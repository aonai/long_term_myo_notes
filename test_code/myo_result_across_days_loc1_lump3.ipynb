{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results of TSD, DANN, SCADANN models across 10 days of inward rotation starting at Day_0~2 for Subject_4\n",
    "\n",
    "Library used can be downloaded from https://github.com/aonai/long_term_EMG_myo   \n",
    "&emsp; Original by UlysseCoteAllard https://github.com/UlysseCoteAllard/LongTermEMG   \n",
    "Dataset recorded by https://github.com/Suguru55/Wearable_Sensor_Long-term_sEMG_Dataset   \n",
    "Extended robot project can be found in https://github.com/aonai/myo_robot_arm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "* weights for TSD are total of 50 training models, 10 for each day\n",
    "* weights for DANN and SCADANN are total of 45 trianing models, 9 for each day\n",
    "\n",
    "\n",
    "* training examples should have shape (1, 8,)\n",
    "* first session has shape (12, 572, 252)\n",
    "* the following sessions have shape (4, 572, 252)\n",
    "* training labels should have shape (1, 8,)\n",
    "\n",
    "\n",
    "* location 0, 1, and 2 corresponds to neutral position, inward rotation, and outward rotation respectively\n",
    "* session mentioned below are days, so number of sessions is 10\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "import torch\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_dir = \"/home/laiy/gitrepos/msr_final/LongTermEMG_myo\"\n",
    "os.chdir(code_dir)\n",
    "from PrepareAndLoadData.process_data import read_data_training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Prepare Data\n",
    "use `switch=2` to train across days and individually on wearing location 0 (`session_in_include=[0]`)\n",
    "\n",
    "### specify the directories used for running the code:\n",
    "* `code_diar`: path to long_term_EMG_myo library\n",
    "* `data_dir`: where raw dataset is loaded; raw data is in csv format\n",
    "* `processed_data_dir`: where processed dataset is loaded; processed data is in npy pickle format\n",
    "    * processed data should be a ndarray of shape   \n",
    "    (controlling_factor_1 x controlling_factor_2 x num_sessions_per_gesture x #examples_window*#mov(26*22=572) x processed_channel_shape(252 for TSD, (4,8,10) for ConvNet)\n",
    "* `path_<model_name>`: where model weights are saved\n",
    "    * weights should be saved in folder `/Weights/<model_name>`. Each folder has subfolders containing weights for the first controlling factor.\n",
    "    * weights for base model (TSD or ConvNet) contain m set of training model\n",
    "    * weights for DANN and SCADANN contain m-1 set of trianing model (these models are trianed based on TSD, so they do not have a best_state_0.pt model). \n",
    "* `save_<model_name>`: where model results are saved\n",
    "    * each result for testing a model on a group of dataset is saved in folder `results`. Each result has corresponding \n",
    "        * `<model_name>.txt` includes predictions, ground truths, array of accuracies for each participant and each session, and overall accuracy\n",
    "        * `predictions_<model_name>.npy` includes array of accuracies, ground truths, predictions, and model outputs (probability array for each prediction)\n",
    "        * remember to make blank files in these names before saving\n",
    "\n",
    "\n",
    "\n",
    "* use `read_data_training` to process raw dataset\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/home/laiy/gitrepos/msr_final/Wearable_Sensor_Long-term_sEMG_Dataset/data\"\n",
    "processed_data_dir = \"/home/laiy/gitrepos/msr_final/LongTermEMG_myo/Processed_datasets_all_across_day_loc_1_lump3\"\n",
    "code_dir = \"/home/laiy/gitrepos/msr_final/LongTermEMG_myo\"\n",
    "save_dir = \"/home/laiy/gitrepos/msr_final/LongTermEMG_myo/Results\"\n",
    "\n",
    "path_TSD =\"/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/TSD\"\n",
    "save_TSD = \"/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/results\"\n",
    "\n",
    "path_DANN =\"/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/DANN\"\n",
    "save_DANN = \"/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/results\"\n",
    "\n",
    "path_SCADANN =\"/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/SCADANN\"\n",
    "save_SCADANN = \"/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing Training datasets...\n",
      "session  1  --- process data in days  [2, 5, 6, 16, 17, 18, 22, 24, 25, 28]\n",
      "index_participant_list  [5]\n",
      "READ  Sub 5 _Loc 1 _Day 2\n",
      "examples_per_session =  (1, 4, 572, 252)\n",
      "READ  Sub 5 _Loc 1 _Day 5\n",
      "Include day  5  in first dataset  (4, 572, 252)\n",
      "examples of first session =  (8, 572, 252)\n",
      "examples_per_session =  (1, 8, 572, 252)\n",
      "READ  Sub 5 _Loc 1 _Day 6\n",
      "Include day  6  in first dataset  (8, 572, 252)\n",
      "examples of first session =  (12, 572, 252)\n",
      "examples_per_session =  (1, 12, 572, 252)\n",
      "READ  Sub 5 _Loc 1 _Day 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laiy/.local/lib/python3.8/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "examples_per_session =  (2,)\n",
      "READ  Sub 5 _Loc 1 _Day 17\n",
      "examples_per_session =  (3,)\n",
      "READ  Sub 5 _Loc 1 _Day 18\n",
      "examples_per_session =  (4,)\n",
      "READ  Sub 5 _Loc 1 _Day 22\n",
      "examples_per_session =  (5,)\n",
      "READ  Sub 5 _Loc 1 _Day 24\n",
      "examples_per_session =  (6,)\n",
      "READ  Sub 5 _Loc 1 _Day 25\n",
      "examples_per_session =  (7,)\n",
      "READ  Sub 5 _Loc 1 _Day 28\n",
      "examples_per_session =  (8,)\n",
      "@ traning sessions =  (1, 8)\n",
      "traning examples  (1, 8)\n",
      "traning labels  (1, 8)\n",
      "all traning examples  (1, 8)\n",
      "all traning labels  (1, 8)\n"
     ]
    }
   ],
   "source": [
    "# read_data_training(path=data_dir, store_path = processed_data_dir,  \n",
    "#                    sessions_to_include =[1], switch=2, include_in_first=3,\n",
    "#                    start_at_participant=5, num_participant=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traning examples  (1, 8)\n",
      "traning labels  (1, 8)\n"
     ]
    }
   ],
   "source": [
    "# check stored pickle \n",
    "with open(processed_data_dir + \"/training_session.pickle\", 'rb') as f:\n",
    "    dataset_training = pickle.load(file=f)\n",
    "\n",
    "examples_datasets_train = dataset_training['examples_training']\n",
    "print('traning examples ', np.shape(examples_datasets_train))\n",
    "labels_datasets_train = dataset_training['labels_training']\n",
    "print('traning labels ', np.shape(labels_datasets_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  examples_per_session =  (12, 572, 252)\n",
      "0  labels_per_session =  (12, 572)\n",
      "1  examples_per_session =  (4, 572, 252)\n",
      "1  labels_per_session =  (4, 572)\n",
      "2  examples_per_session =  (4, 572, 252)\n",
      "2  labels_per_session =  (4, 572)\n",
      "3  examples_per_session =  (4, 572, 252)\n",
      "3  labels_per_session =  (4, 572)\n",
      "4  examples_per_session =  (4, 572, 252)\n",
      "4  labels_per_session =  (4, 572)\n",
      "5  examples_per_session =  (4, 572, 252)\n",
      "5  labels_per_session =  (4, 572)\n",
      "6  examples_per_session =  (4, 572, 252)\n",
      "6  labels_per_session =  (4, 572)\n",
      "7  examples_per_session =  (4, 572, 252)\n",
      "7  labels_per_session =  (4, 572)\n"
     ]
    }
   ],
   "source": [
    "for idx, examples_per_session in enumerate (examples_datasets_train[0]):\n",
    "    print(idx, \" examples_per_session = \", np.shape(examples_per_session))\n",
    "    print(idx, \" labels_per_session = \", np.shape(labels_datasets_train[0][idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify params used for training and testing\n",
    "\n",
    "During training and testing, processed datasets are first put into pytorch dataloders, then feed to the model trainer; following are params for TSD model and dataloaders\n",
    "\n",
    "* `num_kernels`: list of integers defining number of neurons used in each linear layer (linear block has `dropout`=0.5)\n",
    "* `number_of_cycles_total`: number of trails performed for each session (assuming that all session have the same trail size)\n",
    "    * 4 for myo across day training\n",
    "* `number_of_classes`: total number of gestures performed in dataset\n",
    "    * 22 for myo\n",
    "* `batch_size`: number of examples stored in each batch\n",
    "* `feature_vector_input_length`: length of input array or each processed signal; i.e. size of one training example \n",
    "    * 252 for TSD\n",
    "* `learning_rate`= 0.002515\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number_of_cycle_for_first_training  12\n",
      "number_of_cycles_total  4\n"
     ]
    }
   ],
   "source": [
    "num_kernels=[200, 200, 200]                                \n",
    "number_of_cycle_for_first_training = np.shape(examples_datasets_train[0][0])[0]               \n",
    "number_of_cycles_total=np.shape(examples_datasets_train[-1][-1])[0]               \n",
    "print(\"number_of_cycle_for_first_training \", number_of_cycle_for_first_training)\n",
    "print(\"number_of_cycles_total \", number_of_cycles_total)\n",
    "number_of_classes=22\n",
    "batch_size=128          \n",
    "feature_vector_input_length=252                     \n",
    "learning_rate=0.002515"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. TSD_DNN\n",
    "* `train_fine_tuning`: used to train data using a base model (TSD or ConvNet)\n",
    "    * running this function will save num_sessions sets of TSD model weights (each is fine tuned based on the previous training)  \n",
    "    \n",
    "* `test_standard_model_on_training_sessions`: test model result\n",
    "\n",
    "\n",
    "### check if dataloaders are loaded correctly:\n",
    "* each participant has shape (num_session x 40 x 572 x 252)\n",
    "* each session has shape (40 x 572 x 252)\n",
    "* put these data into on group ends up with shape (40*572=22880, 252)\n",
    "    * shuffle on group of data and put into dataloaders\n",
    "    * each participant should have num_sessions sets of dataloaders, each correspond to one session\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TrainingsAndEvaluations.ForTrainingSessions.train_tsd_dnn_standard import \\\n",
    "            test_standard_model_on_training_sessions, train_fine_tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET one participant_examples  (8,)\n",
      "   GET one training_index_examples  (12, 572, 252)  at  0\n",
      "   GOT one group XY  (6864, 252)    (6864,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (6177, 252)    (6177,)\n",
      "       one group XY valid (687, 252)    (687, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  1\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  2\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  3\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  4\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  5\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  6\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  7\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "dataloaders: \n",
      "   train  (1, 8)\n",
      "   valid  (1, 8)\n",
      "   test  (1, 0)\n",
      "START TRAINING\n",
      "Participant:  0\n",
      "Session:  0\n",
      "<generator object Module.parameters at 0x7facc99ad970>\n",
      "Epoch 0/499\n",
      "----------\n",
      "train Loss: 0.01126282 Acc: 0.58968099\n",
      "val Loss: 0.00103498 Acc: 0.77438137\n",
      "New best validation loss: 0.0010349764400521047\n",
      "Epoch 1 of 500 took 0.326s\n",
      "Epoch 1/499\n",
      "----------\n",
      "train Loss: 0.00521092 Acc: 0.78792318\n",
      "val Loss: 0.00060841 Acc: 0.85443959\n",
      "New best validation loss: 0.0006084107675927175\n",
      "Epoch 2 of 500 took 0.324s\n",
      "Epoch 2/499\n",
      "----------\n",
      "train Loss: 0.00422491 Acc: 0.81754557\n",
      "val Loss: 0.00045697 Acc: 0.8937409\n",
      "New best validation loss: 0.0004569747996295557\n",
      "Epoch 3 of 500 took 0.322s\n",
      "Epoch 3/499\n",
      "----------\n",
      "train Loss: 0.00376498 Acc: 0.83300781\n",
      "val Loss: 0.00047323 Acc: 0.87627365\n",
      "Epoch 4 of 500 took 0.322s\n",
      "Epoch 4/499\n",
      "----------\n",
      "train Loss: 0.00345973 Acc: 0.84798177\n",
      "val Loss: 0.00039622 Acc: 0.89519651\n",
      "Epoch 5 of 500 took 0.322s\n",
      "Epoch 5/499\n",
      "----------\n",
      "train Loss: 0.00331020 Acc: 0.85498047\n",
      "val Loss: 0.00037242 Acc: 0.90247453\n",
      "Epoch 6 of 500 took 0.323s\n",
      "Epoch 6/499\n",
      "----------\n",
      "train Loss: 0.00301907 Acc: 0.86181641\n",
      "val Loss: 0.00044735 Acc: 0.87918486\n",
      "Epoch 7 of 500 took 0.320s\n",
      "Epoch 7/499\n",
      "----------\n",
      "train Loss: 0.00293053 Acc: 0.86816406\n",
      "val Loss: 0.00038270 Acc: 0.90247453\n",
      "Epoch 8 of 500 took 0.346s\n",
      "Epoch 8/499\n",
      "----------\n",
      "train Loss: 0.00286239 Acc: 0.87483724\n",
      "val Loss: 0.00037315 Acc: 0.91703057\n",
      "Epoch 9 of 500 took 0.320s\n",
      "Epoch 9/499\n",
      "----------\n",
      "train Loss: 0.00263987 Acc: 0.88004557\n",
      "val Loss: 0.00034209 Acc: 0.91557496\n",
      "New best validation loss: 0.0003420940262782001\n",
      "Epoch 10 of 500 took 0.319s\n",
      "Epoch 10/499\n",
      "----------\n",
      "train Loss: 0.00257644 Acc: 0.88232422\n",
      "val Loss: 0.00038601 Acc: 0.89519651\n",
      "Epoch 11 of 500 took 0.321s\n",
      "Epoch 11/499\n",
      "----------\n",
      "train Loss: 0.00247741 Acc: 0.88850911\n",
      "val Loss: 0.00029876 Acc: 0.92576419\n",
      "Epoch 12 of 500 took 0.329s\n",
      "Epoch 12/499\n",
      "----------\n",
      "train Loss: 0.00231706 Acc: 0.89664714\n",
      "val Loss: 0.00032170 Acc: 0.91557496\n",
      "Epoch 13 of 500 took 0.315s\n",
      "Epoch 13/499\n",
      "----------\n",
      "train Loss: 0.00233759 Acc: 0.89436849\n",
      "val Loss: 0.00033752 Acc: 0.90393013\n",
      "Epoch 14 of 500 took 0.331s\n",
      "Epoch 14/499\n",
      "----------\n",
      "train Loss: 0.00227882 Acc: 0.8976237\n",
      "val Loss: 0.00029819 Acc: 0.92430859\n",
      "Epoch 15 of 500 took 0.320s\n",
      "Epoch 15/499\n",
      "----------\n",
      "train Loss: 0.00216388 Acc: 0.90185547\n",
      "val Loss: 0.00035974 Acc: 0.90538574\n",
      "Epoch 16 of 500 took 0.316s\n",
      "Epoch 16/499\n",
      "----------\n",
      "train Loss: 0.00210125 Acc: 0.90380859\n",
      "val Loss: 0.00035711 Acc: 0.89956332\n",
      "Epoch 17 of 500 took 0.315s\n",
      "Epoch 17/499\n",
      "----------\n",
      "train Loss: 0.00209687 Acc: 0.9054362\n",
      "val Loss: 0.00024361 Acc: 0.93740902\n",
      "Epoch 18 of 500 took 0.318s\n",
      "Epoch 18/499\n",
      "----------\n",
      "train Loss: 0.00202491 Acc: 0.90755208\n",
      "val Loss: 0.00028838 Acc: 0.9286754\n",
      "Epoch 19 of 500 took 0.316s\n",
      "Epoch 19/499\n",
      "----------\n",
      "train Loss: 0.00205353 Acc: 0.90608724\n",
      "val Loss: 0.00029416 Acc: 0.9286754\n",
      "Epoch 20 of 500 took 0.315s\n",
      "Epoch 20/499\n",
      "----------\n",
      "train Loss: 0.00197607 Acc: 0.9148763\n",
      "val Loss: 0.00027103 Acc: 0.93449782\n",
      "Epoch 21 of 500 took 0.317s\n",
      "\n",
      "Training complete in 0m 7s\n",
      "Best val loss: 0.000342\n",
      "Session:  1\n",
      "<generator object Module.parameters at 0x7facca159510>\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/TSD/participant_0/best_state_0.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/TSD/participant_0/best_state_0.pt' (epoch 10)\n",
      "Epoch 0/499\n",
      "----------\n",
      "train Loss: 0.00391734 Acc: 0.82128906\n",
      "val Loss: 0.00131003 Acc: 0.87336245\n",
      "New best validation loss: 0.0013100336993105027\n",
      "Epoch 1 of 500 took 0.113s\n",
      "Epoch 1/499\n",
      "----------\n",
      "train Loss: 0.00289363 Acc: 0.87402344\n",
      "val Loss: 0.00091564 Acc: 0.92139738\n",
      "New best validation loss: 0.0009156354910421579\n",
      "Epoch 2 of 500 took 0.112s\n",
      "Epoch 2/499\n",
      "----------\n",
      "train Loss: 0.00269994 Acc: 0.87939453\n",
      "val Loss: 0.00086093 Acc: 0.93449782\n",
      "Epoch 3 of 500 took 0.109s\n",
      "Epoch 3/499\n",
      "----------\n",
      "train Loss: 0.00204663 Acc: 0.90527344\n",
      "val Loss: 0.00081418 Acc: 0.93449782\n",
      "New best validation loss: 0.0008141766078607484\n",
      "Epoch 4 of 500 took 0.133s\n",
      "Epoch 4/499\n",
      "----------\n",
      "train Loss: 0.00208787 Acc: 0.90917969\n",
      "val Loss: 0.00071159 Acc: 0.95196507\n",
      "New best validation loss: 0.0007115898564392823\n",
      "Epoch 5 of 500 took 0.135s\n",
      "Epoch 5/499\n",
      "----------\n",
      "train Loss: 0.00201727 Acc: 0.91015625\n",
      "val Loss: 0.00190449 Acc: 0.86026201\n",
      "Epoch 6 of 500 took 0.134s\n",
      "Epoch 6/499\n",
      "----------\n",
      "train Loss: 0.00211653 Acc: 0.90673828\n",
      "val Loss: 0.00095142 Acc: 0.91703057\n",
      "Epoch 7 of 500 took 0.127s\n",
      "Epoch 7/499\n",
      "----------\n",
      "train Loss: 0.00176637 Acc: 0.92285156\n",
      "val Loss: 0.00050890 Acc: 0.96069869\n",
      "New best validation loss: 0.0005089018217340828\n",
      "Epoch 8 of 500 took 0.144s\n",
      "Epoch 8/499\n",
      "----------\n",
      "train Loss: 0.00187116 Acc: 0.91748047\n",
      "val Loss: 0.00050448 Acc: 0.94759825\n",
      "Epoch 9 of 500 took 0.150s\n",
      "Epoch 9/499\n",
      "----------\n",
      "train Loss: 0.00162035 Acc: 0.93115234\n",
      "val Loss: 0.00056081 Acc: 0.95633188\n",
      "Epoch 10 of 500 took 0.146s\n",
      "Epoch 10/499\n",
      "----------\n",
      "train Loss: 0.00162060 Acc: 0.93164062\n",
      "val Loss: 0.00055620 Acc: 0.95633188\n",
      "Epoch 11 of 500 took 0.141s\n",
      "Epoch 11/499\n",
      "----------\n",
      "train Loss: 0.00136984 Acc: 0.93945312\n",
      "val Loss: 0.00039100 Acc: 0.96069869\n",
      "New best validation loss: 0.00039099885646953336\n",
      "Epoch 12 of 500 took 0.144s\n",
      "Epoch 12/499\n",
      "----------\n",
      "train Loss: 0.00143897 Acc: 0.93603516\n",
      "val Loss: 0.00042196 Acc: 0.97816594\n",
      "Epoch 13 of 500 took 0.145s\n",
      "Epoch 13/499\n",
      "----------\n",
      "train Loss: 0.00131614 Acc: 0.93994141\n",
      "val Loss: 0.00052589 Acc: 0.95633188\n",
      "Epoch 14 of 500 took 0.144s\n",
      "Epoch 14/499\n",
      "----------\n",
      "train Loss: 0.00146555 Acc: 0.93896484\n",
      "val Loss: 0.00042409 Acc: 0.9650655\n",
      "Epoch 15 of 500 took 0.147s\n",
      "Epoch 15/499\n",
      "----------\n",
      "train Loss: 0.00132728 Acc: 0.94140625\n",
      "val Loss: 0.00120572 Acc: 0.91266376\n",
      "Epoch 16 of 500 took 0.142s\n",
      "Epoch 16/499\n",
      "----------\n",
      "train Loss: 0.00122388 Acc: 0.94091797\n",
      "val Loss: 0.00040127 Acc: 0.97816594\n",
      "Epoch 17 of 500 took 0.141s\n",
      "Epoch 17/499\n",
      "----------\n",
      "train Loss: 0.00114914 Acc: 0.94628906\n",
      "val Loss: 0.00035042 Acc: 0.97379913\n",
      "Epoch 18 of 500 took 0.146s\n",
      "Epoch 18/499\n",
      "----------\n",
      "train Loss: 0.00118158 Acc: 0.94873047\n",
      "val Loss: 0.00035995 Acc: 0.96943231\n",
      "Epoch 19 of 500 took 0.138s\n",
      "Epoch 19/499\n",
      "----------\n",
      "train Loss: 0.00130251 Acc: 0.94238281\n",
      "val Loss: 0.00062125 Acc: 0.94323144\n",
      "Epoch 20 of 500 took 0.115s\n",
      "Epoch 20/499\n",
      "----------\n",
      "train Loss: 0.00110009 Acc: 0.94824219\n",
      "val Loss: 0.00032405 Acc: 0.96069869\n",
      "Epoch 21 of 500 took 0.105s\n",
      "Epoch 21/499\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.00111745 Acc: 0.94921875\n",
      "val Loss: 0.00027095 Acc: 0.98253275\n",
      "New best validation loss: 0.0002709508723046582\n",
      "Epoch 22 of 500 took 0.111s\n",
      "Epoch 22/499\n",
      "----------\n",
      "train Loss: 0.00102116 Acc: 0.95507812\n",
      "val Loss: 0.00042572 Acc: 0.9650655\n",
      "Epoch 23 of 500 took 0.107s\n",
      "Epoch 23/499\n",
      "----------\n",
      "train Loss: 0.00102178 Acc: 0.95361328\n",
      "val Loss: 0.00036273 Acc: 0.9650655\n",
      "Epoch 24 of 500 took 0.110s\n",
      "Epoch 24/499\n",
      "----------\n",
      "train Loss: 0.00091240 Acc: 0.96191406\n",
      "val Loss: 0.00020530 Acc: 0.97816594\n",
      "Epoch 25 of 500 took 0.108s\n",
      "Epoch 25/499\n",
      "----------\n",
      "train Loss: 0.00082657 Acc: 0.96289062\n",
      "val Loss: 0.00036425 Acc: 0.97816594\n",
      "Epoch 26 of 500 took 0.114s\n",
      "Epoch 26/499\n",
      "----------\n",
      "train Loss: 0.00104386 Acc: 0.95214844\n",
      "val Loss: 0.00031966 Acc: 0.97816594\n",
      "Epoch 27 of 500 took 0.112s\n",
      "Epoch 27/499\n",
      "----------\n",
      "train Loss: 0.00084264 Acc: 0.9609375\n",
      "val Loss: 0.00036473 Acc: 0.96943231\n",
      "Epoch 28 of 500 took 0.116s\n",
      "Epoch 28/499\n",
      "----------\n",
      "train Loss: 0.00105088 Acc: 0.95166016\n",
      "val Loss: 0.00041956 Acc: 0.96069869\n",
      "Epoch 29 of 500 took 0.111s\n",
      "Epoch 29/499\n",
      "----------\n",
      "train Loss: 0.00091790 Acc: 0.95947266\n",
      "val Loss: 0.00024093 Acc: 0.97816594\n",
      "Epoch 30 of 500 took 0.122s\n",
      "Epoch 30/499\n",
      "----------\n",
      "train Loss: 0.00081301 Acc: 0.96630859\n",
      "val Loss: 0.00064221 Acc: 0.95196507\n",
      "Epoch    31: reducing learning rate of group 0 to 5.0300e-04.\n",
      "Epoch 31 of 500 took 0.170s\n",
      "Epoch 31/499\n",
      "----------\n",
      "train Loss: 0.00071471 Acc: 0.97363281\n",
      "val Loss: 0.00024773 Acc: 0.98689956\n",
      "Epoch 32 of 500 took 0.150s\n",
      "Epoch 32/499\n",
      "----------\n",
      "train Loss: 0.00068620 Acc: 0.97119141\n",
      "val Loss: 0.00017978 Acc: 0.98689956\n",
      "Epoch 33 of 500 took 0.144s\n",
      "\n",
      "Training complete in 0m 4s\n",
      "Best val loss: 0.000271\n",
      "Session:  2\n",
      "<generator object Module.parameters at 0x7facca159510>\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/TSD/participant_0/best_state_1.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/TSD/participant_0/best_state_1.pt' (epoch 22)\n",
      "Epoch 0/499\n",
      "----------\n",
      "train Loss: 0.00300465 Acc: 0.87695312\n",
      "val Loss: 0.00097345 Acc: 0.930131\n",
      "New best validation loss: 0.000973450629992256\n",
      "Epoch 1 of 500 took 0.133s\n",
      "Epoch 1/499\n",
      "----------\n",
      "train Loss: 0.00236936 Acc: 0.89941406\n",
      "val Loss: 0.00080856 Acc: 0.92139738\n",
      "New best validation loss: 0.0008085644010893643\n",
      "Epoch 2 of 500 took 0.137s\n",
      "Epoch 2/499\n",
      "----------\n",
      "train Loss: 0.00204072 Acc: 0.91308594\n",
      "val Loss: 0.00080572 Acc: 0.93449782\n",
      "Epoch 3 of 500 took 0.118s\n",
      "Epoch 3/499\n",
      "----------\n",
      "train Loss: 0.00178753 Acc: 0.91992188\n",
      "val Loss: 0.00072359 Acc: 0.94759825\n",
      "Epoch 4 of 500 took 0.106s\n",
      "Epoch 4/499\n",
      "----------\n",
      "train Loss: 0.00186342 Acc: 0.92041016\n",
      "val Loss: 0.00077152 Acc: 0.94759825\n",
      "Epoch 5 of 500 took 0.111s\n",
      "Epoch 5/499\n",
      "----------\n",
      "train Loss: 0.00186984 Acc: 0.91943359\n",
      "val Loss: 0.00059259 Acc: 0.94323144\n",
      "New best validation loss: 0.0005925906146978186\n",
      "Epoch 6 of 500 took 0.114s\n",
      "Epoch 6/499\n",
      "----------\n",
      "train Loss: 0.00132553 Acc: 0.93945312\n",
      "val Loss: 0.00069973 Acc: 0.93886463\n",
      "Epoch 7 of 500 took 0.138s\n",
      "Epoch 7/499\n",
      "----------\n",
      "train Loss: 0.00150136 Acc: 0.93115234\n",
      "val Loss: 0.00054989 Acc: 0.96069869\n",
      "Epoch 8 of 500 took 0.128s\n",
      "Epoch 8/499\n",
      "----------\n",
      "train Loss: 0.00145222 Acc: 0.93066406\n",
      "val Loss: 0.00059911 Acc: 0.95196507\n",
      "Epoch 9 of 500 took 0.110s\n",
      "Epoch 9/499\n",
      "----------\n",
      "train Loss: 0.00122217 Acc: 0.94287109\n",
      "val Loss: 0.00058238 Acc: 0.94759825\n",
      "Epoch 10 of 500 took 0.105s\n",
      "Epoch 10/499\n",
      "----------\n",
      "train Loss: 0.00118076 Acc: 0.95263672\n",
      "val Loss: 0.00055838 Acc: 0.96943231\n",
      "Epoch 11 of 500 took 0.111s\n",
      "Epoch 11/499\n",
      "----------\n",
      "train Loss: 0.00118556 Acc: 0.94873047\n",
      "val Loss: 0.00056587 Acc: 0.95196507\n",
      "Epoch 12 of 500 took 0.107s\n",
      "Epoch 12/499\n",
      "----------\n",
      "train Loss: 0.00106135 Acc: 0.95703125\n",
      "val Loss: 0.00057793 Acc: 0.94759825\n",
      "Epoch 13 of 500 took 0.110s\n",
      "Epoch 13/499\n",
      "----------\n",
      "train Loss: 0.00125085 Acc: 0.94433594\n",
      "val Loss: 0.00053307 Acc: 0.96943231\n",
      "Epoch 14 of 500 took 0.107s\n",
      "Epoch 14/499\n",
      "----------\n",
      "train Loss: 0.00114111 Acc: 0.95263672\n",
      "val Loss: 0.00048773 Acc: 0.95196507\n",
      "New best validation loss: 0.00048773055774155663\n",
      "Epoch 15 of 500 took 0.116s\n",
      "Epoch 15/499\n",
      "----------\n",
      "train Loss: 0.00124735 Acc: 0.94287109\n",
      "val Loss: 0.00048989 Acc: 0.9650655\n",
      "Epoch 16 of 500 took 0.106s\n",
      "Epoch 16/499\n",
      "----------\n",
      "train Loss: 0.00125330 Acc: 0.94775391\n",
      "val Loss: 0.00039112 Acc: 0.96943231\n",
      "Epoch 17 of 500 took 0.110s\n",
      "Epoch 17/499\n",
      "----------\n",
      "train Loss: 0.00106741 Acc: 0.95458984\n",
      "val Loss: 0.00063000 Acc: 0.95196507\n",
      "Epoch 18 of 500 took 0.107s\n",
      "Epoch 18/499\n",
      "----------\n",
      "train Loss: 0.00119438 Acc: 0.94726562\n",
      "val Loss: 0.00064645 Acc: 0.95196507\n",
      "Epoch 19 of 500 took 0.110s\n",
      "Epoch 19/499\n",
      "----------\n",
      "train Loss: 0.00100144 Acc: 0.95263672\n",
      "val Loss: 0.00045320 Acc: 0.96943231\n",
      "Epoch 20 of 500 took 0.117s\n",
      "Epoch 20/499\n",
      "----------\n",
      "train Loss: 0.00096825 Acc: 0.953125\n",
      "val Loss: 0.00046435 Acc: 0.96069869\n",
      "Epoch 21 of 500 took 0.141s\n",
      "Epoch 21/499\n",
      "----------\n",
      "train Loss: 0.00087567 Acc: 0.96191406\n",
      "val Loss: 0.00043825 Acc: 0.96069869\n",
      "Epoch 22 of 500 took 0.136s\n",
      "Epoch 22/499\n",
      "----------\n",
      "train Loss: 0.00100793 Acc: 0.95849609\n",
      "val Loss: 0.00059159 Acc: 0.96069869\n",
      "Epoch    23: reducing learning rate of group 0 to 5.0300e-04.\n",
      "Epoch 23 of 500 took 0.112s\n",
      "Epoch 23/499\n",
      "----------\n",
      "train Loss: 0.00078641 Acc: 0.96533203\n",
      "val Loss: 0.00039423 Acc: 0.96069869\n",
      "Epoch 24 of 500 took 0.106s\n",
      "Epoch 24/499\n",
      "----------\n",
      "train Loss: 0.00081379 Acc: 0.96679688\n",
      "val Loss: 0.00042859 Acc: 0.97379913\n",
      "Epoch 25 of 500 took 0.111s\n",
      "Epoch 25/499\n",
      "----------\n",
      "train Loss: 0.00063867 Acc: 0.97460938\n",
      "val Loss: 0.00036223 Acc: 0.97379913\n",
      "New best validation loss: 0.0003622318719672324\n",
      "Epoch 26 of 500 took 0.108s\n",
      "Epoch 26/499\n",
      "----------\n",
      "train Loss: 0.00061774 Acc: 0.97070312\n",
      "val Loss: 0.00034546 Acc: 0.97379913\n",
      "Epoch 27 of 500 took 0.109s\n",
      "Epoch 27/499\n",
      "----------\n",
      "train Loss: 0.00057161 Acc: 0.97607422\n",
      "val Loss: 0.00029171 Acc: 0.97379913\n",
      "Epoch 28 of 500 took 0.108s\n",
      "Epoch 28/499\n",
      "----------\n",
      "train Loss: 0.00063244 Acc: 0.97314453\n",
      "val Loss: 0.00031017 Acc: 0.97816594\n",
      "Epoch 29 of 500 took 0.110s\n",
      "Epoch 29/499\n",
      "----------\n",
      "train Loss: 0.00061992 Acc: 0.97167969\n",
      "val Loss: 0.00035203 Acc: 0.98689956\n",
      "Epoch 30 of 500 took 0.106s\n",
      "Epoch 30/499\n",
      "----------\n",
      "train Loss: 0.00063057 Acc: 0.96972656\n",
      "val Loss: 0.00036894 Acc: 0.96943231\n",
      "Epoch 31 of 500 took 0.112s\n",
      "Epoch 31/499\n",
      "----------\n",
      "train Loss: 0.00051626 Acc: 0.97900391\n",
      "val Loss: 0.00032899 Acc: 0.97816594\n",
      "Epoch 32 of 500 took 0.108s\n",
      "Epoch 32/499\n",
      "----------\n",
      "train Loss: 0.00054055 Acc: 0.97802734\n",
      "val Loss: 0.00029086 Acc: 0.97816594\n",
      "Epoch 33 of 500 took 0.111s\n",
      "Epoch 33/499\n",
      "----------\n",
      "train Loss: 0.00059979 Acc: 0.97558594\n",
      "val Loss: 0.00033906 Acc: 0.97816594\n",
      "Epoch 34 of 500 took 0.107s\n",
      "Epoch 34/499\n",
      "----------\n",
      "train Loss: 0.00055132 Acc: 0.97949219\n",
      "val Loss: 0.00031266 Acc: 0.96943231\n",
      "Epoch 35 of 500 took 0.109s\n",
      "Epoch 35/499\n",
      "----------\n",
      "train Loss: 0.00056512 Acc: 0.9765625\n",
      "val Loss: 0.00032965 Acc: 0.97379913\n",
      "Epoch 36 of 500 took 0.107s\n",
      "Epoch 36/499\n",
      "----------\n",
      "train Loss: 0.00055803 Acc: 0.97705078\n",
      "val Loss: 0.00036072 Acc: 0.97379913\n",
      "Epoch 37 of 500 took 0.110s\n",
      "\n",
      "Training complete in 0m 4s\n",
      "Best val loss: 0.000362\n",
      "Session:  3\n",
      "<generator object Module.parameters at 0x7facca159510>\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/TSD/participant_0/best_state_2.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/TSD/participant_0/best_state_2.pt' (epoch 26)\n",
      "Epoch 0/499\n",
      "----------\n",
      "train Loss: 0.00309621 Acc: 0.87304688\n",
      "val Loss: 0.00110340 Acc: 0.930131\n",
      "New best validation loss: 0.0011033983209768237\n",
      "Epoch 1 of 500 took 0.108s\n",
      "Epoch 1/499\n",
      "----------\n",
      "train Loss: 0.00246713 Acc: 0.89013672\n",
      "val Loss: 0.00077735 Acc: 0.94323144\n",
      "New best validation loss: 0.0007773473793762741\n",
      "Epoch 2 of 500 took 0.113s\n",
      "Epoch 2/499\n",
      "----------\n",
      "train Loss: 0.00189620 Acc: 0.91699219\n",
      "val Loss: 0.00059737 Acc: 0.94759825\n",
      "New best validation loss: 0.0005973732367353147\n",
      "Epoch 3 of 500 took 0.108s\n",
      "Epoch 3/499\n",
      "----------\n",
      "train Loss: 0.00190717 Acc: 0.92333984\n",
      "val Loss: 0.00074394 Acc: 0.93886463\n",
      "Epoch 4 of 500 took 0.110s\n",
      "Epoch 4/499\n",
      "----------\n",
      "train Loss: 0.00179431 Acc: 0.91552734\n",
      "val Loss: 0.00061586 Acc: 0.94759825\n",
      "Epoch 5 of 500 took 0.111s\n",
      "Epoch 5/499\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.00154325 Acc: 0.92822266\n",
      "val Loss: 0.00071722 Acc: 0.95196507\n",
      "Epoch 6 of 500 took 0.109s\n",
      "Epoch 6/499\n",
      "----------\n",
      "train Loss: 0.00164340 Acc: 0.91552734\n",
      "val Loss: 0.00058534 Acc: 0.95633188\n",
      "Epoch 7 of 500 took 0.106s\n",
      "Epoch 7/499\n",
      "----------\n",
      "train Loss: 0.00150723 Acc: 0.92919922\n",
      "val Loss: 0.00073290 Acc: 0.94759825\n",
      "Epoch 8 of 500 took 0.111s\n",
      "Epoch 8/499\n",
      "----------\n",
      "train Loss: 0.00150643 Acc: 0.92724609\n",
      "val Loss: 0.00062049 Acc: 0.94323144\n",
      "Epoch 9 of 500 took 0.106s\n",
      "Epoch 9/499\n",
      "----------\n",
      "train Loss: 0.00168774 Acc: 0.91699219\n",
      "val Loss: 0.00062642 Acc: 0.97379913\n",
      "Epoch 10 of 500 took 0.109s\n",
      "Epoch 10/499\n",
      "----------\n",
      "train Loss: 0.00135631 Acc: 0.93847656\n",
      "val Loss: 0.00056192 Acc: 0.95196507\n",
      "Epoch 11 of 500 took 0.134s\n",
      "Epoch 11/499\n",
      "----------\n",
      "train Loss: 0.00126406 Acc: 0.95068359\n",
      "val Loss: 0.00040191 Acc: 0.9650655\n",
      "New best validation loss: 0.00040190764519845555\n",
      "Epoch 12 of 500 took 0.130s\n",
      "Epoch 12/499\n",
      "----------\n",
      "train Loss: 0.00143555 Acc: 0.93603516\n",
      "val Loss: 0.00046471 Acc: 0.96069869\n",
      "Epoch 13 of 500 took 0.107s\n",
      "Epoch 13/499\n",
      "----------\n",
      "train Loss: 0.00107489 Acc: 0.94970703\n",
      "val Loss: 0.00054230 Acc: 0.96943231\n",
      "Epoch 14 of 500 took 0.120s\n",
      "Epoch 14/499\n",
      "----------\n",
      "train Loss: 0.00132839 Acc: 0.93359375\n",
      "val Loss: 0.00053482 Acc: 0.9650655\n",
      "Epoch 15 of 500 took 0.106s\n",
      "Epoch 15/499\n",
      "----------\n",
      "train Loss: 0.00117375 Acc: 0.94775391\n",
      "val Loss: 0.00060826 Acc: 0.96069869\n",
      "Epoch 16 of 500 took 0.109s\n",
      "Epoch 16/499\n",
      "----------\n",
      "train Loss: 0.00116534 Acc: 0.94384766\n",
      "val Loss: 0.00050089 Acc: 0.96943231\n",
      "Epoch 17 of 500 took 0.106s\n",
      "Epoch 17/499\n",
      "----------\n",
      "train Loss: 0.00100257 Acc: 0.95556641\n",
      "val Loss: 0.00050922 Acc: 0.9650655\n",
      "Epoch    18: reducing learning rate of group 0 to 5.0300e-04.\n",
      "Epoch 18 of 500 took 0.109s\n",
      "Epoch 18/499\n",
      "----------\n",
      "train Loss: 0.00087721 Acc: 0.95800781\n",
      "val Loss: 0.00042909 Acc: 0.97816594\n",
      "Epoch 19 of 500 took 0.106s\n",
      "Epoch 19/499\n",
      "----------\n",
      "train Loss: 0.00083654 Acc: 0.96240234\n",
      "val Loss: 0.00036632 Acc: 0.97816594\n",
      "Epoch 20 of 500 took 0.111s\n",
      "Epoch 20/499\n",
      "----------\n",
      "train Loss: 0.00081404 Acc: 0.95996094\n",
      "val Loss: 0.00032900 Acc: 0.97816594\n",
      "Epoch 21 of 500 took 0.107s\n",
      "Epoch 21/499\n",
      "----------\n",
      "train Loss: 0.00079728 Acc: 0.96240234\n",
      "val Loss: 0.00039482 Acc: 0.97379913\n",
      "Epoch 22 of 500 took 0.110s\n",
      "Epoch 22/499\n",
      "----------\n",
      "train Loss: 0.00078077 Acc: 0.96533203\n",
      "val Loss: 0.00036450 Acc: 0.98253275\n",
      "Epoch 23 of 500 took 0.134s\n",
      "\n",
      "Training complete in 0m 3s\n",
      "Best val loss: 0.000402\n",
      "Session:  4\n",
      "<generator object Module.parameters at 0x7facc90115f0>\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/TSD/participant_0/best_state_3.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/TSD/participant_0/best_state_3.pt' (epoch 12)\n",
      "Epoch 0/499\n",
      "----------\n",
      "train Loss: 0.00367880 Acc: 0.84863281\n",
      "val Loss: 0.00148154 Acc: 0.91703057\n",
      "New best validation loss: 0.0014815447632402312\n",
      "Epoch 1 of 500 took 0.113s\n",
      "Epoch 1/499\n",
      "----------\n",
      "train Loss: 0.00244518 Acc: 0.89013672\n",
      "val Loss: 0.00148185 Acc: 0.92139738\n",
      "Epoch 2 of 500 took 0.107s\n",
      "Epoch 2/499\n",
      "----------\n",
      "train Loss: 0.00242598 Acc: 0.8828125\n",
      "val Loss: 0.00122126 Acc: 0.91703057\n",
      "New best validation loss: 0.0012212560947284948\n",
      "Epoch 3 of 500 took 0.113s\n",
      "Epoch 3/499\n",
      "----------\n",
      "train Loss: 0.00217722 Acc: 0.90039062\n",
      "val Loss: 0.00136607 Acc: 0.92576419\n",
      "Epoch 4 of 500 took 0.106s\n",
      "Epoch 4/499\n",
      "----------\n",
      "train Loss: 0.00202859 Acc: 0.89794922\n",
      "val Loss: 0.00133728 Acc: 0.91703057\n",
      "Epoch 5 of 500 took 0.109s\n",
      "Epoch 5/499\n",
      "----------\n",
      "train Loss: 0.00179183 Acc: 0.91796875\n",
      "val Loss: 0.00173439 Acc: 0.89956332\n",
      "Epoch 6 of 500 took 0.106s\n",
      "Epoch 6/499\n",
      "----------\n",
      "train Loss: 0.00173691 Acc: 0.91699219\n",
      "val Loss: 0.00125553 Acc: 0.90829694\n",
      "Epoch 7 of 500 took 0.110s\n",
      "Epoch 7/499\n",
      "----------\n",
      "train Loss: 0.00178205 Acc: 0.91699219\n",
      "val Loss: 0.00114480 Acc: 0.930131\n",
      "Epoch 8 of 500 took 0.106s\n",
      "Epoch 8/499\n",
      "----------\n",
      "train Loss: 0.00163352 Acc: 0.92041016\n",
      "val Loss: 0.00117999 Acc: 0.930131\n",
      "Epoch 9 of 500 took 0.112s\n",
      "Epoch 9/499\n",
      "----------\n",
      "train Loss: 0.00151690 Acc: 0.92822266\n",
      "val Loss: 0.00148389 Acc: 0.91266376\n",
      "Epoch 10 of 500 took 0.107s\n",
      "Epoch 10/499\n",
      "----------\n",
      "train Loss: 0.00149183 Acc: 0.92724609\n",
      "val Loss: 0.00134737 Acc: 0.92576419\n",
      "Epoch 11 of 500 took 0.119s\n",
      "Epoch 11/499\n",
      "----------\n",
      "train Loss: 0.00145348 Acc: 0.93261719\n",
      "val Loss: 0.00103353 Acc: 0.94323144\n",
      "New best validation loss: 0.001033529964596944\n",
      "Epoch 12 of 500 took 0.139s\n",
      "Epoch 12/499\n",
      "----------\n",
      "train Loss: 0.00142016 Acc: 0.93896484\n",
      "val Loss: 0.00079109 Acc: 0.94323144\n",
      "New best validation loss: 0.0007910888528199175\n",
      "Epoch 13 of 500 took 0.122s\n",
      "Epoch 13/499\n",
      "----------\n",
      "train Loss: 0.00136501 Acc: 0.93994141\n",
      "val Loss: 0.00072763 Acc: 0.95633188\n",
      "Epoch 14 of 500 took 0.126s\n",
      "Epoch 14/499\n",
      "----------\n",
      "train Loss: 0.00136519 Acc: 0.93798828\n",
      "val Loss: 0.00084594 Acc: 0.94759825\n",
      "Epoch 15 of 500 took 0.130s\n",
      "Epoch 15/499\n",
      "----------\n",
      "train Loss: 0.00140340 Acc: 0.93457031\n",
      "val Loss: 0.00079264 Acc: 0.95196507\n",
      "Epoch 16 of 500 took 0.122s\n",
      "Epoch 16/499\n",
      "----------\n",
      "train Loss: 0.00125710 Acc: 0.9453125\n",
      "val Loss: 0.00104342 Acc: 0.94759825\n",
      "Epoch 17 of 500 took 0.151s\n",
      "Epoch 17/499\n",
      "----------\n",
      "train Loss: 0.00126840 Acc: 0.94433594\n",
      "val Loss: 0.00089143 Acc: 0.93449782\n",
      "Epoch 18 of 500 took 0.143s\n",
      "Epoch 18/499\n",
      "----------\n",
      "train Loss: 0.00145172 Acc: 0.92773438\n",
      "val Loss: 0.00080671 Acc: 0.95633188\n",
      "Epoch 19 of 500 took 0.149s\n",
      "Epoch 19/499\n",
      "----------\n",
      "train Loss: 0.00128629 Acc: 0.94189453\n",
      "val Loss: 0.00095535 Acc: 0.93886463\n",
      "Epoch    20: reducing learning rate of group 0 to 5.0300e-04.\n",
      "Epoch 20 of 500 took 0.118s\n",
      "Epoch 20/499\n",
      "----------\n",
      "train Loss: 0.00122719 Acc: 0.93945312\n",
      "val Loss: 0.00075097 Acc: 0.9650655\n",
      "Epoch 21 of 500 took 0.144s\n",
      "Epoch 21/499\n",
      "----------\n",
      "train Loss: 0.00101876 Acc: 0.95361328\n",
      "val Loss: 0.00069765 Acc: 0.96069869\n",
      "Epoch 22 of 500 took 0.122s\n",
      "Epoch 22/499\n",
      "----------\n",
      "train Loss: 0.00111805 Acc: 0.94726562\n",
      "val Loss: 0.00077240 Acc: 0.94759825\n",
      "Epoch 23 of 500 took 0.112s\n",
      "Epoch 23/499\n",
      "----------\n",
      "train Loss: 0.00100925 Acc: 0.95507812\n",
      "val Loss: 0.00072393 Acc: 0.95196507\n",
      "Epoch 24 of 500 took 0.108s\n",
      "\n",
      "Training complete in 0m 3s\n",
      "Best val loss: 0.000791\n",
      "Session:  5\n",
      "<generator object Module.parameters at 0x7facca159510>\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/TSD/participant_0/best_state_4.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/TSD/participant_0/best_state_4.pt' (epoch 13)\n",
      "Epoch 0/499\n",
      "----------\n",
      "train Loss: 0.00380644 Acc: 0.85498047\n",
      "val Loss: 0.00116686 Acc: 0.90393013\n",
      "New best validation loss: 0.0011668607397370984\n",
      "Epoch 1 of 500 took 0.118s\n",
      "Epoch 1/499\n",
      "----------\n",
      "train Loss: 0.00310321 Acc: 0.86132812\n",
      "val Loss: 0.00105363 Acc: 0.91703057\n",
      "New best validation loss: 0.001053628182307081\n",
      "Epoch 2 of 500 took 0.107s\n",
      "Epoch 2/499\n",
      "----------\n",
      "train Loss: 0.00247119 Acc: 0.88867188\n",
      "val Loss: 0.00102201 Acc: 0.89956332\n",
      "Epoch 3 of 500 took 0.109s\n",
      "Epoch 3/499\n",
      "----------\n",
      "train Loss: 0.00239104 Acc: 0.88964844\n",
      "val Loss: 0.00098223 Acc: 0.90393013\n",
      "Epoch 4 of 500 took 0.106s\n",
      "Epoch 4/499\n",
      "----------\n",
      "train Loss: 0.00242661 Acc: 0.89501953\n",
      "val Loss: 0.00092566 Acc: 0.930131\n",
      "New best validation loss: 0.0009256601333618164\n",
      "Epoch 5 of 500 took 0.111s\n",
      "Epoch 5/499\n",
      "----------\n",
      "train Loss: 0.00216596 Acc: 0.89941406\n",
      "val Loss: 0.00080725 Acc: 0.93886463\n",
      "New best validation loss: 0.0008072523831280038\n",
      "Epoch 6 of 500 took 0.107s\n",
      "Epoch 6/499\n",
      "----------\n",
      "train Loss: 0.00191518 Acc: 0.90576172\n",
      "val Loss: 0.00098023 Acc: 0.90829694\n",
      "Epoch 7 of 500 took 0.109s\n",
      "Epoch 7/499\n",
      "----------\n",
      "train Loss: 0.00190348 Acc: 0.91064453\n",
      "val Loss: 0.00085004 Acc: 0.91703057\n",
      "Epoch 8 of 500 took 0.105s\n",
      "Epoch 8/499\n",
      "----------\n",
      "train Loss: 0.00192045 Acc: 0.91455078\n",
      "val Loss: 0.00072328 Acc: 0.93449782\n",
      "Epoch 9 of 500 took 0.110s\n",
      "Epoch 9/499\n",
      "----------\n",
      "train Loss: 0.00166351 Acc: 0.91699219\n",
      "val Loss: 0.00074251 Acc: 0.93449782\n",
      "Epoch 10 of 500 took 0.117s\n",
      "Epoch 10/499\n",
      "----------\n",
      "train Loss: 0.00176715 Acc: 0.91943359\n",
      "val Loss: 0.00071606 Acc: 0.93886463\n",
      "Epoch 11 of 500 took 0.109s\n",
      "Epoch 11/499\n",
      "----------\n",
      "train Loss: 0.00160341 Acc: 0.92871094\n",
      "val Loss: 0.00072027 Acc: 0.93886463\n",
      "Epoch 12 of 500 took 0.110s\n",
      "Epoch 12/499\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.00164227 Acc: 0.92089844\n",
      "val Loss: 0.00065055 Acc: 0.96069869\n",
      "New best validation loss: 0.0006505461108736596\n",
      "Epoch 13 of 500 took 0.111s\n",
      "Epoch 13/499\n",
      "----------\n",
      "train Loss: 0.00156398 Acc: 0.92822266\n",
      "val Loss: 0.00076496 Acc: 0.930131\n",
      "Epoch 14 of 500 took 0.107s\n",
      "Epoch 14/499\n",
      "----------\n",
      "train Loss: 0.00166080 Acc: 0.92529297\n",
      "val Loss: 0.00086471 Acc: 0.93449782\n",
      "Epoch 15 of 500 took 0.111s\n",
      "Epoch 15/499\n",
      "----------\n",
      "train Loss: 0.00154996 Acc: 0.93359375\n",
      "val Loss: 0.00053184 Acc: 0.95196507\n",
      "New best validation loss: 0.0005318422244625841\n",
      "Epoch 16 of 500 took 0.108s\n",
      "Epoch 16/499\n",
      "----------\n",
      "train Loss: 0.00151405 Acc: 0.93164062\n",
      "val Loss: 0.00068292 Acc: 0.94759825\n",
      "Epoch 17 of 500 took 0.109s\n",
      "Epoch 17/499\n",
      "----------\n",
      "train Loss: 0.00152001 Acc: 0.92919922\n",
      "val Loss: 0.00075208 Acc: 0.92139738\n",
      "Epoch 18 of 500 took 0.106s\n",
      "Epoch 18/499\n",
      "----------\n",
      "train Loss: 0.00139514 Acc: 0.93554688\n",
      "val Loss: 0.00040050 Acc: 0.97816594\n",
      "New best validation loss: 0.0004004968175721481\n",
      "Epoch 19 of 500 took 0.116s\n",
      "Epoch 19/499\n",
      "----------\n",
      "train Loss: 0.00141918 Acc: 0.93505859\n",
      "val Loss: 0.00072538 Acc: 0.93886463\n",
      "Epoch 20 of 500 took 0.106s\n",
      "Epoch 20/499\n",
      "----------\n",
      "train Loss: 0.00134449 Acc: 0.94042969\n",
      "val Loss: 0.00046145 Acc: 0.95633188\n",
      "Epoch 21 of 500 took 0.110s\n",
      "Epoch 21/499\n",
      "----------\n",
      "train Loss: 0.00120596 Acc: 0.94335938\n",
      "val Loss: 0.00049460 Acc: 0.96069869\n",
      "Epoch 22 of 500 took 0.107s\n",
      "Epoch 22/499\n",
      "----------\n",
      "train Loss: 0.00116851 Acc: 0.94775391\n",
      "val Loss: 0.00059768 Acc: 0.95633188\n",
      "Epoch 23 of 500 took 0.110s\n",
      "Epoch 23/499\n",
      "----------\n",
      "train Loss: 0.00131508 Acc: 0.93994141\n",
      "val Loss: 0.00051358 Acc: 0.94759825\n",
      "Epoch 24 of 500 took 0.106s\n",
      "Epoch 24/499\n",
      "----------\n",
      "train Loss: 0.00108804 Acc: 0.95019531\n",
      "val Loss: 0.00091230 Acc: 0.930131\n",
      "Epoch    25: reducing learning rate of group 0 to 5.0300e-04.\n",
      "Epoch 25 of 500 took 0.110s\n",
      "Epoch 25/499\n",
      "----------\n",
      "train Loss: 0.00120168 Acc: 0.94287109\n",
      "val Loss: 0.00036467 Acc: 0.97379913\n",
      "Epoch 26 of 500 took 0.107s\n",
      "Epoch 26/499\n",
      "----------\n",
      "train Loss: 0.00096756 Acc: 0.9609375\n",
      "val Loss: 0.00040004 Acc: 0.96943231\n",
      "Epoch 27 of 500 took 0.109s\n",
      "Epoch 27/499\n",
      "----------\n",
      "train Loss: 0.00091924 Acc: 0.96044922\n",
      "val Loss: 0.00034525 Acc: 0.96943231\n",
      "Epoch 28 of 500 took 0.107s\n",
      "Epoch 28/499\n",
      "----------\n",
      "train Loss: 0.00100420 Acc: 0.95507812\n",
      "val Loss: 0.00033975 Acc: 0.97379913\n",
      "Epoch 29 of 500 took 0.115s\n",
      "Epoch 29/499\n",
      "----------\n",
      "train Loss: 0.00101583 Acc: 0.95507812\n",
      "val Loss: 0.00034056 Acc: 0.97379913\n",
      "Epoch 30 of 500 took 0.113s\n",
      "\n",
      "Training complete in 0m 3s\n",
      "Best val loss: 0.000400\n",
      "Session:  6\n",
      "<generator object Module.parameters at 0x7facca159510>\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/TSD/participant_0/best_state_5.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/TSD/participant_0/best_state_5.pt' (epoch 19)\n",
      "Epoch 0/499\n",
      "----------\n",
      "train Loss: 0.00338442 Acc: 0.86572266\n",
      "val Loss: 0.00099933 Acc: 0.89519651\n",
      "New best validation loss: 0.000999329652328158\n",
      "Epoch 1 of 500 took 0.120s\n",
      "Epoch 1/499\n",
      "----------\n",
      "train Loss: 0.00248627 Acc: 0.88183594\n",
      "val Loss: 0.00092921 Acc: 0.90829694\n",
      "Epoch 2 of 500 took 0.127s\n",
      "Epoch 2/499\n",
      "----------\n",
      "train Loss: 0.00233302 Acc: 0.90185547\n",
      "val Loss: 0.00087441 Acc: 0.92139738\n",
      "New best validation loss: 0.0008744145845221641\n",
      "Epoch 3 of 500 took 0.111s\n",
      "Epoch 3/499\n",
      "----------\n",
      "train Loss: 0.00219582 Acc: 0.89990234\n",
      "val Loss: 0.00089880 Acc: 0.90393013\n",
      "Epoch 4 of 500 took 0.111s\n",
      "Epoch 4/499\n",
      "----------\n",
      "train Loss: 0.00202833 Acc: 0.90234375\n",
      "val Loss: 0.00074203 Acc: 0.930131\n",
      "New best validation loss: 0.0007420298053708139\n",
      "Epoch 5 of 500 took 0.113s\n",
      "Epoch 5/499\n",
      "----------\n",
      "train Loss: 0.00185316 Acc: 0.92285156\n",
      "val Loss: 0.00075885 Acc: 0.930131\n",
      "Epoch 6 of 500 took 0.108s\n",
      "Epoch 6/499\n",
      "----------\n",
      "train Loss: 0.00172251 Acc: 0.921875\n",
      "val Loss: 0.00059946 Acc: 0.94323144\n",
      "New best validation loss: 0.0005994610260667759\n",
      "Epoch 7 of 500 took 0.121s\n",
      "Epoch 7/499\n",
      "----------\n",
      "train Loss: 0.00170424 Acc: 0.91748047\n",
      "val Loss: 0.00087156 Acc: 0.91703057\n",
      "Epoch 8 of 500 took 0.134s\n",
      "Epoch 8/499\n",
      "----------\n",
      "train Loss: 0.00164986 Acc: 0.92919922\n",
      "val Loss: 0.00070396 Acc: 0.94323144\n",
      "Epoch 9 of 500 took 0.113s\n",
      "Epoch 9/499\n",
      "----------\n",
      "train Loss: 0.00158325 Acc: 0.93115234\n",
      "val Loss: 0.00060607 Acc: 0.94323144\n",
      "Epoch 10 of 500 took 0.110s\n",
      "Epoch 10/499\n",
      "----------\n",
      "train Loss: 0.00150729 Acc: 0.93066406\n",
      "val Loss: 0.00058896 Acc: 0.93886463\n",
      "Epoch 11 of 500 took 0.115s\n",
      "Epoch 11/499\n",
      "----------\n",
      "train Loss: 0.00150578 Acc: 0.92822266\n",
      "val Loss: 0.00059658 Acc: 0.95196507\n",
      "Epoch 12 of 500 took 0.130s\n",
      "Epoch 12/499\n",
      "----------\n",
      "train Loss: 0.00157953 Acc: 0.92285156\n",
      "val Loss: 0.00067075 Acc: 0.93449782\n",
      "Epoch 13 of 500 took 0.120s\n",
      "Epoch 13/499\n",
      "----------\n",
      "train Loss: 0.00126963 Acc: 0.94482422\n",
      "val Loss: 0.00080431 Acc: 0.93886463\n",
      "Epoch 14 of 500 took 0.114s\n",
      "Epoch 14/499\n",
      "----------\n",
      "train Loss: 0.00123171 Acc: 0.94384766\n",
      "val Loss: 0.00054543 Acc: 0.95196507\n",
      "Epoch 15 of 500 took 0.110s\n",
      "Epoch 15/499\n",
      "----------\n",
      "train Loss: 0.00130508 Acc: 0.9375\n",
      "val Loss: 0.00061708 Acc: 0.930131\n",
      "Epoch 16 of 500 took 0.112s\n",
      "Epoch 16/499\n",
      "----------\n",
      "train Loss: 0.00131480 Acc: 0.94091797\n",
      "val Loss: 0.00042935 Acc: 0.96069869\n",
      "New best validation loss: 0.0004293467995901816\n",
      "Epoch 17 of 500 took 0.117s\n",
      "Epoch 17/499\n",
      "----------\n",
      "train Loss: 0.00121477 Acc: 0.94482422\n",
      "val Loss: 0.00052691 Acc: 0.94759825\n",
      "Epoch 18 of 500 took 0.115s\n",
      "Epoch 18/499\n",
      "----------\n",
      "train Loss: 0.00120611 Acc: 0.94384766\n",
      "val Loss: 0.00046411 Acc: 0.95633188\n",
      "Epoch 19 of 500 took 0.123s\n",
      "Epoch 19/499\n",
      "----------\n",
      "train Loss: 0.00133064 Acc: 0.93847656\n",
      "val Loss: 0.00039069 Acc: 0.9650655\n",
      "Epoch 20 of 500 took 0.114s\n",
      "Epoch 20/499\n",
      "----------\n",
      "train Loss: 0.00105817 Acc: 0.95458984\n",
      "val Loss: 0.00055941 Acc: 0.95633188\n",
      "Epoch 21 of 500 took 0.117s\n",
      "Epoch 21/499\n",
      "----------\n",
      "train Loss: 0.00099741 Acc: 0.95556641\n",
      "val Loss: 0.00054824 Acc: 0.94759825\n",
      "Epoch 22 of 500 took 0.142s\n",
      "Epoch 22/499\n",
      "----------\n",
      "train Loss: 0.00100272 Acc: 0.95410156\n",
      "val Loss: 0.00037909 Acc: 0.95633188\n",
      "Epoch 23 of 500 took 0.134s\n",
      "Epoch 23/499\n",
      "----------\n",
      "train Loss: 0.00100764 Acc: 0.95410156\n",
      "val Loss: 0.00290890 Acc: 0.80786026\n",
      "Epoch 24 of 500 took 0.161s\n",
      "Epoch 24/499\n",
      "----------\n",
      "train Loss: 0.00104494 Acc: 0.95654297\n",
      "val Loss: 0.00041653 Acc: 0.96069869\n",
      "Epoch 25 of 500 took 0.147s\n",
      "Epoch 25/499\n",
      "----------\n",
      "train Loss: 0.00102742 Acc: 0.95166016\n",
      "val Loss: 0.00034100 Acc: 0.96943231\n",
      "Epoch 26 of 500 took 0.140s\n",
      "Epoch 26/499\n",
      "----------\n",
      "train Loss: 0.00099624 Acc: 0.95507812\n",
      "val Loss: 0.00042187 Acc: 0.96069869\n",
      "Epoch 27 of 500 took 0.120s\n",
      "Epoch 27/499\n",
      "----------\n",
      "train Loss: 0.00100599 Acc: 0.95166016\n",
      "val Loss: 0.00043554 Acc: 0.94759825\n",
      "Epoch 28 of 500 took 0.138s\n",
      "\n",
      "Training complete in 0m 3s\n",
      "Best val loss: 0.000429\n",
      "Session:  7\n",
      "<generator object Module.parameters at 0x7facca159510>\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/TSD/participant_0/best_state_6.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/TSD/participant_0/best_state_6.pt' (epoch 17)\n",
      "Epoch 0/499\n",
      "----------\n",
      "train Loss: 0.00362455 Acc: 0.85058594\n",
      "val Loss: 0.00111238 Acc: 0.88646288\n",
      "New best validation loss: 0.0011123754572139556\n",
      "Epoch 1 of 500 took 0.141s\n",
      "Epoch 1/499\n",
      "----------\n",
      "train Loss: 0.00267433 Acc: 0.87109375\n",
      "val Loss: 0.00098687 Acc: 0.88646288\n",
      "New best validation loss: 0.0009868688328297378\n",
      "Epoch 2 of 500 took 0.110s\n",
      "Epoch 2/499\n",
      "----------\n",
      "train Loss: 0.00245732 Acc: 0.88623047\n",
      "val Loss: 0.00087512 Acc: 0.91266376\n",
      "New best validation loss: 0.0008751221619318667\n",
      "Epoch 3 of 500 took 0.120s\n",
      "Epoch 3/499\n",
      "----------\n",
      "train Loss: 0.00227407 Acc: 0.89306641\n",
      "val Loss: 0.00087114 Acc: 0.92576419\n",
      "Epoch 4 of 500 took 0.113s\n",
      "Epoch 4/499\n",
      "----------\n",
      "train Loss: 0.00218972 Acc: 0.89697266\n",
      "val Loss: 0.00090694 Acc: 0.91703057\n",
      "Epoch 5 of 500 took 0.113s\n",
      "Epoch 5/499\n",
      "----------\n",
      "train Loss: 0.00181798 Acc: 0.90966797\n",
      "val Loss: 0.00059433 Acc: 0.93449782\n",
      "New best validation loss: 0.0005943252660301575\n",
      "Epoch 6 of 500 took 0.114s\n",
      "Epoch 6/499\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.00187480 Acc: 0.91650391\n",
      "val Loss: 0.00084520 Acc: 0.92139738\n",
      "Epoch 7 of 500 took 0.118s\n",
      "Epoch 7/499\n",
      "----------\n",
      "train Loss: 0.00175920 Acc: 0.91455078\n",
      "val Loss: 0.00086803 Acc: 0.93886463\n",
      "Epoch 8 of 500 took 0.113s\n",
      "Epoch 8/499\n",
      "----------\n",
      "train Loss: 0.00171825 Acc: 0.91650391\n",
      "val Loss: 0.00084143 Acc: 0.90829694\n",
      "Epoch 9 of 500 took 0.131s\n",
      "Epoch 9/499\n",
      "----------\n",
      "train Loss: 0.00152375 Acc: 0.92578125\n",
      "val Loss: 0.00062306 Acc: 0.94759825\n",
      "Epoch 10 of 500 took 0.142s\n",
      "Epoch 10/499\n",
      "----------\n",
      "train Loss: 0.00158520 Acc: 0.92480469\n",
      "val Loss: 0.00061745 Acc: 0.95196507\n",
      "Epoch 11 of 500 took 0.117s\n",
      "Epoch 11/499\n",
      "----------\n",
      "train Loss: 0.00144008 Acc: 0.93554688\n",
      "val Loss: 0.00068516 Acc: 0.93886463\n",
      "Epoch    12: reducing learning rate of group 0 to 5.0300e-04.\n",
      "Epoch 12 of 500 took 0.110s\n",
      "Epoch 12/499\n",
      "----------\n",
      "train Loss: 0.00136370 Acc: 0.93408203\n",
      "val Loss: 0.00060438 Acc: 0.93449782\n",
      "Epoch 13 of 500 took 0.111s\n",
      "Epoch 13/499\n",
      "----------\n",
      "train Loss: 0.00127513 Acc: 0.93701172\n",
      "val Loss: 0.00052586 Acc: 0.95196507\n",
      "Epoch 14 of 500 took 0.118s\n",
      "Epoch 14/499\n",
      "----------\n",
      "train Loss: 0.00125861 Acc: 0.94335938\n",
      "val Loss: 0.00048269 Acc: 0.95196507\n",
      "New best validation loss: 0.0004826922408878543\n",
      "Epoch 15 of 500 took 0.112s\n",
      "Epoch 15/499\n",
      "----------\n",
      "train Loss: 0.00121475 Acc: 0.93994141\n",
      "val Loss: 0.00055614 Acc: 0.94759825\n",
      "Epoch 16 of 500 took 0.107s\n",
      "Epoch 16/499\n",
      "----------\n",
      "train Loss: 0.00117888 Acc: 0.94287109\n",
      "val Loss: 0.00050804 Acc: 0.94759825\n",
      "Epoch 17 of 500 took 0.109s\n",
      "Epoch 17/499\n",
      "----------\n",
      "train Loss: 0.00122454 Acc: 0.94091797\n",
      "val Loss: 0.00056482 Acc: 0.93886463\n",
      "Epoch 18 of 500 took 0.107s\n",
      "Epoch 18/499\n",
      "----------\n",
      "train Loss: 0.00117659 Acc: 0.94726562\n",
      "val Loss: 0.00057017 Acc: 0.93449782\n",
      "Epoch 19 of 500 took 0.110s\n",
      "Epoch 19/499\n",
      "----------\n",
      "train Loss: 0.00120608 Acc: 0.9453125\n",
      "val Loss: 0.00048709 Acc: 0.94759825\n",
      "Epoch 20 of 500 took 0.107s\n",
      "Epoch 20/499\n",
      "----------\n",
      "train Loss: 0.00113885 Acc: 0.94091797\n",
      "val Loss: 0.00043949 Acc: 0.96069869\n",
      "Epoch 21 of 500 took 0.111s\n",
      "Epoch 21/499\n",
      "----------\n",
      "train Loss: 0.00108571 Acc: 0.9453125\n",
      "val Loss: 0.00048745 Acc: 0.94759825\n",
      "Epoch 22 of 500 took 0.107s\n",
      "Epoch 22/499\n",
      "----------\n",
      "train Loss: 0.00095977 Acc: 0.95605469\n",
      "val Loss: 0.00048875 Acc: 0.96069869\n",
      "Epoch 23 of 500 took 0.109s\n",
      "Epoch 23/499\n",
      "----------\n",
      "train Loss: 0.00117262 Acc: 0.94238281\n",
      "val Loss: 0.00056887 Acc: 0.94323144\n",
      "Epoch 24 of 500 took 0.107s\n",
      "Epoch 24/499\n",
      "----------\n",
      "train Loss: 0.00105244 Acc: 0.95263672\n",
      "val Loss: 0.00058308 Acc: 0.93449782\n",
      "Epoch 25 of 500 took 0.110s\n",
      "Epoch 25/499\n",
      "----------\n",
      "train Loss: 0.00110708 Acc: 0.94873047\n",
      "val Loss: 0.00049131 Acc: 0.94759825\n",
      "Epoch 26 of 500 took 0.106s\n",
      "\n",
      "Training complete in 0m 3s\n",
      "Best val loss: 0.000483\n"
     ]
    }
   ],
   "source": [
    "# train_fine_tuning(examples_datasets_train, labels_datasets_train,\n",
    "#                   num_kernels=num_kernels, path_weight_to_save_to=path_TSD,\n",
    "#                   number_of_classes=number_of_classes, \n",
    "#                   number_of_cycles_total=number_of_cycles_total,\n",
    "#                   number_of_cycle_for_first_training = number_of_cycle_for_first_training,\n",
    "#                   batch_size=batch_size,\n",
    "#                   feature_vector_input_length=feature_vector_input_length,\n",
    "#                   learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET one participant_examples  (8,)\n",
      "   GET one training_index_examples  (12, 572, 252)  at  0\n",
      "   GOT one group XY  (6864, 252)    (6864,)\n",
      "       one group XY test  (1716, 252)    (1716, 252)\n",
      "       one group XY train (6177, 252)    (6177,)\n",
      "       one group XY valid (687, 252)    (687, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  1\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  2\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  3\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  4\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  5\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  6\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  7\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "dataloaders: \n",
      "   train  (1, 8)\n",
      "   valid  (1, 8)\n",
      "   test  (1, 8)\n",
      "0  SESSION   data =  1716\n",
      "Participant:  0  Accuracy:  0.916083916083916\n",
      "1  SESSION   data =  572\n",
      "Participant:  0  Accuracy:  0.6905594405594405\n",
      "2  SESSION   data =  572\n",
      "Participant:  0  Accuracy:  0.7167832167832168\n",
      "3  SESSION   data =  572\n",
      "Participant:  0  Accuracy:  0.5681818181818182\n",
      "4  SESSION   data =  572\n",
      "Participant:  0  Accuracy:  0.6188811188811189\n",
      "5  SESSION   data =  572\n",
      "Participant:  0  Accuracy:  0.44405594405594406\n",
      "6  SESSION   data =  572\n",
      "Participant:  0  Accuracy:  0.458041958041958\n",
      "7  SESSION   data =  572\n",
      "Participant:  0  Accuracy:  0.4195804195804196\n",
      "ACCURACY PARTICIPANT  0 :  [0.916083916083916, 0.6905594405594405, 0.7167832167832168, 0.5681818181818182, 0.6188811188811189, 0.44405594405594406, 0.458041958041958, 0.4195804195804196]\n",
      "[array([0.91608392, 0.69055944, 0.71678322, 0.56818182, 0.61888112,\n",
      "       0.44405594, 0.45804196, 0.41958042])]\n",
      "OVERALL ACCURACY: 0.604020979020979\n"
     ]
    }
   ],
   "source": [
    "algo_name = \"standard_TSD\"\n",
    "test_standard_model_on_training_sessions(examples_datasets_train, labels_datasets_train,\n",
    "                                  num_neurons=num_kernels, use_only_first_training=True,\n",
    "                                  path_weights=path_TSD,\n",
    "                                  feature_vector_input_length=feature_vector_input_length,\n",
    "                                  save_path = save_TSD, algo_name=algo_name,\n",
    "                                  number_of_cycles_total=number_of_cycles_total,\n",
    "                                  number_of_cycle_for_first_training = number_of_cycle_for_first_training,\n",
    "                                  number_of_classes=number_of_classes, cycle_for_test=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Day_0~2</th>\n",
       "      <td>0.916084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_3</th>\n",
       "      <td>0.690559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_4</th>\n",
       "      <td>0.716783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_5</th>\n",
       "      <td>0.568182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_6</th>\n",
       "      <td>0.618881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_7</th>\n",
       "      <td>0.444056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_8</th>\n",
       "      <td>0.458042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_9</th>\n",
       "      <td>0.41958</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Participant_5\n",
       "Day_0~2      0.916084\n",
       "Day_3        0.690559\n",
       "Day_4        0.716783\n",
       "Day_5        0.568182\n",
       "Day_6        0.618881\n",
       "Day_7        0.444056\n",
       "Day_8        0.458042\n",
       "Day_9         0.41958"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_filename = save_TSD + '/predictions_' + algo_name + \"_no_retraining.npy\"\n",
    "results = np.load(results_filename, allow_pickle=True)\n",
    "TSD_acc = results[0]\n",
    "TSD_acc_overall = np.mean(TSD_acc)\n",
    "index_participant_list = ['0~2', 3, 4, 5, 6, 7, 8, 9]\n",
    "TSD_df = pd.DataFrame(TSD_acc.transpose(), \n",
    "                       index = [f'Day_{i}' for i in index_participant_list],\n",
    "                        columns = ['Participant_5'])\n",
    "TSD_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "TSD_df.transpose().plot.bar(rot=0, figsize=(10,5))\n",
    "plt.title(\"TSD Accuracies\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TrainingsAndEvaluations.ForTrainingSessions.utils import get_gesture_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ground_truths  =  (1, 8)\n",
      "predictions =  (1, 8)\n",
      "index_participant_list  ['0~2', 3, 4, 5, 6, 7, 8, 9]\n",
      "accuracies_gestures =  (22, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Loc1_Sub5_Day0~2-&gt;0~2</th>\n",
       "      <th>Loc1_Sub5_Day0~2-&gt;3</th>\n",
       "      <th>Loc1_Sub5_Day0~2-&gt;4</th>\n",
       "      <th>Loc1_Sub5_Day0~2-&gt;5</th>\n",
       "      <th>Loc1_Sub5_Day0~2-&gt;6</th>\n",
       "      <th>Loc1_Sub5_Day0~2-&gt;7</th>\n",
       "      <th>Loc1_Sub5_Day0~2-&gt;8</th>\n",
       "      <th>Loc1_Sub5_Day0~2-&gt;9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M1</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.384615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M2</td>\n",
       "      <td>0.871795</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.346154</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>0.346154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M3</td>\n",
       "      <td>0.948718</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M4</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>M5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.038462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>M6</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.346154</td>\n",
       "      <td>0.192308</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.769231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>M7</td>\n",
       "      <td>0.987179</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.769231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>M8</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.384615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>M9</td>\n",
       "      <td>0.897436</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.615385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>M10</td>\n",
       "      <td>0.910256</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.076923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>M11</td>\n",
       "      <td>0.935897</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>M12</td>\n",
       "      <td>0.910256</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.269231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>M13</td>\n",
       "      <td>0.743590</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.576923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>M14</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>M15</td>\n",
       "      <td>0.641026</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>M16</td>\n",
       "      <td>0.987179</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>M17</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>M18</td>\n",
       "      <td>0.987179</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>0.346154</td>\n",
       "      <td>0.576923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>M19</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>M20</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.769231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>M21</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.653846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Mean</td>\n",
       "      <td>0.916084</td>\n",
       "      <td>0.690559</td>\n",
       "      <td>0.716783</td>\n",
       "      <td>0.568182</td>\n",
       "      <td>0.618881</td>\n",
       "      <td>0.444056</td>\n",
       "      <td>0.458042</td>\n",
       "      <td>0.419580</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Loc1_Sub5_Day0~2->0~2  Loc1_Sub5_Day0~2->3  \\\n",
       "0          M0               1.000000             1.000000   \n",
       "1          M1               0.961538             0.692308   \n",
       "2          M2               0.871795             0.500000   \n",
       "3          M3               0.948718             0.076923   \n",
       "4          M4               0.833333             0.000000   \n",
       "5          M5               1.000000             1.000000   \n",
       "6          M6               1.000000             0.230769   \n",
       "7          M7               0.987179             0.961538   \n",
       "8          M8               0.923077             1.000000   \n",
       "9          M9               0.897436             0.807692   \n",
       "10        M10               0.910256             0.461538   \n",
       "11        M11               0.935897             0.538462   \n",
       "12        M12               0.910256             0.884615   \n",
       "13        M13               0.743590             0.961538   \n",
       "14        M14               0.923077             0.000000   \n",
       "15        M15               0.641026             0.538462   \n",
       "16        M16               0.987179             1.000000   \n",
       "17        M17               1.000000             1.000000   \n",
       "18        M18               0.987179             1.000000   \n",
       "19        M19               1.000000             0.961538   \n",
       "20        M20               0.807692             0.884615   \n",
       "21        M21               0.884615             0.692308   \n",
       "22       Mean               0.916084             0.690559   \n",
       "\n",
       "    Loc1_Sub5_Day0~2->4  Loc1_Sub5_Day0~2->5  Loc1_Sub5_Day0~2->6  \\\n",
       "0              1.000000             1.000000             1.000000   \n",
       "1              0.884615             0.961538             0.500000   \n",
       "2              0.769231             0.384615             0.346154   \n",
       "3              0.230769             0.038462             0.000000   \n",
       "4              0.730769             0.000000             0.000000   \n",
       "5              0.538462             0.538462             0.884615   \n",
       "6              1.000000             0.538462             0.346154   \n",
       "7              0.923077             0.884615             1.000000   \n",
       "8              0.961538             1.000000             0.923077   \n",
       "9              0.846154             0.615385             1.000000   \n",
       "10             0.884615             0.807692             0.576923   \n",
       "11             0.038462             0.000000             0.500000   \n",
       "12             0.884615             0.692308             0.653846   \n",
       "13             0.961538             1.000000             1.000000   \n",
       "14             0.000000             0.000000             0.000000   \n",
       "15             0.000000             0.000000             0.000000   \n",
       "16             1.000000             0.884615             1.000000   \n",
       "17             0.769231             1.000000             1.000000   \n",
       "18             1.000000             0.076923             0.538462   \n",
       "19             0.846154             0.269231             0.615385   \n",
       "20             0.884615             0.884615             0.807692   \n",
       "21             0.615385             0.923077             0.923077   \n",
       "22             0.716783             0.568182             0.618881   \n",
       "\n",
       "    Loc1_Sub5_Day0~2->7  Loc1_Sub5_Day0~2->8  Loc1_Sub5_Day0~2->9  \n",
       "0              1.000000             1.000000             1.000000  \n",
       "1              0.269231             0.423077             0.384615  \n",
       "2              0.000000             0.269231             0.346154  \n",
       "3              0.000000             0.000000             0.000000  \n",
       "4              0.000000             0.000000             0.000000  \n",
       "5              0.000000             0.461538             0.038462  \n",
       "6              0.192308             0.115385             0.769231  \n",
       "7              0.846154             0.692308             0.769231  \n",
       "8              0.961538             0.807692             0.384615  \n",
       "9              0.807692             0.769231             0.615385  \n",
       "10             0.269231             0.230769             0.076923  \n",
       "11             0.000000             0.000000             0.000000  \n",
       "12             0.230769             0.115385             0.269231  \n",
       "13             0.923077             1.000000             0.576923  \n",
       "14             0.000000             0.000000             0.000000  \n",
       "15             0.000000             0.000000             0.000000  \n",
       "16             1.000000             1.000000             1.000000  \n",
       "17             1.000000             1.000000             1.000000  \n",
       "18             0.653846             0.346154             0.576923  \n",
       "19             0.000000             0.038462             0.000000  \n",
       "20             0.846154             0.807692             0.769231  \n",
       "21             0.769231             1.000000             0.653846  \n",
       "22             0.444056             0.458042             0.419580  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truths = results[1]\n",
    "predictions = results[2]\n",
    "print(\"ground_truths  = \", np.shape(ground_truths))\n",
    "print(\"predictions = \", np.shape(predictions))\n",
    "m_name = \"Loc1_Sub\"\n",
    "n_name = \"Day0~2->\"\n",
    "df = get_gesture_accuracies(ground_truths, predictions, number_of_classes=number_of_classes, \n",
    "                            m_name=m_name, n_name=n_name, path=save_TSD, algo_name=algo_name,\n",
    "                           index_participant_list_customized=index_participant_list,\n",
    "                           lump_day_at_participant=5)\n",
    "df = pd.read_csv(save_TSD+'/'+algo_name+'.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. DANN\n",
    "* `train_DANN`: train DANN model using the first set of training weights from base model\n",
    "    * num_sessions-1 sets of training weights will be saved\n",
    "* `test_DANN_on_training_sessions`: test DANN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TrainingsAndEvaluations.ForTrainingSessions.train_tsd_dnn_DA import train_DANN, test_DANN_on_training_sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET one participant_examples  (8,)\n",
      "   GET one training_index_examples  (12, 572, 252)  at  0\n",
      "   GOT one group XY  (6864, 252)    (6864,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (6177, 252)    (6177,)\n",
      "       one group XY valid (687, 252)    (687, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  1\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  2\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  3\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  4\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  5\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  6\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  7\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "dataloaders: \n",
      "   train  (1, 8)\n",
      "   valid  (1, 8)\n",
      "   test  (1, 0)\n",
      "SHAPE SESSIONS:  (8,)\n",
      "()\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/TSD/participant_0/best_state_0.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/TSD/participant_0/best_state_0.pt' (epoch 10)\n",
      "STARTING TRAINING\n",
      "Accuracy source 0.884277, main loss classifier 0.251182, source classification loss 0.336140, loss domain distinction 0.340514, accuracy domain distinction 0.489990\n",
      "VALIDATION Loss: 0.33308536 Acc: 0.87336245\n",
      "New best validation loss:  0.3330853581428528\n",
      "Epoch 1 of 500 took 0.237s\n",
      "Accuracy source 0.884277, main loss classifier 0.242566, source classification loss 0.337394, loss domain distinction 0.194225, accuracy domain distinction 0.500244\n",
      "VALIDATION Loss: 0.39134204 Acc: 0.87190684\n",
      "Epoch 2 of 500 took 0.236s\n",
      "Accuracy source 0.889160, main loss classifier 0.231520, source classification loss 0.318665, loss domain distinction 0.189679, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.30693555 Acc: 0.8937409\n",
      "New best validation loss:  0.30693554878234863\n",
      "Epoch 3 of 500 took 0.241s\n",
      "Accuracy source 0.873535, main loss classifier 0.246539, source classification loss 0.349735, loss domain distinction 0.188108, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.28351718 Acc: 0.88646288\n",
      "New best validation loss:  0.28351718187332153\n",
      "Epoch 4 of 500 took 0.239s\n",
      "Accuracy source 0.877441, main loss classifier 0.236027, source classification loss 0.328011, loss domain distinction 0.190533, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.23560157 Acc: 0.91411936\n",
      "New best validation loss:  0.23560157418251038\n",
      "Epoch 5 of 500 took 0.236s\n",
      "Accuracy source 0.889648, main loss classifier 0.238627, source classification loss 0.334697, loss domain distinction 0.188651, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.26109099 Acc: 0.90393013\n",
      "Epoch    16: reducing learning rate of group 0 to 5.0300e-04.\n",
      "Epoch 6 of 500 took 0.236s\n",
      "Accuracy source 0.902344, main loss classifier 0.218448, source classification loss 0.299478, loss domain distinction 0.184662, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.28068441 Acc: 0.90393013\n",
      "Epoch 7 of 500 took 0.235s\n",
      "Accuracy source 0.899414, main loss classifier 0.210734, source classification loss 0.284479, loss domain distinction 0.184891, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.22698948 Acc: 0.90538574\n",
      "New best validation loss:  0.2269894778728485\n",
      "Epoch 8 of 500 took 0.240s\n",
      "Accuracy source 0.905273, main loss classifier 0.206500, source classification loss 0.274749, loss domain distinction 0.184527, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.20983645 Acc: 0.92139738\n",
      "New best validation loss:  0.2098364531993866\n",
      "Epoch 9 of 500 took 0.235s\n",
      "Accuracy source 0.908691, main loss classifier 0.197086, source classification loss 0.256517, loss domain distinction 0.184733, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.22073022 Acc: 0.91266376\n",
      "Epoch 10 of 500 took 0.235s\n",
      "Accuracy source 0.916504, main loss classifier 0.194649, source classification loss 0.252445, loss domain distinction 0.184343, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.20668030 Acc: 0.92139738\n",
      "New best validation loss:  0.2066802978515625\n",
      "Epoch 11 of 500 took 0.254s\n",
      "Accuracy source 0.918945, main loss classifier 0.187235, source classification loss 0.237337, loss domain distinction 0.184516, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.23625699 Acc: 0.91703057\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0060e-04.\n",
      "Epoch 12 of 500 took 0.236s\n",
      "Accuracy source 0.907715, main loss classifier 0.205221, source classification loss 0.273775, loss domain distinction 0.183717, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.23749390 Acc: 0.92285298\n",
      "Epoch 13 of 500 took 0.240s\n",
      "Accuracy source 0.920410, main loss classifier 0.183305, source classification loss 0.230552, loss domain distinction 0.183541, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.23290223 Acc: 0.91557496\n",
      "Epoch 14 of 500 took 0.238s\n",
      "Accuracy source 0.906250, main loss classifier 0.199330, source classification loss 0.262591, loss domain distinction 0.183027, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.28328758 Acc: 0.8937409\n",
      "Epoch 15 of 500 took 0.284s\n",
      "Accuracy source 0.919434, main loss classifier 0.180373, source classification loss 0.224300, loss domain distinction 0.185142, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.24539357 Acc: 0.91411936\n",
      "Epoch 16 of 500 took 0.248s\n",
      "Accuracy source 0.923828, main loss classifier 0.185394, source classification loss 0.235314, loss domain distinction 0.182308, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.26750544 Acc: 0.89810771\n",
      "Epoch 17 of 500 took 0.254s\n",
      "Accuracy source 0.906738, main loss classifier 0.199690, source classification loss 0.263196, loss domain distinction 0.183480, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.20607512 Acc: 0.92430859\n",
      "New best validation loss:  0.20607511699199677\n",
      "Epoch 18 of 500 took 0.287s\n",
      "Accuracy source 0.919434, main loss classifier 0.181605, source classification loss 0.227615, loss domain distinction 0.181901, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.21816279 Acc: 0.91994178\n",
      "Epoch 19 of 500 took 0.252s\n",
      "Accuracy source 0.915039, main loss classifier 0.191063, source classification loss 0.245994, loss domain distinction 0.184550, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.22037068 Acc: 0.90975255\n",
      "Epoch 20 of 500 took 0.237s\n",
      "Accuracy source 0.905762, main loss classifier 0.197409, source classification loss 0.259904, loss domain distinction 0.181954, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.22407320 Acc: 0.92285298\n",
      "Epoch 21 of 500 took 0.238s\n",
      "Accuracy source 0.920898, main loss classifier 0.185659, source classification loss 0.235679, loss domain distinction 0.183374, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.26020369 Acc: 0.91848617\n",
      "Epoch 22 of 500 took 0.240s\n",
      "Accuracy source 0.917969, main loss classifier 0.186211, source classification loss 0.236411, loss domain distinction 0.184121, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.20538159 Acc: 0.9286754\n",
      "New best validation loss:  0.2053815871477127\n",
      "Epoch 23 of 500 took 0.235s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy source 0.909180, main loss classifier 0.186234, source classification loss 0.236088, loss domain distinction 0.184229, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.20573461 Acc: 0.92430859\n",
      "Epoch 24 of 500 took 0.240s\n",
      "Accuracy source 0.920410, main loss classifier 0.182453, source classification loss 0.229586, loss domain distinction 0.182879, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.20957269 Acc: 0.91994178\n",
      "Epoch 25 of 500 took 0.237s\n",
      "Accuracy source 0.918945, main loss classifier 0.184206, source classification loss 0.232206, loss domain distinction 0.183857, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.22245811 Acc: 0.92139738\n",
      "Epoch 26 of 500 took 0.233s\n",
      "Accuracy source 0.916992, main loss classifier 0.189791, source classification loss 0.244248, loss domain distinction 0.183358, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.25003240 Acc: 0.90975255\n",
      "Epoch 27 of 500 took 0.232s\n",
      "Accuracy source 0.906250, main loss classifier 0.197462, source classification loss 0.258764, loss domain distinction 0.184268, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.21039933 Acc: 0.92139738\n",
      "Epoch 28 of 500 took 0.232s\n",
      "Accuracy source 0.923340, main loss classifier 0.180454, source classification loss 0.225544, loss domain distinction 0.182333, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.22284627 Acc: 0.92139738\n",
      "Epoch 29 of 500 took 0.239s\n",
      "Accuracy source 0.919434, main loss classifier 0.181983, source classification loss 0.228458, loss domain distinction 0.182576, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.21545279 Acc: 0.9286754\n",
      "Epoch 30 of 500 took 0.232s\n",
      "Accuracy source 0.909668, main loss classifier 0.192856, source classification loss 0.249291, loss domain distinction 0.183804, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.23410685 Acc: 0.91411936\n",
      "Epoch 31 of 500 took 0.233s\n",
      "Accuracy source 0.910645, main loss classifier 0.193677, source classification loss 0.251543, loss domain distinction 0.183478, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.26161397 Acc: 0.90975255\n",
      "Epoch 32 of 500 took 0.232s\n",
      "Accuracy source 0.909180, main loss classifier 0.193201, source classification loss 0.249897, loss domain distinction 0.185090, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.29203889 Acc: 0.89519651\n",
      "Epoch 33 of 500 took 0.234s\n",
      "Accuracy source 0.921387, main loss classifier 0.182888, source classification loss 0.229897, loss domain distinction 0.183765, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.31011656 Acc: 0.89810771\n",
      "Training complete in 0m 8s\n",
      "()\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/TSD/participant_0/best_state_0.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/TSD/participant_0/best_state_0.pt' (epoch 10)\n",
      "STARTING TRAINING\n",
      "Accuracy source 0.890625, main loss classifier 0.243839, source classification loss 0.321820, loss domain distinction 0.343983, accuracy domain distinction 0.481934\n",
      "VALIDATION Loss: 0.34464085 Acc: 0.86462882\n",
      "New best validation loss:  0.344640851020813\n",
      "Epoch 1 of 500 took 0.235s\n",
      "Accuracy source 0.878418, main loss classifier 0.238635, source classification loss 0.331872, loss domain distinction 0.190235, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.28736779 Acc: 0.8937409\n",
      "New best validation loss:  0.2873677909374237\n",
      "Epoch 2 of 500 took 0.266s\n",
      "Accuracy source 0.888184, main loss classifier 0.241861, source classification loss 0.339100, loss domain distinction 0.192223, accuracy domain distinction 0.500244\n",
      "VALIDATION Loss: 0.26124921 Acc: 0.89665211\n",
      "New best validation loss:  0.26124921441078186\n",
      "Epoch 3 of 500 took 0.313s\n",
      "Accuracy source 0.880371, main loss classifier 0.244378, source classification loss 0.344317, loss domain distinction 0.189048, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.28933057 Acc: 0.89082969\n",
      "Epoch 4 of 500 took 0.304s\n",
      "Accuracy source 0.895996, main loss classifier 0.228812, source classification loss 0.315029, loss domain distinction 0.187908, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.36976019 Acc: 0.87190684\n",
      "Epoch 5 of 500 took 0.304s\n",
      "Accuracy source 0.888184, main loss classifier 0.229345, source classification loss 0.316921, loss domain distinction 0.187251, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.28280640 Acc: 0.89810771\n",
      "Epoch    16: reducing learning rate of group 0 to 5.0300e-04.\n",
      "Epoch 6 of 500 took 0.309s\n",
      "Accuracy source 0.892090, main loss classifier 0.225154, source classification loss 0.313350, loss domain distinction 0.183837, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.23827916 Acc: 0.91266376\n",
      "New best validation loss:  0.23827916383743286\n",
      "Epoch 7 of 500 took 0.306s\n",
      "Accuracy source 0.899414, main loss classifier 0.214945, source classification loss 0.291554, loss domain distinction 0.187467, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.31624013 Acc: 0.88500728\n",
      "Epoch 8 of 500 took 0.306s\n",
      "Accuracy source 0.905273, main loss classifier 0.203420, source classification loss 0.269383, loss domain distinction 0.184516, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.23543383 Acc: 0.91120815\n",
      "New best validation loss:  0.23543383181095123\n",
      "Epoch 9 of 500 took 0.263s\n",
      "Accuracy source 0.894531, main loss classifier 0.205827, source classification loss 0.274795, loss domain distinction 0.183457, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.27646729 Acc: 0.89956332\n",
      "Epoch 10 of 500 took 0.325s\n",
      "Accuracy source 0.916016, main loss classifier 0.189458, source classification loss 0.241247, loss domain distinction 0.186053, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.27148226 Acc: 0.8937409\n",
      "Epoch 11 of 500 took 0.323s\n",
      "Accuracy source 0.910645, main loss classifier 0.194659, source classification loss 0.252869, loss domain distinction 0.182555, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.25247735 Acc: 0.90393013\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0060e-04.\n",
      "Epoch 12 of 500 took 0.311s\n",
      "Accuracy source 0.916992, main loss classifier 0.180263, source classification loss 0.224501, loss domain distinction 0.184229, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.24886952 Acc: 0.89519651\n",
      "Epoch 13 of 500 took 0.308s\n",
      "Accuracy source 0.908691, main loss classifier 0.200916, source classification loss 0.265600, loss domain distinction 0.183413, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.27879331 Acc: 0.89519651\n",
      "Epoch 14 of 500 took 0.281s\n",
      "Accuracy source 0.910645, main loss classifier 0.194127, source classification loss 0.252106, loss domain distinction 0.183054, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.25375372 Acc: 0.90101892\n",
      "Epoch 15 of 500 took 0.282s\n",
      "Accuracy source 0.915527, main loss classifier 0.193261, source classification loss 0.250444, loss domain distinction 0.184208, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.25944963 Acc: 0.8937409\n",
      "Epoch 16 of 500 took 0.278s\n",
      "Accuracy source 0.913574, main loss classifier 0.190892, source classification loss 0.245888, loss domain distinction 0.183202, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.22465444 Acc: 0.91703057\n",
      "New best validation loss:  0.2246544361114502\n",
      "Epoch 17 of 500 took 0.240s\n",
      "Accuracy source 0.919922, main loss classifier 0.183283, source classification loss 0.230531, loss domain distinction 0.183566, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.26847908 Acc: 0.88209607\n",
      "Epoch 18 of 500 took 0.235s\n",
      "Accuracy source 0.913574, main loss classifier 0.192309, source classification loss 0.248357, loss domain distinction 0.182342, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.24031897 Acc: 0.90975255\n",
      "Epoch 19 of 500 took 0.274s\n",
      "Accuracy source 0.920410, main loss classifier 0.183600, source classification loss 0.231490, loss domain distinction 0.182708, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.24571134 Acc: 0.90684134\n",
      "Epoch 20 of 500 took 0.247s\n",
      "Accuracy source 0.914551, main loss classifier 0.187710, source classification loss 0.239577, loss domain distinction 0.182620, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.24855290 Acc: 0.89956332\n",
      "Epoch 21 of 500 took 0.234s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy source 0.919922, main loss classifier 0.186812, source classification loss 0.237444, loss domain distinction 0.182024, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.26939195 Acc: 0.89810771\n",
      "Epoch 22 of 500 took 0.235s\n",
      "Accuracy source 0.923828, main loss classifier 0.182447, source classification loss 0.229423, loss domain distinction 0.184050, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.30199572 Acc: 0.8922853\n",
      "Epoch 23 of 500 took 0.234s\n",
      "Accuracy source 0.909180, main loss classifier 0.191037, source classification loss 0.246043, loss domain distinction 0.183423, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.24531004 Acc: 0.90538574\n",
      "Epoch 24 of 500 took 0.234s\n",
      "Accuracy source 0.918457, main loss classifier 0.181897, source classification loss 0.228031, loss domain distinction 0.183290, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.25577092 Acc: 0.89956332\n",
      "Epoch 25 of 500 took 0.236s\n",
      "Accuracy source 0.913574, main loss classifier 0.192613, source classification loss 0.248930, loss domain distinction 0.184349, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.26003760 Acc: 0.8922853\n",
      "Epoch 26 of 500 took 0.235s\n",
      "Accuracy source 0.920898, main loss classifier 0.179812, source classification loss 0.223946, loss domain distinction 0.183529, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.25198072 Acc: 0.89956332\n",
      "Epoch 27 of 500 took 0.233s\n",
      "Accuracy source 0.918945, main loss classifier 0.182173, source classification loss 0.228614, loss domain distinction 0.182770, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.27809033 Acc: 0.8937409\n",
      "Training complete in 0m 8s\n",
      "()\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/TSD/participant_0/best_state_0.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/TSD/participant_0/best_state_0.pt' (epoch 10)\n",
      "STARTING TRAINING\n",
      "Accuracy source 0.862793, main loss classifier 0.263187, source classification loss 0.360205, loss domain distinction 0.337309, accuracy domain distinction 0.489014\n",
      "VALIDATION Loss: 0.27895978 Acc: 0.90538574\n",
      "New best validation loss:  0.2789597809314728\n",
      "Epoch 1 of 500 took 0.240s\n",
      "Accuracy source 0.867188, main loss classifier 0.252996, source classification loss 0.359958, loss domain distinction 0.192162, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.31108105 Acc: 0.88209607\n",
      "Epoch 2 of 500 took 0.238s\n",
      "Accuracy source 0.889160, main loss classifier 0.231840, source classification loss 0.318159, loss domain distinction 0.190086, accuracy domain distinction 0.499756\n",
      "VALIDATION Loss: 0.40420407 Acc: 0.83988355\n",
      "Epoch 3 of 500 took 0.241s\n",
      "Accuracy source 0.880371, main loss classifier 0.240512, source classification loss 0.338314, loss domain distinction 0.188531, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.33735934 Acc: 0.86754003\n",
      "Epoch 4 of 500 took 0.280s\n",
      "Accuracy source 0.888672, main loss classifier 0.241494, source classification loss 0.340658, loss domain distinction 0.188178, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.35762095 Acc: 0.86171761\n",
      "Epoch 5 of 500 took 0.235s\n",
      "Accuracy source 0.889648, main loss classifier 0.231858, source classification loss 0.321408, loss domain distinction 0.188216, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.56800288 Acc: 0.80786026\n",
      "Epoch    16: reducing learning rate of group 0 to 5.0300e-04.\n",
      "Epoch 6 of 500 took 0.233s\n",
      "Accuracy source 0.896484, main loss classifier 0.219999, source classification loss 0.302808, loss domain distinction 0.184298, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.27599436 Acc: 0.89956332\n",
      "New best validation loss:  0.27599436044692993\n",
      "Epoch 7 of 500 took 0.277s\n",
      "Accuracy source 0.900391, main loss classifier 0.213573, source classification loss 0.288579, loss domain distinction 0.186528, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.28559077 Acc: 0.88791849\n",
      "Epoch 8 of 500 took 0.232s\n",
      "Accuracy source 0.895020, main loss classifier 0.219885, source classification loss 0.302411, loss domain distinction 0.183598, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.32664293 Acc: 0.89082969\n",
      "Epoch 9 of 500 took 0.235s\n",
      "Accuracy source 0.913086, main loss classifier 0.188779, source classification loss 0.240923, loss domain distinction 0.183575, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.38118246 Acc: 0.87627365\n",
      "Epoch 10 of 500 took 0.236s\n",
      "Accuracy source 0.902344, main loss classifier 0.198434, source classification loss 0.260820, loss domain distinction 0.181249, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.27976754 Acc: 0.90247453\n",
      "Epoch 11 of 500 took 0.233s\n",
      "Accuracy source 0.911621, main loss classifier 0.197005, source classification loss 0.256925, loss domain distinction 0.184818, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.32166386 Acc: 0.88209607\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0060e-04.\n",
      "Epoch 12 of 500 took 0.234s\n",
      "Accuracy source 0.918457, main loss classifier 0.187152, source classification loss 0.238528, loss domain distinction 0.183418, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.27913937 Acc: 0.88937409\n",
      "Epoch 13 of 500 took 0.234s\n",
      "Accuracy source 0.914551, main loss classifier 0.193225, source classification loss 0.248760, loss domain distinction 0.185022, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.28451347 Acc: 0.8937409\n",
      "Epoch 14 of 500 took 0.239s\n",
      "Accuracy source 0.905762, main loss classifier 0.197271, source classification loss 0.259389, loss domain distinction 0.180526, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.30518982 Acc: 0.8937409\n",
      "Epoch 15 of 500 took 0.233s\n",
      "Accuracy source 0.904297, main loss classifier 0.197004, source classification loss 0.257815, loss domain distinction 0.184620, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.29313466 Acc: 0.8937409\n",
      "Epoch 16 of 500 took 0.234s\n",
      "Accuracy source 0.907227, main loss classifier 0.197440, source classification loss 0.259528, loss domain distinction 0.182252, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.29513338 Acc: 0.90247453\n",
      "Epoch 17 of 500 took 0.234s\n",
      "Accuracy source 0.913574, main loss classifier 0.186419, source classification loss 0.237857, loss domain distinction 0.181002, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.31465694 Acc: 0.87336245\n",
      "Training complete in 0m 4s\n",
      "()\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/TSD/participant_0/best_state_0.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/TSD/participant_0/best_state_0.pt' (epoch 10)\n",
      "STARTING TRAINING\n",
      "Accuracy source 0.869141, main loss classifier 0.261184, source classification loss 0.357191, loss domain distinction 0.337976, accuracy domain distinction 0.488281\n",
      "VALIDATION Loss: 0.35597941 Acc: 0.87918486\n",
      "New best validation loss:  0.35597941279411316\n",
      "Epoch 1 of 500 took 0.235s\n",
      "Accuracy source 0.865723, main loss classifier 0.256986, source classification loss 0.366668, loss domain distinction 0.192794, accuracy domain distinction 0.499268\n",
      "VALIDATION Loss: 0.36212370 Acc: 0.86462882\n",
      "Epoch 2 of 500 took 0.237s\n",
      "Accuracy source 0.889648, main loss classifier 0.233409, source classification loss 0.323180, loss domain distinction 0.188255, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.47559053 Acc: 0.84861718\n",
      "Epoch 3 of 500 took 0.243s\n",
      "Accuracy source 0.877441, main loss classifier 0.240712, source classification loss 0.337748, loss domain distinction 0.188750, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.38228282 Acc: 0.85443959\n",
      "Epoch 4 of 500 took 0.236s\n",
      "Accuracy source 0.888184, main loss classifier 0.232287, source classification loss 0.322099, loss domain distinction 0.187355, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.44498870 Acc: 0.86171761\n",
      "Epoch 5 of 500 took 0.235s\n",
      "Accuracy source 0.875000, main loss classifier 0.240351, source classification loss 0.338638, loss domain distinction 0.186934, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.36897916 Acc: 0.86899563\n",
      "Epoch    16: reducing learning rate of group 0 to 5.0300e-04.\n",
      "Epoch 6 of 500 took 0.235s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy source 0.900879, main loss classifier 0.213484, source classification loss 0.290429, loss domain distinction 0.185253, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.38634521 Acc: 0.88064047\n",
      "Epoch 7 of 500 took 0.278s\n",
      "Accuracy source 0.907715, main loss classifier 0.200277, source classification loss 0.265026, loss domain distinction 0.180695, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.34472072 Acc: 0.88355167\n",
      "New best validation loss:  0.344720721244812\n",
      "Epoch 8 of 500 took 0.252s\n",
      "Accuracy source 0.903320, main loss classifier 0.206140, source classification loss 0.275308, loss domain distinction 0.182606, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.42704976 Acc: 0.86317322\n",
      "Epoch 9 of 500 took 0.236s\n",
      "Accuracy source 0.911621, main loss classifier 0.196938, source classification loss 0.257674, loss domain distinction 0.181819, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.44507423 Acc: 0.85152838\n",
      "Epoch 10 of 500 took 0.234s\n",
      "Accuracy source 0.899414, main loss classifier 0.201748, source classification loss 0.267542, loss domain distinction 0.182116, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.42176208 Acc: 0.86608443\n",
      "Epoch 11 of 500 took 0.235s\n",
      "Accuracy source 0.908203, main loss classifier 0.195702, source classification loss 0.255502, loss domain distinction 0.182533, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.46813387 Acc: 0.84716157\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0060e-04.\n",
      "Epoch 12 of 500 took 0.235s\n",
      "Accuracy source 0.917480, main loss classifier 0.185659, source classification loss 0.235936, loss domain distinction 0.181332, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.36975551 Acc: 0.87336245\n",
      "Epoch 13 of 500 took 0.252s\n",
      "Accuracy source 0.907715, main loss classifier 0.188270, source classification loss 0.241996, loss domain distinction 0.180920, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.36570913 Acc: 0.87772926\n",
      "Epoch 14 of 500 took 0.268s\n",
      "Accuracy source 0.906250, main loss classifier 0.192987, source classification loss 0.250168, loss domain distinction 0.182284, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.37012053 Acc: 0.87190684\n",
      "Epoch 15 of 500 took 0.234s\n",
      "Accuracy source 0.906250, main loss classifier 0.193393, source classification loss 0.252365, loss domain distinction 0.180643, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.49808809 Acc: 0.85007278\n",
      "Epoch 16 of 500 took 0.234s\n",
      "Accuracy source 0.912598, main loss classifier 0.193829, source classification loss 0.252018, loss domain distinction 0.181909, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.33541125 Acc: 0.87918486\n",
      "New best validation loss:  0.3354112505912781\n",
      "Epoch 17 of 500 took 0.236s\n",
      "Accuracy source 0.918457, main loss classifier 0.191483, source classification loss 0.247343, loss domain distinction 0.181717, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.36653334 Acc: 0.87336245\n",
      "Epoch 18 of 500 took 0.236s\n",
      "Accuracy source 0.911621, main loss classifier 0.187789, source classification loss 0.240285, loss domain distinction 0.180649, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.28839403 Acc: 0.90101892\n",
      "New best validation loss:  0.288394033908844\n",
      "Epoch 19 of 500 took 0.235s\n",
      "Accuracy source 0.912109, main loss classifier 0.189197, source classification loss 0.243503, loss domain distinction 0.181234, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.44650900 Acc: 0.85443959\n",
      "Epoch 20 of 500 took 0.263s\n",
      "Accuracy source 0.910156, main loss classifier 0.187976, source classification loss 0.241053, loss domain distinction 0.181141, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.33856249 Acc: 0.8922853\n",
      "Epoch 21 of 500 took 0.281s\n",
      "Accuracy source 0.911621, main loss classifier 0.187859, source classification loss 0.240510, loss domain distinction 0.181692, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.30402583 Acc: 0.88500728\n",
      "Epoch 22 of 500 took 0.252s\n",
      "Accuracy source 0.923828, main loss classifier 0.181867, source classification loss 0.228649, loss domain distinction 0.181920, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.44313779 Acc: 0.86171761\n",
      "Epoch 23 of 500 took 0.252s\n",
      "Accuracy source 0.910156, main loss classifier 0.190210, source classification loss 0.244599, loss domain distinction 0.181024, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.37376797 Acc: 0.88064047\n",
      "Epoch 24 of 500 took 0.247s\n",
      "Accuracy source 0.902344, main loss classifier 0.198115, source classification loss 0.261204, loss domain distinction 0.182697, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.43944806 Acc: 0.85152838\n",
      "Epoch 25 of 500 took 0.249s\n",
      "Accuracy source 0.916504, main loss classifier 0.192376, source classification loss 0.249731, loss domain distinction 0.181709, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.31713280 Acc: 0.89519651\n",
      "Epoch 26 of 500 took 0.238s\n",
      "Accuracy source 0.918945, main loss classifier 0.180956, source classification loss 0.226551, loss domain distinction 0.182375, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.35550150 Acc: 0.87481805\n",
      "Epoch 27 of 500 took 0.257s\n",
      "Accuracy source 0.920410, main loss classifier 0.183954, source classification loss 0.232548, loss domain distinction 0.181130, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.31621388 Acc: 0.89082969\n",
      "Epoch 28 of 500 took 0.279s\n",
      "Accuracy source 0.918945, main loss classifier 0.185903, source classification loss 0.237071, loss domain distinction 0.182021, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.34753686 Acc: 0.87336245\n",
      "Epoch 29 of 500 took 0.246s\n",
      "Accuracy source 0.922852, main loss classifier 0.179863, source classification loss 0.224596, loss domain distinction 0.181907, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.30113199 Acc: 0.8937409\n",
      "Training complete in 0m 7s\n",
      "()\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/TSD/participant_0/best_state_0.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/TSD/participant_0/best_state_0.pt' (epoch 10)\n",
      "STARTING TRAINING\n",
      "Accuracy source 0.885254, main loss classifier 0.244034, source classification loss 0.326024, loss domain distinction 0.331323, accuracy domain distinction 0.490967\n",
      "VALIDATION Loss: 0.52239430 Acc: 0.83260553\n",
      "New best validation loss:  0.5223942995071411\n",
      "Epoch 1 of 500 took 0.235s\n",
      "Accuracy source 0.877441, main loss classifier 0.255032, source classification loss 0.364095, loss domain distinction 0.191849, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.59101987 Acc: 0.80494905\n",
      "Epoch 2 of 500 took 0.235s\n",
      "Accuracy source 0.877930, main loss classifier 0.245420, source classification loss 0.346430, loss domain distinction 0.187544, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.46511939 Acc: 0.83842795\n",
      "New best validation loss:  0.4651193916797638\n",
      "Epoch 3 of 500 took 0.241s\n",
      "Accuracy source 0.890625, main loss classifier 0.222505, source classification loss 0.302656, loss domain distinction 0.186474, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.61683977 Acc: 0.81513828\n",
      "Epoch 4 of 500 took 0.234s\n",
      "Accuracy source 0.872070, main loss classifier 0.251148, source classification loss 0.360049, loss domain distinction 0.188053, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.48736763 Acc: 0.84425036\n",
      "Epoch 5 of 500 took 0.234s\n",
      "Accuracy source 0.879883, main loss classifier 0.237737, source classification loss 0.334562, loss domain distinction 0.185411, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.79583919 Acc: 0.79330422\n",
      "Epoch    16: reducing learning rate of group 0 to 5.0300e-04.\n",
      "Epoch 6 of 500 took 0.233s\n",
      "Accuracy source 0.886719, main loss classifier 0.219179, source classification loss 0.303323, loss domain distinction 0.181159, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.54585582 Acc: 0.82387191\n",
      "Epoch 7 of 500 took 0.235s\n",
      "Accuracy source 0.908203, main loss classifier 0.197699, source classification loss 0.258397, loss domain distinction 0.183413, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.43748087 Acc: 0.85443959\n",
      "New best validation loss:  0.4374808669090271\n",
      "Epoch 8 of 500 took 0.234s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy source 0.912109, main loss classifier 0.195021, source classification loss 0.253623, loss domain distinction 0.181886, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.69591123 Acc: 0.80494905\n",
      "Epoch 9 of 500 took 0.233s\n",
      "Accuracy source 0.914551, main loss classifier 0.191476, source classification loss 0.246525, loss domain distinction 0.182607, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.64037222 Acc: 0.81222707\n",
      "Epoch 10 of 500 took 0.232s\n",
      "Accuracy source 0.901855, main loss classifier 0.200258, source classification loss 0.263817, loss domain distinction 0.183982, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.68408507 Acc: 0.80349345\n",
      "Epoch 11 of 500 took 0.235s\n",
      "Accuracy source 0.917969, main loss classifier 0.191941, source classification loss 0.248142, loss domain distinction 0.180819, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.46111664 Acc: 0.85152838\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0060e-04.\n",
      "Epoch 12 of 500 took 0.234s\n",
      "Accuracy source 0.904785, main loss classifier 0.200053, source classification loss 0.264747, loss domain distinction 0.183200, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.57580072 Acc: 0.83551674\n",
      "Epoch 13 of 500 took 0.233s\n",
      "Accuracy source 0.914551, main loss classifier 0.195196, source classification loss 0.255674, loss domain distinction 0.180412, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.48023093 Acc: 0.84279476\n",
      "Epoch 14 of 500 took 0.239s\n",
      "Accuracy source 0.916992, main loss classifier 0.191784, source classification loss 0.248983, loss domain distinction 0.181468, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.62553060 Acc: 0.81659389\n",
      "Epoch 15 of 500 took 0.233s\n",
      "Accuracy source 0.897949, main loss classifier 0.200034, source classification loss 0.265684, loss domain distinction 0.180853, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.54128683 Acc: 0.82969432\n",
      "Epoch 16 of 500 took 0.239s\n",
      "Accuracy source 0.921875, main loss classifier 0.184917, source classification loss 0.234918, loss domain distinction 0.182043, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.51835859 Acc: 0.84425036\n",
      "Epoch 17 of 500 took 0.235s\n",
      "Accuracy source 0.920898, main loss classifier 0.184140, source classification loss 0.233398, loss domain distinction 0.181617, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.60101902 Acc: 0.83260553\n",
      "Epoch 18 of 500 took 0.233s\n",
      "Accuracy source 0.909668, main loss classifier 0.191557, source classification loss 0.248046, loss domain distinction 0.181682, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.52673608 Acc: 0.83551674\n",
      "Training complete in 0m 5s\n",
      "()\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/TSD/participant_0/best_state_0.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/TSD/participant_0/best_state_0.pt' (epoch 10)\n",
      "STARTING TRAINING\n",
      "Accuracy source 0.875488, main loss classifier 0.265042, source classification loss 0.365751, loss domain distinction 0.337511, accuracy domain distinction 0.489746\n",
      "VALIDATION Loss: 1.13746703 Acc: 0.6768559\n",
      "New best validation loss:  1.1374670267105103\n",
      "Epoch 1 of 500 took 0.238s\n",
      "Accuracy source 0.875488, main loss classifier 0.257256, source classification loss 0.368844, loss domain distinction 0.190444, accuracy domain distinction 0.500244\n",
      "VALIDATION Loss: 0.50715464 Acc: 0.83260553\n",
      "New best validation loss:  0.507154643535614\n",
      "Epoch 2 of 500 took 0.272s\n",
      "Accuracy source 0.885254, main loss classifier 0.236757, source classification loss 0.328950, loss domain distinction 0.188879, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.53705841 Acc: 0.8224163\n",
      "Epoch 3 of 500 took 0.259s\n",
      "Accuracy source 0.877441, main loss classifier 0.256130, source classification loss 0.370209, loss domain distinction 0.185997, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.32087618 Acc: 0.88355167\n",
      "New best validation loss:  0.32087618112564087\n",
      "Epoch 4 of 500 took 0.265s\n",
      "Accuracy source 0.889160, main loss classifier 0.235677, source classification loss 0.329053, loss domain distinction 0.186242, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.60366613 Acc: 0.80786026\n",
      "Epoch 5 of 500 took 0.276s\n",
      "Accuracy source 0.881348, main loss classifier 0.234602, source classification loss 0.328470, loss domain distinction 0.185436, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.42316750 Acc: 0.86026201\n",
      "Epoch    16: reducing learning rate of group 0 to 5.0300e-04.\n",
      "Epoch 6 of 500 took 0.247s\n",
      "Accuracy source 0.884277, main loss classifier 0.231506, source classification loss 0.326501, loss domain distinction 0.181671, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.48024100 Acc: 0.84133916\n",
      "Epoch 7 of 500 took 0.287s\n",
      "Accuracy source 0.898438, main loss classifier 0.206645, source classification loss 0.277791, loss domain distinction 0.179859, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.37291422 Acc: 0.86608443\n",
      "Epoch 8 of 500 took 0.235s\n",
      "Accuracy source 0.905273, main loss classifier 0.203150, source classification loss 0.270895, loss domain distinction 0.180595, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.43151328 Acc: 0.85152838\n",
      "Epoch 9 of 500 took 0.238s\n",
      "Accuracy source 0.911621, main loss classifier 0.206177, source classification loss 0.277454, loss domain distinction 0.179234, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.42907453 Acc: 0.86026201\n",
      "Epoch 10 of 500 took 0.234s\n",
      "Accuracy source 0.901367, main loss classifier 0.201774, source classification loss 0.268076, loss domain distinction 0.181308, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.33050671 Acc: 0.89082969\n",
      "Epoch 11 of 500 took 0.247s\n",
      "Accuracy source 0.915527, main loss classifier 0.187502, source classification loss 0.238704, loss domain distinction 0.181672, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.47019377 Acc: 0.85298399\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0060e-04.\n",
      "Epoch 12 of 500 took 0.234s\n",
      "Accuracy source 0.914062, main loss classifier 0.198314, source classification loss 0.261834, loss domain distinction 0.180235, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.52067333 Acc: 0.83551674\n",
      "Epoch 13 of 500 took 0.240s\n",
      "Accuracy source 0.905762, main loss classifier 0.199906, source classification loss 0.265113, loss domain distinction 0.181289, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.35657859 Acc: 0.88064047\n",
      "Epoch 14 of 500 took 0.247s\n",
      "Accuracy source 0.910156, main loss classifier 0.192740, source classification loss 0.251213, loss domain distinction 0.180050, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.44499058 Acc: 0.8573508\n",
      "Training complete in 0m 4s\n",
      "()\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/TSD/participant_0/best_state_0.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/TSD/participant_0/best_state_0.pt' (epoch 10)\n",
      "STARTING TRAINING\n",
      "Accuracy source 0.885254, main loss classifier 0.242353, source classification loss 0.321860, loss domain distinction 0.328874, accuracy domain distinction 0.489258\n",
      "VALIDATION Loss: 0.47220737 Acc: 0.82532751\n",
      "New best validation loss:  0.47220736742019653\n",
      "Epoch 1 of 500 took 0.238s\n",
      "Accuracy source 0.882324, main loss classifier 0.231486, source classification loss 0.316522, loss domain distinction 0.192209, accuracy domain distinction 0.500244\n",
      "VALIDATION Loss: 0.42420632 Acc: 0.85443959\n",
      "New best validation loss:  0.42420631647109985\n",
      "Epoch 2 of 500 took 0.246s\n",
      "Accuracy source 0.865234, main loss classifier 0.252904, source classification loss 0.361391, loss domain distinction 0.188323, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.53121740 Acc: 0.81222707\n",
      "Epoch 3 of 500 took 0.234s\n",
      "Accuracy source 0.865723, main loss classifier 0.254263, source classification loss 0.365229, loss domain distinction 0.187983, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.53736490 Acc: 0.81513828\n",
      "Epoch 4 of 500 took 0.232s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy source 0.880859, main loss classifier 0.245708, source classification loss 0.349462, loss domain distinction 0.187467, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.38185325 Acc: 0.85152838\n",
      "New best validation loss:  0.38185325264930725\n",
      "Epoch 5 of 500 took 0.235s\n",
      "Accuracy source 0.890137, main loss classifier 0.231133, source classification loss 0.321543, loss domain distinction 0.186466, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.39181498 Acc: 0.86608443\n",
      "Epoch    16: reducing learning rate of group 0 to 5.0300e-04.\n",
      "Epoch 6 of 500 took 0.249s\n",
      "Accuracy source 0.885254, main loss classifier 0.221879, source classification loss 0.306963, loss domain distinction 0.183776, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.36503983 Acc: 0.87918486\n",
      "New best validation loss:  0.3650398254394531\n",
      "Epoch 7 of 500 took 0.234s\n",
      "Accuracy source 0.904297, main loss classifier 0.199359, source classification loss 0.262886, loss domain distinction 0.181294, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.51580346 Acc: 0.82823872\n",
      "Epoch 8 of 500 took 0.233s\n",
      "Accuracy source 0.888672, main loss classifier 0.218318, source classification loss 0.300510, loss domain distinction 0.181647, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.38037193 Acc: 0.86754003\n",
      "Epoch 9 of 500 took 0.233s\n",
      "Accuracy source 0.908203, main loss classifier 0.202738, source classification loss 0.268787, loss domain distinction 0.183505, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.44572389 Acc: 0.85007278\n",
      "Epoch 10 of 500 took 0.234s\n",
      "Accuracy source 0.904785, main loss classifier 0.207311, source classification loss 0.278021, loss domain distinction 0.182403, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.47921753 Acc: 0.84425036\n",
      "Epoch 11 of 500 took 0.234s\n",
      "Accuracy source 0.903809, main loss classifier 0.198568, source classification loss 0.261250, loss domain distinction 0.181499, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.33435145 Acc: 0.88209607\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0060e-04.\n",
      "New best validation loss:  0.33435145020484924\n",
      "Epoch 12 of 500 took 0.235s\n",
      "Accuracy source 0.910645, main loss classifier 0.194495, source classification loss 0.253646, loss domain distinction 0.182657, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.36051258 Acc: 0.87918486\n",
      "Epoch 13 of 500 took 0.234s\n",
      "Accuracy source 0.915527, main loss classifier 0.189850, source classification loss 0.244531, loss domain distinction 0.181320, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.35670114 Acc: 0.87918486\n",
      "Epoch 14 of 500 took 0.233s\n",
      "Accuracy source 0.906250, main loss classifier 0.197613, source classification loss 0.260411, loss domain distinction 0.181021, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.50934947 Acc: 0.83551674\n",
      "Epoch 15 of 500 took 0.236s\n",
      "Accuracy source 0.916504, main loss classifier 0.184059, source classification loss 0.233700, loss domain distinction 0.180265, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.37996411 Acc: 0.86754003\n",
      "Epoch 16 of 500 took 0.232s\n",
      "Accuracy source 0.926758, main loss classifier 0.179414, source classification loss 0.224107, loss domain distinction 0.181288, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.46808076 Acc: 0.84716157\n",
      "Epoch 17 of 500 took 0.233s\n",
      "Accuracy source 0.910156, main loss classifier 0.197568, source classification loss 0.259836, loss domain distinction 0.180675, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.45514879 Acc: 0.8588064\n",
      "Epoch 18 of 500 took 0.233s\n",
      "Accuracy source 0.913574, main loss classifier 0.191570, source classification loss 0.248099, loss domain distinction 0.181967, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.41730434 Acc: 0.86026201\n",
      "Epoch 19 of 500 took 0.247s\n",
      "Accuracy source 0.913086, main loss classifier 0.192232, source classification loss 0.249694, loss domain distinction 0.181192, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.51028049 Acc: 0.83260553\n",
      "Epoch 20 of 500 took 0.234s\n",
      "Accuracy source 0.921875, main loss classifier 0.182863, source classification loss 0.230629, loss domain distinction 0.181909, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.38851959 Acc: 0.86462882\n",
      "Epoch 21 of 500 took 0.239s\n",
      "Accuracy source 0.913574, main loss classifier 0.188288, source classification loss 0.242011, loss domain distinction 0.179829, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.35957104 Acc: 0.88064047\n",
      "Epoch 22 of 500 took 0.233s\n",
      "Accuracy source 0.913086, main loss classifier 0.194040, source classification loss 0.254047, loss domain distinction 0.181694, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.42345873 Acc: 0.84861718\n",
      "Training complete in 0m 5s\n"
     ]
    }
   ],
   "source": [
    "# train_DANN(examples_datasets_train, labels_datasets_train, \n",
    "#           num_kernels=num_kernels,\n",
    "#           path_weights_fine_tuning=path_TSD,\n",
    "#           number_of_classes=number_of_classes,\n",
    "#           number_of_cycles_total = number_of_cycles_total,\n",
    "#           number_of_cycle_for_first_training = number_of_cycle_for_first_training,\n",
    "#           batch_size=batch_size,\n",
    "#           feature_vector_input_length=feature_vector_input_length,\n",
    "#           path_weights_to_save_to=path_DANN, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET one participant_examples  (8,)\n",
      "   GET one training_index_examples  (12, 572, 252)  at  0\n",
      "   GOT one group XY  (6864, 252)    (6864,)\n",
      "       one group XY test  (1716, 252)    (1716, 252)\n",
      "       one group XY train (6177, 252)    (6177,)\n",
      "       one group XY valid (687, 252)    (687, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  1\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  2\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  3\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  4\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  5\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  6\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  7\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "dataloaders: \n",
      "   train  (1, 8)\n",
      "   valid  (1, 8)\n",
      "   test  (1, 8)\n",
      "(8,)\n",
      "Participant ID:  0  Session ID:  0  Accuracy:  0.916083916083916\n",
      "Participant ID:  0  Session ID:  1  Accuracy:  0.7674825174825175\n",
      "Participant ID:  0  Session ID:  2  Accuracy:  0.7727272727272727\n",
      "Participant ID:  0  Session ID:  3  Accuracy:  0.5961538461538461\n",
      "Participant ID:  0  Session ID:  4  Accuracy:  0.6958041958041958\n",
      "Participant ID:  0  Session ID:  5  Accuracy:  0.5367132867132867\n",
      "Participant ID:  0  Session ID:  6  Accuracy:  0.548951048951049\n",
      "Participant ID:  0  Session ID:  7  Accuracy:  0.4755244755244755\n",
      "ACCURACY PARTICIPANT:  [0.916083916083916, 0.7674825174825175, 0.7727272727272727, 0.5961538461538461, 0.6958041958041958, 0.5367132867132867, 0.548951048951049, 0.4755244755244755]\n",
      "[[0.91608392 0.76748252 0.77272727 0.59615385 0.6958042  0.53671329\n",
      "  0.54895105 0.47552448]]\n",
      "[array([0.91608392, 0.76748252, 0.77272727, 0.59615385, 0.6958042 ,\n",
      "       0.53671329, 0.54895105, 0.47552448])]\n",
      "OVERALL ACCURACY: 0.66368006993007\n"
     ]
    }
   ],
   "source": [
    "algo_name = \"DANN\"\n",
    "test_DANN_on_training_sessions(examples_datasets_train, labels_datasets_train,\n",
    "                              feature_vector_input_length=feature_vector_input_length,\n",
    "                              num_neurons=num_kernels, path_weights_DA=path_DANN,\n",
    "                              algo_name=algo_name, save_path = save_DANN, \n",
    "                              number_of_cycles_total=number_of_cycles_total,\n",
    "                              number_of_cycle_for_first_training = number_of_cycle_for_first_training,\n",
    "                              path_weights_normal=path_TSD, number_of_classes=number_of_classes,\n",
    "                              cycle_for_test=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Day_0~2</th>\n",
       "      <td>0.916084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_3</th>\n",
       "      <td>0.767483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_4</th>\n",
       "      <td>0.772727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_5</th>\n",
       "      <td>0.596154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_6</th>\n",
       "      <td>0.695804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_7</th>\n",
       "      <td>0.536713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_8</th>\n",
       "      <td>0.548951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_9</th>\n",
       "      <td>0.475524</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Participant_5\n",
       "Day_0~2      0.916084\n",
       "Day_3        0.767483\n",
       "Day_4        0.772727\n",
       "Day_5        0.596154\n",
       "Day_6        0.695804\n",
       "Day_7        0.536713\n",
       "Day_8        0.548951\n",
       "Day_9        0.475524"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_filename = save_DANN + '/predictions_' + algo_name + \".npy\"\n",
    "results = np.load(results_filename, allow_pickle=True)\n",
    "DANN_acc = results[0]\n",
    "DANN_acc_overall = np.mean(DANN_acc)\n",
    "DANN_df = pd.DataFrame(DANN_acc.transpose(), \n",
    "                       index = [f'Day_{i}' for i in index_participant_list],\n",
    "                        columns = ['Participant_5'])\n",
    "DANN_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "DANN_df.transpose().plot.bar(rot=0, figsize=(10,5))\n",
    "plt.title(\"DANN Accuracies\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ground_truths  =  (1, 8)\n",
      "predictions =  (1, 8)\n",
      "index_participant_list  ['0~2', 3, 4, 5, 6, 7, 8, 9]\n",
      "accuracies_gestures =  (22, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Loc1_Sub5_Day0~2-&gt;0~2</th>\n",
       "      <th>Loc1_Sub5_Day0~2-&gt;3</th>\n",
       "      <th>Loc1_Sub5_Day0~2-&gt;4</th>\n",
       "      <th>Loc1_Sub5_Day0~2-&gt;5</th>\n",
       "      <th>Loc1_Sub5_Day0~2-&gt;6</th>\n",
       "      <th>Loc1_Sub5_Day0~2-&gt;7</th>\n",
       "      <th>Loc1_Sub5_Day0~2-&gt;8</th>\n",
       "      <th>Loc1_Sub5_Day0~2-&gt;9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M1</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.730769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M2</td>\n",
       "      <td>0.871795</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.192308</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.346154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M3</td>\n",
       "      <td>0.948718</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M4</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>M5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.576923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>M6</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>0.692308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>M7</td>\n",
       "      <td>0.987179</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.730769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>M8</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.615385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>M9</td>\n",
       "      <td>0.897436</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.346154</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.192308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>M10</td>\n",
       "      <td>0.910256</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.115385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>M11</td>\n",
       "      <td>0.935897</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.346154</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.230769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>M12</td>\n",
       "      <td>0.910256</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>M13</td>\n",
       "      <td>0.743590</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.807692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>M14</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>M15</td>\n",
       "      <td>0.641026</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.346154</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.230769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>M16</td>\n",
       "      <td>0.987179</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.961538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>M17</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>M18</td>\n",
       "      <td>0.987179</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.461538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>M19</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>0.653846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>M20</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.076923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>M21</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.615385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Mean</td>\n",
       "      <td>0.916084</td>\n",
       "      <td>0.767483</td>\n",
       "      <td>0.772727</td>\n",
       "      <td>0.596154</td>\n",
       "      <td>0.695804</td>\n",
       "      <td>0.536713</td>\n",
       "      <td>0.548951</td>\n",
       "      <td>0.475524</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Loc1_Sub5_Day0~2->0~2  Loc1_Sub5_Day0~2->3  \\\n",
       "0          M0               1.000000             1.000000   \n",
       "1          M1               0.961538             0.807692   \n",
       "2          M2               0.871795             0.538462   \n",
       "3          M3               0.948718             0.423077   \n",
       "4          M4               0.833333             0.000000   \n",
       "5          M5               1.000000             1.000000   \n",
       "6          M6               1.000000             0.423077   \n",
       "7          M7               0.987179             1.000000   \n",
       "8          M8               0.923077             1.000000   \n",
       "9          M9               0.897436             0.769231   \n",
       "10        M10               0.910256             0.423077   \n",
       "11        M11               0.935897             0.692308   \n",
       "12        M12               0.910256             0.884615   \n",
       "13        M13               0.743590             1.000000   \n",
       "14        M14               0.923077             0.538462   \n",
       "15        M15               0.641026             0.730769   \n",
       "16        M16               0.987179             1.000000   \n",
       "17        M17               1.000000             1.000000   \n",
       "18        M18               0.987179             1.000000   \n",
       "19        M19               1.000000             1.000000   \n",
       "20        M20               0.807692             0.961538   \n",
       "21        M21               0.884615             0.692308   \n",
       "22       Mean               0.916084             0.767483   \n",
       "\n",
       "    Loc1_Sub5_Day0~2->4  Loc1_Sub5_Day0~2->5  Loc1_Sub5_Day0~2->6  \\\n",
       "0              1.000000             1.000000             1.000000   \n",
       "1              0.923077             1.000000             0.846154   \n",
       "2              0.846154             0.384615             0.500000   \n",
       "3              0.653846             0.038462             0.000000   \n",
       "4              0.961538             0.000000             0.000000   \n",
       "5              0.615385             0.692308             0.923077   \n",
       "6              0.923077             0.576923             0.461538   \n",
       "7              1.000000             0.730769             0.923077   \n",
       "8              0.961538             1.000000             0.807692   \n",
       "9              0.615385             0.346154             0.730769   \n",
       "10             0.692308             0.769231             0.615385   \n",
       "11             0.307692             0.307692             0.807692   \n",
       "12             0.769231             0.576923             0.730769   \n",
       "13             0.846154             1.000000             1.000000   \n",
       "14             0.076923             0.000000             0.230769   \n",
       "15             0.346154             0.615385             0.423077   \n",
       "16             1.000000             0.576923             1.000000   \n",
       "17             0.846154             0.961538             1.000000   \n",
       "18             1.000000             0.076923             0.769231   \n",
       "19             1.000000             0.769231             1.000000   \n",
       "20             0.923077             0.769231             0.576923   \n",
       "21             0.692308             0.923077             0.961538   \n",
       "22             0.772727             0.596154             0.695804   \n",
       "\n",
       "    Loc1_Sub5_Day0~2->7  Loc1_Sub5_Day0~2->8  Loc1_Sub5_Day0~2->9  \n",
       "0              1.000000             1.000000             1.000000  \n",
       "1              0.615385             0.730769             0.730769  \n",
       "2              0.192308             0.000000             0.346154  \n",
       "3              0.000000             0.000000             0.000000  \n",
       "4              0.000000             0.000000             0.000000  \n",
       "5              0.307692             0.846154             0.576923  \n",
       "6              0.461538             0.269231             0.692308  \n",
       "7              0.769231             0.807692             0.730769  \n",
       "8              1.000000             0.961538             0.615385  \n",
       "9              0.038462             0.576923             0.192308  \n",
       "10             0.923077             0.576923             0.115385  \n",
       "11             0.346154             0.384615             0.230769  \n",
       "12             0.730769             0.653846             0.500000  \n",
       "13             1.000000             1.000000             0.807692  \n",
       "14             0.000000             0.000000             0.000000  \n",
       "15             0.000000             0.000000             0.230769  \n",
       "16             0.923077             1.000000             0.961538  \n",
       "17             1.000000             0.846154             0.923077  \n",
       "18             0.884615             0.423077             0.461538  \n",
       "19             0.307692             0.653846             0.653846  \n",
       "20             0.807692             0.384615             0.076923  \n",
       "21             0.500000             0.961538             0.615385  \n",
       "22             0.536713             0.548951             0.475524  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truths = results[1]\n",
    "predictions = results[2]\n",
    "print(\"ground_truths  = \", np.shape(ground_truths))\n",
    "print(\"predictions = \", np.shape(predictions))\n",
    "df = get_gesture_accuracies(ground_truths, predictions, number_of_classes=number_of_classes, \n",
    "                            m_name=m_name, n_name=n_name, path=save_TSD, algo_name=algo_name,\n",
    "                           index_participant_list_customized=index_participant_list,\n",
    "                           lump_day_at_participant=5)\n",
    "df = pd.read_csv(save_DANN+'/'+algo_name+'.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. SCADANN\n",
    "\n",
    "* `run_SCADANN_training_sessions`: train SCADANN model. The first session uses TSD model_0 wegits; others use DANN weights\n",
    "    * specify `percentage_same_gesture_stable` based on the performance of most pseudo labels: \n",
    "        * print accuracies out and check what percentage will optimize `ACCURACY MODEL` and `ACCURACY PSEUDO` without cutting out too much data \n",
    "    * num_sessions-1 sets of training weights will be saved\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TrainingsAndEvaluations.ForTrainingSessions.train_tsd_dnn_SCADANN import \\\n",
    "    run_SCADANN_training_sessions, test_network_SCADANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET one participant_examples  (8,)\n",
      "   GET one training_index_examples  (12, 572, 252)  at  0\n",
      "   GOT one group XY  (6864, 252)    (6864,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (6864, 252)    (6864,)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  1\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2288, 252)    (2288,)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  2\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2288, 252)    (2288,)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  3\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2288, 252)    (2288,)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  4\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2288, 252)    (2288,)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  5\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2288, 252)    (2288,)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  6\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2288, 252)    (2288,)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  7\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2288, 252)    (2288,)\n",
      "dataloaders: \n",
      "   train  (1, 8)\n",
      "   valid  (0,)\n",
      "   test  (1, 0)\n",
      "participants_train =  1\n",
      "Optimizer =  <generator object Module.parameters at 0x7facc9889eb0>\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/DANN/participant_0/best_state_1.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/DANN/participant_0/best_state_1.pt' (epoch 23)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/TSD/participant_0/best_state_0.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/TSD/participant_0/best_state_0.pt' (epoch 10)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/DANN/participant_0/best_state_1.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/DANN/participant_0/best_state_1.pt' (epoch 23)\n",
      "==== models_array =  (2,)  @ session  1\n",
      "HANDLING NEW SESSION  0\n",
      "HANDLING NEW SESSION  1\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.6538461538461539  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  1\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  5\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.5   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.0  len before:  26   len after:  2\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  16\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.5555555555555556  len before:  26   len after:  9\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.5384615384615384  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.0  len before:  26   len after:  5\n",
      "BEFORE:  0.0   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  1.0  len before:  26   len after:  7\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.3333333333333333  len before:  26   len after:  15\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.6923076923076923  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.8098776223776224   Accuracy pseudo: 0.9407927747114903  len pseudo:  1993    len predictions 2288\n",
      "STARTING TRAINING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laiy/.local/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/laiy/.local/lib/python3.8/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy total 0.875977, main loss classifier 0.497155, source accuracy 0.916667 source classification loss 0.241856, target accuracy 0.835286 target loss 0.525237 accuracy domain distinction 0.500000 loss domain distinction 1.136086,\n",
      "VALIDATION Loss: 0.28911076 Acc: 0.91478697\n",
      "New best validation loss:  0.28911075794271063\n",
      "Epoch 2 of 500 took 0.329s\n",
      "Accuracy total 0.884115, main loss classifier 0.440483, source accuracy 0.914062 source classification loss 0.249852, target accuracy 0.854167 target loss 0.417600 accuracy domain distinction 0.500000 loss domain distinction 1.067576,\n",
      "VALIDATION Loss: 0.33220528 Acc: 0.90977444\n",
      "Epoch 3 of 500 took 0.314s\n",
      "Accuracy total 0.888021, main loss classifier 0.425458, source accuracy 0.903646 source classification loss 0.266874, target accuracy 0.872396 target loss 0.375112 accuracy domain distinction 0.500000 loss domain distinction 1.044651,\n",
      "VALIDATION Loss: 0.32067026 Acc: 0.9122807\n",
      "Epoch 4 of 500 took 0.318s\n",
      "Accuracy total 0.890951, main loss classifier 0.429278, source accuracy 0.906901 source classification loss 0.274540, target accuracy 0.875000 target loss 0.375745 accuracy domain distinction 0.500000 loss domain distinction 1.041354,\n",
      "VALIDATION Loss: 0.26179738 Acc: 0.91729323\n",
      "New best validation loss:  0.2617973791701453\n",
      "Epoch 5 of 500 took 0.321s\n",
      "Accuracy total 0.898438, main loss classifier 0.392637, source accuracy 0.910156 source classification loss 0.256041, target accuracy 0.886719 target loss 0.322410 accuracy domain distinction 0.500000 loss domain distinction 1.034118,\n",
      "VALIDATION Loss: 0.34270753 Acc: 0.89724311\n",
      "Epoch 6 of 500 took 0.320s\n",
      "Accuracy total 0.891276, main loss classifier 0.418147, source accuracy 0.912760 source classification loss 0.256919, target accuracy 0.869792 target loss 0.372021 accuracy domain distinction 0.500000 loss domain distinction 1.036779,\n",
      "VALIDATION Loss: 0.29290809 Acc: 0.89473684\n",
      "Epoch 7 of 500 took 0.315s\n",
      "Accuracy total 0.904297, main loss classifier 0.407421, source accuracy 0.910807 source classification loss 0.284258, target accuracy 0.897786 target loss 0.323512 accuracy domain distinction 0.500000 loss domain distinction 1.035364,\n",
      "VALIDATION Loss: 0.28599183 Acc: 0.90977444\n",
      "Epoch 8 of 500 took 0.317s\n",
      "Accuracy total 0.900065, main loss classifier 0.403513, source accuracy 0.908854 source classification loss 0.269686, target accuracy 0.891276 target loss 0.329354 accuracy domain distinction 0.500000 loss domain distinction 1.039926,\n",
      "VALIDATION Loss: 0.27633290 Acc: 0.9122807\n",
      "Epoch 9 of 500 took 0.324s\n",
      "Accuracy total 0.897461, main loss classifier 0.397104, source accuracy 0.906250 source classification loss 0.266495, target accuracy 0.888672 target loss 0.320504 accuracy domain distinction 0.500000 loss domain distinction 1.036045,\n",
      "VALIDATION Loss: 0.23050803 Acc: 0.92481203\n",
      "New best validation loss:  0.23050802733216966\n",
      "Epoch 10 of 500 took 0.318s\n",
      "Accuracy total 0.900065, main loss classifier 0.393769, source accuracy 0.902344 source classification loss 0.253396, target accuracy 0.897786 target loss 0.326604 accuracy domain distinction 0.500000 loss domain distinction 1.037688,\n",
      "VALIDATION Loss: 0.27812622 Acc: 0.89724311\n",
      "Epoch 11 of 500 took 0.315s\n",
      "Accuracy total 0.897461, main loss classifier 0.387246, source accuracy 0.906901 source classification loss 0.250487, target accuracy 0.888021 target loss 0.317220 accuracy domain distinction 0.500000 loss domain distinction 1.033921,\n",
      "VALIDATION Loss: 0.28464043 Acc: 0.92230576\n",
      "Epoch 12 of 500 took 0.324s\n",
      "Accuracy total 0.914714, main loss classifier 0.361319, source accuracy 0.926432 source classification loss 0.224179, target accuracy 0.902995 target loss 0.291221 accuracy domain distinction 0.500000 loss domain distinction 1.036190,\n",
      "VALIDATION Loss: 0.28533489 Acc: 0.9122807\n",
      "Epoch 13 of 500 took 0.320s\n",
      "Accuracy total 0.896484, main loss classifier 0.394963, source accuracy 0.900391 source classification loss 0.262325, target accuracy 0.892578 target loss 0.320043 accuracy domain distinction 0.500000 loss domain distinction 1.037796,\n",
      "VALIDATION Loss: 0.22327311 Acc: 0.92481203\n",
      "New best validation loss:  0.2232731091124671\n",
      "Epoch 14 of 500 took 0.316s\n",
      "Accuracy total 0.902018, main loss classifier 0.406088, source accuracy 0.916016 source classification loss 0.275358, target accuracy 0.888021 target loss 0.329257 accuracy domain distinction 0.500000 loss domain distinction 1.037808,\n",
      "VALIDATION Loss: 0.25966586 Acc: 0.91478697\n",
      "Epoch 15 of 500 took 0.321s\n",
      "Accuracy total 0.901367, main loss classifier 0.389157, source accuracy 0.910807 source classification loss 0.258955, target accuracy 0.891927 target loss 0.312769 accuracy domain distinction 0.500000 loss domain distinction 1.032948,\n",
      "VALIDATION Loss: 0.23489544 Acc: 0.92230576\n",
      "Epoch 16 of 500 took 0.315s\n",
      "Accuracy total 0.905599, main loss classifier 0.386857, source accuracy 0.916667 source classification loss 0.245377, target accuracy 0.894531 target loss 0.320811 accuracy domain distinction 0.500000 loss domain distinction 1.037628,\n",
      "VALIDATION Loss: 0.24721296 Acc: 0.9273183\n",
      "Epoch 17 of 500 took 0.315s\n",
      "Accuracy total 0.905273, main loss classifier 0.380978, source accuracy 0.910807 source classification loss 0.262633, target accuracy 0.899740 target loss 0.291999 accuracy domain distinction 0.500000 loss domain distinction 1.036617,\n",
      "VALIDATION Loss: 0.19221006 Acc: 0.93483709\n",
      "New best validation loss:  0.19221005961298943\n",
      "Epoch 18 of 500 took 0.322s\n",
      "Accuracy total 0.905599, main loss classifier 0.382412, source accuracy 0.910156 source classification loss 0.257660, target accuracy 0.901042 target loss 0.300031 accuracy domain distinction 0.500000 loss domain distinction 1.035664,\n",
      "VALIDATION Loss: 0.22025532 Acc: 0.9273183\n",
      "Epoch 19 of 500 took 0.317s\n",
      "Accuracy total 0.900065, main loss classifier 0.399646, source accuracy 0.901693 source classification loss 0.287527, target accuracy 0.898438 target loss 0.304646 accuracy domain distinction 0.500000 loss domain distinction 1.035594,\n",
      "VALIDATION Loss: 0.21020909 Acc: 0.92481203\n",
      "Epoch 20 of 500 took 0.317s\n",
      "Accuracy total 0.904948, main loss classifier 0.366180, source accuracy 0.908854 source classification loss 0.248971, target accuracy 0.901042 target loss 0.276282 accuracy domain distinction 0.500000 loss domain distinction 1.035533,\n",
      "VALIDATION Loss: 0.21135312 Acc: 0.92481203\n",
      "Epoch 21 of 500 took 0.320s\n",
      "Accuracy total 0.908854, main loss classifier 0.368127, source accuracy 0.908854 source classification loss 0.255556, target accuracy 0.908854 target loss 0.272893 accuracy domain distinction 0.500000 loss domain distinction 1.039025,\n",
      "VALIDATION Loss: 0.27622190 Acc: 0.89473684\n",
      "Epoch 22 of 500 took 0.318s\n",
      "Accuracy total 0.911458, main loss classifier 0.368622, source accuracy 0.919271 source classification loss 0.243730, target accuracy 0.903646 target loss 0.286415 accuracy domain distinction 0.500000 loss domain distinction 1.035501,\n",
      "VALIDATION Loss: 0.23445036 Acc: 0.91478697\n",
      "Epoch 23 of 500 took 0.320s\n",
      "Accuracy total 0.907552, main loss classifier 0.384464, source accuracy 0.910807 source classification loss 0.265018, target accuracy 0.904297 target loss 0.297369 accuracy domain distinction 0.500000 loss domain distinction 1.032707,\n",
      "VALIDATION Loss: 0.19472555 Acc: 0.9273183\n",
      "Epoch    23: reducing learning rate of group 0 to 2.0120e-05.\n",
      "Epoch 24 of 500 took 0.319s\n",
      "Accuracy total 0.913086, main loss classifier 0.364259, source accuracy 0.923828 source classification loss 0.232219, target accuracy 0.902344 target loss 0.289248 accuracy domain distinction 0.500000 loss domain distinction 1.035258,\n",
      "VALIDATION Loss: 0.22781702 Acc: 0.92982456\n",
      "Epoch 25 of 500 took 0.320s\n",
      "Accuracy total 0.909831, main loss classifier 0.374765, source accuracy 0.911458 source classification loss 0.261643, target accuracy 0.908203 target loss 0.281152 accuracy domain distinction 0.500000 loss domain distinction 1.033680,\n",
      "VALIDATION Loss: 0.21107974 Acc: 0.9197995\n",
      "Epoch 26 of 500 took 0.314s\n",
      "Accuracy total 0.912760, main loss classifier 0.357665, source accuracy 0.927734 source classification loss 0.225380, target accuracy 0.897786 target loss 0.283691 accuracy domain distinction 0.500000 loss domain distinction 1.031297,\n",
      "VALIDATION Loss: 0.27592285 Acc: 0.9273183\n",
      "Epoch 27 of 500 took 0.318s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy total 0.916016, main loss classifier 0.357803, source accuracy 0.911458 source classification loss 0.273458, target accuracy 0.920573 target loss 0.235845 accuracy domain distinction 0.500000 loss domain distinction 1.031521,\n",
      "VALIDATION Loss: 0.24165157 Acc: 0.90977444\n",
      "Epoch 28 of 500 took 0.404s\n",
      "Accuracy total 0.912435, main loss classifier 0.370423, source accuracy 0.911458 source classification loss 0.254882, target accuracy 0.913411 target loss 0.279962 accuracy domain distinction 0.500000 loss domain distinction 1.030018,\n",
      "VALIDATION Loss: 0.19044457 Acc: 0.92982456\n",
      "New best validation loss:  0.1904445689703737\n",
      "Epoch 29 of 500 took 0.318s\n",
      "Accuracy total 0.909831, main loss classifier 0.368859, source accuracy 0.907552 source classification loss 0.265976, target accuracy 0.912109 target loss 0.265179 accuracy domain distinction 0.500000 loss domain distinction 1.032817,\n",
      "VALIDATION Loss: 0.22388798 Acc: 0.9122807\n",
      "Epoch 30 of 500 took 0.317s\n",
      "Accuracy total 0.902344, main loss classifier 0.388392, source accuracy 0.901693 source classification loss 0.281780, target accuracy 0.902995 target loss 0.287925 accuracy domain distinction 0.500000 loss domain distinction 1.035394,\n",
      "VALIDATION Loss: 0.28834881 Acc: 0.90726817\n",
      "Epoch 31 of 500 took 0.320s\n",
      "Accuracy total 0.913086, main loss classifier 0.355687, source accuracy 0.923177 source classification loss 0.220164, target accuracy 0.902995 target loss 0.284316 accuracy domain distinction 0.500000 loss domain distinction 1.034465,\n",
      "VALIDATION Loss: 0.21080343 Acc: 0.93483709\n",
      "Epoch 32 of 500 took 0.315s\n",
      "Accuracy total 0.913086, main loss classifier 0.379565, source accuracy 0.914062 source classification loss 0.262338, target accuracy 0.912109 target loss 0.290697 accuracy domain distinction 0.500000 loss domain distinction 1.030476,\n",
      "VALIDATION Loss: 0.23448786 Acc: 0.9273183\n",
      "Epoch 33 of 500 took 0.317s\n",
      "Accuracy total 0.908529, main loss classifier 0.374144, source accuracy 0.908854 source classification loss 0.266259, target accuracy 0.908203 target loss 0.276251 accuracy domain distinction 0.500000 loss domain distinction 1.028890,\n",
      "VALIDATION Loss: 0.20165630 Acc: 0.92982456\n",
      "Epoch 34 of 500 took 0.340s\n",
      "Accuracy total 0.914062, main loss classifier 0.352604, source accuracy 0.909505 source classification loss 0.244037, target accuracy 0.918620 target loss 0.254788 accuracy domain distinction 0.500000 loss domain distinction 1.031913,\n",
      "VALIDATION Loss: 0.21577931 Acc: 0.93233083\n",
      "Epoch    34: reducing learning rate of group 0 to 4.0240e-06.\n",
      "Epoch 35 of 500 took 0.314s\n",
      "Accuracy total 0.909180, main loss classifier 0.365974, source accuracy 0.907552 source classification loss 0.261571, target accuracy 0.910807 target loss 0.264209 accuracy domain distinction 0.500000 loss domain distinction 1.030839,\n",
      "VALIDATION Loss: 0.35757986 Acc: 0.91478697\n",
      "Epoch 36 of 500 took 0.315s\n",
      "Accuracy total 0.913737, main loss classifier 0.375129, source accuracy 0.903646 source classification loss 0.289641, target accuracy 0.923828 target loss 0.253834 accuracy domain distinction 0.500000 loss domain distinction 1.033916,\n",
      "VALIDATION Loss: 0.20570231 Acc: 0.93233083\n",
      "Epoch 37 of 500 took 0.322s\n",
      "Accuracy total 0.913086, main loss classifier 0.371775, source accuracy 0.914062 source classification loss 0.266514, target accuracy 0.912109 target loss 0.269726 accuracy domain distinction 0.500000 loss domain distinction 1.036550,\n",
      "VALIDATION Loss: 0.19083787 Acc: 0.92481203\n",
      "Epoch 38 of 500 took 0.315s\n",
      "Accuracy total 0.914062, main loss classifier 0.351469, source accuracy 0.915365 source classification loss 0.243426, target accuracy 0.912760 target loss 0.253189 accuracy domain distinction 0.500000 loss domain distinction 1.031621,\n",
      "VALIDATION Loss: 0.27023398 Acc: 0.92481203\n",
      "Epoch 39 of 500 took 0.324s\n",
      "Accuracy total 0.912760, main loss classifier 0.361536, source accuracy 0.908854 source classification loss 0.267288, target accuracy 0.916667 target loss 0.249414 accuracy domain distinction 0.500000 loss domain distinction 1.031856,\n",
      "VALIDATION Loss: 0.23337559 Acc: 0.9273183\n",
      "Epoch 40 of 500 took 0.327s\n",
      "Training complete in 0m 13s\n",
      "['participant_0']\n",
      "Optimizer =  <generator object Module.parameters at 0x7facc4a4bcf0>\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/DANN/participant_0/best_state_2.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/DANN/participant_0/best_state_2.pt' (epoch 17)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/TSD/participant_0/best_state_0.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/TSD/participant_0/best_state_0.pt' (epoch 10)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/DANN/participant_0/best_state_1.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/DANN/participant_0/best_state_1.pt' (epoch 23)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/DANN/participant_0/best_state_2.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/DANN/participant_0/best_state_2.pt' (epoch 17)\n",
      "==== models_array =  (3,)  @ session  2\n",
      "HANDLING NEW SESSION  0\n",
      "HANDLING NEW SESSION  1\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.6538461538461539  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  1\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  5\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.5   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.0  len before:  26   len after:  2\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  16\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.5555555555555556  len before:  26   len after:  9\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.5384615384615384  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.0  len before:  26   len after:  5\n",
      "BEFORE:  0.0   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  1.0  len before:  26   len after:  7\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.3333333333333333  len before:  26   len after:  15\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.6923076923076923  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.8098776223776224   Accuracy pseudo: 0.9407927747114903  len pseudo:  1993    len predictions 2288\n",
      "HANDLING NEW SESSION  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.6538461538461539  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  5\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  8\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.4230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.0  len before:  26   len after:  7\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  13\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.0  len before:  26   len after:  9\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.0  len before:  26   len after:  6\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  1\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.6923076923076923  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  14\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  16\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.9  len before:  26   len after:  10\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  5\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.8846153846153846  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  15\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.5769230769230769  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.0  len before:  26   len after:  18\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.75  len before:  26   len after:  8\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  14\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.7666083916083916   Accuracy pseudo: 0.8792585170340681  len pseudo:  1996    len predictions 2288\n",
      "STARTING TRAINING\n",
      "Accuracy total 0.863281, main loss classifier 0.552411, source accuracy 0.888672 source classification loss 0.357826, target accuracy 0.837891 target loss 0.518537 accuracy domain distinction 0.500000 loss domain distinction 1.142295,\n",
      "VALIDATION Loss: 0.26718089 Acc: 0.9175\n",
      "New best validation loss:  0.26718089410236906\n",
      "Epoch 2 of 500 took 0.390s\n",
      "Accuracy total 0.876953, main loss classifier 0.493287, source accuracy 0.884766 source classification loss 0.349119, target accuracy 0.869141 target loss 0.426621 accuracy domain distinction 0.500000 loss domain distinction 1.054173,\n",
      "VALIDATION Loss: 0.19622384 Acc: 0.95\n",
      "New best validation loss:  0.1962238433105605\n",
      "Epoch 3 of 500 took 0.395s\n",
      "Accuracy total 0.879232, main loss classifier 0.492993, source accuracy 0.894531 source classification loss 0.340934, target accuracy 0.863932 target loss 0.434166 accuracy domain distinction 0.500000 loss domain distinction 1.054431,\n",
      "VALIDATION Loss: 0.22845208 Acc: 0.9425\n",
      "Epoch 4 of 500 took 0.397s\n",
      "Accuracy total 0.879883, main loss classifier 0.467669, source accuracy 0.887370 source classification loss 0.322652, target accuracy 0.872396 target loss 0.404177 accuracy domain distinction 0.500000 loss domain distinction 1.042537,\n",
      "VALIDATION Loss: 0.20266494 Acc: 0.9375\n",
      "Epoch 5 of 500 took 0.399s\n",
      "Accuracy total 0.880859, main loss classifier 0.455234, source accuracy 0.894531 source classification loss 0.314068, target accuracy 0.867188 target loss 0.389051 accuracy domain distinction 0.500000 loss domain distinction 1.036739,\n",
      "VALIDATION Loss: 0.19503200 Acc: 0.94\n",
      "New best validation loss:  0.19503200479916163\n",
      "Epoch 6 of 500 took 0.401s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy total 0.872070, main loss classifier 0.486121, source accuracy 0.886068 source classification loss 0.346757, target accuracy 0.858073 target loss 0.417656 accuracy domain distinction 0.500000 loss domain distinction 1.039139,\n",
      "VALIDATION Loss: 0.21594267 Acc: 0.95\n",
      "Epoch 7 of 500 took 0.353s\n",
      "Accuracy total 0.881836, main loss classifier 0.448027, source accuracy 0.885417 source classification loss 0.311780, target accuracy 0.878255 target loss 0.377031 accuracy domain distinction 0.500000 loss domain distinction 1.036210,\n",
      "VALIDATION Loss: 0.20218140 Acc: 0.9425\n",
      "Epoch 8 of 500 took 0.321s\n",
      "Accuracy total 0.885417, main loss classifier 0.445777, source accuracy 0.880859 source classification loss 0.330696, target accuracy 0.889974 target loss 0.353015 accuracy domain distinction 0.500000 loss domain distinction 1.039217,\n",
      "VALIDATION Loss: 0.21203234 Acc: 0.9425\n",
      "Epoch 9 of 500 took 0.314s\n",
      "Accuracy total 0.885742, main loss classifier 0.449204, source accuracy 0.890625 source classification loss 0.329299, target accuracy 0.880859 target loss 0.361254 accuracy domain distinction 0.500000 loss domain distinction 1.039279,\n",
      "VALIDATION Loss: 0.18377446 Acc: 0.945\n",
      "New best validation loss:  0.18377445957490376\n",
      "Epoch 10 of 500 took 0.316s\n",
      "Accuracy total 0.880859, main loss classifier 0.443757, source accuracy 0.890625 source classification loss 0.312213, target accuracy 0.871094 target loss 0.367879 accuracy domain distinction 0.500000 loss domain distinction 1.037108,\n",
      "VALIDATION Loss: 0.16819372 Acc: 0.9425\n",
      "New best validation loss:  0.16819372081330844\n",
      "Epoch 11 of 500 took 0.322s\n",
      "Accuracy total 0.902995, main loss classifier 0.395627, source accuracy 0.911458 source classification loss 0.276155, target accuracy 0.894531 target loss 0.307351 accuracy domain distinction 0.500000 loss domain distinction 1.038742,\n",
      "VALIDATION Loss: 0.16060393 Acc: 0.955\n",
      "New best validation loss:  0.160603934100696\n",
      "Epoch 12 of 500 took 0.316s\n",
      "Accuracy total 0.890625, main loss classifier 0.432928, source accuracy 0.894531 source classification loss 0.339634, target accuracy 0.886719 target loss 0.317427 accuracy domain distinction 0.500000 loss domain distinction 1.043975,\n",
      "VALIDATION Loss: 0.15851843 Acc: 0.95\n",
      "New best validation loss:  0.15851842984557152\n",
      "Epoch 13 of 500 took 0.318s\n",
      "Accuracy total 0.890299, main loss classifier 0.424078, source accuracy 0.890625 source classification loss 0.314701, target accuracy 0.889974 target loss 0.326045 accuracy domain distinction 0.500000 loss domain distinction 1.037053,\n",
      "VALIDATION Loss: 0.14474202 Acc: 0.955\n",
      "New best validation loss:  0.14474201521703176\n",
      "Epoch 14 of 500 took 0.320s\n",
      "Accuracy total 0.886393, main loss classifier 0.434749, source accuracy 0.885417 source classification loss 0.327023, target accuracy 0.887370 target loss 0.335438 accuracy domain distinction 0.500000 loss domain distinction 1.035191,\n",
      "VALIDATION Loss: 0.14933670 Acc: 0.9575\n",
      "Epoch 15 of 500 took 0.319s\n",
      "Accuracy total 0.893880, main loss classifier 0.408827, source accuracy 0.891276 source classification loss 0.302138, target accuracy 0.896484 target loss 0.308741 accuracy domain distinction 0.500000 loss domain distinction 1.033876,\n",
      "VALIDATION Loss: 0.25982735 Acc: 0.9425\n",
      "Epoch 16 of 500 took 0.316s\n",
      "Accuracy total 0.897461, main loss classifier 0.414607, source accuracy 0.886068 source classification loss 0.325514, target accuracy 0.908854 target loss 0.296452 accuracy domain distinction 0.500000 loss domain distinction 1.036242,\n",
      "VALIDATION Loss: 0.23427306 Acc: 0.9475\n",
      "Epoch 17 of 500 took 0.314s\n",
      "Accuracy total 0.894531, main loss classifier 0.410655, source accuracy 0.895182 source classification loss 0.294127, target accuracy 0.893880 target loss 0.319255 accuracy domain distinction 0.500000 loss domain distinction 1.039637,\n",
      "VALIDATION Loss: 0.13829572 Acc: 0.955\n",
      "New best validation loss:  0.13829572445579938\n",
      "Epoch 18 of 500 took 0.355s\n",
      "Accuracy total 0.891602, main loss classifier 0.402505, source accuracy 0.888021 source classification loss 0.298135, target accuracy 0.895182 target loss 0.300539 accuracy domain distinction 0.500000 loss domain distinction 1.031684,\n",
      "VALIDATION Loss: 0.15070184 Acc: 0.9475\n",
      "Epoch 19 of 500 took 0.314s\n",
      "Accuracy total 0.897135, main loss classifier 0.410358, source accuracy 0.897135 source classification loss 0.303238, target accuracy 0.897135 target loss 0.309501 accuracy domain distinction 0.500000 loss domain distinction 1.039888,\n",
      "VALIDATION Loss: 0.18363721 Acc: 0.955\n",
      "Epoch 20 of 500 took 0.316s\n",
      "Accuracy total 0.894531, main loss classifier 0.423581, source accuracy 0.889974 source classification loss 0.326520, target accuracy 0.899089 target loss 0.314021 accuracy domain distinction 0.500000 loss domain distinction 1.033110,\n",
      "VALIDATION Loss: 0.20964117 Acc: 0.9575\n",
      "Epoch 21 of 500 took 0.319s\n",
      "Accuracy total 0.897461, main loss classifier 0.422523, source accuracy 0.890625 source classification loss 0.338812, target accuracy 0.904297 target loss 0.299207 accuracy domain distinction 0.500000 loss domain distinction 1.035138,\n",
      "VALIDATION Loss: 0.19246577 Acc: 0.955\n",
      "Epoch 22 of 500 took 0.315s\n",
      "Accuracy total 0.898438, main loss classifier 0.394546, source accuracy 0.897135 source classification loss 0.289172, target accuracy 0.899740 target loss 0.293749 accuracy domain distinction 0.500000 loss domain distinction 1.030852,\n",
      "VALIDATION Loss: 0.17933770 Acc: 0.9425\n",
      "Epoch 23 of 500 took 0.315s\n",
      "Accuracy total 0.892904, main loss classifier 0.410670, source accuracy 0.876953 source classification loss 0.353855, target accuracy 0.908854 target loss 0.260855 accuracy domain distinction 0.500000 loss domain distinction 1.033154,\n",
      "VALIDATION Loss: 0.15115394 Acc: 0.9575\n",
      "Epoch    23: reducing learning rate of group 0 to 2.0120e-05.\n",
      "Epoch 24 of 500 took 0.318s\n",
      "Accuracy total 0.898112, main loss classifier 0.392978, source accuracy 0.895833 source classification loss 0.284257, target accuracy 0.900391 target loss 0.295182 accuracy domain distinction 0.500000 loss domain distinction 1.032584,\n",
      "VALIDATION Loss: 0.15071508 Acc: 0.95\n",
      "Epoch 25 of 500 took 0.314s\n",
      "Accuracy total 0.892578, main loss classifier 0.406224, source accuracy 0.895833 source classification loss 0.295608, target accuracy 0.889323 target loss 0.310173 accuracy domain distinction 0.500000 loss domain distinction 1.033335,\n",
      "VALIDATION Loss: 0.15986051 Acc: 0.95\n",
      "Epoch 26 of 500 took 0.313s\n",
      "Accuracy total 0.894531, main loss classifier 0.413666, source accuracy 0.886068 source classification loss 0.342000, target accuracy 0.902995 target loss 0.278003 accuracy domain distinction 0.500000 loss domain distinction 1.036651,\n",
      "VALIDATION Loss: 0.18512678 Acc: 0.9325\n",
      "Epoch 27 of 500 took 0.319s\n",
      "Accuracy total 0.898438, main loss classifier 0.403825, source accuracy 0.895182 source classification loss 0.302268, target accuracy 0.901693 target loss 0.299190 accuracy domain distinction 0.500000 loss domain distinction 1.030955,\n",
      "VALIDATION Loss: 0.18950337 Acc: 0.95\n",
      "Epoch 28 of 500 took 0.316s\n",
      "Accuracy total 0.891927, main loss classifier 0.406762, source accuracy 0.880208 source classification loss 0.327054, target accuracy 0.903646 target loss 0.279754 accuracy domain distinction 0.500000 loss domain distinction 1.033574,\n",
      "VALIDATION Loss: 0.21860889 Acc: 0.945\n",
      "Epoch 29 of 500 took 0.315s\n",
      "Training complete in 0m 9s\n",
      "['participant_0']\n",
      "Optimizer =  <generator object Module.parameters at 0x7facc4a50eb0>\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/DANN/participant_0/best_state_3.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/DANN/participant_0/best_state_3.pt' (epoch 7)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/TSD/participant_0/best_state_0.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/TSD/participant_0/best_state_0.pt' (epoch 10)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/DANN/participant_0/best_state_1.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/DANN/participant_0/best_state_1.pt' (epoch 23)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/DANN/participant_0/best_state_2.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/DANN/participant_0/best_state_2.pt' (epoch 17)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/DANN/participant_0/best_state_3.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/DANN/participant_0/best_state_3.pt' (epoch 7)\n",
      "==== models_array =  (4,)  @ session  3\n",
      "HANDLING NEW SESSION  0\n",
      "HANDLING NEW SESSION  1\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.6538461538461539  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  1\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  5\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.5   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.0  len before:  26   len after:  2\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  16\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.5555555555555556  len before:  26   len after:  9\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.5384615384615384  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.0  len before:  26   len after:  5\n",
      "BEFORE:  0.0   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  1.0  len before:  26   len after:  7\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.3333333333333333  len before:  26   len after:  15\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.6923076923076923  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.8098776223776224   Accuracy pseudo: 0.9407927747114903  len pseudo:  1993    len predictions 2288\n",
      "HANDLING NEW SESSION  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.6538461538461539  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  5\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  8\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.4230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.0  len before:  26   len after:  7\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  13\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.0  len before:  26   len after:  9\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.0  len before:  26   len after:  6\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  1\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.6923076923076923  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  14\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  16\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.9  len before:  26   len after:  10\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  5\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.8846153846153846  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  15\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.5769230769230769  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.0  len before:  26   len after:  18\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.75  len before:  26   len after:  8\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  14\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.7666083916083916   Accuracy pseudo: 0.8792585170340681  len pseudo:  1996    len predictions 2288\n",
      "HANDLING NEW SESSION  3\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.0  len before:  26   len after:  3\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.6538461538461539  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.5769230769230769  len before:  26   len after:  26\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.0  len before:  26   len after:  7\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  8\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  11\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  15\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.0  len before:  26   len after:  9\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.11538461538461539   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  7\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.8076923076923077  len before:  26   len after:  26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.5555555555555556  len before:  26   len after:  18\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  6\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  7\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  16\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.625  len before:  26   len after:  16\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.46153846153846156  len before:  26   len after:  26\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  6\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.9  len before:  26   len after:  10\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.4230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.4230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.11764705882352941  len before:  26   len after:  17\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.5769230769230769  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  7\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.6289335664335665   Accuracy pseudo: 0.7052845528455285  len pseudo:  1968    len predictions 2288\n",
      "STARTING TRAINING\n",
      "Accuracy total 0.822591, main loss classifier 0.675818, source accuracy 0.858724 source classification loss 0.435277, target accuracy 0.786458 target loss 0.699197 accuracy domain distinction 0.500326 loss domain distinction 1.085809,\n",
      "VALIDATION Loss: 0.41394824 Acc: 0.86548223\n",
      "New best validation loss:  0.4139482378959656\n",
      "Epoch 2 of 500 took 0.333s\n",
      "Accuracy total 0.838542, main loss classifier 0.580440, source accuracy 0.850911 source classification loss 0.424855, target accuracy 0.826172 target loss 0.526077 accuracy domain distinction 0.500000 loss domain distinction 1.049742,\n",
      "VALIDATION Loss: 0.28917962 Acc: 0.90101523\n",
      "New best validation loss:  0.28917962312698364\n",
      "Epoch 3 of 500 took 0.316s\n",
      "Accuracy total 0.862956, main loss classifier 0.511985, source accuracy 0.878906 source classification loss 0.356887, target accuracy 0.847005 target loss 0.455796 accuracy domain distinction 0.500000 loss domain distinction 1.056435,\n",
      "VALIDATION Loss: 0.23557143 Acc: 0.9035533\n",
      "New best validation loss:  0.23557143339088985\n",
      "Epoch 4 of 500 took 0.336s\n",
      "Accuracy total 0.847982, main loss classifier 0.524418, source accuracy 0.860677 source classification loss 0.385436, target accuracy 0.835286 target loss 0.452857 accuracy domain distinction 0.500000 loss domain distinction 1.052714,\n",
      "VALIDATION Loss: 0.28809060 Acc: 0.8857868\n",
      "Epoch 5 of 500 took 0.318s\n",
      "Accuracy total 0.861003, main loss classifier 0.516435, source accuracy 0.864583 source classification loss 0.376606, target accuracy 0.857422 target loss 0.447450 accuracy domain distinction 0.500000 loss domain distinction 1.044076,\n",
      "VALIDATION Loss: 0.21668018 Acc: 0.91370558\n",
      "New best validation loss:  0.21668017549174173\n",
      "Epoch 6 of 500 took 0.318s\n",
      "Accuracy total 0.863607, main loss classifier 0.492449, source accuracy 0.869792 source classification loss 0.367250, target accuracy 0.857422 target loss 0.407782 accuracy domain distinction 0.500000 loss domain distinction 1.049333,\n",
      "VALIDATION Loss: 0.23537218 Acc: 0.91878173\n",
      "Epoch 7 of 500 took 0.316s\n",
      "Accuracy total 0.862630, main loss classifier 0.485887, source accuracy 0.873047 source classification loss 0.362831, target accuracy 0.852214 target loss 0.399304 accuracy domain distinction 0.500000 loss domain distinction 1.048192,\n",
      "VALIDATION Loss: 0.21326097 Acc: 0.90609137\n",
      "New best validation loss:  0.21326096994536264\n",
      "Epoch 8 of 500 took 0.317s\n",
      "Accuracy total 0.862956, main loss classifier 0.488605, source accuracy 0.867188 source classification loss 0.365308, target accuracy 0.858724 target loss 0.401897 accuracy domain distinction 0.500000 loss domain distinction 1.050024,\n",
      "VALIDATION Loss: 0.21568381 Acc: 0.91624365\n",
      "Epoch 9 of 500 took 0.314s\n",
      "Accuracy total 0.867839, main loss classifier 0.486772, source accuracy 0.880208 source classification loss 0.361931, target accuracy 0.855469 target loss 0.403066 accuracy domain distinction 0.500000 loss domain distinction 1.042740,\n",
      "VALIDATION Loss: 0.18535725 Acc: 0.92385787\n",
      "New best validation loss:  0.18535724708012172\n",
      "Epoch 10 of 500 took 0.322s\n",
      "Accuracy total 0.879232, main loss classifier 0.456650, source accuracy 0.878906 source classification loss 0.369149, target accuracy 0.879557 target loss 0.335284 accuracy domain distinction 0.500000 loss domain distinction 1.044335,\n",
      "VALIDATION Loss: 0.27760439 Acc: 0.87817259\n",
      "Epoch 11 of 500 took 0.316s\n",
      "Accuracy total 0.865234, main loss classifier 0.478772, source accuracy 0.875000 source classification loss 0.366846, target accuracy 0.855469 target loss 0.382027 accuracy domain distinction 0.500000 loss domain distinction 1.043350,\n",
      "VALIDATION Loss: 0.18992496 Acc: 0.93401015\n",
      "Epoch 12 of 500 took 0.317s\n",
      "Accuracy total 0.878255, main loss classifier 0.460985, source accuracy 0.875000 source classification loss 0.366747, target accuracy 0.881510 target loss 0.346497 accuracy domain distinction 0.500000 loss domain distinction 1.043631,\n",
      "VALIDATION Loss: 0.17966791 Acc: 0.93654822\n",
      "New best validation loss:  0.17966791135924204\n",
      "Epoch 13 of 500 took 0.323s\n",
      "Accuracy total 0.880208, main loss classifier 0.447249, source accuracy 0.881510 source classification loss 0.351441, target accuracy 0.878906 target loss 0.334376 accuracy domain distinction 0.500000 loss domain distinction 1.043402,\n",
      "VALIDATION Loss: 0.24504093 Acc: 0.88832487\n",
      "Epoch 14 of 500 took 0.316s\n",
      "Accuracy total 0.872070, main loss classifier 0.456287, source accuracy 0.863281 source classification loss 0.374244, target accuracy 0.880859 target loss 0.329659 accuracy domain distinction 0.500000 loss domain distinction 1.043353,\n",
      "VALIDATION Loss: 0.26964624 Acc: 0.89086294\n",
      "Epoch 15 of 500 took 0.315s\n",
      "Accuracy total 0.880534, main loss classifier 0.441981, source accuracy 0.880208 source classification loss 0.343989, target accuracy 0.880859 target loss 0.331648 accuracy domain distinction 0.500000 loss domain distinction 1.041621,\n",
      "VALIDATION Loss: 0.22433559 Acc: 0.9213198\n",
      "Epoch 16 of 500 took 0.316s\n",
      "Accuracy total 0.891276, main loss classifier 0.432498, source accuracy 0.903646 source classification loss 0.300034, target accuracy 0.878906 target loss 0.356353 accuracy domain distinction 0.500000 loss domain distinction 1.043044,\n",
      "VALIDATION Loss: 0.25355110 Acc: 0.91878173\n",
      "Epoch 17 of 500 took 0.320s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy total 0.880208, main loss classifier 0.437932, source accuracy 0.878906 source classification loss 0.335210, target accuracy 0.881510 target loss 0.331918 accuracy domain distinction 0.500000 loss domain distinction 1.043676,\n",
      "VALIDATION Loss: 0.24720393 Acc: 0.88832487\n",
      "Epoch 18 of 500 took 0.319s\n",
      "Accuracy total 0.891602, main loss classifier 0.418097, source accuracy 0.889323 source classification loss 0.333491, target accuracy 0.893880 target loss 0.294271 accuracy domain distinction 0.500000 loss domain distinction 1.042155,\n",
      "VALIDATION Loss: 0.20563929 Acc: 0.92893401\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0060e-04.\n",
      "Epoch 19 of 500 took 0.315s\n",
      "Accuracy total 0.882812, main loss classifier 0.434845, source accuracy 0.882812 source classification loss 0.336965, target accuracy 0.882812 target loss 0.326439 accuracy domain distinction 0.500000 loss domain distinction 1.031428,\n",
      "VALIDATION Loss: 0.15913741 Acc: 0.94162437\n",
      "New best validation loss:  0.15913741290569305\n",
      "Epoch 20 of 500 took 0.324s\n",
      "Accuracy total 0.889648, main loss classifier 0.424443, source accuracy 0.889974 source classification loss 0.326998, target accuracy 0.889323 target loss 0.314853 accuracy domain distinction 0.500000 loss domain distinction 1.035173,\n",
      "VALIDATION Loss: 0.25632993 Acc: 0.90101523\n",
      "Epoch 21 of 500 took 0.316s\n",
      "Accuracy total 0.894531, main loss classifier 0.397825, source accuracy 0.887370 source classification loss 0.325040, target accuracy 0.901693 target loss 0.265146 accuracy domain distinction 0.500000 loss domain distinction 1.027319,\n",
      "VALIDATION Loss: 0.23767022 Acc: 0.91116751\n",
      "Epoch 22 of 500 took 0.315s\n",
      "Accuracy total 0.891276, main loss classifier 0.404026, source accuracy 0.897786 source classification loss 0.289039, target accuracy 0.884766 target loss 0.312700 accuracy domain distinction 0.500000 loss domain distinction 1.031562,\n",
      "VALIDATION Loss: 0.18683233 Acc: 0.91116751\n",
      "Epoch 23 of 500 took 0.317s\n",
      "Accuracy total 0.891602, main loss classifier 0.414921, source accuracy 0.887370 source classification loss 0.331206, target accuracy 0.895833 target loss 0.292068 accuracy domain distinction 0.500000 loss domain distinction 1.032843,\n",
      "VALIDATION Loss: 0.18535830 Acc: 0.92639594\n",
      "Epoch 24 of 500 took 0.313s\n",
      "Accuracy total 0.893229, main loss classifier 0.397073, source accuracy 0.894531 source classification loss 0.300382, target accuracy 0.891927 target loss 0.287437 accuracy domain distinction 0.500000 loss domain distinction 1.031634,\n",
      "VALIDATION Loss: 0.16791419 Acc: 0.93908629\n",
      "Epoch 25 of 500 took 0.314s\n",
      "Accuracy total 0.897786, main loss classifier 0.391644, source accuracy 0.899740 source classification loss 0.288641, target accuracy 0.895833 target loss 0.288755 accuracy domain distinction 0.500000 loss domain distinction 1.029463,\n",
      "VALIDATION Loss: 0.25335715 Acc: 0.89847716\n",
      "Epoch    25: reducing learning rate of group 0 to 2.0120e-05.\n",
      "Epoch 26 of 500 took 0.319s\n",
      "Accuracy total 0.904948, main loss classifier 0.387726, source accuracy 0.896484 source classification loss 0.313410, target accuracy 0.913411 target loss 0.255537 accuracy domain distinction 0.500000 loss domain distinction 1.032525,\n",
      "VALIDATION Loss: 0.24230775 Acc: 0.91878173\n",
      "Epoch 27 of 500 took 0.314s\n",
      "Accuracy total 0.895182, main loss classifier 0.389221, source accuracy 0.887370 source classification loss 0.305946, target accuracy 0.902995 target loss 0.266230 accuracy domain distinction 0.500000 loss domain distinction 1.031334,\n",
      "VALIDATION Loss: 0.19726168 Acc: 0.93147208\n",
      "Epoch 28 of 500 took 0.317s\n",
      "Accuracy total 0.896810, main loss classifier 0.400575, source accuracy 0.897135 source classification loss 0.312649, target accuracy 0.896484 target loss 0.281861 accuracy domain distinction 0.500000 loss domain distinction 1.033197,\n",
      "VALIDATION Loss: 0.25091881 Acc: 0.91624365\n",
      "Epoch 29 of 500 took 0.316s\n",
      "Accuracy total 0.900716, main loss classifier 0.401396, source accuracy 0.897135 source classification loss 0.304637, target accuracy 0.904297 target loss 0.290803 accuracy domain distinction 0.500000 loss domain distinction 1.036757,\n",
      "VALIDATION Loss: 0.16605707 Acc: 0.94162437\n",
      "Epoch 30 of 500 took 0.313s\n",
      "Accuracy total 0.882812, main loss classifier 0.420256, source accuracy 0.884115 source classification loss 0.322407, target accuracy 0.881510 target loss 0.310493 accuracy domain distinction 0.500000 loss domain distinction 1.038057,\n",
      "VALIDATION Loss: 0.17945238 Acc: 0.91878173\n",
      "Epoch 31 of 500 took 0.317s\n",
      "Training complete in 0m 10s\n",
      "['participant_0']\n",
      "Optimizer =  <generator object Module.parameters at 0x7facc4a4bcf0>\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/DANN/participant_0/best_state_4.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/DANN/participant_0/best_state_4.pt' (epoch 19)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/TSD/participant_0/best_state_0.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/TSD/participant_0/best_state_0.pt' (epoch 10)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/DANN/participant_0/best_state_1.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/DANN/participant_0/best_state_1.pt' (epoch 23)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/DANN/participant_0/best_state_2.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/DANN/participant_0/best_state_2.pt' (epoch 17)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/DANN/participant_0/best_state_3.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/DANN/participant_0/best_state_3.pt' (epoch 7)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/DANN/participant_0/best_state_4.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/DANN/participant_0/best_state_4.pt' (epoch 19)\n",
      "==== models_array =  (5,)  @ session  4\n",
      "HANDLING NEW SESSION  0\n",
      "HANDLING NEW SESSION  1\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.6538461538461539  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  1\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  5\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.5   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.0  len before:  26   len after:  2\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  16\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.5555555555555556  len before:  26   len after:  9\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.5384615384615384  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.0  len before:  26   len after:  5\n",
      "BEFORE:  0.0   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  1.0  len before:  26   len after:  7\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.3333333333333333  len before:  26   len after:  15\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.6923076923076923  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.8098776223776224   Accuracy pseudo: 0.9407927747114903  len pseudo:  1993    len predictions 2288\n",
      "HANDLING NEW SESSION  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.6538461538461539  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  5\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  8\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.4230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.0  len before:  26   len after:  7\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  13\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.0  len before:  26   len after:  9\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.0  len before:  26   len after:  6\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  1\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.6923076923076923  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  14\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  16\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.9  len before:  26   len after:  10\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  5\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.8846153846153846  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  15\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.5769230769230769  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.0  len before:  26   len after:  18\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.75  len before:  26   len after:  8\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  14\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.7666083916083916   Accuracy pseudo: 0.8792585170340681  len pseudo:  1996    len predictions 2288\n",
      "HANDLING NEW SESSION  3\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.0  len before:  26   len after:  3\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.6538461538461539  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.5769230769230769  len before:  26   len after:  26\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.0  len before:  26   len after:  7\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  8\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  11\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  15\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.0  len before:  26   len after:  9\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.11538461538461539   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  7\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.5555555555555556  len before:  26   len after:  18\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  6\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  7\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  16\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.625  len before:  26   len after:  16\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.46153846153846156  len before:  26   len after:  26\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  6\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.9  len before:  26   len after:  10\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.4230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.4230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.11764705882352941  len before:  26   len after:  17\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.5769230769230769  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  7\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.6289335664335665   Accuracy pseudo: 0.7052845528455285  len pseudo:  1968    len predictions 2288\n",
      "HANDLING NEW SESSION  4\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  7\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  6\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.34615384615384615   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.46153846153846156  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  14\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  10\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.038461538461538464   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  11\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.6923076923076923  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.4230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.5  len before:  26   len after:  26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  16\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  18\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  1.0  len before:  26   len after:  4\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  14\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  17\n",
      "ACCURACY MODEL:  0.6940559440559441   Accuracy pseudo: 0.8220380302291566  len pseudo:  2051    len predictions 2288\n",
      "STARTING TRAINING\n",
      "Accuracy total 0.829688, main loss classifier 0.646091, source accuracy 0.855000 source classification loss 0.444378, target accuracy 0.804375 target loss 0.621745 accuracy domain distinction 0.500000 loss domain distinction 1.130293,\n",
      "VALIDATION Loss: 0.32595506 Acc: 0.87591241\n",
      "New best validation loss:  0.3259550631046295\n",
      "Epoch 2 of 500 took 0.339s\n",
      "Accuracy total 0.866875, main loss classifier 0.520275, source accuracy 0.878125 source classification loss 0.377743, target accuracy 0.855625 target loss 0.450463 accuracy domain distinction 0.500000 loss domain distinction 1.061725,\n",
      "VALIDATION Loss: 0.18899937 Acc: 0.93917275\n",
      "New best validation loss:  0.18899936654738017\n",
      "Epoch 3 of 500 took 0.341s\n",
      "Accuracy total 0.858437, main loss classifier 0.509164, source accuracy 0.871250 source classification loss 0.363822, target accuracy 0.845625 target loss 0.446212 accuracy domain distinction 0.500000 loss domain distinction 1.041470,\n",
      "VALIDATION Loss: 0.26071871 Acc: 0.90997567\n",
      "Epoch 4 of 500 took 0.330s\n",
      "Accuracy total 0.859062, main loss classifier 0.515632, source accuracy 0.860000 source classification loss 0.401745, target accuracy 0.858125 target loss 0.421190 accuracy domain distinction 0.500000 loss domain distinction 1.041644,\n",
      "VALIDATION Loss: 0.20441259 Acc: 0.92457421\n",
      "Epoch 5 of 500 took 0.331s\n",
      "Accuracy total 0.869062, main loss classifier 0.505945, source accuracy 0.870000 source classification loss 0.393174, target accuracy 0.868125 target loss 0.411901 accuracy domain distinction 0.500000 loss domain distinction 1.034077,\n",
      "VALIDATION Loss: 0.17309882 Acc: 0.94160584\n",
      "New best validation loss:  0.1730988206607955\n",
      "Epoch 6 of 500 took 0.334s\n",
      "Accuracy total 0.870625, main loss classifier 0.496064, source accuracy 0.881875 source classification loss 0.364711, target accuracy 0.859375 target loss 0.421197 accuracy domain distinction 0.500000 loss domain distinction 1.031097,\n",
      "VALIDATION Loss: 0.17627304 Acc: 0.95377129\n",
      "Epoch 7 of 500 took 0.329s\n",
      "Accuracy total 0.871563, main loss classifier 0.488221, source accuracy 0.877500 source classification loss 0.365536, target accuracy 0.865625 target loss 0.403418 accuracy domain distinction 0.500000 loss domain distinction 1.037441,\n",
      "VALIDATION Loss: 0.16957560 Acc: 0.93673966\n",
      "New best validation loss:  0.16957560288054602\n",
      "Epoch 8 of 500 took 0.330s\n",
      "Accuracy total 0.870938, main loss classifier 0.505318, source accuracy 0.869375 source classification loss 0.410699, target accuracy 0.872500 target loss 0.392279 accuracy domain distinction 0.500000 loss domain distinction 1.038296,\n",
      "VALIDATION Loss: 0.16999845 Acc: 0.94160584\n",
      "Epoch 9 of 500 took 0.330s\n",
      "Accuracy total 0.872500, main loss classifier 0.474141, source accuracy 0.879375 source classification loss 0.350640, target accuracy 0.865625 target loss 0.391819 accuracy domain distinction 0.500000 loss domain distinction 1.029118,\n",
      "VALIDATION Loss: 0.17669700 Acc: 0.94647202\n",
      "Epoch 10 of 500 took 0.329s\n",
      "Accuracy total 0.878125, main loss classifier 0.473031, source accuracy 0.881250 source classification loss 0.369762, target accuracy 0.875000 target loss 0.368960 accuracy domain distinction 0.500000 loss domain distinction 1.036700,\n",
      "VALIDATION Loss: 0.15728079 Acc: 0.95620438\n",
      "New best validation loss:  0.15728079101869039\n",
      "Epoch 11 of 500 took 0.331s\n",
      "Accuracy total 0.874375, main loss classifier 0.485777, source accuracy 0.866875 source classification loss 0.382339, target accuracy 0.881875 target loss 0.382062 accuracy domain distinction 0.500000 loss domain distinction 1.035758,\n",
      "VALIDATION Loss: 0.15648752 Acc: 0.93917275\n",
      "New best validation loss:  0.15648751812321798\n",
      "Epoch 12 of 500 took 0.330s\n",
      "Accuracy total 0.871250, main loss classifier 0.481124, source accuracy 0.865625 source classification loss 0.375742, target accuracy 0.876875 target loss 0.379695 accuracy domain distinction 0.500000 loss domain distinction 1.034054,\n",
      "VALIDATION Loss: 0.12536546 Acc: 0.95620438\n",
      "New best validation loss:  0.1253654576305832\n",
      "Epoch 13 of 500 took 0.332s\n",
      "Accuracy total 0.879375, main loss classifier 0.468413, source accuracy 0.885625 source classification loss 0.341143, target accuracy 0.873125 target loss 0.388181 accuracy domain distinction 0.500000 loss domain distinction 1.037513,\n",
      "VALIDATION Loss: 0.27048725 Acc: 0.88807786\n",
      "Epoch 14 of 500 took 0.330s\n",
      "Accuracy total 0.871563, main loss classifier 0.475329, source accuracy 0.868750 source classification loss 0.369122, target accuracy 0.874375 target loss 0.375622 accuracy domain distinction 0.500000 loss domain distinction 1.029565,\n",
      "VALIDATION Loss: 0.16505278 Acc: 0.94647202\n",
      "Epoch 15 of 500 took 0.326s\n",
      "Accuracy total 0.878125, main loss classifier 0.494433, source accuracy 0.870625 source classification loss 0.425460, target accuracy 0.885625 target loss 0.357482 accuracy domain distinction 0.500000 loss domain distinction 1.029617,\n",
      "VALIDATION Loss: 0.18952753 Acc: 0.94160584\n",
      "Epoch 16 of 500 took 0.331s\n",
      "Accuracy total 0.888437, main loss classifier 0.458404, source accuracy 0.883750 source classification loss 0.352975, target accuracy 0.893125 target loss 0.356934 accuracy domain distinction 0.500000 loss domain distinction 1.034494,\n",
      "VALIDATION Loss: 0.16080220 Acc: 0.95377129\n",
      "Epoch 17 of 500 took 0.331s\n",
      "Accuracy total 0.874687, main loss classifier 0.467098, source accuracy 0.872500 source classification loss 0.368861, target accuracy 0.876875 target loss 0.357812 accuracy domain distinction 0.500000 loss domain distinction 1.037618,\n",
      "VALIDATION Loss: 0.13258121 Acc: 0.96836983\n",
      "Epoch 18 of 500 took 0.334s\n",
      "Accuracy total 0.879062, main loss classifier 0.463314, source accuracy 0.876875 source classification loss 0.345867, target accuracy 0.881250 target loss 0.374649 accuracy domain distinction 0.500000 loss domain distinction 1.030563,\n",
      "VALIDATION Loss: 0.15191091 Acc: 0.95377129\n",
      "Epoch    18: reducing learning rate of group 0 to 2.0120e-05.\n",
      "Epoch 19 of 500 took 0.331s\n",
      "Accuracy total 0.879375, main loss classifier 0.474544, source accuracy 0.873750 source classification loss 0.403165, target accuracy 0.885000 target loss 0.340847 accuracy domain distinction 0.500000 loss domain distinction 1.025380,\n",
      "VALIDATION Loss: 0.14004896 Acc: 0.95863747\n",
      "Epoch 20 of 500 took 0.359s\n",
      "Accuracy total 0.884375, main loss classifier 0.434534, source accuracy 0.887500 source classification loss 0.322886, target accuracy 0.881250 target loss 0.340447 accuracy domain distinction 0.500000 loss domain distinction 1.028669,\n",
      "VALIDATION Loss: 0.18157830 Acc: 0.93673966\n",
      "Epoch 21 of 500 took 0.339s\n",
      "Accuracy total 0.893437, main loss classifier 0.436499, source accuracy 0.900625 source classification loss 0.311829, target accuracy 0.886250 target loss 0.354721 accuracy domain distinction 0.500000 loss domain distinction 1.032238,\n",
      "VALIDATION Loss: 0.17735019 Acc: 0.93917275\n",
      "Epoch 22 of 500 took 0.359s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy total 0.880000, main loss classifier 0.459466, source accuracy 0.875000 source classification loss 0.361882, target accuracy 0.885000 target loss 0.350423 accuracy domain distinction 0.500000 loss domain distinction 1.033133,\n",
      "VALIDATION Loss: 0.20745029 Acc: 0.94160584\n",
      "Epoch 23 of 500 took 0.329s\n",
      "Accuracy total 0.887500, main loss classifier 0.437701, source accuracy 0.894375 source classification loss 0.326355, target accuracy 0.880625 target loss 0.343219 accuracy domain distinction 0.500000 loss domain distinction 1.029146,\n",
      "VALIDATION Loss: 0.16463831 Acc: 0.95377129\n",
      "Epoch 24 of 500 took 0.341s\n",
      "Training complete in 0m 8s\n",
      "['participant_0']\n",
      "Optimizer =  <generator object Module.parameters at 0x7facc55f49e0>\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/DANN/participant_0/best_state_5.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/DANN/participant_0/best_state_5.pt' (epoch 8)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/TSD/participant_0/best_state_0.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/TSD/participant_0/best_state_0.pt' (epoch 10)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/DANN/participant_0/best_state_1.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/DANN/participant_0/best_state_1.pt' (epoch 23)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/DANN/participant_0/best_state_2.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/DANN/participant_0/best_state_2.pt' (epoch 17)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/DANN/participant_0/best_state_3.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/DANN/participant_0/best_state_3.pt' (epoch 7)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/DANN/participant_0/best_state_4.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/DANN/participant_0/best_state_4.pt' (epoch 19)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/DANN/participant_0/best_state_5.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/DANN/participant_0/best_state_5.pt' (epoch 8)\n",
      "==== models_array =  (6,)  @ session  5\n",
      "HANDLING NEW SESSION  0\n",
      "HANDLING NEW SESSION  1\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.6538461538461539  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  1\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  5\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.5   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.0  len before:  26   len after:  2\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  16\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.5555555555555556  len before:  26   len after:  9\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.5384615384615384  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.0  len before:  26   len after:  5\n",
      "BEFORE:  0.0   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  1.0  len before:  26   len after:  7\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.3333333333333333  len before:  26   len after:  15\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.6923076923076923  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.8098776223776224   Accuracy pseudo: 0.9407927747114903  len pseudo:  1993    len predictions 2288\n",
      "HANDLING NEW SESSION  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.6538461538461539  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  5\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  8\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.4230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.0  len before:  26   len after:  7\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  13\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.0  len before:  26   len after:  9\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.0  len before:  26   len after:  6\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  1\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.6923076923076923  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  14\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  16\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.9  len before:  26   len after:  10\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  5\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.8846153846153846  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  15\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.5769230769230769  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.0  len before:  26   len after:  18\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.75  len before:  26   len after:  8\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  14\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.7666083916083916   Accuracy pseudo: 0.8792585170340681  len pseudo:  1996    len predictions 2288\n",
      "HANDLING NEW SESSION  3\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.0  len before:  26   len after:  3\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.6538461538461539  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.5769230769230769  len before:  26   len after:  26\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.0  len before:  26   len after:  7\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  8\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  11\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  15\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.0  len before:  26   len after:  9\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.11538461538461539   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  7\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.5555555555555556  len before:  26   len after:  18\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  6\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  7\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  16\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.625  len before:  26   len after:  16\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.46153846153846156  len before:  26   len after:  26\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  6\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.9  len before:  26   len after:  10\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.4230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.4230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.11764705882352941  len before:  26   len after:  17\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.5769230769230769  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  7\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.6289335664335665   Accuracy pseudo: 0.7052845528455285  len pseudo:  1968    len predictions 2288\n",
      "HANDLING NEW SESSION  4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  7\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  6\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.34615384615384615   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.46153846153846156  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  14\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  10\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.038461538461538464   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  11\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.6923076923076923  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.4230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  16\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  18\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  1.0  len before:  26   len after:  4\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  14\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  17\n",
      "ACCURACY MODEL:  0.6940559440559441   Accuracy pseudo: 0.8220380302291566  len pseudo:  2051    len predictions 2288\n",
      "HANDLING NEW SESSION  5\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.5769230769230769  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  11\n",
      "BEFORE:  0.5   AFTER:  0.46153846153846156  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  7\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.0  len before:  26   len after:  9\n",
      "BEFORE:  0.34615384615384615   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.6  len before:  26   len after:  10\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  12\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.11538461538461539   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  14\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.0  len before:  26   len after:  8\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  9\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  2\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.5769230769230769  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  12\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  18\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  18\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  8\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.8461538461538461  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.46153846153846156  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.6923076923076923  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.0  len before:  26   len after:  16\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  12\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  2\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  9\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  5\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  1.0  len before:  26   len after:  1\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.6470588235294118  len before:  26   len after:  17\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  5\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  1.0  len before:  26   len after:  8\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  13\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  15\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  10\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  5\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.0  len before:  26   len after:  14\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.21428571428571427  len before:  26   len after:  14\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  11\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  12\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  1.0  len before:  26   len after:  2\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  9\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.46153846153846156  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.9230769230769231  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.5415209790209791   Accuracy pseudo: 0.691715976331361  len pseudo:  1690    len predictions 2288\n",
      "STARTING TRAINING\n",
      "Accuracy total 0.812872, main loss classifier 0.719422, source accuracy 0.840774 source classification loss 0.497718, target accuracy 0.784970 target loss 0.724959 accuracy domain distinction 0.500000 loss domain distinction 1.080844,\n",
      "VALIDATION Loss: 0.33791501 Acc: 0.88461538\n",
      "New best validation loss:  0.3379150132338206\n",
      "Epoch 2 of 500 took 0.292s\n",
      "Accuracy total 0.834821, main loss classifier 0.631056, source accuracy 0.847470 source classification loss 0.485684, target accuracy 0.822173 target loss 0.566652 accuracy domain distinction 0.500000 loss domain distinction 1.048880,\n",
      "VALIDATION Loss: 0.31014187 Acc: 0.88757396\n",
      "New best validation loss:  0.3101418676475684\n",
      "Epoch 3 of 500 took 0.278s\n",
      "Accuracy total 0.851935, main loss classifier 0.558183, source accuracy 0.853423 source classification loss 0.437396, target accuracy 0.850446 target loss 0.469211 accuracy domain distinction 0.500000 loss domain distinction 1.048793,\n",
      "VALIDATION Loss: 0.26870150 Acc: 0.89940828\n",
      "New best validation loss:  0.26870150367418927\n",
      "Epoch 4 of 500 took 0.281s\n",
      "Accuracy total 0.864955, main loss classifier 0.536266, source accuracy 0.874256 source classification loss 0.410515, target accuracy 0.855655 target loss 0.452059 accuracy domain distinction 0.500000 loss domain distinction 1.049795,\n",
      "VALIDATION Loss: 0.40950543 Acc: 0.84615385\n",
      "Epoch 5 of 500 took 0.279s\n",
      "Accuracy total 0.856771, main loss classifier 0.532691, source accuracy 0.850446 source classification loss 0.430431, target accuracy 0.863095 target loss 0.424893 accuracy domain distinction 0.500000 loss domain distinction 1.050289,\n",
      "VALIDATION Loss: 0.27984090 Acc: 0.90828402\n",
      "Epoch 6 of 500 took 0.280s\n",
      "Accuracy total 0.867188, main loss classifier 0.490219, source accuracy 0.862351 source classification loss 0.404288, target accuracy 0.872024 target loss 0.367299 accuracy domain distinction 0.500000 loss domain distinction 1.044250,\n",
      "VALIDATION Loss: 0.24762767 Acc: 0.9112426\n",
      "New best validation loss:  0.24762767180800438\n",
      "Epoch 7 of 500 took 0.281s\n",
      "Accuracy total 0.873140, main loss classifier 0.464551, source accuracy 0.863095 source classification loss 0.357228, target accuracy 0.883185 target loss 0.363067 accuracy domain distinction 0.500000 loss domain distinction 1.044035,\n",
      "VALIDATION Loss: 0.23871487 Acc: 0.91715976\n",
      "New best validation loss:  0.2387148713072141\n",
      "Epoch 8 of 500 took 0.280s\n",
      "Accuracy total 0.860491, main loss classifier 0.526120, source accuracy 0.841518 source classification loss 0.463917, target accuracy 0.879464 target loss 0.380530 accuracy domain distinction 0.500000 loss domain distinction 1.038969,\n",
      "VALIDATION Loss: 0.30415369 Acc: 0.90236686\n",
      "Epoch 9 of 500 took 0.289s\n",
      "Accuracy total 0.882812, main loss classifier 0.469148, source accuracy 0.877232 source classification loss 0.372074, target accuracy 0.888393 target loss 0.357890 accuracy domain distinction 0.500000 loss domain distinction 1.041651,\n",
      "VALIDATION Loss: 0.23883217 Acc: 0.92011834\n",
      "Epoch 10 of 500 took 0.279s\n",
      "Accuracy total 0.877604, main loss classifier 0.466561, source accuracy 0.869792 source classification loss 0.382653, target accuracy 0.885417 target loss 0.342523 accuracy domain distinction 0.500000 loss domain distinction 1.039731,\n",
      "VALIDATION Loss: 0.22949051 Acc: 0.92899408\n",
      "New best validation loss:  0.22949051360289255\n",
      "Epoch 11 of 500 took 0.278s\n",
      "Accuracy total 0.888021, main loss classifier 0.464189, source accuracy 0.876488 source classification loss 0.398938, target accuracy 0.899554 target loss 0.321852 accuracy domain distinction 0.500000 loss domain distinction 1.037937,\n",
      "VALIDATION Loss: 0.25740332 Acc: 0.91715976\n",
      "Epoch 12 of 500 took 0.277s\n",
      "Accuracy total 0.880208, main loss classifier 0.468229, source accuracy 0.862351 source classification loss 0.403320, target accuracy 0.898065 target loss 0.325063 accuracy domain distinction 0.500000 loss domain distinction 1.040378,\n",
      "VALIDATION Loss: 0.18461630 Acc: 0.93195266\n",
      "New best validation loss:  0.18461629996697107\n",
      "Epoch 13 of 500 took 0.282s\n",
      "Accuracy total 0.869792, main loss classifier 0.469071, source accuracy 0.863095 source classification loss 0.376481, target accuracy 0.876488 target loss 0.353212 accuracy domain distinction 0.500000 loss domain distinction 1.042247,\n",
      "VALIDATION Loss: 0.25143569 Acc: 0.89940828\n",
      "Epoch 14 of 500 took 0.277s\n",
      "Accuracy total 0.881324, main loss classifier 0.468608, source accuracy 0.854911 source classification loss 0.420816, target accuracy 0.907738 target loss 0.308701 accuracy domain distinction 0.500000 loss domain distinction 1.038491,\n",
      "VALIDATION Loss: 0.26626292 Acc: 0.90828402\n",
      "Epoch 15 of 500 took 0.279s\n",
      "Accuracy total 0.883185, main loss classifier 0.444412, source accuracy 0.869792 source classification loss 0.364914, target accuracy 0.896577 target loss 0.314666 accuracy domain distinction 0.500000 loss domain distinction 1.046219,\n",
      "VALIDATION Loss: 0.23047365 Acc: 0.9112426\n",
      "Epoch 16 of 500 took 0.278s\n",
      "Accuracy total 0.882068, main loss classifier 0.445014, source accuracy 0.872024 source classification loss 0.361213, target accuracy 0.892113 target loss 0.320754 accuracy domain distinction 0.500000 loss domain distinction 1.040301,\n",
      "VALIDATION Loss: 0.26393638 Acc: 0.89940828\n",
      "Epoch 17 of 500 took 0.281s\n",
      "Accuracy total 0.882812, main loss classifier 0.460109, source accuracy 0.875744 source classification loss 0.374957, target accuracy 0.889881 target loss 0.336422 accuracy domain distinction 0.500000 loss domain distinction 1.044201,\n",
      "VALIDATION Loss: 0.20912904 Acc: 0.9408284\n",
      "Epoch 18 of 500 took 0.280s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy total 0.892857, main loss classifier 0.428776, source accuracy 0.873512 source classification loss 0.366935, target accuracy 0.912202 target loss 0.282822 accuracy domain distinction 0.500000 loss domain distinction 1.038974,\n",
      "VALIDATION Loss: 0.23578949 Acc: 0.90828402\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0060e-04.\n",
      "Epoch 19 of 500 took 0.280s\n",
      "Accuracy total 0.885417, main loss classifier 0.436927, source accuracy 0.875000 source classification loss 0.367725, target accuracy 0.895833 target loss 0.300887 accuracy domain distinction 0.500000 loss domain distinction 1.026209,\n",
      "VALIDATION Loss: 0.18509543 Acc: 0.92307692\n",
      "Epoch 20 of 500 took 0.281s\n",
      "Accuracy total 0.885789, main loss classifier 0.437997, source accuracy 0.872768 source classification loss 0.360027, target accuracy 0.898810 target loss 0.309332 accuracy domain distinction 0.500000 loss domain distinction 1.033177,\n",
      "VALIDATION Loss: 0.18138962 Acc: 0.92307692\n",
      "New best validation loss:  0.18138962363203368\n",
      "Epoch 21 of 500 took 0.278s\n",
      "Accuracy total 0.893973, main loss classifier 0.427506, source accuracy 0.873512 source classification loss 0.367364, target accuracy 0.914435 target loss 0.281534 accuracy domain distinction 0.500000 loss domain distinction 1.030570,\n",
      "VALIDATION Loss: 0.18371312 Acc: 0.92307692\n",
      "Epoch 22 of 500 took 0.280s\n",
      "Accuracy total 0.889509, main loss classifier 0.419992, source accuracy 0.872024 source classification loss 0.356346, target accuracy 0.906994 target loss 0.277611 accuracy domain distinction 0.500000 loss domain distinction 1.030137,\n",
      "VALIDATION Loss: 0.18597138 Acc: 0.93786982\n",
      "Epoch 23 of 500 took 0.277s\n",
      "Accuracy total 0.894345, main loss classifier 0.414794, source accuracy 0.877976 source classification loss 0.348284, target accuracy 0.910714 target loss 0.275453 accuracy domain distinction 0.500000 loss domain distinction 1.029260,\n",
      "VALIDATION Loss: 0.21442954 Acc: 0.93195266\n",
      "Epoch 24 of 500 took 0.282s\n",
      "Accuracy total 0.893973, main loss classifier 0.428238, source accuracy 0.878720 source classification loss 0.361784, target accuracy 0.909226 target loss 0.288027 accuracy domain distinction 0.500000 loss domain distinction 1.033329,\n",
      "VALIDATION Loss: 0.20768377 Acc: 0.9408284\n",
      "Epoch 25 of 500 took 0.278s\n",
      "Accuracy total 0.894717, main loss classifier 0.419304, source accuracy 0.877232 source classification loss 0.356745, target accuracy 0.912202 target loss 0.274627 accuracy domain distinction 0.500000 loss domain distinction 1.036182,\n",
      "VALIDATION Loss: 0.24603536 Acc: 0.93491124\n",
      "Epoch 26 of 500 took 0.284s\n",
      "Accuracy total 0.892113, main loss classifier 0.416186, source accuracy 0.867560 source classification loss 0.358009, target accuracy 0.916667 target loss 0.268339 accuracy domain distinction 0.500000 loss domain distinction 1.030120,\n",
      "VALIDATION Loss: 0.20722870 Acc: 0.92899408\n",
      "Epoch    26: reducing learning rate of group 0 to 2.0120e-05.\n",
      "Epoch 27 of 500 took 0.281s\n",
      "Accuracy total 0.896949, main loss classifier 0.422621, source accuracy 0.882440 source classification loss 0.375563, target accuracy 0.911458 target loss 0.264078 accuracy domain distinction 0.500000 loss domain distinction 1.028003,\n",
      "VALIDATION Loss: 0.18357042 Acc: 0.9260355\n",
      "Epoch 28 of 500 took 0.294s\n",
      "Accuracy total 0.905134, main loss classifier 0.405675, source accuracy 0.889881 source classification loss 0.344996, target accuracy 0.920387 target loss 0.259965 accuracy domain distinction 0.500000 loss domain distinction 1.031946,\n",
      "VALIDATION Loss: 0.18616262 Acc: 0.95266272\n",
      "Epoch 29 of 500 took 0.298s\n",
      "Accuracy total 0.895833, main loss classifier 0.418302, source accuracy 0.884673 source classification loss 0.356948, target accuracy 0.906994 target loss 0.273707 accuracy domain distinction 0.500000 loss domain distinction 1.029746,\n",
      "VALIDATION Loss: 0.16728438 Acc: 0.93195266\n",
      "New best validation loss:  0.16728438002367815\n",
      "Epoch 30 of 500 took 0.279s\n",
      "Accuracy total 0.891369, main loss classifier 0.416109, source accuracy 0.864583 source classification loss 0.391333, target accuracy 0.918155 target loss 0.234569 accuracy domain distinction 0.500000 loss domain distinction 1.031581,\n",
      "VALIDATION Loss: 0.20460079 Acc: 0.92899408\n",
      "Epoch 31 of 500 took 0.281s\n",
      "Accuracy total 0.904390, main loss classifier 0.399201, source accuracy 0.885417 source classification loss 0.338133, target accuracy 0.923363 target loss 0.254374 accuracy domain distinction 0.500000 loss domain distinction 1.029470,\n",
      "VALIDATION Loss: 0.19427413 Acc: 0.92899408\n",
      "Epoch 32 of 500 took 0.281s\n",
      "Accuracy total 0.902902, main loss classifier 0.385610, source accuracy 0.892113 source classification loss 0.296888, target accuracy 0.913690 target loss 0.267254 accuracy domain distinction 0.500000 loss domain distinction 1.035382,\n",
      "VALIDATION Loss: 0.18280710 Acc: 0.9260355\n",
      "Epoch 33 of 500 took 0.290s\n",
      "Accuracy total 0.911086, main loss classifier 0.372599, source accuracy 0.897321 source classification loss 0.299141, target accuracy 0.924851 target loss 0.240638 accuracy domain distinction 0.500000 loss domain distinction 1.027092,\n",
      "VALIDATION Loss: 0.17025197 Acc: 0.93195266\n",
      "Epoch 34 of 500 took 0.277s\n",
      "Accuracy total 0.896949, main loss classifier 0.416284, source accuracy 0.877232 source classification loss 0.361867, target accuracy 0.916667 target loss 0.265021 accuracy domain distinction 0.500000 loss domain distinction 1.028404,\n",
      "VALIDATION Loss: 0.20465684 Acc: 0.92899408\n",
      "Epoch 35 of 500 took 0.279s\n",
      "Accuracy total 0.901042, main loss classifier 0.396168, source accuracy 0.886905 source classification loss 0.320762, target accuracy 0.915179 target loss 0.266365 accuracy domain distinction 0.500000 loss domain distinction 1.026045,\n",
      "VALIDATION Loss: 0.24732706 Acc: 0.92307692\n",
      "Epoch    35: reducing learning rate of group 0 to 4.0240e-06.\n",
      "Epoch 36 of 500 took 0.297s\n",
      "Accuracy total 0.898065, main loss classifier 0.400720, source accuracy 0.892113 source classification loss 0.323827, target accuracy 0.904018 target loss 0.271924 accuracy domain distinction 0.500000 loss domain distinction 1.028448,\n",
      "VALIDATION Loss: 0.18554700 Acc: 0.93786982\n",
      "Epoch 37 of 500 took 0.278s\n",
      "Accuracy total 0.899926, main loss classifier 0.409264, source accuracy 0.894345 source classification loss 0.347781, target accuracy 0.905506 target loss 0.264578 accuracy domain distinction 0.500000 loss domain distinction 1.030844,\n",
      "VALIDATION Loss: 0.24282629 Acc: 0.9112426\n",
      "Epoch 38 of 500 took 0.281s\n",
      "Accuracy total 0.898810, main loss classifier 0.402989, source accuracy 0.887649 source classification loss 0.340155, target accuracy 0.909970 target loss 0.260797 accuracy domain distinction 0.500000 loss domain distinction 1.025131,\n",
      "VALIDATION Loss: 0.18731308 Acc: 0.93491124\n",
      "Epoch 39 of 500 took 0.279s\n",
      "Accuracy total 0.905506, main loss classifier 0.394360, source accuracy 0.895089 source classification loss 0.333527, target accuracy 0.915923 target loss 0.249311 accuracy domain distinction 0.500000 loss domain distinction 1.029404,\n",
      "VALIDATION Loss: 0.16008540 Acc: 0.94674556\n",
      "New best validation loss:  0.1600854049126307\n",
      "Epoch 40 of 500 took 0.278s\n",
      "Accuracy total 0.899926, main loss classifier 0.403278, source accuracy 0.894345 source classification loss 0.309195, target accuracy 0.905506 target loss 0.290878 accuracy domain distinction 0.500000 loss domain distinction 1.032407,\n",
      "VALIDATION Loss: 0.18980925 Acc: 0.93491124\n",
      "Epoch 41 of 500 took 0.278s\n",
      "Accuracy total 0.900298, main loss classifier 0.398182, source accuracy 0.886161 source classification loss 0.335283, target accuracy 0.914435 target loss 0.255459 accuracy domain distinction 0.500000 loss domain distinction 1.028108,\n",
      "VALIDATION Loss: 0.16932442 Acc: 0.93491124\n",
      "Epoch 42 of 500 took 0.282s\n",
      "Accuracy total 0.901042, main loss classifier 0.402558, source accuracy 0.883929 source classification loss 0.348232, target accuracy 0.918155 target loss 0.250981 accuracy domain distinction 0.500000 loss domain distinction 1.029515,\n",
      "VALIDATION Loss: 0.18334748 Acc: 0.93786982\n",
      "Epoch 43 of 500 took 0.279s\n",
      "Accuracy total 0.904390, main loss classifier 0.396248, source accuracy 0.896577 source classification loss 0.322478, target accuracy 0.912202 target loss 0.264634 accuracy domain distinction 0.500000 loss domain distinction 1.026927,\n",
      "VALIDATION Loss: 0.16255547 Acc: 0.94674556\n",
      "Epoch 44 of 500 took 0.286s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy total 0.901786, main loss classifier 0.398286, source accuracy 0.890625 source classification loss 0.322654, target accuracy 0.912946 target loss 0.268175 accuracy domain distinction 0.500000 loss domain distinction 1.028713,\n",
      "VALIDATION Loss: 0.23882873 Acc: 0.89349112\n",
      "Epoch 45 of 500 took 0.282s\n",
      "Accuracy total 0.905134, main loss classifier 0.402548, source accuracy 0.897321 source classification loss 0.312352, target accuracy 0.912946 target loss 0.288236 accuracy domain distinction 0.500000 loss domain distinction 1.022539,\n",
      "VALIDATION Loss: 0.16390830 Acc: 0.93491124\n",
      "Epoch    45: reducing learning rate of group 0 to 8.0480e-07.\n",
      "Epoch 46 of 500 took 0.280s\n",
      "Accuracy total 0.897321, main loss classifier 0.411888, source accuracy 0.884673 source classification loss 0.360859, target accuracy 0.909970 target loss 0.258150 accuracy domain distinction 0.500000 loss domain distinction 1.023836,\n",
      "VALIDATION Loss: 0.16870676 Acc: 0.93195266\n",
      "Epoch 47 of 500 took 0.280s\n",
      "Accuracy total 0.900298, main loss classifier 0.405599, source accuracy 0.880208 source classification loss 0.358310, target accuracy 0.920387 target loss 0.247349 accuracy domain distinction 0.500000 loss domain distinction 1.027699,\n",
      "VALIDATION Loss: 0.22760737 Acc: 0.93491124\n",
      "Epoch 48 of 500 took 0.278s\n",
      "Accuracy total 0.899554, main loss classifier 0.406673, source accuracy 0.886161 source classification loss 0.341784, target accuracy 0.912946 target loss 0.263858 accuracy domain distinction 0.500000 loss domain distinction 1.038518,\n",
      "VALIDATION Loss: 0.18426136 Acc: 0.9260355\n",
      "Epoch 49 of 500 took 0.282s\n",
      "Accuracy total 0.898810, main loss classifier 0.406838, source accuracy 0.885417 source classification loss 0.343839, target accuracy 0.912202 target loss 0.264733 accuracy domain distinction 0.500000 loss domain distinction 1.025519,\n",
      "VALIDATION Loss: 0.17803694 Acc: 0.92307692\n",
      "Epoch 50 of 500 took 0.278s\n",
      "Accuracy total 0.900298, main loss classifier 0.400361, source accuracy 0.886161 source classification loss 0.338126, target accuracy 0.914435 target loss 0.256314 accuracy domain distinction 0.500000 loss domain distinction 1.031409,\n",
      "VALIDATION Loss: 0.20761394 Acc: 0.9260355\n",
      "Epoch 51 of 500 took 0.277s\n",
      "Training complete in 0m 14s\n",
      "['participant_0']\n",
      "Optimizer =  <generator object Module.parameters at 0x7facc55f49e0>\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/DANN/participant_0/best_state_6.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/DANN/participant_0/best_state_6.pt' (epoch 4)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/TSD/participant_0/best_state_0.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/TSD/participant_0/best_state_0.pt' (epoch 10)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/DANN/participant_0/best_state_1.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/DANN/participant_0/best_state_1.pt' (epoch 23)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/DANN/participant_0/best_state_2.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/DANN/participant_0/best_state_2.pt' (epoch 17)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/DANN/participant_0/best_state_3.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/DANN/participant_0/best_state_3.pt' (epoch 7)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/DANN/participant_0/best_state_4.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/DANN/participant_0/best_state_4.pt' (epoch 19)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/DANN/participant_0/best_state_5.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/DANN/participant_0/best_state_5.pt' (epoch 8)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/DANN/participant_0/best_state_6.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/DANN/participant_0/best_state_6.pt' (epoch 4)\n",
      "==== models_array =  (7,)  @ session  6\n",
      "HANDLING NEW SESSION  0\n",
      "HANDLING NEW SESSION  1\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.6538461538461539  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  1\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  5\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.5   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.0  len before:  26   len after:  2\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  16\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.5555555555555556  len before:  26   len after:  9\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.5384615384615384  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.0  len before:  26   len after:  5\n",
      "BEFORE:  0.0   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  1.0  len before:  26   len after:  7\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.3333333333333333  len before:  26   len after:  15\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.6923076923076923  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.8098776223776224   Accuracy pseudo: 0.9407927747114903  len pseudo:  1993    len predictions 2288\n",
      "HANDLING NEW SESSION  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.6538461538461539  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  5\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  8\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.4230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.0  len before:  26   len after:  7\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  13\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.0  len before:  26   len after:  9\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.0  len before:  26   len after:  6\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  1\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.6923076923076923  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  14\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  16\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.9  len before:  26   len after:  10\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  5\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.8846153846153846  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  15\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.5769230769230769  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.0  len before:  26   len after:  18\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.75  len before:  26   len after:  8\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  14\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.7666083916083916   Accuracy pseudo: 0.8792585170340681  len pseudo:  1996    len predictions 2288\n",
      "HANDLING NEW SESSION  3\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.0  len before:  26   len after:  3\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.6538461538461539  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.5769230769230769  len before:  26   len after:  26\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.0  len before:  26   len after:  7\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  8\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  11\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  15\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.0  len before:  26   len after:  9\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.11538461538461539   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  7\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.5555555555555556  len before:  26   len after:  18\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  6\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  7\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  16\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.625  len before:  26   len after:  16\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.46153846153846156  len before:  26   len after:  26\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  6\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.9  len before:  26   len after:  10\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.4230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.4230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.11764705882352941  len before:  26   len after:  17\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.5769230769230769  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  7\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.6289335664335665   Accuracy pseudo: 0.7052845528455285  len pseudo:  1968    len predictions 2288\n",
      "HANDLING NEW SESSION  4\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  7\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  6\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.34615384615384615   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.46153846153846156  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  14\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  10\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.038461538461538464   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  11\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.6923076923076923  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.4230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  16\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  18\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  1.0  len before:  26   len after:  4\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  14\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  17\n",
      "ACCURACY MODEL:  0.6940559440559441   Accuracy pseudo: 0.8220380302291566  len pseudo:  2051    len predictions 2288\n",
      "HANDLING NEW SESSION  5\n",
      "Finish segment dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.5769230769230769  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  11\n",
      "BEFORE:  0.5   AFTER:  0.46153846153846156  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  7\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.0  len before:  26   len after:  9\n",
      "BEFORE:  0.34615384615384615   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.6  len before:  26   len after:  10\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  12\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.11538461538461539   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  14\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.0  len before:  26   len after:  8\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  9\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  2\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.5769230769230769  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  12\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  18\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  18\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  8\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.8461538461538461  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.46153846153846156  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.6923076923076923  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.0  len before:  26   len after:  16\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  12\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  2\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  9\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  5\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  1.0  len before:  26   len after:  1\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.6470588235294118  len before:  26   len after:  17\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  5\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  1.0  len before:  26   len after:  8\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  13\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  15\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  10\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  5\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.0  len before:  26   len after:  14\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.21428571428571427  len before:  26   len after:  14\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  11\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  12\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  1.0  len before:  26   len after:  2\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  9\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.46153846153846156  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.9230769230769231  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.5415209790209791   Accuracy pseudo: 0.691715976331361  len pseudo:  1690    len predictions 2288\n",
      "HANDLING NEW SESSION  6\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.6923076923076923  len before:  26   len after:  26\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  16\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  4\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  1\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  12\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  7\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  10\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.0  len before:  26   len after:  7\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  4\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  1.0  len before:  26   len after:  7\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  15\n",
      "BEFORE:  0.11538461538461539   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  12\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  15\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.5384615384615384  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.0  len before:  26   len after:  6\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.0  len before:  26   len after:  2\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.34615384615384615   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  17\n",
      "BEFORE:  0.11538461538461539   AFTER:  0.0  len before:  26   len after:  18\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.6923076923076923  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.0  len before:  26   len after:  12\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.0  len before:  26   len after:  10\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.5493881118881119   Accuracy pseudo: 0.6553640911617565  len pseudo:  1799    len predictions 2288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING TRAINING\n",
      "Accuracy total 0.761009, main loss classifier 0.875407, source accuracy 0.762074 source classification loss 0.759696, target accuracy 0.759943 target loss 0.762534 accuracy domain distinction 0.499645 loss domain distinction 1.142921,\n",
      "VALIDATION Loss: 0.38915802 Acc: 0.83888889\n",
      "New best validation loss:  0.3891580179333687\n",
      "Epoch 2 of 500 took 0.322s\n",
      "Accuracy total 0.797940, main loss classifier 0.710247, source accuracy 0.812500 source classification loss 0.583133, target accuracy 0.783381 target loss 0.613677 accuracy domain distinction 0.500000 loss domain distinction 1.118423,\n",
      "VALIDATION Loss: 0.25361298 Acc: 0.93333333\n",
      "New best validation loss:  0.2536129802465439\n",
      "Epoch 3 of 500 took 0.300s\n",
      "Accuracy total 0.804688, main loss classifier 0.685816, source accuracy 0.804688 source classification loss 0.586041, target accuracy 0.804688 target loss 0.564074 accuracy domain distinction 0.499645 loss domain distinction 1.107578,\n",
      "VALIDATION Loss: 0.27979566 Acc: 0.89166667\n",
      "Epoch 4 of 500 took 0.293s\n",
      "Accuracy total 0.806463, main loss classifier 0.705160, source accuracy 0.786222 source classification loss 0.648474, target accuracy 0.826705 target loss 0.542386 accuracy domain distinction 0.499645 loss domain distinction 1.097301,\n",
      "VALIDATION Loss: 0.48184639 Acc: 0.84722222\n",
      "Epoch 5 of 500 took 0.295s\n",
      "Accuracy total 0.808239, main loss classifier 0.649291, source accuracy 0.787642 source classification loss 0.593493, target accuracy 0.828835 target loss 0.486351 accuracy domain distinction 0.500000 loss domain distinction 1.093691,\n",
      "VALIDATION Loss: 0.32739150 Acc: 0.86944444\n",
      "Epoch 6 of 500 took 0.296s\n",
      "Accuracy total 0.831676, main loss classifier 0.604773, source accuracy 0.819602 source classification loss 0.538374, target accuracy 0.843750 target loss 0.453034 accuracy domain distinction 0.500000 loss domain distinction 1.090692,\n",
      "VALIDATION Loss: 0.28720299 Acc: 0.87777778\n",
      "Epoch 7 of 500 took 0.296s\n",
      "Accuracy total 0.826349, main loss classifier 0.616612, source accuracy 0.794034 source classification loss 0.582712, target accuracy 0.858665 target loss 0.431387 accuracy domain distinction 0.500000 loss domain distinction 1.095621,\n",
      "VALIDATION Loss: 0.24138804 Acc: 0.91388889\n",
      "New best validation loss:  0.24138803780078888\n",
      "Epoch 8 of 500 took 0.302s\n",
      "Accuracy total 0.816051, main loss classifier 0.628971, source accuracy 0.803267 source classification loss 0.581204, target accuracy 0.828835 target loss 0.458884 accuracy domain distinction 0.500000 loss domain distinction 1.089273,\n",
      "VALIDATION Loss: 0.18674020 Acc: 0.93888889\n",
      "New best validation loss:  0.1867402046918869\n",
      "Epoch 9 of 500 took 0.298s\n",
      "Accuracy total 0.844460, main loss classifier 0.582069, source accuracy 0.829545 source classification loss 0.534378, target accuracy 0.859375 target loss 0.413181 accuracy domain distinction 0.500355 loss domain distinction 1.082892,\n",
      "VALIDATION Loss: 0.24460308 Acc: 0.91944444\n",
      "Epoch 10 of 500 took 0.298s\n",
      "Accuracy total 0.842685, main loss classifier 0.573767, source accuracy 0.828835 source classification loss 0.519081, target accuracy 0.856534 target loss 0.410072 accuracy domain distinction 0.500000 loss domain distinction 1.091905,\n",
      "VALIDATION Loss: 0.22488260 Acc: 0.9\n",
      "Epoch 11 of 500 took 0.294s\n",
      "Accuracy total 0.858310, main loss classifier 0.510841, source accuracy 0.852983 source classification loss 0.434895, target accuracy 0.863636 target loss 0.369243 accuracy domain distinction 0.500000 loss domain distinction 1.087723,\n",
      "VALIDATION Loss: 0.19606588 Acc: 0.925\n",
      "Epoch 12 of 500 took 0.293s\n",
      "Accuracy total 0.852983, main loss classifier 0.543179, source accuracy 0.829545 source classification loss 0.505568, target accuracy 0.876420 target loss 0.364294 accuracy domain distinction 0.500000 loss domain distinction 1.082483,\n",
      "VALIDATION Loss: 0.19061485 Acc: 0.925\n",
      "Epoch 13 of 500 took 0.298s\n",
      "Accuracy total 0.860795, main loss classifier 0.526697, source accuracy 0.842330 source classification loss 0.467172, target accuracy 0.879261 target loss 0.369407 accuracy domain distinction 0.500000 loss domain distinction 1.084075,\n",
      "VALIDATION Loss: 0.19931149 Acc: 0.92777778\n",
      "Epoch 14 of 500 took 0.295s\n",
      "Accuracy total 0.839844, main loss classifier 0.567596, source accuracy 0.816051 source classification loss 0.531357, target accuracy 0.863636 target loss 0.386552 accuracy domain distinction 0.500000 loss domain distinction 1.086413,\n",
      "VALIDATION Loss: 0.27876379 Acc: 0.88611111\n",
      "Epoch    14: reducing learning rate of group 0 to 5.0300e-04.\n",
      "Epoch 15 of 500 took 0.296s\n",
      "Accuracy total 0.857244, main loss classifier 0.503940, source accuracy 0.835227 source classification loss 0.456673, target accuracy 0.879261 target loss 0.341871 accuracy domain distinction 0.500000 loss domain distinction 1.046679,\n",
      "VALIDATION Loss: 0.19114193 Acc: 0.93055556\n",
      "Epoch 16 of 500 took 0.306s\n",
      "Accuracy total 0.873580, main loss classifier 0.486661, source accuracy 0.856534 source classification loss 0.439605, target accuracy 0.890625 target loss 0.325551 accuracy domain distinction 0.500000 loss domain distinction 1.040827,\n",
      "VALIDATION Loss: 0.22109556 Acc: 0.92777778\n",
      "Epoch 17 of 500 took 0.294s\n",
      "Accuracy total 0.875355, main loss classifier 0.473610, source accuracy 0.853693 source classification loss 0.420069, target accuracy 0.897017 target loss 0.318634 accuracy domain distinction 0.500000 loss domain distinction 1.042585,\n",
      "VALIDATION Loss: 0.22158234 Acc: 0.91388889\n",
      "Epoch 18 of 500 took 0.295s\n",
      "Accuracy total 0.868963, main loss classifier 0.476308, source accuracy 0.845170 source classification loss 0.439082, target accuracy 0.892756 target loss 0.305964 accuracy domain distinction 0.500000 loss domain distinction 1.037856,\n",
      "VALIDATION Loss: 0.16017665 Acc: 0.94444444\n",
      "New best validation loss:  0.16017665093143782\n",
      "Epoch 19 of 500 took 0.295s\n",
      "Accuracy total 0.877131, main loss classifier 0.460521, source accuracy 0.854403 source classification loss 0.425161, target accuracy 0.899858 target loss 0.289145 accuracy domain distinction 0.500000 loss domain distinction 1.033680,\n",
      "VALIDATION Loss: 0.14270435 Acc: 0.95\n",
      "New best validation loss:  0.14270434776941934\n",
      "Epoch 20 of 500 took 0.296s\n",
      "Accuracy total 0.868608, main loss classifier 0.475315, source accuracy 0.848722 source classification loss 0.423748, target accuracy 0.888494 target loss 0.319105 accuracy domain distinction 0.500000 loss domain distinction 1.038886,\n",
      "VALIDATION Loss: 0.14094246 Acc: 0.93888889\n",
      "New best validation loss:  0.14094246303041777\n",
      "Epoch 21 of 500 took 0.296s\n",
      "Accuracy total 0.877131, main loss classifier 0.456246, source accuracy 0.854403 source classification loss 0.405079, target accuracy 0.899858 target loss 0.300757 accuracy domain distinction 0.500000 loss domain distinction 1.033282,\n",
      "VALIDATION Loss: 0.14950292 Acc: 0.95833333\n",
      "Epoch 22 of 500 took 0.294s\n",
      "Accuracy total 0.884588, main loss classifier 0.445542, source accuracy 0.865057 source classification loss 0.398898, target accuracy 0.904119 target loss 0.285107 accuracy domain distinction 0.500000 loss domain distinction 1.035395,\n",
      "VALIDATION Loss: 0.15488605 Acc: 0.94166667\n",
      "Epoch 23 of 500 took 0.294s\n",
      "Accuracy total 0.879972, main loss classifier 0.447839, source accuracy 0.855114 source classification loss 0.394577, target accuracy 0.904830 target loss 0.293165 accuracy domain distinction 0.500000 loss domain distinction 1.039679,\n",
      "VALIDATION Loss: 0.23464976 Acc: 0.91388889\n",
      "Epoch 24 of 500 took 0.294s\n",
      "Accuracy total 0.883878, main loss classifier 0.443599, source accuracy 0.861506 source classification loss 0.404661, target accuracy 0.906250 target loss 0.276396 accuracy domain distinction 0.500000 loss domain distinction 1.030704,\n",
      "VALIDATION Loss: 0.15537727 Acc: 0.94166667\n",
      "Epoch 25 of 500 took 0.299s\n",
      "Accuracy total 0.887429, main loss classifier 0.426645, source accuracy 0.864347 source classification loss 0.391145, target accuracy 0.910511 target loss 0.254962 accuracy domain distinction 0.500000 loss domain distinction 1.035915,\n",
      "VALIDATION Loss: 0.15732127 Acc: 0.94722222\n",
      "Epoch 26 of 500 took 0.293s\n",
      "Accuracy total 0.883523, main loss classifier 0.442084, source accuracy 0.865057 source classification loss 0.406168, target accuracy 0.901989 target loss 0.271602 accuracy domain distinction 0.500000 loss domain distinction 1.031992,\n",
      "VALIDATION Loss: 0.13621629 Acc: 0.95555556\n",
      "New best validation loss:  0.1362162878115972\n",
      "Epoch 27 of 500 took 0.297s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy total 0.889205, main loss classifier 0.438455, source accuracy 0.870028 source classification loss 0.399571, target accuracy 0.908381 target loss 0.269919 accuracy domain distinction 0.500000 loss domain distinction 1.037102,\n",
      "VALIDATION Loss: 0.12169811 Acc: 0.95555556\n",
      "New best validation loss:  0.1216981088121732\n",
      "Epoch 28 of 500 took 0.296s\n",
      "Accuracy total 0.887784, main loss classifier 0.429647, source accuracy 0.858665 source classification loss 0.394565, target accuracy 0.916903 target loss 0.257430 accuracy domain distinction 0.500000 loss domain distinction 1.036495,\n",
      "VALIDATION Loss: 0.17878845 Acc: 0.93055556\n",
      "Epoch 29 of 500 took 0.293s\n",
      "Accuracy total 0.886364, main loss classifier 0.430028, source accuracy 0.865057 source classification loss 0.377443, target accuracy 0.907670 target loss 0.275423 accuracy domain distinction 0.500000 loss domain distinction 1.035956,\n",
      "VALIDATION Loss: 0.16878742 Acc: 0.94722222\n",
      "Epoch 30 of 500 took 0.295s\n",
      "Accuracy total 0.886009, main loss classifier 0.433924, source accuracy 0.859375 source classification loss 0.414701, target accuracy 0.912642 target loss 0.244259 accuracy domain distinction 0.500000 loss domain distinction 1.044446,\n",
      "VALIDATION Loss: 0.17952765 Acc: 0.93333333\n",
      "Epoch 31 of 500 took 0.292s\n",
      "Accuracy total 0.877131, main loss classifier 0.465191, source accuracy 0.858665 source classification loss 0.425993, target accuracy 0.895597 target loss 0.297535 accuracy domain distinction 0.500000 loss domain distinction 1.034269,\n",
      "VALIDATION Loss: 0.21970228 Acc: 0.91388889\n",
      "Epoch 32 of 500 took 0.295s\n",
      "Accuracy total 0.883523, main loss classifier 0.439431, source accuracy 0.851562 source classification loss 0.417109, target accuracy 0.915483 target loss 0.254340 accuracy domain distinction 0.500000 loss domain distinction 1.037070,\n",
      "VALIDATION Loss: 0.13802503 Acc: 0.94444444\n",
      "Epoch 33 of 500 took 0.294s\n",
      "Accuracy total 0.890625, main loss classifier 0.426956, source accuracy 0.871449 source classification loss 0.387613, target accuracy 0.909801 target loss 0.258510 accuracy domain distinction 0.500000 loss domain distinction 1.038945,\n",
      "VALIDATION Loss: 0.16821160 Acc: 0.93055556\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0060e-04.\n",
      "Epoch 34 of 500 took 0.295s\n",
      "Accuracy total 0.876420, main loss classifier 0.450098, source accuracy 0.840909 source classification loss 0.426325, target accuracy 0.911932 target loss 0.268155 accuracy domain distinction 0.500000 loss domain distinction 1.028579,\n",
      "VALIDATION Loss: 0.16377910 Acc: 0.94166667\n",
      "Epoch 35 of 500 took 0.294s\n",
      "Accuracy total 0.887429, main loss classifier 0.433257, source accuracy 0.860795 source classification loss 0.405274, target accuracy 0.914062 target loss 0.254933 accuracy domain distinction 0.500000 loss domain distinction 1.031530,\n",
      "VALIDATION Loss: 0.16897843 Acc: 0.925\n",
      "Epoch 36 of 500 took 0.295s\n",
      "Accuracy total 0.881037, main loss classifier 0.443954, source accuracy 0.850142 source classification loss 0.434197, target accuracy 0.911932 target loss 0.248742 accuracy domain distinction 0.500000 loss domain distinction 1.024850,\n",
      "VALIDATION Loss: 0.15500894 Acc: 0.94444444\n",
      "Epoch 37 of 500 took 0.294s\n",
      "Accuracy total 0.891690, main loss classifier 0.406806, source accuracy 0.875000 source classification loss 0.354670, target accuracy 0.908381 target loss 0.253783 accuracy domain distinction 0.500000 loss domain distinction 1.025789,\n",
      "VALIDATION Loss: 0.13544398 Acc: 0.95\n",
      "Epoch 38 of 500 took 0.294s\n",
      "Accuracy total 0.888139, main loss classifier 0.428077, source accuracy 0.859375 source classification loss 0.396906, target accuracy 0.916903 target loss 0.254092 accuracy domain distinction 0.500000 loss domain distinction 1.025777,\n",
      "VALIDATION Loss: 0.15543132 Acc: 0.93888889\n",
      "Epoch 39 of 500 took 0.293s\n",
      "Training complete in 0m 11s\n",
      "['participant_0']\n",
      "Optimizer =  <generator object Module.parameters at 0x7facc55792e0>\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/DANN/participant_0/best_state_7.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/DANN/participant_0/best_state_7.pt' (epoch 12)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/TSD/participant_0/best_state_0.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/TSD/participant_0/best_state_0.pt' (epoch 10)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/DANN/participant_0/best_state_1.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/DANN/participant_0/best_state_1.pt' (epoch 23)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/DANN/participant_0/best_state_2.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/DANN/participant_0/best_state_2.pt' (epoch 17)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/DANN/participant_0/best_state_3.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/DANN/participant_0/best_state_3.pt' (epoch 7)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/DANN/participant_0/best_state_4.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/DANN/participant_0/best_state_4.pt' (epoch 19)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/DANN/participant_0/best_state_5.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/DANN/participant_0/best_state_5.pt' (epoch 8)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/DANN/participant_0/best_state_6.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/DANN/participant_0/best_state_6.pt' (epoch 4)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/DANN/participant_0/best_state_7.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump3/DANN/participant_0/best_state_7.pt' (epoch 12)\n",
      "==== models_array =  (8,)  @ session  7\n",
      "HANDLING NEW SESSION  0\n",
      "HANDLING NEW SESSION  1\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.6538461538461539  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  1\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  5\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.5   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.0  len before:  26   len after:  2\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  16\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.5555555555555556  len before:  26   len after:  9\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.5384615384615384  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.0  len before:  26   len after:  5\n",
      "BEFORE:  0.0   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  1.0  len before:  26   len after:  7\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.3333333333333333  len before:  26   len after:  15\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.6923076923076923  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.8098776223776224   Accuracy pseudo: 0.9407927747114903  len pseudo:  1993    len predictions 2288\n",
      "HANDLING NEW SESSION  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.6538461538461539  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  5\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  8\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.4230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.0  len before:  26   len after:  7\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  13\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.0  len before:  26   len after:  9\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.0  len before:  26   len after:  6\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  1\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.6923076923076923  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  14\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  16\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.9  len before:  26   len after:  10\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  5\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.8846153846153846  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  15\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.5769230769230769  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.0  len before:  26   len after:  18\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.75  len before:  26   len after:  8\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  14\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.7666083916083916   Accuracy pseudo: 0.8792585170340681  len pseudo:  1996    len predictions 2288\n",
      "HANDLING NEW SESSION  3\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.0  len before:  26   len after:  3\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.6538461538461539  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.5769230769230769  len before:  26   len after:  26\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.0  len before:  26   len after:  7\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  8\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  11\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  15\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.0  len before:  26   len after:  9\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.11538461538461539   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  7\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.5555555555555556  len before:  26   len after:  18\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  6\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  7\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  16\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.625  len before:  26   len after:  16\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.46153846153846156  len before:  26   len after:  26\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  6\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.9  len before:  26   len after:  10\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.4230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.4230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.11764705882352941  len before:  26   len after:  17\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.5769230769230769  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  7\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.6289335664335665   Accuracy pseudo: 0.7052845528455285  len pseudo:  1968    len predictions 2288\n",
      "HANDLING NEW SESSION  4\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  7\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  6\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.34615384615384615   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.46153846153846156  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  14\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  10\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.038461538461538464   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  11\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.6923076923076923  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.4230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  16\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  18\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  1.0  len before:  26   len after:  4\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  14\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  17\n",
      "ACCURACY MODEL:  0.6940559440559441   Accuracy pseudo: 0.8220380302291566  len pseudo:  2051    len predictions 2288\n",
      "HANDLING NEW SESSION  5\n",
      "Finish segment dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.5769230769230769  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  11\n",
      "BEFORE:  0.5   AFTER:  0.46153846153846156  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  7\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.0  len before:  26   len after:  9\n",
      "BEFORE:  0.34615384615384615   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.6  len before:  26   len after:  10\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  12\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.11538461538461539   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  14\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.0  len before:  26   len after:  8\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  9\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  2\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.5769230769230769  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  12\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  18\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  18\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  8\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.8461538461538461  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.46153846153846156  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.6923076923076923  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.0  len before:  26   len after:  16\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  12\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  2\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  9\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  5\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  1.0  len before:  26   len after:  1\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.6470588235294118  len before:  26   len after:  17\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  5\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  1.0  len before:  26   len after:  8\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  13\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  15\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  10\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  5\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.0  len before:  26   len after:  14\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.21428571428571427  len before:  26   len after:  14\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  11\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  12\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  1.0  len before:  26   len after:  2\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  9\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.46153846153846156  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.9230769230769231  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.5415209790209791   Accuracy pseudo: 0.691715976331361  len pseudo:  1690    len predictions 2288\n",
      "HANDLING NEW SESSION  6\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.6923076923076923  len before:  26   len after:  26\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  16\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  4\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  1\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  12\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  7\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  10\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.0  len before:  26   len after:  7\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  4\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  1.0  len before:  26   len after:  7\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  15\n",
      "BEFORE:  0.11538461538461539   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  12\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  15\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.5384615384615384  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.0  len before:  26   len after:  6\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.0  len before:  26   len after:  2\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.34615384615384615   AFTER:  nan  len before:  26   len after:  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  17\n",
      "BEFORE:  0.11538461538461539   AFTER:  0.0  len before:  26   len after:  18\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.6923076923076923  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.0  len before:  26   len after:  12\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.0  len before:  26   len after:  10\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.5493881118881119   Accuracy pseudo: 0.6553640911617565  len pseudo:  1799    len predictions 2288\n",
      "HANDLING NEW SESSION  7\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.5769230769230769  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.0  len before:  26   len after:  18\n",
      "BEFORE:  0.5   AFTER:  1.0  len before:  26   len after:  5\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.0  len before:  26   len after:  10\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.4230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  1.0  len before:  26   len after:  7\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  15\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.5384615384615384  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.11538461538461539   AFTER:  0.0  len before:  26   len after:  9\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.23076923076923078  len before:  26   len after:  26\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  15\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  6\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.0  len before:  26   len after:  5\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  7\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.46153846153846156  len before:  26   len after:  26\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.5769230769230769  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.6538461538461539  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  8\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  1.0  len before:  26   len after:  13\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  0.4230769230769231   AFTER:  1.0  len before:  26   len after:  13\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  8\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.0  len before:  26   len after:  5\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.46153846153846156  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.0  len before:  26   len after:  10\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.5769230769230769  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  1\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.11538461538461539   AFTER:  0.0  len before:  26   len after:  18\n",
      "BEFORE:  0.23076923076923078   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.5   AFTER:  0.6666666666666666  len before:  26   len after:  6\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  18\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  7\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  1.0  len before:  26   len after:  6\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.8846153846153846  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.4593531468531469   Accuracy pseudo: 0.5426398696360674  len pseudo:  1841    len predictions 2288\n",
      "STARTING TRAINING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy total 0.740829, main loss classifier 0.981251, source accuracy 0.803668 source classification loss 0.580117, target accuracy 0.677989 target loss 1.154726 accuracy domain distinction 0.500000 loss domain distinction 1.138293,\n",
      "VALIDATION Loss: 0.52906536 Acc: 0.84281843\n",
      "New best validation loss:  0.5290653556585312\n",
      "Epoch 2 of 500 took 0.328s\n",
      "Accuracy total 0.769701, main loss classifier 0.833193, source accuracy 0.828804 source classification loss 0.515397, target accuracy 0.710598 target loss 0.938145 accuracy domain distinction 0.500000 loss domain distinction 1.064223,\n",
      "VALIDATION Loss: 0.36047622 Acc: 0.88888889\n",
      "New best validation loss:  0.36047621568044025\n",
      "Epoch 3 of 500 took 0.308s\n",
      "Accuracy total 0.783628, main loss classifier 0.815982, source accuracy 0.832880 source classification loss 0.527751, target accuracy 0.734375 target loss 0.895152 accuracy domain distinction 0.500000 loss domain distinction 1.045300,\n",
      "VALIDATION Loss: 0.47428527 Acc: 0.82926829\n",
      "Epoch 4 of 500 took 0.305s\n",
      "Accuracy total 0.802649, main loss classifier 0.745196, source accuracy 0.846467 source classification loss 0.485392, target accuracy 0.758832 target loss 0.796432 accuracy domain distinction 0.500000 loss domain distinction 1.042836,\n",
      "VALIDATION Loss: 0.42364899 Acc: 0.85365854\n",
      "Epoch 5 of 500 took 0.309s\n",
      "Accuracy total 0.797215, main loss classifier 0.743727, source accuracy 0.822011 source classification loss 0.533704, target accuracy 0.772418 target loss 0.745382 accuracy domain distinction 0.500000 loss domain distinction 1.041840,\n",
      "VALIDATION Loss: 0.50230878 Acc: 0.82113821\n",
      "Epoch 6 of 500 took 0.306s\n",
      "Accuracy total 0.812500, main loss classifier 0.694629, source accuracy 0.849185 source classification loss 0.477246, target accuracy 0.775815 target loss 0.703779 accuracy domain distinction 0.500000 loss domain distinction 1.041171,\n",
      "VALIDATION Loss: 0.32442125 Acc: 0.88888889\n",
      "New best validation loss:  0.3244212518135707\n",
      "Epoch 7 of 500 took 0.307s\n",
      "Accuracy total 0.798234, main loss classifier 0.713958, source accuracy 0.828125 source classification loss 0.490918, target accuracy 0.768342 target loss 0.729458 accuracy domain distinction 0.500000 loss domain distinction 1.037703,\n",
      "VALIDATION Loss: 0.34666057 Acc: 0.87533875\n",
      "Epoch 8 of 500 took 0.308s\n",
      "Accuracy total 0.825408, main loss classifier 0.642230, source accuracy 0.857337 source classification loss 0.428834, target accuracy 0.793478 target loss 0.647897 accuracy domain distinction 0.500000 loss domain distinction 1.038652,\n",
      "VALIDATION Loss: 0.34064230 Acc: 0.90243902\n",
      "Epoch 9 of 500 took 0.306s\n",
      "Accuracy total 0.824389, main loss classifier 0.659963, source accuracy 0.845788 source classification loss 0.469259, target accuracy 0.802989 target loss 0.644038 accuracy domain distinction 0.500000 loss domain distinction 1.033145,\n",
      "VALIDATION Loss: 0.32268915 Acc: 0.91056911\n",
      "New best validation loss:  0.32268914580345154\n",
      "Epoch 10 of 500 took 0.308s\n",
      "Accuracy total 0.811481, main loss classifier 0.689718, source accuracy 0.845109 source classification loss 0.488713, target accuracy 0.777853 target loss 0.683049 accuracy domain distinction 0.500000 loss domain distinction 1.038367,\n",
      "VALIDATION Loss: 0.36194779 Acc: 0.88617886\n",
      "Epoch 11 of 500 took 0.306s\n",
      "Accuracy total 0.813179, main loss classifier 0.699281, source accuracy 0.845109 source classification loss 0.504038, target accuracy 0.781250 target loss 0.687814 accuracy domain distinction 0.500000 loss domain distinction 1.033546,\n",
      "VALIDATION Loss: 0.40399571 Acc: 0.84281843\n",
      "Epoch 12 of 500 took 0.306s\n",
      "Accuracy total 0.824389, main loss classifier 0.670233, source accuracy 0.849185 source classification loss 0.471485, target accuracy 0.799592 target loss 0.661203 accuracy domain distinction 0.500000 loss domain distinction 1.038892,\n",
      "VALIDATION Loss: 0.33794691 Acc: 0.88346883\n",
      "Epoch 13 of 500 took 0.303s\n",
      "Accuracy total 0.828465, main loss classifier 0.644962, source accuracy 0.847147 source classification loss 0.481601, target accuracy 0.809783 target loss 0.601606 accuracy domain distinction 0.500000 loss domain distinction 1.033591,\n",
      "VALIDATION Loss: 0.32583538 Acc: 0.88617886\n",
      "Epoch 14 of 500 took 0.307s\n",
      "Accuracy total 0.836277, main loss classifier 0.604404, source accuracy 0.862772 source classification loss 0.414482, target accuracy 0.809783 target loss 0.587622 accuracy domain distinction 0.500000 loss domain distinction 1.033521,\n",
      "VALIDATION Loss: 0.34296119 Acc: 0.8699187\n",
      "Epoch 15 of 500 took 0.306s\n",
      "Accuracy total 0.815217, main loss classifier 0.672946, source accuracy 0.836277 source classification loss 0.508485, target accuracy 0.794158 target loss 0.630977 accuracy domain distinction 0.500000 loss domain distinction 1.032144,\n",
      "VALIDATION Loss: 0.37816127 Acc: 0.86720867\n",
      "Epoch    15: reducing learning rate of group 0 to 2.0120e-05.\n",
      "Epoch 16 of 500 took 0.313s\n",
      "Accuracy total 0.839674, main loss classifier 0.602078, source accuracy 0.861413 source classification loss 0.427914, target accuracy 0.817935 target loss 0.570865 accuracy domain distinction 0.500000 loss domain distinction 1.026883,\n",
      "VALIDATION Loss: 0.32029310 Acc: 0.90785908\n",
      "New best validation loss:  0.3202930986881256\n",
      "Epoch 17 of 500 took 0.308s\n",
      "Accuracy total 0.832541, main loss classifier 0.630253, source accuracy 0.850543 source classification loss 0.449968, target accuracy 0.814538 target loss 0.604834 accuracy domain distinction 0.500000 loss domain distinction 1.028521,\n",
      "VALIDATION Loss: 0.32943906 Acc: 0.89430894\n",
      "Epoch 18 of 500 took 0.307s\n",
      "Accuracy total 0.828125, main loss classifier 0.634769, source accuracy 0.857337 source classification loss 0.441434, target accuracy 0.798913 target loss 0.620944 accuracy domain distinction 0.500000 loss domain distinction 1.035802,\n",
      "VALIDATION Loss: 0.34167377 Acc: 0.899729\n",
      "Epoch 19 of 500 took 0.306s\n",
      "Accuracy total 0.834579, main loss classifier 0.611013, source accuracy 0.847147 source classification loss 0.431003, target accuracy 0.822011 target loss 0.583941 accuracy domain distinction 0.500000 loss domain distinction 1.035408,\n",
      "VALIDATION Loss: 0.41300055 Acc: 0.85365854\n",
      "Epoch 20 of 500 took 0.307s\n",
      "Accuracy total 0.830842, main loss classifier 0.610489, source accuracy 0.852582 source classification loss 0.432095, target accuracy 0.809103 target loss 0.583671 accuracy domain distinction 0.500000 loss domain distinction 1.026056,\n",
      "VALIDATION Loss: 0.34232469 Acc: 0.88617886\n",
      "Epoch 21 of 500 took 0.309s\n",
      "Accuracy total 0.825408, main loss classifier 0.632628, source accuracy 0.853261 source classification loss 0.447663, target accuracy 0.797554 target loss 0.610474 accuracy domain distinction 0.500000 loss domain distinction 1.035596,\n",
      "VALIDATION Loss: 0.38484565 Acc: 0.84823848\n",
      "Epoch 22 of 500 took 0.306s\n",
      "Accuracy total 0.828804, main loss classifier 0.607518, source accuracy 0.853940 source classification loss 0.434160, target accuracy 0.803668 target loss 0.574169 accuracy domain distinction 0.500000 loss domain distinction 1.033537,\n",
      "VALIDATION Loss: 0.38358015 Acc: 0.84552846\n",
      "Epoch    22: reducing learning rate of group 0 to 4.0240e-06.\n",
      "Epoch 23 of 500 took 0.308s\n",
      "Accuracy total 0.833560, main loss classifier 0.620167, source accuracy 0.849185 source classification loss 0.461925, target accuracy 0.817935 target loss 0.571756 accuracy domain distinction 0.500000 loss domain distinction 1.033259,\n",
      "VALIDATION Loss: 0.31718745 Acc: 0.89701897\n",
      "New best validation loss:  0.31718744585911435\n",
      "Epoch 24 of 500 took 0.309s\n",
      "Accuracy total 0.836617, main loss classifier 0.629577, source accuracy 0.861413 source classification loss 0.430584, target accuracy 0.811821 target loss 0.621943 accuracy domain distinction 0.500000 loss domain distinction 1.033129,\n",
      "VALIDATION Loss: 0.34419932 Acc: 0.89701897\n",
      "Epoch 25 of 500 took 0.310s\n",
      "Accuracy total 0.823030, main loss classifier 0.640630, source accuracy 0.853940 source classification loss 0.464878, target accuracy 0.792120 target loss 0.610676 accuracy domain distinction 0.500000 loss domain distinction 1.028531,\n",
      "VALIDATION Loss: 0.30720076 Acc: 0.90243902\n",
      "New best validation loss:  0.30720075716574985\n",
      "Epoch 26 of 500 took 0.306s\n",
      "Accuracy total 0.831182, main loss classifier 0.610256, source accuracy 0.849864 source classification loss 0.442123, target accuracy 0.812500 target loss 0.572158 accuracy domain distinction 0.500000 loss domain distinction 1.031154,\n",
      "VALIDATION Loss: 0.30170294 Acc: 0.90514905\n",
      "New best validation loss:  0.30170293649037677\n",
      "Epoch 27 of 500 took 0.309s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy total 0.834239, main loss classifier 0.588783, source accuracy 0.853261 source classification loss 0.418302, target accuracy 0.815217 target loss 0.553147 accuracy domain distinction 0.500000 loss domain distinction 1.030584,\n",
      "VALIDATION Loss: 0.32314016 Acc: 0.90785908\n",
      "Epoch 28 of 500 took 0.310s\n",
      "Accuracy total 0.835598, main loss classifier 0.599527, source accuracy 0.854620 source classification loss 0.437885, target accuracy 0.816576 target loss 0.554047 accuracy domain distinction 0.500000 loss domain distinction 1.035615,\n",
      "VALIDATION Loss: 0.30938488 Acc: 0.89430894\n",
      "Epoch 29 of 500 took 0.307s\n",
      "Accuracy total 0.832201, main loss classifier 0.604691, source accuracy 0.851902 source classification loss 0.427771, target accuracy 0.812500 target loss 0.573536 accuracy domain distinction 0.500000 loss domain distinction 1.040372,\n",
      "VALIDATION Loss: 0.38662131 Acc: 0.85907859\n",
      "Epoch 30 of 500 took 0.305s\n",
      "Accuracy total 0.831522, main loss classifier 0.633480, source accuracy 0.845788 source classification loss 0.468803, target accuracy 0.817255 target loss 0.591422 accuracy domain distinction 0.500000 loss domain distinction 1.033677,\n",
      "VALIDATION Loss: 0.36442192 Acc: 0.87804878\n",
      "Epoch 31 of 500 took 0.310s\n",
      "Accuracy total 0.817935, main loss classifier 0.657658, source accuracy 0.845109 source classification loss 0.474946, target accuracy 0.790761 target loss 0.633337 accuracy domain distinction 0.500000 loss domain distinction 1.035166,\n",
      "VALIDATION Loss: 0.32413147 Acc: 0.88346883\n",
      "Epoch 32 of 500 took 0.314s\n",
      "Accuracy total 0.834579, main loss classifier 0.621050, source accuracy 0.862092 source classification loss 0.427948, target accuracy 0.807065 target loss 0.608883 accuracy domain distinction 0.500000 loss domain distinction 1.026350,\n",
      "VALIDATION Loss: 0.30837973 Acc: 0.89701897\n",
      "Epoch    32: reducing learning rate of group 0 to 8.0480e-07.\n",
      "Epoch 33 of 500 took 0.307s\n",
      "Accuracy total 0.834918, main loss classifier 0.615058, source accuracy 0.857337 source classification loss 0.450967, target accuracy 0.812500 target loss 0.573701 accuracy domain distinction 0.500000 loss domain distinction 1.027233,\n",
      "VALIDATION Loss: 0.40434292 Acc: 0.85636856\n",
      "Epoch 34 of 500 took 0.308s\n",
      "Accuracy total 0.831182, main loss classifier 0.612577, source accuracy 0.856658 source classification loss 0.429433, target accuracy 0.805707 target loss 0.589748 accuracy domain distinction 0.500000 loss domain distinction 1.029857,\n",
      "VALIDATION Loss: 0.41159501 Acc: 0.84552846\n",
      "Epoch 35 of 500 took 0.306s\n",
      "Accuracy total 0.832541, main loss classifier 0.618294, source accuracy 0.859375 source classification loss 0.437653, target accuracy 0.805707 target loss 0.592443 accuracy domain distinction 0.500000 loss domain distinction 1.032459,\n",
      "VALIDATION Loss: 0.43053372 Acc: 0.87533875\n",
      "Epoch 36 of 500 took 0.306s\n",
      "Accuracy total 0.810122, main loss classifier 0.672836, source accuracy 0.832880 source classification loss 0.490440, target accuracy 0.787364 target loss 0.648102 accuracy domain distinction 0.500000 loss domain distinction 1.035647,\n",
      "VALIDATION Loss: 0.33529823 Acc: 0.899729\n",
      "Epoch 37 of 500 took 0.306s\n",
      "Accuracy total 0.835258, main loss classifier 0.625692, source accuracy 0.860734 source classification loss 0.465467, target accuracy 0.809783 target loss 0.580330 accuracy domain distinction 0.500000 loss domain distinction 1.027929,\n",
      "VALIDATION Loss: 0.30123020 Acc: 0.90514905\n",
      "New best validation loss:  0.3012302021185557\n",
      "Epoch 38 of 500 took 0.311s\n",
      "Accuracy total 0.824389, main loss classifier 0.652139, source accuracy 0.845109 source classification loss 0.482072, target accuracy 0.803668 target loss 0.615685 accuracy domain distinction 0.500000 loss domain distinction 1.032605,\n",
      "VALIDATION Loss: 0.36174720 Acc: 0.88888889\n",
      "Epoch 39 of 500 took 0.305s\n",
      "Accuracy total 0.836277, main loss classifier 0.608723, source accuracy 0.867527 source classification loss 0.417158, target accuracy 0.805027 target loss 0.593890 accuracy domain distinction 0.500000 loss domain distinction 1.031988,\n",
      "VALIDATION Loss: 0.30540449 Acc: 0.89159892\n",
      "Epoch 40 of 500 took 0.305s\n",
      "Accuracy total 0.823370, main loss classifier 0.640110, source accuracy 0.835598 source classification loss 0.475066, target accuracy 0.811141 target loss 0.598709 accuracy domain distinction 0.500000 loss domain distinction 1.032222,\n",
      "VALIDATION Loss: 0.31595029 Acc: 0.90243902\n",
      "Epoch 41 of 500 took 0.309s\n",
      "Accuracy total 0.827106, main loss classifier 0.619529, source accuracy 0.840353 source classification loss 0.463211, target accuracy 0.813859 target loss 0.569772 accuracy domain distinction 0.500000 loss domain distinction 1.030378,\n",
      "VALIDATION Loss: 0.34028240 Acc: 0.88346883\n",
      "Epoch 42 of 500 took 0.307s\n",
      "Accuracy total 0.830503, main loss classifier 0.624155, source accuracy 0.853261 source classification loss 0.447378, target accuracy 0.807745 target loss 0.594871 accuracy domain distinction 0.500000 loss domain distinction 1.030313,\n",
      "VALIDATION Loss: 0.32846824 Acc: 0.89701897\n",
      "Epoch 43 of 500 took 0.306s\n",
      "Accuracy total 0.839334, main loss classifier 0.605365, source accuracy 0.853261 source classification loss 0.455879, target accuracy 0.825408 target loss 0.547492 accuracy domain distinction 0.500000 loss domain distinction 1.036799,\n",
      "VALIDATION Loss: 0.33087227 Acc: 0.87804878\n",
      "Epoch    43: reducing learning rate of group 0 to 1.6096e-07.\n",
      "Epoch 44 of 500 took 0.312s\n",
      "Accuracy total 0.832880, main loss classifier 0.612980, source accuracy 0.862092 source classification loss 0.428167, target accuracy 0.803668 target loss 0.591425 accuracy domain distinction 0.500000 loss domain distinction 1.031836,\n",
      "VALIDATION Loss: 0.37534690 Acc: 0.85365854\n",
      "Epoch 45 of 500 took 0.306s\n",
      "Accuracy total 0.819293, main loss classifier 0.644378, source accuracy 0.849185 source classification loss 0.455288, target accuracy 0.789402 target loss 0.626893 accuracy domain distinction 0.500000 loss domain distinction 1.032882,\n",
      "VALIDATION Loss: 0.48014067 Acc: 0.8401084\n",
      "Epoch 46 of 500 took 0.306s\n",
      "Accuracy total 0.830163, main loss classifier 0.598364, source accuracy 0.841712 source classification loss 0.464898, target accuracy 0.818614 target loss 0.525087 accuracy domain distinction 0.500000 loss domain distinction 1.033714,\n",
      "VALIDATION Loss: 0.38018811 Acc: 0.85094851\n",
      "Epoch 47 of 500 took 0.306s\n",
      "Accuracy total 0.837976, main loss classifier 0.612666, source accuracy 0.866848 source classification loss 0.411943, target accuracy 0.809103 target loss 0.608089 accuracy domain distinction 0.500000 loss domain distinction 1.026494,\n",
      "VALIDATION Loss: 0.34333215 Acc: 0.89430894\n",
      "Epoch 48 of 500 took 0.314s\n",
      "Accuracy total 0.844090, main loss classifier 0.619144, source accuracy 0.865489 source classification loss 0.428316, target accuracy 0.822690 target loss 0.602550 accuracy domain distinction 0.500000 loss domain distinction 1.037113,\n",
      "VALIDATION Loss: 0.32459811 Acc: 0.899729\n",
      "Epoch 49 of 500 took 0.307s\n",
      "Training complete in 0m 15s\n",
      "['participant_0']\n"
     ]
    }
   ],
   "source": [
    "# percentage_same_gesture_stable = 0.75 \n",
    "# run_SCADANN_training_sessions(examples_datasets=examples_datasets_train, labels_datasets=labels_datasets_train,\n",
    "#                               num_kernels=num_kernels, feature_vector_input_length=feature_vector_input_length,\n",
    "#                               path_weights_to_save_to=path_SCADANN,\n",
    "#                               path_weights_Adversarial_training=path_DANN,\n",
    "#                               path_weights_Normal_training=path_TSD,\n",
    "#                               number_of_cycles_total = number_of_cycles_total, \n",
    "#                               number_of_cycle_for_first_training = number_of_cycle_for_first_training,\n",
    "#                               number_of_classes=number_of_classes,\n",
    "#                               learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET one participant_examples  (8,)\n",
      "   GET one training_index_examples  (12, 572, 252)  at  0\n",
      "   GOT one group XY  (6864, 252)    (6864,)\n",
      "       one group XY test  (1716, 252)    (1716, 252)\n",
      "       one group XY train (6177, 252)    (6177,)\n",
      "       one group XY valid (687, 252)    (687, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  1\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  2\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  3\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  4\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  5\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  6\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  7\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "dataloaders: \n",
      "   train  (1, 8)\n",
      "   valid  (1, 8)\n",
      "   test  (1, 8)\n",
      "Participant:  0  Accuracy:  0.916083916083916\n",
      "Participant:  0  Accuracy:  0.8146853146853147\n",
      "Participant:  0  Accuracy:  0.8199300699300699\n",
      "Participant:  0  Accuracy:  0.6538461538461539\n",
      "Participant:  0  Accuracy:  0.7622377622377622\n",
      "Participant:  0  Accuracy:  0.6520979020979021\n",
      "Participant:  0  Accuracy:  0.6066433566433567\n",
      "Participant:  0  Accuracy:  0.527972027972028\n",
      "ACCURACY PARTICIPANT:  [0.916083916083916, 0.8146853146853147, 0.8199300699300699, 0.6538461538461539, 0.7622377622377622, 0.6520979020979021, 0.6066433566433567, 0.527972027972028]\n",
      "[[0.91608392 0.81468531 0.81993007 0.65384615 0.76223776 0.6520979\n",
      "  0.60664336 0.52797203]]\n",
      "[array([0.91608392, 0.81468531, 0.81993007, 0.65384615, 0.76223776,\n",
      "       0.6520979 , 0.60664336, 0.52797203])]\n",
      "OVERALL ACCURACY: 0.7191870629370629\n"
     ]
    }
   ],
   "source": [
    "algo_name = \"SCADANN\"\n",
    "test_network_SCADANN(examples_datasets_train=examples_datasets_train, labels_datasets_train=labels_datasets_train,\n",
    "                     num_neurons=num_kernels, feature_vector_input_length=feature_vector_input_length,\n",
    "                     path_weights_SCADANN =path_SCADANN, path_weights_normal=path_TSD,\n",
    "                     algo_name=algo_name, cycle_test=3, number_of_cycles_total=number_of_cycles_total,\n",
    "                     number_of_cycle_for_first_training = number_of_cycle_for_first_training,\n",
    "                     number_of_classes=number_of_classes, save_path = save_SCADANN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Day_0~2</th>\n",
       "      <td>0.916084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_3</th>\n",
       "      <td>0.814685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_4</th>\n",
       "      <td>0.81993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_5</th>\n",
       "      <td>0.653846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_6</th>\n",
       "      <td>0.762238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_7</th>\n",
       "      <td>0.652098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_8</th>\n",
       "      <td>0.606643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_9</th>\n",
       "      <td>0.527972</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Participant_5\n",
       "Day_0~2      0.916084\n",
       "Day_3        0.814685\n",
       "Day_4         0.81993\n",
       "Day_5        0.653846\n",
       "Day_6        0.762238\n",
       "Day_7        0.652098\n",
       "Day_8        0.606643\n",
       "Day_9        0.527972"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_filename = save_SCADANN + '/predictions_' + algo_name + \".npy\"\n",
    "results = np.load(results_filename, allow_pickle=True)\n",
    "SCADANN_acc = results[0]\n",
    "SCADANN_acc_overall = np.mean(SCADANN_acc)\n",
    "SCADANN_df = pd.DataFrame(SCADANN_acc.transpose(), \n",
    "                       index = [f'Day_{i}' for i in index_participant_list],\n",
    "                        columns = ['Participant_5'])\n",
    "SCADANN_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "SCADANN_df.transpose().plot.bar(rot=0, figsize=(10,5))\n",
    "plt.title(\"SCADANN Accuracies\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ground_truths  =  (1, 8)\n",
      "predictions =  (1, 8)\n",
      "index_participant_list  ['0~2', 3, 4, 5, 6, 7, 8, 9]\n",
      "accuracies_gestures =  (22, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Loc1_Sub5_Day0~2-&gt;0~2</th>\n",
       "      <th>Loc1_Sub5_Day0~2-&gt;3</th>\n",
       "      <th>Loc1_Sub5_Day0~2-&gt;4</th>\n",
       "      <th>Loc1_Sub5_Day0~2-&gt;5</th>\n",
       "      <th>Loc1_Sub5_Day0~2-&gt;6</th>\n",
       "      <th>Loc1_Sub5_Day0~2-&gt;7</th>\n",
       "      <th>Loc1_Sub5_Day0~2-&gt;8</th>\n",
       "      <th>Loc1_Sub5_Day0~2-&gt;9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M1</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.730769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M2</td>\n",
       "      <td>0.871795</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.230769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M3</td>\n",
       "      <td>0.948718</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M4</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>M5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.538462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>M6</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>M7</td>\n",
       "      <td>0.987179</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.807692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>M8</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>M9</td>\n",
       "      <td>0.897436</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.192308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>M10</td>\n",
       "      <td>0.910256</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.076923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>M11</td>\n",
       "      <td>0.935897</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>0.346154</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.192308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>M12</td>\n",
       "      <td>0.910256</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.384615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>M13</td>\n",
       "      <td>0.743590</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>M14</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>M15</td>\n",
       "      <td>0.641026</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.192308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>M16</td>\n",
       "      <td>0.987179</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>M17</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>M18</td>\n",
       "      <td>0.987179</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.192308</td>\n",
       "      <td>0.576923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>M19</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.692308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>M20</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.038462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>M21</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.961538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Mean</td>\n",
       "      <td>0.916084</td>\n",
       "      <td>0.814685</td>\n",
       "      <td>0.819930</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>0.762238</td>\n",
       "      <td>0.652098</td>\n",
       "      <td>0.606643</td>\n",
       "      <td>0.527972</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Loc1_Sub5_Day0~2->0~2  Loc1_Sub5_Day0~2->3  \\\n",
       "0          M0               1.000000             1.000000   \n",
       "1          M1               0.961538             0.884615   \n",
       "2          M2               0.871795             0.615385   \n",
       "3          M3               0.948718             0.576923   \n",
       "4          M4               0.833333             0.000000   \n",
       "5          M5               1.000000             1.000000   \n",
       "6          M6               1.000000             0.846154   \n",
       "7          M7               0.987179             1.000000   \n",
       "8          M8               0.923077             1.000000   \n",
       "9          M9               0.897436             0.730769   \n",
       "10        M10               0.910256             0.500000   \n",
       "11        M11               0.935897             0.653846   \n",
       "12        M12               0.910256             0.923077   \n",
       "13        M13               0.743590             1.000000   \n",
       "14        M14               0.923077             0.692308   \n",
       "15        M15               0.641026             0.807692   \n",
       "16        M16               0.987179             1.000000   \n",
       "17        M17               1.000000             1.000000   \n",
       "18        M18               0.987179             1.000000   \n",
       "19        M19               1.000000             1.000000   \n",
       "20        M20               0.807692             1.000000   \n",
       "21        M21               0.884615             0.692308   \n",
       "22       Mean               0.916084             0.814685   \n",
       "\n",
       "    Loc1_Sub5_Day0~2->4  Loc1_Sub5_Day0~2->5  Loc1_Sub5_Day0~2->6  \\\n",
       "0              1.000000             1.000000             1.000000   \n",
       "1              0.923077             1.000000             0.884615   \n",
       "2              0.846154             0.230769             0.500000   \n",
       "3              0.730769             0.038462             0.000000   \n",
       "4              1.000000             0.000000             0.000000   \n",
       "5              0.576923             0.730769             1.000000   \n",
       "6              1.000000             0.423077             0.692308   \n",
       "7              1.000000             0.961538             1.000000   \n",
       "8              1.000000             1.000000             0.846154   \n",
       "9              0.730769             0.423077             0.769231   \n",
       "10             0.923077             0.961538             1.000000   \n",
       "11             0.269231             0.346154             0.846154   \n",
       "12             0.846154             0.538462             0.884615   \n",
       "13             1.000000             1.000000             1.000000   \n",
       "14             0.000000             0.000000             0.115385   \n",
       "15             0.423077             0.884615             0.461538   \n",
       "16             1.000000             0.576923             1.000000   \n",
       "17             0.923077             1.000000             1.000000   \n",
       "18             1.000000             0.384615             1.000000   \n",
       "19             1.000000             1.000000             1.000000   \n",
       "20             0.961538             0.923077             0.807692   \n",
       "21             0.884615             0.961538             0.961538   \n",
       "22             0.819930             0.653846             0.762238   \n",
       "\n",
       "    Loc1_Sub5_Day0~2->7  Loc1_Sub5_Day0~2->8  Loc1_Sub5_Day0~2->9  \n",
       "0              1.000000             1.000000             1.000000  \n",
       "1              0.807692             0.730769             0.730769  \n",
       "2              0.307692             0.038462             0.230769  \n",
       "3              0.000000             0.000000             0.000000  \n",
       "4              0.000000             0.000000             0.000000  \n",
       "5              0.461538             0.961538             0.538462  \n",
       "6              0.576923             0.461538             1.000000  \n",
       "7              0.961538             1.000000             0.807692  \n",
       "8              1.000000             1.000000             1.000000  \n",
       "9              0.038462             0.615385             0.192308  \n",
       "10             1.000000             0.461538             0.076923  \n",
       "11             0.807692             0.500000             0.192308  \n",
       "12             0.961538             0.769231             0.384615  \n",
       "13             1.000000             1.000000             1.000000  \n",
       "14             0.000000             0.000000             0.000000  \n",
       "15             0.000000             0.000000             0.192308  \n",
       "16             1.000000             1.000000             1.000000  \n",
       "17             1.000000             1.000000             1.000000  \n",
       "18             1.000000             0.192308             0.576923  \n",
       "19             0.538462             1.000000             0.692308  \n",
       "20             0.884615             0.615385             0.038462  \n",
       "21             1.000000             1.000000             0.961538  \n",
       "22             0.652098             0.606643             0.527972  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truths = results[1]\n",
    "predictions = results[2]\n",
    "print(\"ground_truths  = \", np.shape(ground_truths))\n",
    "print(\"predictions = \", np.shape(predictions))\n",
    "df = get_gesture_accuracies(ground_truths, predictions, number_of_classes=number_of_classes, \n",
    "                            m_name=m_name, n_name=n_name, path=save_TSD, algo_name=algo_name,\n",
    "                           index_participant_list_customized=index_participant_list,\n",
    "                           lump_day_at_participant=5)\n",
    "df = pd.read_csv(save_SCADANN+'/'+algo_name+'.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "Suppose there is a ndarray of NxM dataloaders, then N group of models will be trained, and each group will have M model. Each group is independent of the other, and each model within a group is dependent on its previous training weights.\n",
    "\n",
    "In general, overall accuracies of SCADANN are better than DANN, and DANN is better than TSD.\n",
    "Occasionally accuracies of SCADANN end up a little smaller than DANN, reasons may be lack of datasets put into training model (fixed) and non-optimal percentage_same_gesture_sable (fixed). Code should be reproducible if processed dataset sticks to the shape defined above.  \n",
    "\n",
    "The amount of increase in accuracies from DANN to SCADANN looks random. But if the base model is better at classifying one session, then its corresponding SCADANN is also better at classifying the same session. Given such result, to obtain the best performance from SCADANN, a good model trained with good data should be the starting point.\n",
    "\n",
    "* What to check if sth goes wrong:\n",
    "    * percentage_same_gesture_sable\n",
    "    * number of cycles or sessions\n",
    "    * shape of dataloaders (combination of train, test, valid should include all dataset)\n",
    "    * shape of procssed datasets\n",
    "    * directory paths of weights and results\n",
    "    * if weights are stored or loaded correcltyTSD_acc_overall_one = np.mean(TSD_acc, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TSD\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Day_0~2</th>\n",
       "      <td>0.916084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_3</th>\n",
       "      <td>0.690559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_4</th>\n",
       "      <td>0.716783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_5</th>\n",
       "      <td>0.568182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_6</th>\n",
       "      <td>0.618881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_7</th>\n",
       "      <td>0.444056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_8</th>\n",
       "      <td>0.458042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_9</th>\n",
       "      <td>0.41958</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Participant_5\n",
       "Day_0~2      0.916084\n",
       "Day_3        0.690559\n",
       "Day_4        0.716783\n",
       "Day_5        0.568182\n",
       "Day_6        0.618881\n",
       "Day_7        0.444056\n",
       "Day_8        0.458042\n",
       "Day_9         0.41958"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DANN\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Day_0~2</th>\n",
       "      <td>0.916084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_3</th>\n",
       "      <td>0.767483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_4</th>\n",
       "      <td>0.772727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_5</th>\n",
       "      <td>0.596154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_6</th>\n",
       "      <td>0.695804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_7</th>\n",
       "      <td>0.536713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_8</th>\n",
       "      <td>0.548951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_9</th>\n",
       "      <td>0.475524</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Participant_5\n",
       "Day_0~2      0.916084\n",
       "Day_3        0.767483\n",
       "Day_4        0.772727\n",
       "Day_5        0.596154\n",
       "Day_6        0.695804\n",
       "Day_7        0.536713\n",
       "Day_8        0.548951\n",
       "Day_9        0.475524"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCADANN\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Day_0~2</th>\n",
       "      <td>0.916084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_3</th>\n",
       "      <td>0.814685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_4</th>\n",
       "      <td>0.81993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_5</th>\n",
       "      <td>0.653846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_6</th>\n",
       "      <td>0.762238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_7</th>\n",
       "      <td>0.652098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_8</th>\n",
       "      <td>0.606643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_9</th>\n",
       "      <td>0.527972</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Participant_5\n",
       "Day_0~2      0.916084\n",
       "Day_3        0.814685\n",
       "Day_4         0.81993\n",
       "Day_5        0.653846\n",
       "Day_6        0.762238\n",
       "Day_7        0.652098\n",
       "Day_8        0.606643\n",
       "Day_9        0.527972"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"TSD\")\n",
    "display(TSD_df)\n",
    "print(\"DANN\")\n",
    "display(DANN_df)\n",
    "print(\"SCADANN\")\n",
    "display(SCADANN_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Day_3</th>\n",
       "      <td>0.124126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_4</th>\n",
       "      <td>0.103147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_5</th>\n",
       "      <td>0.085664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_6</th>\n",
       "      <td>0.143357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_7</th>\n",
       "      <td>0.208042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_8</th>\n",
       "      <td>0.148601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_9</th>\n",
       "      <td>0.108392</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Participant_5\n",
       "Day_3      0.124126\n",
       "Day_4      0.103147\n",
       "Day_5      0.085664\n",
       "Day_6      0.143357\n",
       "Day_7      0.208042\n",
       "Day_8      0.148601\n",
       "Day_9      0.108392"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "diff_df = SCADANN_df-TSD_df\n",
    "diff_df = diff_df.drop('Day_'+index_participant_list[0])\n",
    "display(diff_df)\n",
    "diff_df.to_csv(save_TSD+'/diff_results/across_day_loc1_lump3_diff.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Overall_Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TSD</th>\n",
       "      <td>0.604021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DANN</th>\n",
       "      <td>0.663680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SCADANN</th>\n",
       "      <td>0.719187</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Overall_Acc\n",
       "TSD         0.604021\n",
       "DANN        0.663680\n",
       "SCADANN     0.719187"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_acc_df = pd.DataFrame([TSD_acc_overall, DANN_acc_overall, SCADANN_acc_overall],\n",
    "                             index = [\"TSD\", \"DANN\", \"SCADANN\"],\n",
    "                             columns = [\"Overall_Acc\"])\n",
    "overall_acc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1800x1800 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(25,25))\n",
    "acc_list = [TSD_df, DANN_df, SCADANN_df, overall_acc_df]\n",
    "title_list = [\"TSD\", \"DANN\", \"SCADANN\", \"Overall\"]\n",
    "for idx, ax in enumerate(axes.reshape(-1)): \n",
    "    acc_list[idx].transpose().plot.bar(ax = ax, rot=0)\n",
    "    ax.set_title(title_list[idx])\n",
    "    ax.set_ylim([0, 1.0])\n",
    "    for p in ax.patches:\n",
    "        ax.annotate(str(np.round(p.get_height(),2)), (p.get_x()+p.get_width()/2., p.get_height()),\n",
    "                    ha='center', va='center', xytext=(0, 8),textcoords='offset points')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
