{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results of TSD, DANN, SCADANN models across 10 days of inward rotation starting at Day_0~4 for Subject_4\n",
    "\n",
    "Library used can be downloaded from https://github.com/aonai/long_term_EMG_myo   \n",
    "&emsp; Original by UlysseCoteAllard https://github.com/UlysseCoteAllard/LongTermEMG   \n",
    "Dataset recorded by https://github.com/Suguru55/Wearable_Sensor_Long-term_sEMG_Dataset   \n",
    "Extended robot project can be found in https://github.com/aonai/myo_robot_arm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "* weights for TSD are total of 50 training models, 10 for each day\n",
    "* weights for DANN and SCADANN are total of 45 trianing models, 9 for each day\n",
    "\n",
    "\n",
    "* training examples should have shape (1, 6,)\n",
    "* first session has shape (20, 572, 252)\n",
    "* the following sessions have shape (4, 572, 252)\n",
    "* training labels should have shape (1, 6,)\n",
    "\n",
    "\n",
    "* location 0, 1, and 2 corresponds to neutral position, inward rotation, and outward rotation respectively\n",
    "* session mentioned below are days, so number of sessions is 10\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "import torch\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_dir = \"/home/laiy/gitrepos/msr_final/LongTermEMG_myo\"\n",
    "os.chdir(code_dir)\n",
    "from PrepareAndLoadData.process_data import read_data_training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Prepare Data\n",
    "use `switch=2` to train across days and individually on wearing location 0 (`session_in_include=[0]`)\n",
    "\n",
    "### specify the directories used for running the code:\n",
    "* `code_diar`: path to long_term_EMG_myo library\n",
    "* `data_dir`: where raw dataset is loaded; raw data is in csv format\n",
    "* `processed_data_dir`: where processed dataset is loaded; processed data is in npy pickle format\n",
    "    * processed data should be a ndarray of shape   \n",
    "    (controlling_factor_1 x controlling_factor_2 x num_sessions_per_gesture x #examples_window*#mov(26*22=572) x processed_channel_shape(252 for TSD, (4,8,10) for ConvNet)\n",
    "* `path_<model_name>`: where model weights are saved\n",
    "    * weights should be saved in folder `/Weights/<model_name>`. Each folder has subfolders containing weights for the first controlling factor.\n",
    "    * weights for base model (TSD or ConvNet) contain m set of training model\n",
    "    * weights for DANN and SCADANN contain m-1 set of trianing model (these models are trianed based on TSD, so they do not have a best_state_0.pt model). \n",
    "* `save_<model_name>`: where model results are saved\n",
    "    * each result for testing a model on a group of dataset is saved in folder `results`. Each result has corresponding \n",
    "        * `<model_name>.txt` includes predictions, ground truths, array of accuracies for each participant and each session, and overall accuracy\n",
    "        * `predictions_<model_name>.npy` includes array of accuracies, ground truths, predictions, and model outputs (probability array for each prediction)\n",
    "        * remember to make blank files in these names before saving\n",
    "\n",
    "\n",
    "\n",
    "* use `read_data_training` to process raw dataset\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/home/laiy/gitrepos/msr_final/Wearable_Sensor_Long-term_sEMG_Dataset/data\"\n",
    "processed_data_dir = \"/home/laiy/gitrepos/msr_final/LongTermEMG_myo/Processed_datasets_all_across_day_loc_1_lump5\"\n",
    "code_dir = \"/home/laiy/gitrepos/msr_final/LongTermEMG_myo\"\n",
    "save_dir = \"/home/laiy/gitrepos/msr_final/LongTermEMG_myo/Results\"\n",
    "\n",
    "path_TSD =\"/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump5/TSD\"\n",
    "save_TSD = \"/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/results\"\n",
    "\n",
    "path_DANN =\"/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump5/DANN\"\n",
    "save_DANN = \"/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/results\"\n",
    "\n",
    "path_SCADANN =\"/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump5/SCADANN\"\n",
    "save_SCADANN = \"/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing Training datasets...\n",
      "session  1  --- process data in days  [2, 5, 6, 16, 17, 18, 22, 24, 25, 28]\n",
      "index_participant_list  [5]\n",
      "READ  Sub 5 _Loc 1 _Day 2\n",
      "examples_per_session =  (1, 4, 572, 252)\n",
      "READ  Sub 5 _Loc 1 _Day 5\n",
      "Include day  5  in first dataset  (4, 572, 252)\n",
      "examples of first session =  (8, 572, 252)\n",
      "examples_per_session =  (1, 8, 572, 252)\n",
      "READ  Sub 5 _Loc 1 _Day 6\n",
      "Include day  6  in first dataset  (8, 572, 252)\n",
      "examples of first session =  (12, 572, 252)\n",
      "examples_per_session =  (1, 12, 572, 252)\n",
      "READ  Sub 5 _Loc 1 _Day 16\n",
      "Include day  16  in first dataset  (12, 572, 252)\n",
      "examples of first session =  (16, 572, 252)\n",
      "examples_per_session =  (1, 16, 572, 252)\n",
      "READ  Sub 5 _Loc 1 _Day 17\n",
      "Include day  17  in first dataset  (16, 572, 252)\n",
      "examples of first session =  (20, 572, 252)\n",
      "examples_per_session =  (1, 20, 572, 252)\n",
      "READ  Sub 5 _Loc 1 _Day 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laiy/.local/lib/python3.8/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "examples_per_session =  (2,)\n",
      "READ  Sub 5 _Loc 1 _Day 22\n",
      "examples_per_session =  (3,)\n",
      "READ  Sub 5 _Loc 1 _Day 24\n",
      "examples_per_session =  (4,)\n",
      "READ  Sub 5 _Loc 1 _Day 25\n",
      "examples_per_session =  (5,)\n",
      "READ  Sub 5 _Loc 1 _Day 28\n",
      "examples_per_session =  (6,)\n",
      "@ traning sessions =  (1, 6)\n",
      "traning examples  (1, 6)\n",
      "traning labels  (1, 6)\n",
      "all traning examples  (1, 6)\n",
      "all traning labels  (1, 6)\n"
     ]
    }
   ],
   "source": [
    "read_data_training(path=data_dir, store_path = processed_data_dir,  \n",
    "                   sessions_to_include =[1], switch=2, include_in_first=5,\n",
    "                   start_at_participant=5, num_participant=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traning examples  (1, 6)\n",
      "traning labels  (1, 6)\n"
     ]
    }
   ],
   "source": [
    "# check stored pickle \n",
    "with open(processed_data_dir + \"/training_session.pickle\", 'rb') as f:\n",
    "    dataset_training = pickle.load(file=f)\n",
    "\n",
    "examples_datasets_train = dataset_training['examples_training']\n",
    "print('traning examples ', np.shape(examples_datasets_train))\n",
    "labels_datasets_train = dataset_training['labels_training']\n",
    "print('traning labels ', np.shape(labels_datasets_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  examples_per_session =  (20, 572, 252)\n",
      "0  labels_per_session =  (20, 572)\n",
      "1  examples_per_session =  (4, 572, 252)\n",
      "1  labels_per_session =  (4, 572)\n",
      "2  examples_per_session =  (4, 572, 252)\n",
      "2  labels_per_session =  (4, 572)\n",
      "3  examples_per_session =  (4, 572, 252)\n",
      "3  labels_per_session =  (4, 572)\n",
      "4  examples_per_session =  (4, 572, 252)\n",
      "4  labels_per_session =  (4, 572)\n",
      "5  examples_per_session =  (4, 572, 252)\n",
      "5  labels_per_session =  (4, 572)\n"
     ]
    }
   ],
   "source": [
    "for idx, examples_per_session in enumerate (examples_datasets_train[0]):\n",
    "    print(idx, \" examples_per_session = \", np.shape(examples_per_session))\n",
    "    print(idx, \" labels_per_session = \", np.shape(labels_datasets_train[0][idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify params used for training and testing\n",
    "\n",
    "During training and testing, processed datasets are first put into pytorch dataloders, then feed to the model trainer; following are params for TSD model and dataloaders\n",
    "\n",
    "* `num_kernels`: list of integers defining number of neurons used in each linear layer (linear block has `dropout`=0.5)\n",
    "* `number_of_cycles_total`: number of trails performed for each session (assuming that all session have the same trail size)\n",
    "    * 4 for myo across day training\n",
    "* `number_of_classes`: total number of gestures performed in dataset\n",
    "    * 22 for myo\n",
    "* `batch_size`: number of examples stored in each batch\n",
    "* `feature_vector_input_length`: length of input array or each processed signal; i.e. size of one training example \n",
    "    * 252 for TSD\n",
    "* `learning_rate`= 0.002515\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number_of_cycle_for_first_training  20\n",
      "number_of_cycles_total  4\n"
     ]
    }
   ],
   "source": [
    "num_kernels=[200, 200, 200]                                \n",
    "number_of_cycle_for_first_training = np.shape(examples_datasets_train[0][0])[0]               \n",
    "number_of_cycles_total=np.shape(examples_datasets_train[-1][-1])[0]               \n",
    "print(\"number_of_cycle_for_first_training \", number_of_cycle_for_first_training)\n",
    "print(\"number_of_cycles_total \", number_of_cycles_total)\n",
    "number_of_classes=22\n",
    "batch_size=128          \n",
    "feature_vector_input_length=252                     \n",
    "learning_rate=0.002515"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. TSD_DNN\n",
    "* `train_fine_tuning`: used to train data using a base model (TSD or ConvNet)\n",
    "    * running this function will save num_sessions sets of TSD model weights (each is fine tuned based on the previous training)  \n",
    "    \n",
    "* `test_standard_model_on_training_sessions`: test model result\n",
    "\n",
    "\n",
    "### check if dataloaders are loaded correctly:\n",
    "* each participant has shape (num_session x 40 x 572 x 252)\n",
    "* each session has shape (40 x 572 x 252)\n",
    "* put these data into on group ends up with shape (40*572=22880, 252)\n",
    "    * shuffle on group of data and put into dataloaders\n",
    "    * each participant should have num_sessions sets of dataloaders, each correspond to one session\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TrainingsAndEvaluations.ForTrainingSessions.train_tsd_dnn_standard import \\\n",
    "            test_standard_model_on_training_sessions, train_fine_tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET one participant_examples  (6,)\n",
      "   GET one training_index_examples  (20, 572, 252)  at  0\n",
      "   GOT one group XY  (11440, 252)    (11440,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (10296, 252)    (10296,)\n",
      "       one group XY valid (1144, 252)    (1144, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  1\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  2\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  3\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  4\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  5\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "dataloaders: \n",
      "   train  (1, 6)\n",
      "   valid  (1, 6)\n",
      "   test  (1, 0)\n",
      "START TRAINING\n",
      "Participant:  0\n",
      "Session:  0\n",
      "<generator object Module.parameters at 0x7f58616e9ba0>\n",
      "Epoch 0/499\n",
      "----------\n",
      "train Loss: 0.00942886 Acc: 0.64160156\n",
      "val Loss: 0.00038615 Acc: 0.85926573\n",
      "New best validation loss: 0.0003861480399028405\n",
      "Epoch 1 of 500 took 0.527s\n",
      "Epoch 1/499\n",
      "----------\n",
      "train Loss: 0.00478278 Acc: 0.79287109\n",
      "val Loss: 0.00028492 Acc: 0.89073427\n",
      "New best validation loss: 0.00028492419140322225\n",
      "Epoch 2 of 500 took 0.519s\n",
      "Epoch 2/499\n",
      "----------\n",
      "train Loss: 0.00397032 Acc: 0.825\n",
      "val Loss: 0.00025281 Acc: 0.89685315\n",
      "Epoch 3 of 500 took 0.522s\n",
      "Epoch 3/499\n",
      "----------\n",
      "train Loss: 0.00362948 Acc: 0.83613281\n",
      "val Loss: 0.00027232 Acc: 0.89073427\n",
      "Epoch 4 of 500 took 0.547s\n",
      "Epoch 4/499\n",
      "----------\n",
      "train Loss: 0.00341241 Acc: 0.84746094\n",
      "val Loss: 0.00021779 Acc: 0.90734266\n",
      "Epoch 5 of 500 took 0.592s\n",
      "Epoch 5/499\n",
      "----------\n",
      "train Loss: 0.00315301 Acc: 0.85839844\n",
      "val Loss: 0.00020426 Acc: 0.91520979\n",
      "Epoch 6 of 500 took 0.520s\n",
      "Epoch 6/499\n",
      "----------\n",
      "train Loss: 0.00299115 Acc: 0.86660156\n",
      "val Loss: 0.00021181 Acc: 0.91258741\n",
      "Epoch 7 of 500 took 0.520s\n",
      "Epoch 7/499\n",
      "----------\n",
      "train Loss: 0.00289447 Acc: 0.87265625\n",
      "val Loss: 0.00019355 Acc: 0.92482517\n",
      "Epoch 8 of 500 took 0.584s\n",
      "Epoch 8/499\n",
      "----------\n",
      "train Loss: 0.00285712 Acc: 0.87197266\n",
      "val Loss: 0.00018925 Acc: 0.92482517\n",
      "Epoch 9 of 500 took 0.577s\n",
      "Epoch 9/499\n",
      "----------\n",
      "train Loss: 0.00281075 Acc: 0.8734375\n",
      "val Loss: 0.00018707 Acc: 0.9256993\n",
      "Epoch 10 of 500 took 0.609s\n",
      "Epoch 10/499\n",
      "----------\n",
      "train Loss: 0.00261039 Acc: 0.88105469\n",
      "val Loss: 0.00016991 Acc: 0.93269231\n",
      "New best validation loss: 0.00016991148045013\n",
      "Epoch 11 of 500 took 0.527s\n",
      "Epoch 11/499\n",
      "----------\n",
      "train Loss: 0.00251965 Acc: 0.88603516\n",
      "val Loss: 0.00017403 Acc: 0.93006993\n",
      "Epoch 12 of 500 took 0.528s\n",
      "Epoch 12/499\n",
      "----------\n",
      "train Loss: 0.00251404 Acc: 0.88896484\n",
      "val Loss: 0.00017739 Acc: 0.91870629\n",
      "Epoch 13 of 500 took 0.584s\n",
      "Epoch 13/499\n",
      "----------\n",
      "train Loss: 0.00248030 Acc: 0.88691406\n",
      "val Loss: 0.00018428 Acc: 0.91870629\n",
      "Epoch 14 of 500 took 0.524s\n",
      "Epoch 14/499\n",
      "----------\n",
      "train Loss: 0.00233025 Acc: 0.89355469\n",
      "val Loss: 0.00017374 Acc: 0.9291958\n",
      "Epoch 15 of 500 took 0.521s\n",
      "Epoch 15/499\n",
      "----------\n",
      "train Loss: 0.00227741 Acc: 0.89863281\n",
      "val Loss: 0.00019487 Acc: 0.92132867\n",
      "Epoch 16 of 500 took 0.520s\n",
      "Epoch 16/499\n",
      "----------\n",
      "train Loss: 0.00234971 Acc: 0.89296875\n",
      "val Loss: 0.00014810 Acc: 0.93706294\n",
      "Epoch 17 of 500 took 0.522s\n",
      "Epoch 17/499\n",
      "----------\n",
      "train Loss: 0.00226474 Acc: 0.89970703\n",
      "val Loss: 0.00015274 Acc: 0.93968531\n",
      "Epoch 18 of 500 took 0.520s\n",
      "Epoch 18/499\n",
      "----------\n",
      "train Loss: 0.00208533 Acc: 0.90673828\n",
      "val Loss: 0.00017939 Acc: 0.9222028\n",
      "Epoch 19 of 500 took 0.568s\n",
      "Epoch 19/499\n",
      "----------\n",
      "train Loss: 0.00210624 Acc: 0.90634766\n",
      "val Loss: 0.00015914 Acc: 0.93706294\n",
      "Epoch 20 of 500 took 0.547s\n",
      "Epoch 20/499\n",
      "----------\n",
      "train Loss: 0.00210631 Acc: 0.90449219\n",
      "val Loss: 0.00014203 Acc: 0.93706294\n",
      "Epoch 21 of 500 took 0.523s\n",
      "Epoch 21/499\n",
      "----------\n",
      "train Loss: 0.00207641 Acc: 0.90888672\n",
      "val Loss: 0.00015805 Acc: 0.93181818\n",
      "Epoch 22 of 500 took 0.525s\n",
      "\n",
      "Training complete in 0m 12s\n",
      "Best val loss: 0.000170\n",
      "Session:  1\n",
      "<generator object Module.parameters at 0x7f58610c2660>\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump5/TSD/participant_0/best_state_0.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump5/TSD/participant_0/best_state_0.pt' (epoch 11)\n",
      "Epoch 0/499\n",
      "----------\n",
      "train Loss: 0.00283879 Acc: 0.87158203\n",
      "val Loss: 0.00087568 Acc: 0.94759825\n",
      "New best validation loss: 0.0008756817037882242\n",
      "Epoch 1 of 500 took 0.110s\n",
      "Epoch 1/499\n",
      "----------\n",
      "train Loss: 0.00235671 Acc: 0.89746094\n",
      "val Loss: 0.00107128 Acc: 0.92139738\n",
      "Epoch 2 of 500 took 0.109s\n",
      "Epoch 2/499\n",
      "----------\n",
      "train Loss: 0.00213356 Acc: 0.90136719\n",
      "val Loss: 0.00076015 Acc: 0.94759825\n",
      "New best validation loss: 0.0007601524154171673\n",
      "Epoch 3 of 500 took 0.110s\n",
      "Epoch 3/499\n",
      "----------\n",
      "train Loss: 0.00191824 Acc: 0.91015625\n",
      "val Loss: 0.00059848 Acc: 0.96069869\n",
      "New best validation loss: 0.0005984760528048053\n",
      "Epoch 4 of 500 took 0.133s\n",
      "Epoch 4/499\n",
      "----------\n",
      "train Loss: 0.00191496 Acc: 0.9140625\n",
      "val Loss: 0.00058989 Acc: 0.95196507\n",
      "Epoch 5 of 500 took 0.128s\n",
      "Epoch 5/499\n",
      "----------\n",
      "train Loss: 0.00203760 Acc: 0.90576172\n",
      "val Loss: 0.00071988 Acc: 0.95633188\n",
      "Epoch 6 of 500 took 0.137s\n",
      "Epoch 6/499\n",
      "----------\n",
      "train Loss: 0.00185165 Acc: 0.91210938\n",
      "val Loss: 0.00053227 Acc: 0.95633188\n",
      "Epoch 7 of 500 took 0.143s\n",
      "Epoch 7/499\n",
      "----------\n",
      "train Loss: 0.00155762 Acc: 0.93457031\n",
      "val Loss: 0.00056548 Acc: 0.94323144\n",
      "Epoch 8 of 500 took 0.148s\n",
      "Epoch 8/499\n",
      "----------\n",
      "train Loss: 0.00147663 Acc: 0.92675781\n",
      "val Loss: 0.00060469 Acc: 0.96943231\n",
      "Epoch 9 of 500 took 0.136s\n",
      "Epoch 9/499\n",
      "----------\n",
      "train Loss: 0.00145350 Acc: 0.93066406\n",
      "val Loss: 0.00050047 Acc: 0.97379913\n",
      "Epoch 10 of 500 took 0.135s\n",
      "Epoch 10/499\n",
      "----------\n",
      "train Loss: 0.00138576 Acc: 0.93994141\n",
      "val Loss: 0.00042033 Acc: 0.96943231\n",
      "New best validation loss: 0.0004203253828281919\n",
      "Epoch 11 of 500 took 0.136s\n",
      "Epoch 11/499\n",
      "----------\n",
      "train Loss: 0.00134749 Acc: 0.93994141\n",
      "val Loss: 0.00037914 Acc: 0.96943231\n",
      "Epoch 12 of 500 took 0.140s\n",
      "Epoch 12/499\n",
      "----------\n",
      "train Loss: 0.00132846 Acc: 0.94042969\n",
      "val Loss: 0.00052317 Acc: 0.96069869\n",
      "Epoch 13 of 500 took 0.128s\n",
      "Epoch 13/499\n",
      "----------\n",
      "train Loss: 0.00125029 Acc: 0.94091797\n",
      "val Loss: 0.00044799 Acc: 0.96069869\n",
      "Epoch 14 of 500 took 0.138s\n",
      "Epoch 14/499\n",
      "----------\n",
      "train Loss: 0.00136902 Acc: 0.93505859\n",
      "val Loss: 0.00039775 Acc: 0.97816594\n",
      "Epoch 15 of 500 took 0.135s\n",
      "Epoch 15/499\n",
      "----------\n",
      "train Loss: 0.00146138 Acc: 0.93115234\n",
      "val Loss: 0.00056778 Acc: 0.94323144\n",
      "Epoch 16 of 500 took 0.136s\n",
      "Epoch 16/499\n",
      "----------\n",
      "train Loss: 0.00117674 Acc: 0.95117188\n",
      "val Loss: 0.00044310 Acc: 0.96943231\n",
      "Epoch 17 of 500 took 0.134s\n",
      "Epoch 17/499\n",
      "----------\n",
      "train Loss: 0.00121184 Acc: 0.94726562\n",
      "val Loss: 0.00057970 Acc: 0.93886463\n",
      "Epoch    18: reducing learning rate of group 0 to 5.0300e-04.\n",
      "Epoch 18 of 500 took 0.142s\n",
      "Epoch 18/499\n",
      "----------\n",
      "train Loss: 0.00115222 Acc: 0.94921875\n",
      "val Loss: 0.00034942 Acc: 0.98253275\n",
      "Epoch 19 of 500 took 0.128s\n",
      "Epoch 19/499\n",
      "----------\n",
      "train Loss: 0.00092860 Acc: 0.95947266\n",
      "val Loss: 0.00037359 Acc: 0.97379913\n",
      "Epoch 20 of 500 took 0.136s\n",
      "Epoch 20/499\n",
      "----------\n",
      "train Loss: 0.00092784 Acc: 0.95654297\n",
      "val Loss: 0.00030125 Acc: 0.97816594\n",
      "New best validation loss: 0.00030124913006370245\n",
      "Epoch 21 of 500 took 0.135s\n",
      "Epoch 21/499\n",
      "----------\n",
      "train Loss: 0.00084317 Acc: 0.96386719\n",
      "val Loss: 0.00029462 Acc: 0.97816594\n",
      "Epoch 22 of 500 took 0.125s\n",
      "Epoch 22/499\n",
      "----------\n",
      "train Loss: 0.00078243 Acc: 0.96533203\n",
      "val Loss: 0.00030561 Acc: 0.97816594\n",
      "Epoch 23 of 500 took 0.130s\n",
      "Epoch 23/499\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.00083889 Acc: 0.95751953\n",
      "val Loss: 0.00026176 Acc: 0.97379913\n",
      "Epoch 24 of 500 took 0.118s\n",
      "Epoch 24/499\n",
      "----------\n",
      "train Loss: 0.00081533 Acc: 0.96582031\n",
      "val Loss: 0.00027236 Acc: 0.97379913\n",
      "Epoch 25 of 500 took 0.106s\n",
      "Epoch 25/499\n",
      "----------\n",
      "train Loss: 0.00074101 Acc: 0.96679688\n",
      "val Loss: 0.00024066 Acc: 0.98689956\n",
      "Epoch 26 of 500 took 0.108s\n",
      "Epoch 26/499\n",
      "----------\n",
      "train Loss: 0.00072684 Acc: 0.96728516\n",
      "val Loss: 0.00027464 Acc: 0.98253275\n",
      "Epoch 27 of 500 took 0.106s\n",
      "Epoch 27/499\n",
      "----------\n",
      "train Loss: 0.00074109 Acc: 0.96630859\n",
      "val Loss: 0.00025871 Acc: 0.98253275\n",
      "Epoch 28 of 500 took 0.108s\n",
      "Epoch 28/499\n",
      "----------\n",
      "train Loss: 0.00083892 Acc: 0.96826172\n",
      "val Loss: 0.00024510 Acc: 0.97816594\n",
      "Epoch 29 of 500 took 0.107s\n",
      "Epoch 29/499\n",
      "----------\n",
      "train Loss: 0.00065410 Acc: 0.96923828\n",
      "val Loss: 0.00021956 Acc: 0.98689956\n",
      "Epoch 30 of 500 took 0.110s\n",
      "Epoch 30/499\n",
      "----------\n",
      "train Loss: 0.00067853 Acc: 0.97216797\n",
      "val Loss: 0.00030607 Acc: 0.97816594\n",
      "Epoch 31 of 500 took 0.105s\n",
      "Epoch 31/499\n",
      "----------\n",
      "train Loss: 0.00073333 Acc: 0.96826172\n",
      "val Loss: 0.00022419 Acc: 0.97816594\n",
      "Epoch 32 of 500 took 0.107s\n",
      "\n",
      "Training complete in 0m 4s\n",
      "Best val loss: 0.000301\n",
      "Session:  2\n",
      "<generator object Module.parameters at 0x7f58610c2900>\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump5/TSD/participant_0/best_state_1.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump5/TSD/participant_0/best_state_1.pt' (epoch 21)\n",
      "Epoch 0/499\n",
      "----------\n",
      "train Loss: 0.00348049 Acc: 0.86279297\n",
      "val Loss: 0.00138954 Acc: 0.92576419\n",
      "New best validation loss: 0.0013895431720533746\n",
      "Epoch 1 of 500 took 0.111s\n",
      "Epoch 1/499\n",
      "----------\n",
      "train Loss: 0.00273656 Acc: 0.87890625\n",
      "val Loss: 0.00133586 Acc: 0.92139738\n",
      "Epoch 2 of 500 took 0.110s\n",
      "Epoch 2/499\n",
      "----------\n",
      "train Loss: 0.00233193 Acc: 0.88916016\n",
      "val Loss: 0.00158706 Acc: 0.89082969\n",
      "Epoch 3 of 500 took 0.106s\n",
      "Epoch 3/499\n",
      "----------\n",
      "train Loss: 0.00214261 Acc: 0.89941406\n",
      "val Loss: 0.00114653 Acc: 0.93449782\n",
      "New best validation loss: 0.0011465326927634827\n",
      "Epoch 4 of 500 took 0.111s\n",
      "Epoch 4/499\n",
      "----------\n",
      "train Loss: 0.00209503 Acc: 0.89941406\n",
      "val Loss: 0.00117207 Acc: 0.93449782\n",
      "Epoch 5 of 500 took 0.107s\n",
      "Epoch 5/499\n",
      "----------\n",
      "train Loss: 0.00207769 Acc: 0.89990234\n",
      "val Loss: 0.00123302 Acc: 0.94759825\n",
      "Epoch 6 of 500 took 0.110s\n",
      "Epoch 6/499\n",
      "----------\n",
      "train Loss: 0.00203719 Acc: 0.90380859\n",
      "val Loss: 0.00100256 Acc: 0.96069869\n",
      "New best validation loss: 0.0010025581938731098\n",
      "Epoch 7 of 500 took 0.107s\n",
      "Epoch 7/499\n",
      "----------\n",
      "train Loss: 0.00165403 Acc: 0.92041016\n",
      "val Loss: 0.00099418 Acc: 0.95633188\n",
      "Epoch 8 of 500 took 0.114s\n",
      "Epoch 8/499\n",
      "----------\n",
      "train Loss: 0.00188486 Acc: 0.91992188\n",
      "val Loss: 0.00089927 Acc: 0.95633188\n",
      "New best validation loss: 0.0008992663377237112\n",
      "Epoch 9 of 500 took 0.107s\n",
      "Epoch 9/499\n",
      "----------\n",
      "train Loss: 0.00165372 Acc: 0.92578125\n",
      "val Loss: 0.00097778 Acc: 0.93886463\n",
      "Epoch 10 of 500 took 0.114s\n",
      "Epoch 10/499\n",
      "----------\n",
      "train Loss: 0.00166290 Acc: 0.92285156\n",
      "val Loss: 0.00088138 Acc: 0.94323144\n",
      "Epoch 11 of 500 took 0.106s\n",
      "Epoch 11/499\n",
      "----------\n",
      "train Loss: 0.00144063 Acc: 0.93896484\n",
      "val Loss: 0.00079732 Acc: 0.95196507\n",
      "New best validation loss: 0.0007973235246916525\n",
      "Epoch 12 of 500 took 0.110s\n",
      "Epoch 12/499\n",
      "----------\n",
      "train Loss: 0.00145169 Acc: 0.93505859\n",
      "val Loss: 0.00094129 Acc: 0.930131\n",
      "Epoch 13 of 500 took 0.112s\n",
      "Epoch 13/499\n",
      "----------\n",
      "train Loss: 0.00145071 Acc: 0.93261719\n",
      "val Loss: 0.00091060 Acc: 0.92139738\n",
      "Epoch 14 of 500 took 0.109s\n",
      "Epoch 14/499\n",
      "----------\n",
      "train Loss: 0.00152177 Acc: 0.93310547\n",
      "val Loss: 0.00080163 Acc: 0.97379913\n",
      "Epoch 15 of 500 took 0.106s\n",
      "Epoch 15/499\n",
      "----------\n",
      "train Loss: 0.00148355 Acc: 0.93261719\n",
      "val Loss: 0.00081982 Acc: 0.94759825\n",
      "Epoch 16 of 500 took 0.116s\n",
      "Epoch 16/499\n",
      "----------\n",
      "train Loss: 0.00139489 Acc: 0.93408203\n",
      "val Loss: 0.00070482 Acc: 0.9650655\n",
      "Epoch 17 of 500 took 0.106s\n",
      "Epoch 17/499\n",
      "----------\n",
      "train Loss: 0.00132128 Acc: 0.93847656\n",
      "val Loss: 0.00075863 Acc: 0.95196507\n",
      "Epoch 18 of 500 took 0.109s\n",
      "Epoch 18/499\n",
      "----------\n",
      "train Loss: 0.00119036 Acc: 0.9453125\n",
      "val Loss: 0.00102465 Acc: 0.92139738\n",
      "Epoch 19 of 500 took 0.110s\n",
      "Epoch 19/499\n",
      "----------\n",
      "train Loss: 0.00116332 Acc: 0.94726562\n",
      "val Loss: 0.00076944 Acc: 0.95196507\n",
      "Epoch 20 of 500 took 0.109s\n",
      "Epoch 20/499\n",
      "----------\n",
      "train Loss: 0.00127601 Acc: 0.9375\n",
      "val Loss: 0.00132076 Acc: 0.93449782\n",
      "Epoch 21 of 500 took 0.106s\n",
      "Epoch 21/499\n",
      "----------\n",
      "train Loss: 0.00127691 Acc: 0.94140625\n",
      "val Loss: 0.00072420 Acc: 0.9650655\n",
      "Epoch 22 of 500 took 0.110s\n",
      "Epoch 22/499\n",
      "----------\n",
      "train Loss: 0.00127809 Acc: 0.94238281\n",
      "val Loss: 0.00068689 Acc: 0.94759825\n",
      "New best validation loss: 0.0006868904334488915\n",
      "Epoch 23 of 500 took 0.112s\n",
      "Epoch 23/499\n",
      "----------\n",
      "train Loss: 0.00126792 Acc: 0.93994141\n",
      "val Loss: 0.00057735 Acc: 0.97379913\n",
      "New best validation loss: 0.0005773487439842724\n",
      "Epoch 24 of 500 took 0.110s\n",
      "Epoch 24/499\n",
      "----------\n",
      "train Loss: 0.00105934 Acc: 0.95117188\n",
      "val Loss: 0.00049543 Acc: 0.97379913\n",
      "Epoch 25 of 500 took 0.106s\n",
      "Epoch 25/499\n",
      "----------\n",
      "train Loss: 0.00090310 Acc: 0.95849609\n",
      "val Loss: 0.00069765 Acc: 0.96069869\n",
      "Epoch 26 of 500 took 0.110s\n",
      "Epoch 26/499\n",
      "----------\n",
      "train Loss: 0.00115089 Acc: 0.93847656\n",
      "val Loss: 0.00081220 Acc: 0.95633188\n",
      "Epoch 27 of 500 took 0.107s\n",
      "Epoch 27/499\n",
      "----------\n",
      "train Loss: 0.00102086 Acc: 0.95263672\n",
      "val Loss: 0.00046967 Acc: 0.97816594\n",
      "New best validation loss: 0.0004696697647394572\n",
      "Epoch 28 of 500 took 0.111s\n",
      "Epoch 28/499\n",
      "----------\n",
      "train Loss: 0.00101719 Acc: 0.95117188\n",
      "val Loss: 0.00048649 Acc: 0.97816594\n",
      "Epoch 29 of 500 took 0.108s\n",
      "Epoch 29/499\n",
      "----------\n",
      "train Loss: 0.00105032 Acc: 0.95019531\n",
      "val Loss: 0.00078954 Acc: 0.93886463\n",
      "Epoch 30 of 500 took 0.109s\n",
      "Epoch 30/499\n",
      "----------\n",
      "train Loss: 0.00100957 Acc: 0.95263672\n",
      "val Loss: 0.00080956 Acc: 0.96069869\n",
      "Epoch 31 of 500 took 0.106s\n",
      "Epoch 31/499\n",
      "----------\n",
      "train Loss: 0.00097279 Acc: 0.95410156\n",
      "val Loss: 0.00058204 Acc: 0.96069869\n",
      "Epoch 32 of 500 took 0.111s\n",
      "Epoch 32/499\n",
      "----------\n",
      "train Loss: 0.00101344 Acc: 0.95410156\n",
      "val Loss: 0.00048610 Acc: 0.98253275\n",
      "Epoch 33 of 500 took 0.106s\n",
      "Epoch 33/499\n",
      "----------\n",
      "train Loss: 0.00095035 Acc: 0.95751953\n",
      "val Loss: 0.00055557 Acc: 0.9650655\n",
      "Epoch    34: reducing learning rate of group 0 to 5.0300e-04.\n",
      "Epoch 34 of 500 took 0.110s\n",
      "Epoch 34/499\n",
      "----------\n",
      "train Loss: 0.00083983 Acc: 0.96240234\n",
      "val Loss: 0.00044846 Acc: 0.98253275\n",
      "Epoch 35 of 500 took 0.106s\n",
      "Epoch 35/499\n",
      "----------\n",
      "train Loss: 0.00066400 Acc: 0.96826172\n",
      "val Loss: 0.00049172 Acc: 0.98253275\n",
      "Epoch 36 of 500 took 0.110s\n",
      "Epoch 36/499\n",
      "----------\n",
      "train Loss: 0.00068637 Acc: 0.97167969\n",
      "val Loss: 0.00051969 Acc: 0.97816594\n",
      "Epoch 37 of 500 took 0.107s\n",
      "Epoch 37/499\n",
      "----------\n",
      "train Loss: 0.00064146 Acc: 0.97363281\n",
      "val Loss: 0.00043271 Acc: 0.98689956\n",
      "Epoch 38 of 500 took 0.112s\n",
      "Epoch 38/499\n",
      "----------\n",
      "train Loss: 0.00064620 Acc: 0.97412109\n",
      "val Loss: 0.00046847 Acc: 0.97379913\n",
      "Epoch 39 of 500 took 0.105s\n",
      "\n",
      "Training complete in 0m 4s\n",
      "Best val loss: 0.000470\n",
      "Session:  3\n",
      "<generator object Module.parameters at 0x7f58610c2900>\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump5/TSD/participant_0/best_state_2.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump5/TSD/participant_0/best_state_2.pt' (epoch 28)\n",
      "Epoch 0/499\n",
      "----------\n",
      "train Loss: 0.00461659 Acc: 0.83935547\n",
      "val Loss: 0.00125737 Acc: 0.89956332\n",
      "New best validation loss: 0.0012573718243811328\n",
      "Epoch 1 of 500 took 0.112s\n",
      "Epoch 1/499\n",
      "----------\n",
      "train Loss: 0.00303358 Acc: 0.87402344\n",
      "val Loss: 0.00102907 Acc: 0.91703057\n",
      "New best validation loss: 0.0010290724351416508\n",
      "Epoch 2 of 500 took 0.108s\n",
      "Epoch 2/499\n",
      "----------\n",
      "train Loss: 0.00261469 Acc: 0.87939453\n",
      "val Loss: 0.00114950 Acc: 0.91703057\n",
      "Epoch 3 of 500 took 0.110s\n",
      "Epoch 3/499\n",
      "----------\n",
      "train Loss: 0.00234083 Acc: 0.89404297\n",
      "val Loss: 0.00117610 Acc: 0.89956332\n",
      "Epoch 4 of 500 took 0.108s\n",
      "Epoch 4/499\n",
      "----------\n",
      "train Loss: 0.00228190 Acc: 0.89355469\n",
      "val Loss: 0.00092356 Acc: 0.92139738\n",
      "New best validation loss: 0.0009235598504803587\n",
      "Epoch 5 of 500 took 0.112s\n",
      "Epoch 5/499\n",
      "----------\n",
      "train Loss: 0.00204789 Acc: 0.90917969\n",
      "val Loss: 0.00076930 Acc: 0.92139738\n",
      "New best validation loss: 0.0007693042010719599\n",
      "Epoch 6 of 500 took 0.107s\n",
      "Epoch 6/499\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.00193523 Acc: 0.90576172\n",
      "val Loss: 0.00081714 Acc: 0.90829694\n",
      "Epoch 7 of 500 took 0.110s\n",
      "Epoch 7/499\n",
      "----------\n",
      "train Loss: 0.00200798 Acc: 0.91162109\n",
      "val Loss: 0.00078526 Acc: 0.90393013\n",
      "Epoch 8 of 500 took 0.108s\n",
      "Epoch 8/499\n",
      "----------\n",
      "train Loss: 0.00195299 Acc: 0.91552734\n",
      "val Loss: 0.00110107 Acc: 0.90829694\n",
      "Epoch 9 of 500 took 0.109s\n",
      "Epoch 9/499\n",
      "----------\n",
      "train Loss: 0.00175766 Acc: 0.91748047\n",
      "val Loss: 0.00089880 Acc: 0.92576419\n",
      "Epoch 10 of 500 took 0.107s\n",
      "Epoch 10/499\n",
      "----------\n",
      "train Loss: 0.00163436 Acc: 0.92822266\n",
      "val Loss: 0.00054111 Acc: 0.95196507\n",
      "New best validation loss: 0.0005411119179954695\n",
      "Epoch 11 of 500 took 0.110s\n",
      "Epoch 11/499\n",
      "----------\n",
      "train Loss: 0.00181145 Acc: 0.92041016\n",
      "val Loss: 0.00077278 Acc: 0.92139738\n",
      "Epoch 12 of 500 took 0.106s\n",
      "Epoch 12/499\n",
      "----------\n",
      "train Loss: 0.00178472 Acc: 0.91015625\n",
      "val Loss: 0.00077553 Acc: 0.930131\n",
      "Epoch 13 of 500 took 0.109s\n",
      "Epoch 13/499\n",
      "----------\n",
      "train Loss: 0.00142295 Acc: 0.93017578\n",
      "val Loss: 0.00089605 Acc: 0.91266376\n",
      "Epoch 14 of 500 took 0.106s\n",
      "Epoch 14/499\n",
      "----------\n",
      "train Loss: 0.00154692 Acc: 0.92480469\n",
      "val Loss: 0.00071621 Acc: 0.93449782\n",
      "Epoch 15 of 500 took 0.116s\n",
      "Epoch 15/499\n",
      "----------\n",
      "train Loss: 0.00145348 Acc: 0.93261719\n",
      "val Loss: 0.00084885 Acc: 0.93886463\n",
      "Epoch 16 of 500 took 0.107s\n",
      "Epoch 16/499\n",
      "----------\n",
      "train Loss: 0.00142911 Acc: 0.93505859\n",
      "val Loss: 0.00087654 Acc: 0.92139738\n",
      "Epoch    17: reducing learning rate of group 0 to 5.0300e-04.\n",
      "Epoch 17 of 500 took 0.110s\n",
      "Epoch 17/499\n",
      "----------\n",
      "train Loss: 0.00136364 Acc: 0.93701172\n",
      "val Loss: 0.00053042 Acc: 0.95196507\n",
      "Epoch 18 of 500 took 0.106s\n",
      "Epoch 18/499\n",
      "----------\n",
      "train Loss: 0.00121090 Acc: 0.94873047\n",
      "val Loss: 0.00074195 Acc: 0.930131\n",
      "Epoch 19 of 500 took 0.109s\n",
      "Epoch 19/499\n",
      "----------\n",
      "train Loss: 0.00119624 Acc: 0.94873047\n",
      "val Loss: 0.00051134 Acc: 0.94759825\n",
      "Epoch 20 of 500 took 0.106s\n",
      "Epoch 20/499\n",
      "----------\n",
      "train Loss: 0.00121481 Acc: 0.94482422\n",
      "val Loss: 0.00052485 Acc: 0.95196507\n",
      "Epoch 21 of 500 took 0.109s\n",
      "Epoch 21/499\n",
      "----------\n",
      "train Loss: 0.00099298 Acc: 0.9609375\n",
      "val Loss: 0.00045504 Acc: 0.95633188\n",
      "Epoch 22 of 500 took 0.106s\n",
      "\n",
      "Training complete in 0m 2s\n",
      "Best val loss: 0.000541\n",
      "Session:  4\n",
      "<generator object Module.parameters at 0x7f58610c2900>\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump5/TSD/participant_0/best_state_3.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump5/TSD/participant_0/best_state_3.pt' (epoch 11)\n",
      "Epoch 0/499\n",
      "----------\n",
      "train Loss: 0.00336180 Acc: 0.85986328\n",
      "val Loss: 0.00115969 Acc: 0.89082969\n",
      "New best validation loss: 0.001159690613309369\n",
      "Epoch 1 of 500 took 0.114s\n",
      "Epoch 1/499\n",
      "----------\n",
      "train Loss: 0.00242849 Acc: 0.88525391\n",
      "val Loss: 0.00099362 Acc: 0.90393013\n",
      "New best validation loss: 0.0009936196444857068\n",
      "Epoch 2 of 500 took 0.108s\n",
      "Epoch 2/499\n",
      "----------\n",
      "train Loss: 0.00223152 Acc: 0.89453125\n",
      "val Loss: 0.00082474 Acc: 0.89519651\n",
      "New best validation loss: 0.0008247440038289566\n",
      "Epoch 3 of 500 took 0.111s\n",
      "Epoch 3/499\n",
      "----------\n",
      "train Loss: 0.00229841 Acc: 0.89550781\n",
      "val Loss: 0.00088015 Acc: 0.92139738\n",
      "Epoch 4 of 500 took 0.107s\n",
      "Epoch 4/499\n",
      "----------\n",
      "train Loss: 0.00195923 Acc: 0.90917969\n",
      "val Loss: 0.00080718 Acc: 0.92576419\n",
      "Epoch 5 of 500 took 0.111s\n",
      "Epoch 5/499\n",
      "----------\n",
      "train Loss: 0.00205888 Acc: 0.90722656\n",
      "val Loss: 0.00072281 Acc: 0.94759825\n",
      "New best validation loss: 0.0007228109513828327\n",
      "Epoch 6 of 500 took 0.107s\n",
      "Epoch 6/499\n",
      "----------\n",
      "train Loss: 0.00175864 Acc: 0.91601562\n",
      "val Loss: 0.00089150 Acc: 0.91266376\n",
      "Epoch 7 of 500 took 0.110s\n",
      "Epoch 7/499\n",
      "----------\n",
      "train Loss: 0.00182807 Acc: 0.9140625\n",
      "val Loss: 0.00078310 Acc: 0.93886463\n",
      "Epoch 8 of 500 took 0.106s\n",
      "Epoch 8/499\n",
      "----------\n",
      "train Loss: 0.00170763 Acc: 0.91992188\n",
      "val Loss: 0.00083594 Acc: 0.93449782\n",
      "Epoch 9 of 500 took 0.110s\n",
      "Epoch 9/499\n",
      "----------\n",
      "train Loss: 0.00184021 Acc: 0.91748047\n",
      "val Loss: 0.00096354 Acc: 0.91703057\n",
      "Epoch 10 of 500 took 0.107s\n",
      "Epoch 10/499\n",
      "----------\n",
      "train Loss: 0.00173058 Acc: 0.91796875\n",
      "val Loss: 0.00051771 Acc: 0.96069869\n",
      "New best validation loss: 0.0005177067487000378\n",
      "Epoch 11 of 500 took 0.111s\n",
      "Epoch 11/499\n",
      "----------\n",
      "train Loss: 0.00173465 Acc: 0.91894531\n",
      "val Loss: 0.00047185 Acc: 0.9650655\n",
      "Epoch 12 of 500 took 0.106s\n",
      "Epoch 12/499\n",
      "----------\n",
      "train Loss: 0.00154711 Acc: 0.92333984\n",
      "val Loss: 0.00052883 Acc: 0.93886463\n",
      "Epoch 13 of 500 took 0.110s\n",
      "Epoch 13/499\n",
      "----------\n",
      "train Loss: 0.00139192 Acc: 0.93457031\n",
      "val Loss: 0.00053101 Acc: 0.95633188\n",
      "Epoch 14 of 500 took 0.111s\n",
      "Epoch 14/499\n",
      "----------\n",
      "train Loss: 0.00137463 Acc: 0.93652344\n",
      "val Loss: 0.00037942 Acc: 0.96943231\n",
      "New best validation loss: 0.00037941818154014356\n",
      "Epoch 15 of 500 took 0.111s\n",
      "Epoch 15/499\n",
      "----------\n",
      "train Loss: 0.00118746 Acc: 0.94433594\n",
      "val Loss: 0.00050121 Acc: 0.95196507\n",
      "Epoch 16 of 500 took 0.106s\n",
      "Epoch 16/499\n",
      "----------\n",
      "train Loss: 0.00122103 Acc: 0.94580078\n",
      "val Loss: 0.00049033 Acc: 0.96069869\n",
      "Epoch 17 of 500 took 0.110s\n",
      "Epoch 17/499\n",
      "----------\n",
      "train Loss: 0.00125621 Acc: 0.93994141\n",
      "val Loss: 0.00101887 Acc: 0.91266376\n",
      "Epoch 18 of 500 took 0.131s\n",
      "Epoch 18/499\n",
      "----------\n",
      "train Loss: 0.00122742 Acc: 0.9375\n",
      "val Loss: 0.00046156 Acc: 0.96069869\n",
      "Epoch 19 of 500 took 0.133s\n",
      "Epoch 19/499\n",
      "----------\n",
      "train Loss: 0.00109126 Acc: 0.95263672\n",
      "val Loss: 0.00030964 Acc: 0.97816594\n",
      "Epoch 20 of 500 took 0.122s\n",
      "Epoch 20/499\n",
      "----------\n",
      "train Loss: 0.00115566 Acc: 0.94580078\n",
      "val Loss: 0.00062529 Acc: 0.94759825\n",
      "Epoch 21 of 500 took 0.109s\n",
      "Epoch 21/499\n",
      "----------\n",
      "train Loss: 0.00117860 Acc: 0.94384766\n",
      "val Loss: 0.00065990 Acc: 0.93449782\n",
      "Epoch 22 of 500 took 0.106s\n",
      "Epoch 22/499\n",
      "----------\n",
      "train Loss: 0.00130323 Acc: 0.94091797\n",
      "val Loss: 0.00043980 Acc: 0.97816594\n",
      "Epoch 23 of 500 took 0.114s\n",
      "Epoch 23/499\n",
      "----------\n",
      "train Loss: 0.00109552 Acc: 0.95166016\n",
      "val Loss: 0.00035402 Acc: 0.96943231\n",
      "Epoch 24 of 500 took 0.105s\n",
      "Epoch 24/499\n",
      "----------\n",
      "train Loss: 0.00115224 Acc: 0.94677734\n",
      "val Loss: 0.00046772 Acc: 0.96069869\n",
      "Epoch 25 of 500 took 0.109s\n",
      "Epoch 25/499\n",
      "----------\n",
      "train Loss: 0.00122285 Acc: 0.94287109\n",
      "val Loss: 0.00045464 Acc: 0.96069869\n",
      "Epoch    26: reducing learning rate of group 0 to 5.0300e-04.\n",
      "Epoch 26 of 500 took 0.107s\n",
      "\n",
      "Training complete in 0m 3s\n",
      "Best val loss: 0.000379\n",
      "Session:  5\n",
      "<generator object Module.parameters at 0x7f58613385f0>\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump5/TSD/participant_0/best_state_4.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump5/TSD/participant_0/best_state_4.pt' (epoch 15)\n",
      "Epoch 0/499\n",
      "----------\n",
      "train Loss: 0.00368940 Acc: 0.84814453\n",
      "val Loss: 0.00178691 Acc: 0.8558952\n",
      "New best validation loss: 0.0017869070367521594\n",
      "Epoch 1 of 500 took 0.112s\n",
      "Epoch 1/499\n",
      "----------\n",
      "train Loss: 0.00243473 Acc: 0.88476562\n",
      "val Loss: 0.00092090 Acc: 0.90393013\n",
      "New best validation loss: 0.0009208959913670235\n",
      "Epoch 2 of 500 took 0.110s\n",
      "Epoch 2/499\n",
      "----------\n",
      "train Loss: 0.00215285 Acc: 0.90087891\n",
      "val Loss: 0.00083878 Acc: 0.92576419\n",
      "Epoch 3 of 500 took 0.110s\n",
      "Epoch 3/499\n",
      "----------\n",
      "train Loss: 0.00214655 Acc: 0.90185547\n",
      "val Loss: 0.00086977 Acc: 0.930131\n",
      "Epoch 4 of 500 took 0.106s\n",
      "Epoch 4/499\n",
      "----------\n",
      "train Loss: 0.00193876 Acc: 0.90966797\n",
      "val Loss: 0.00072879 Acc: 0.92576419\n",
      "New best validation loss: 0.0007287887897033358\n",
      "Epoch 5 of 500 took 0.112s\n",
      "Epoch 5/499\n",
      "----------\n",
      "train Loss: 0.00186404 Acc: 0.90234375\n",
      "val Loss: 0.00105256 Acc: 0.89519651\n",
      "Epoch 6 of 500 took 0.109s\n",
      "Epoch 6/499\n",
      "----------\n",
      "train Loss: 0.00169871 Acc: 0.91699219\n",
      "val Loss: 0.00086923 Acc: 0.90829694\n",
      "Epoch 7 of 500 took 0.109s\n",
      "Epoch 7/499\n",
      "----------\n",
      "train Loss: 0.00168392 Acc: 0.92431641\n",
      "val Loss: 0.00059667 Acc: 0.95633188\n",
      "New best validation loss: 0.0005966741835706619\n",
      "Epoch 8 of 500 took 0.107s\n",
      "Epoch 8/499\n",
      "----------\n",
      "train Loss: 0.00179879 Acc: 0.92089844\n",
      "val Loss: 0.00067578 Acc: 0.94759825\n",
      "Epoch 9 of 500 took 0.109s\n",
      "Epoch 9/499\n",
      "----------\n",
      "train Loss: 0.00158579 Acc: 0.92871094\n",
      "val Loss: 0.00062854 Acc: 0.93886463\n",
      "Epoch 10 of 500 took 0.106s\n",
      "Epoch 10/499\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.00164055 Acc: 0.92138672\n",
      "val Loss: 0.00065289 Acc: 0.93886463\n",
      "Epoch 11 of 500 took 0.108s\n",
      "Epoch 11/499\n",
      "----------\n",
      "train Loss: 0.00150564 Acc: 0.92871094\n",
      "val Loss: 0.00061576 Acc: 0.95196507\n",
      "Epoch 12 of 500 took 0.113s\n",
      "Epoch 12/499\n",
      "----------\n",
      "train Loss: 0.00148788 Acc: 0.93017578\n",
      "val Loss: 0.00047445 Acc: 0.95633188\n",
      "New best validation loss: 0.0004744538508648435\n",
      "Epoch 13 of 500 took 0.111s\n",
      "Epoch 13/499\n",
      "----------\n",
      "train Loss: 0.00148121 Acc: 0.93066406\n",
      "val Loss: 0.00066965 Acc: 0.93886463\n",
      "Epoch 14 of 500 took 0.105s\n",
      "Epoch 14/499\n",
      "----------\n",
      "train Loss: 0.00147054 Acc: 0.93115234\n",
      "val Loss: 0.00063037 Acc: 0.93886463\n",
      "Epoch 15 of 500 took 0.113s\n",
      "Epoch 15/499\n",
      "----------\n",
      "train Loss: 0.00136328 Acc: 0.93945312\n",
      "val Loss: 0.00061014 Acc: 0.94323144\n",
      "Epoch 16 of 500 took 0.108s\n",
      "Epoch 16/499\n",
      "----------\n",
      "train Loss: 0.00124518 Acc: 0.94140625\n",
      "val Loss: 0.00044561 Acc: 0.96943231\n",
      "Epoch 17 of 500 took 0.109s\n",
      "Epoch 17/499\n",
      "----------\n",
      "train Loss: 0.00131231 Acc: 0.93554688\n",
      "val Loss: 0.00049020 Acc: 0.9650655\n",
      "Epoch 18 of 500 took 0.106s\n",
      "Epoch 18/499\n",
      "----------\n",
      "train Loss: 0.00124617 Acc: 0.94335938\n",
      "val Loss: 0.00035903 Acc: 0.9650655\n",
      "New best validation loss: 0.000359033685844538\n",
      "Epoch 19 of 500 took 0.112s\n",
      "Epoch 19/499\n",
      "----------\n",
      "train Loss: 0.00122801 Acc: 0.94677734\n",
      "val Loss: 0.00052131 Acc: 0.94759825\n",
      "Epoch 20 of 500 took 0.105s\n",
      "Epoch 20/499\n",
      "----------\n",
      "train Loss: 0.00127648 Acc: 0.93798828\n",
      "val Loss: 0.00069633 Acc: 0.93449782\n",
      "Epoch 21 of 500 took 0.109s\n",
      "Epoch 21/499\n",
      "----------\n",
      "train Loss: 0.00120942 Acc: 0.95019531\n",
      "val Loss: 0.00045415 Acc: 0.95633188\n",
      "Epoch 22 of 500 took 0.106s\n",
      "Epoch 22/499\n",
      "----------\n",
      "train Loss: 0.00119095 Acc: 0.94433594\n",
      "val Loss: 0.00035842 Acc: 0.97379913\n",
      "Epoch 23 of 500 took 0.108s\n",
      "Epoch 23/499\n",
      "----------\n",
      "train Loss: 0.00109496 Acc: 0.94873047\n",
      "val Loss: 0.00038607 Acc: 0.9650655\n",
      "Epoch 24 of 500 took 0.109s\n",
      "Epoch 24/499\n",
      "----------\n",
      "train Loss: 0.00095876 Acc: 0.953125\n",
      "val Loss: 0.00053841 Acc: 0.96069869\n",
      "Epoch 25 of 500 took 0.110s\n",
      "Epoch 25/499\n",
      "----------\n",
      "train Loss: 0.00094530 Acc: 0.95410156\n",
      "val Loss: 0.00028447 Acc: 0.97379913\n",
      "Epoch 26 of 500 took 0.108s\n",
      "Epoch 26/499\n",
      "----------\n",
      "train Loss: 0.00118705 Acc: 0.94287109\n",
      "val Loss: 0.00038702 Acc: 0.97379913\n",
      "Epoch 27 of 500 took 0.109s\n",
      "Epoch 27/499\n",
      "----------\n",
      "train Loss: 0.00100304 Acc: 0.953125\n",
      "val Loss: 0.00039060 Acc: 0.97816594\n",
      "Epoch 28 of 500 took 0.106s\n",
      "Epoch 28/499\n",
      "----------\n",
      "train Loss: 0.00094914 Acc: 0.95214844\n",
      "val Loss: 0.00037541 Acc: 0.95633188\n",
      "Epoch 29 of 500 took 0.110s\n",
      "Epoch 29/499\n",
      "----------\n",
      "train Loss: 0.00096229 Acc: 0.95507812\n",
      "val Loss: 0.00058856 Acc: 0.94759825\n",
      "Epoch 30 of 500 took 0.105s\n",
      "\n",
      "Training complete in 0m 3s\n",
      "Best val loss: 0.000359\n"
     ]
    }
   ],
   "source": [
    "train_fine_tuning(examples_datasets_train, labels_datasets_train,\n",
    "                  num_kernels=num_kernels, path_weight_to_save_to=path_TSD,\n",
    "                  number_of_classes=number_of_classes, \n",
    "                  number_of_cycles_total=number_of_cycles_total,\n",
    "                  number_of_cycle_for_first_training = number_of_cycle_for_first_training,\n",
    "                  batch_size=batch_size,\n",
    "                  feature_vector_input_length=feature_vector_input_length,\n",
    "                  learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET one participant_examples  (6,)\n",
      "   GET one training_index_examples  (20, 572, 252)  at  0\n",
      "   GOT one group XY  (11440, 252)    (11440,)\n",
      "       one group XY test  (2860, 252)    (2860, 252)\n",
      "       one group XY train (10296, 252)    (10296,)\n",
      "       one group XY valid (1144, 252)    (1144, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  1\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  2\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  3\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  4\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  5\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "dataloaders: \n",
      "   train  (1, 6)\n",
      "   valid  (1, 6)\n",
      "   test  (1, 6)\n",
      "0  SESSION   data =  2860\n",
      "Participant:  0  Accuracy:  0.9157342657342658\n",
      "1  SESSION   data =  572\n",
      "Participant:  0  Accuracy:  0.8146853146853147\n",
      "2  SESSION   data =  572\n",
      "Participant:  0  Accuracy:  0.8041958041958042\n",
      "3  SESSION   data =  572\n",
      "Participant:  0  Accuracy:  0.7290209790209791\n",
      "4  SESSION   data =  572\n",
      "Participant:  0  Accuracy:  0.6538461538461539\n",
      "5  SESSION   data =  572\n",
      "Participant:  0  Accuracy:  0.6730769230769231\n",
      "ACCURACY PARTICIPANT  0 :  [0.9157342657342658, 0.8146853146853147, 0.8041958041958042, 0.7290209790209791, 0.6538461538461539, 0.6730769230769231]\n",
      "[array([0.91573427, 0.81468531, 0.8041958 , 0.72902098, 0.65384615,\n",
      "       0.67307692])]\n",
      "OVERALL ACCURACY: 0.7650932400932401\n"
     ]
    }
   ],
   "source": [
    "algo_name = \"standard_TSD\"\n",
    "test_standard_model_on_training_sessions(examples_datasets_train, labels_datasets_train,\n",
    "                                  num_neurons=num_kernels, use_only_first_training=True,\n",
    "                                  path_weights=path_TSD,\n",
    "                                  feature_vector_input_length=feature_vector_input_length,\n",
    "                                  save_path = save_TSD, algo_name=algo_name,\n",
    "                                  number_of_cycles_total=number_of_cycles_total,\n",
    "                                  number_of_cycle_for_first_training = number_of_cycle_for_first_training,\n",
    "                                  number_of_classes=number_of_classes, cycle_for_test=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Day_0~4</th>\n",
       "      <td>0.915734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_5</th>\n",
       "      <td>0.814685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_6</th>\n",
       "      <td>0.804196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_7</th>\n",
       "      <td>0.729021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_8</th>\n",
       "      <td>0.653846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_9</th>\n",
       "      <td>0.673077</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Participant_5\n",
       "Day_0~4      0.915734\n",
       "Day_5        0.814685\n",
       "Day_6        0.804196\n",
       "Day_7        0.729021\n",
       "Day_8        0.653846\n",
       "Day_9        0.673077"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_filename = save_TSD + '/predictions_' + algo_name + \"_no_retraining.npy\"\n",
    "results = np.load(results_filename, allow_pickle=True)\n",
    "TSD_acc = results[0]\n",
    "TSD_acc_overall = np.mean(TSD_acc)\n",
    "index_participant_list = ['0~4', 5, 6, 7, 8, 9]\n",
    "TSD_df = pd.DataFrame(TSD_acc.transpose(), \n",
    "                       index = [f'Day_{i}' for i in index_participant_list],\n",
    "                        columns = ['Participant_5'])\n",
    "TSD_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "TSD_df.transpose().plot.bar(rot=0, figsize=(10,5))\n",
    "plt.title(\"TSD Accuracies\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TrainingsAndEvaluations.ForTrainingSessions.utils import get_gesture_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ground_truths  =  (1, 6)\n",
      "predictions =  (1, 6)\n",
      "index_participant_list  ['0~4', 5, 6, 7, 8, 9]\n",
      "accuracies_gestures =  (22, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Loc1_Sub5_Day0~4-&gt;0~4</th>\n",
       "      <th>Loc1_Sub5_Day0~4-&gt;5</th>\n",
       "      <th>Loc1_Sub5_Day0~4-&gt;6</th>\n",
       "      <th>Loc1_Sub5_Day0~4-&gt;7</th>\n",
       "      <th>Loc1_Sub5_Day0~4-&gt;8</th>\n",
       "      <th>Loc1_Sub5_Day0~4-&gt;9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M1</td>\n",
       "      <td>0.969231</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.384615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M2</td>\n",
       "      <td>0.946154</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.807692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M3</td>\n",
       "      <td>0.876923</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.076923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M4</td>\n",
       "      <td>0.869231</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>M5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.884615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>M6</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>M7</td>\n",
       "      <td>0.992308</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.884615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>M8</td>\n",
       "      <td>0.915385</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.884615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>M9</td>\n",
       "      <td>0.892308</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.653846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>M10</td>\n",
       "      <td>0.892308</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.961538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>M11</td>\n",
       "      <td>0.838462</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.807692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>M12</td>\n",
       "      <td>0.738462</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.153846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>M13</td>\n",
       "      <td>0.830769</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.884615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>M14</td>\n",
       "      <td>0.823077</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.076923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>M15</td>\n",
       "      <td>0.823077</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.538462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>M16</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>M17</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>M18</td>\n",
       "      <td>0.992308</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.730769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>M19</td>\n",
       "      <td>0.992308</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>0.346154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>M20</td>\n",
       "      <td>0.876923</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.884615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>M21</td>\n",
       "      <td>0.876923</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Mean</td>\n",
       "      <td>0.915734</td>\n",
       "      <td>0.814685</td>\n",
       "      <td>0.804196</td>\n",
       "      <td>0.729021</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>0.673077</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Loc1_Sub5_Day0~4->0~4  Loc1_Sub5_Day0~4->5  \\\n",
       "0          M0               1.000000             1.000000   \n",
       "1          M1               0.969231             1.000000   \n",
       "2          M2               0.946154             0.692308   \n",
       "3          M3               0.876923             1.000000   \n",
       "4          M4               0.869231             0.615385   \n",
       "5          M5               1.000000             0.769231   \n",
       "6          M6               1.000000             1.000000   \n",
       "7          M7               0.992308             0.884615   \n",
       "8          M8               0.915385             1.000000   \n",
       "9          M9               0.892308             0.846154   \n",
       "10        M10               0.892308             1.000000   \n",
       "11        M11               0.838462             0.692308   \n",
       "12        M12               0.738462             0.269231   \n",
       "13        M13               0.830769             0.846154   \n",
       "14        M14               0.823077             0.269231   \n",
       "15        M15               0.823077             0.692308   \n",
       "16        M16               1.000000             1.000000   \n",
       "17        M17               1.000000             1.000000   \n",
       "18        M18               0.992308             0.769231   \n",
       "19        M19               0.992308             0.730769   \n",
       "20        M20               0.876923             0.923077   \n",
       "21        M21               0.876923             0.923077   \n",
       "22       Mean               0.915734             0.814685   \n",
       "\n",
       "    Loc1_Sub5_Day0~4->6  Loc1_Sub5_Day0~4->7  Loc1_Sub5_Day0~4->8  \\\n",
       "0              1.000000             1.000000             1.000000   \n",
       "1              0.769231             0.500000             0.423077   \n",
       "2              0.730769             0.730769             0.884615   \n",
       "3              0.884615             0.230769             0.807692   \n",
       "4              0.000000             0.076923             0.076923   \n",
       "5              1.000000             0.730769             0.961538   \n",
       "6              0.538462             0.423077             0.461538   \n",
       "7              1.000000             1.000000             0.923077   \n",
       "8              0.961538             0.884615             0.923077   \n",
       "9              0.961538             0.884615             0.884615   \n",
       "10             1.000000             0.923077             1.000000   \n",
       "11             0.923077             0.884615             0.769231   \n",
       "12             0.461538             0.615385             0.076923   \n",
       "13             0.884615             1.000000             1.000000   \n",
       "14             0.307692             0.653846             0.000000   \n",
       "15             0.615385             0.307692             0.153846   \n",
       "16             1.000000             1.000000             1.000000   \n",
       "17             1.000000             1.000000             1.000000   \n",
       "18             0.769231             0.884615             0.038462   \n",
       "19             1.000000             0.384615             0.269231   \n",
       "20             0.884615             1.000000             0.730769   \n",
       "21             1.000000             0.923077             1.000000   \n",
       "22             0.804196             0.729021             0.653846   \n",
       "\n",
       "    Loc1_Sub5_Day0~4->9  \n",
       "0              1.000000  \n",
       "1              0.384615  \n",
       "2              0.807692  \n",
       "3              0.076923  \n",
       "4              0.000000  \n",
       "5              0.884615  \n",
       "6              1.000000  \n",
       "7              0.884615  \n",
       "8              0.884615  \n",
       "9              0.653846  \n",
       "10             0.961538  \n",
       "11             0.807692  \n",
       "12             0.153846  \n",
       "13             0.884615  \n",
       "14             0.076923  \n",
       "15             0.538462  \n",
       "16             1.000000  \n",
       "17             1.000000  \n",
       "18             0.730769  \n",
       "19             0.346154  \n",
       "20             0.884615  \n",
       "21             0.846154  \n",
       "22             0.673077  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truths = results[1]\n",
    "predictions = results[2]\n",
    "print(\"ground_truths  = \", np.shape(ground_truths))\n",
    "print(\"predictions = \", np.shape(predictions))\n",
    "m_name = \"Loc1_Sub\"\n",
    "n_name = \"Day0~4->\"\n",
    "df = get_gesture_accuracies(ground_truths, predictions, number_of_classes=number_of_classes, \n",
    "                            m_name=m_name, n_name=n_name, path=save_TSD, algo_name=algo_name,\n",
    "                           index_participant_list_customized=index_participant_list,\n",
    "                           lump_day_at_participant=5)\n",
    "df = pd.read_csv(save_TSD+'/'+algo_name+'.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. DANN\n",
    "* `train_DANN`: train DANN model using the first set of training weights from base model\n",
    "    * num_sessions-1 sets of training weights will be saved\n",
    "* `test_DANN_on_training_sessions`: test DANN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TrainingsAndEvaluations.ForTrainingSessions.train_tsd_dnn_DA import train_DANN, test_DANN_on_training_sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET one participant_examples  (6,)\n",
      "   GET one training_index_examples  (20, 572, 252)  at  0\n",
      "   GOT one group XY  (11440, 252)    (11440,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (10296, 252)    (10296,)\n",
      "       one group XY valid (1144, 252)    (1144, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  1\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  2\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  3\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  4\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  5\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "dataloaders: \n",
      "   train  (1, 6)\n",
      "   valid  (1, 6)\n",
      "   test  (1, 0)\n",
      "SHAPE SESSIONS:  (6,)\n",
      "()\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump5/TSD/participant_0/best_state_0.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump5/TSD/participant_0/best_state_0.pt' (epoch 11)\n",
      "STARTING TRAINING\n",
      "Accuracy source 0.890625, main loss classifier 0.233914, source classification loss 0.313052, loss domain distinction 0.235487, accuracy domain distinction 0.495605\n",
      "VALIDATION Loss: 0.23055486 Acc: 0.90734266\n",
      "New best validation loss:  0.23055486381053925\n",
      "Epoch 1 of 500 took 0.237s\n",
      "Accuracy source 0.880371, main loss classifier 0.244716, source classification loss 0.342819, loss domain distinction 0.190745, accuracy domain distinction 0.499756\n",
      "VALIDATION Loss: 0.25153393 Acc: 0.90384615\n",
      "Epoch 2 of 500 took 0.236s\n",
      "Accuracy source 0.873047, main loss classifier 0.250416, source classification loss 0.355861, loss domain distinction 0.190824, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.21075368 Acc: 0.91783217\n",
      "New best validation loss:  0.2107536792755127\n",
      "Epoch 3 of 500 took 0.236s\n",
      "Accuracy source 0.870605, main loss classifier 0.251162, source classification loss 0.358293, loss domain distinction 0.189117, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.25624886 Acc: 0.90122378\n",
      "Epoch 4 of 500 took 0.247s\n",
      "Accuracy source 0.873535, main loss classifier 0.244663, source classification loss 0.347067, loss domain distinction 0.188928, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.21245193 Acc: 0.92482517\n",
      "Epoch 5 of 500 took 0.265s\n",
      "Accuracy source 0.860840, main loss classifier 0.264428, source classification loss 0.387693, loss domain distinction 0.186661, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.20548280 Acc: 0.93181818\n",
      "Epoch    17: reducing learning rate of group 0 to 5.0300e-04.\n",
      "New best validation loss:  0.20548279583454132\n",
      "Epoch 6 of 500 took 0.238s\n",
      "Accuracy source 0.884766, main loss classifier 0.232527, source classification loss 0.327768, loss domain distinction 0.186032, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.21715580 Acc: 0.91958042\n",
      "Epoch 7 of 500 took 0.234s\n",
      "Accuracy source 0.903320, main loss classifier 0.213604, source classification loss 0.289856, loss domain distinction 0.185142, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.21508001 Acc: 0.91783217\n",
      "Epoch 8 of 500 took 0.239s\n",
      "Accuracy source 0.896484, main loss classifier 0.216935, source classification loss 0.296414, loss domain distinction 0.184200, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.20435484 Acc: 0.92307692\n",
      "New best validation loss:  0.20435483753681183\n",
      "Epoch 9 of 500 took 0.244s\n",
      "Accuracy source 0.891602, main loss classifier 0.228852, source classification loss 0.321298, loss domain distinction 0.183434, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.18548508 Acc: 0.93094406\n",
      "New best validation loss:  0.1854850798845291\n",
      "Epoch 10 of 500 took 0.238s\n",
      "Accuracy source 0.906250, main loss classifier 0.206890, source classification loss 0.277391, loss domain distinction 0.182470, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.18475772 Acc: 0.9291958\n",
      "New best validation loss:  0.18475772440433502\n",
      "Epoch 11 of 500 took 0.242s\n",
      "Accuracy source 0.905273, main loss classifier 0.204826, source classification loss 0.272785, loss domain distinction 0.183773, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.18552738 Acc: 0.93006993\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0060e-04.\n",
      "Epoch 12 of 500 took 0.239s\n",
      "Accuracy source 0.898438, main loss classifier 0.203038, source classification loss 0.269771, loss domain distinction 0.183744, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.16740191 Acc: 0.93618881\n",
      "New best validation loss:  0.16740190982818604\n",
      "Epoch 13 of 500 took 0.238s\n",
      "Accuracy source 0.909180, main loss classifier 0.204805, source classification loss 0.273776, loss domain distinction 0.183728, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.18038414 Acc: 0.93618881\n",
      "Epoch 14 of 500 took 0.235s\n",
      "Accuracy source 0.908691, main loss classifier 0.199179, source classification loss 0.262351, loss domain distinction 0.183475, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.17591792 Acc: 0.93356643\n",
      "Epoch 15 of 500 took 0.241s\n",
      "Accuracy source 0.899902, main loss classifier 0.205693, source classification loss 0.275744, loss domain distinction 0.183388, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.17801036 Acc: 0.93968531\n",
      "Epoch 16 of 500 took 0.234s\n",
      "Accuracy source 0.903320, main loss classifier 0.198741, source classification loss 0.262361, loss domain distinction 0.182429, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.16957535 Acc: 0.93793706\n",
      "Epoch 17 of 500 took 0.234s\n",
      "Accuracy source 0.905273, main loss classifier 0.204857, source classification loss 0.273950, loss domain distinction 0.182280, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.17277563 Acc: 0.93706294\n",
      "Epoch 18 of 500 took 0.284s\n",
      "Accuracy source 0.896484, main loss classifier 0.209527, source classification loss 0.283345, loss domain distinction 0.184758, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.17363425 Acc: 0.93793706\n",
      "Epoch 19 of 500 took 0.236s\n",
      "Accuracy source 0.912598, main loss classifier 0.199193, source classification loss 0.263159, loss domain distinction 0.182202, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.17948148 Acc: 0.93356643\n",
      "Epoch 20 of 500 took 0.233s\n",
      "Accuracy source 0.909180, main loss classifier 0.197413, source classification loss 0.259098, loss domain distinction 0.182944, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.16871315 Acc: 0.93793706\n",
      "Epoch 21 of 500 took 0.238s\n",
      "Accuracy source 0.907715, main loss classifier 0.199973, source classification loss 0.264476, loss domain distinction 0.183747, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.17945893 Acc: 0.93618881\n",
      "Epoch 22 of 500 took 0.236s\n",
      "Accuracy source 0.902344, main loss classifier 0.203336, source classification loss 0.270890, loss domain distinction 0.183658, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.16955477 Acc: 0.94405594\n",
      "Epoch 23 of 500 took 0.234s\n",
      "Accuracy source 0.914062, main loss classifier 0.192981, source classification loss 0.250066, loss domain distinction 0.185303, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.17084339 Acc: 0.93706294\n",
      "Training complete in 0m 6s\n",
      "()\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump5/TSD/participant_0/best_state_0.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump5/TSD/participant_0/best_state_0.pt' (epoch 11)\n",
      "STARTING TRAINING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy source 0.879395, main loss classifier 0.250042, source classification loss 0.346536, loss domain distinction 0.231800, accuracy domain distinction 0.494873\n",
      "VALIDATION Loss: 0.32476029 Acc: 0.88199301\n",
      "New best validation loss:  0.3247602880001068\n",
      "Epoch 1 of 500 took 0.241s\n",
      "Accuracy source 0.884766, main loss classifier 0.232763, source classification loss 0.318454, loss domain distinction 0.191903, accuracy domain distinction 0.499756\n",
      "VALIDATION Loss: 0.26539195 Acc: 0.8951049\n",
      "New best validation loss:  0.2653919458389282\n",
      "Epoch 2 of 500 took 0.260s\n",
      "Accuracy source 0.876953, main loss classifier 0.246131, source classification loss 0.348003, loss domain distinction 0.189323, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.32168034 Acc: 0.87587413\n",
      "Epoch 3 of 500 took 0.271s\n",
      "Accuracy source 0.876953, main loss classifier 0.246347, source classification loss 0.349526, loss domain distinction 0.187165, accuracy domain distinction 0.500244\n",
      "VALIDATION Loss: 0.44362652 Acc: 0.84877622\n",
      "Epoch 4 of 500 took 0.278s\n",
      "Accuracy source 0.889648, main loss classifier 0.235317, source classification loss 0.329171, loss domain distinction 0.186499, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.30129689 Acc: 0.88986014\n",
      "Epoch 5 of 500 took 0.282s\n",
      "Accuracy source 0.875488, main loss classifier 0.246261, source classification loss 0.351285, loss domain distinction 0.185857, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.30735573 Acc: 0.88024476\n",
      "Epoch    17: reducing learning rate of group 0 to 5.0300e-04.\n",
      "Epoch 6 of 500 took 0.277s\n",
      "Accuracy source 0.888184, main loss classifier 0.230441, source classification loss 0.325718, loss domain distinction 0.182006, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.26455519 Acc: 0.89685315\n",
      "New best validation loss:  0.2645551860332489\n",
      "Epoch 7 of 500 took 0.319s\n",
      "Accuracy source 0.892090, main loss classifier 0.222841, source classification loss 0.309252, loss domain distinction 0.182917, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.24994670 Acc: 0.90297203\n",
      "New best validation loss:  0.2499466985464096\n",
      "Epoch 8 of 500 took 0.255s\n",
      "Accuracy source 0.899414, main loss classifier 0.212135, source classification loss 0.288928, loss domain distinction 0.182127, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.26982760 Acc: 0.89772727\n",
      "Epoch 9 of 500 took 0.234s\n",
      "Accuracy source 0.908691, main loss classifier 0.201313, source classification loss 0.265758, loss domain distinction 0.182952, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.24547574 Acc: 0.90821678\n",
      "New best validation loss:  0.24547573924064636\n",
      "Epoch 10 of 500 took 0.236s\n",
      "Accuracy source 0.897461, main loss classifier 0.214171, source classification loss 0.292491, loss domain distinction 0.181093, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.25073585 Acc: 0.90472028\n",
      "Epoch 11 of 500 took 0.232s\n",
      "Accuracy source 0.908203, main loss classifier 0.200919, source classification loss 0.265421, loss domain distinction 0.183396, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.24530266 Acc: 0.90384615\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0060e-04.\n",
      "New best validation loss:  0.24530266225337982\n",
      "Epoch 12 of 500 took 0.234s\n",
      "Accuracy source 0.914551, main loss classifier 0.205648, source classification loss 0.275712, loss domain distinction 0.183466, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.22062419 Acc: 0.91433566\n",
      "New best validation loss:  0.2206241935491562\n",
      "Epoch 13 of 500 took 0.240s\n",
      "Accuracy source 0.915527, main loss classifier 0.199008, source classification loss 0.262983, loss domain distinction 0.183475, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.28064817 Acc: 0.89335664\n",
      "Epoch 14 of 500 took 0.233s\n",
      "Accuracy source 0.900391, main loss classifier 0.208161, source classification loss 0.281678, loss domain distinction 0.182707, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.27246803 Acc: 0.89685315\n",
      "Epoch 15 of 500 took 0.236s\n",
      "Accuracy source 0.912109, main loss classifier 0.194677, source classification loss 0.254372, loss domain distinction 0.181320, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.27194390 Acc: 0.89073427\n",
      "Epoch 16 of 500 took 0.233s\n",
      "Accuracy source 0.901367, main loss classifier 0.199738, source classification loss 0.264335, loss domain distinction 0.183173, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.27560523 Acc: 0.89685315\n",
      "Epoch 17 of 500 took 0.289s\n",
      "Accuracy source 0.902832, main loss classifier 0.210007, source classification loss 0.284528, loss domain distinction 0.181836, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.22945906 Acc: 0.91083916\n",
      "Epoch 18 of 500 took 0.235s\n",
      "Accuracy source 0.917480, main loss classifier 0.194422, source classification loss 0.254203, loss domain distinction 0.181531, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.33480051 Acc: 0.87325175\n",
      "Epoch 19 of 500 took 0.250s\n",
      "Accuracy source 0.911133, main loss classifier 0.190375, source classification loss 0.245277, loss domain distinction 0.181578, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.22253181 Acc: 0.91520979\n",
      "Epoch 20 of 500 took 0.235s\n",
      "Accuracy source 0.911133, main loss classifier 0.195773, source classification loss 0.255939, loss domain distinction 0.182350, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.23517361 Acc: 0.90909091\n",
      "Epoch 21 of 500 took 0.234s\n",
      "Accuracy source 0.914551, main loss classifier 0.192554, source classification loss 0.250127, loss domain distinction 0.182295, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.30471694 Acc: 0.88461538\n",
      "Epoch 22 of 500 took 0.244s\n",
      "Accuracy source 0.915039, main loss classifier 0.188278, source classification loss 0.241461, loss domain distinction 0.180991, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.28195050 Acc: 0.88636364\n",
      "Epoch 23 of 500 took 0.236s\n",
      "Accuracy source 0.898438, main loss classifier 0.212714, source classification loss 0.290345, loss domain distinction 0.182118, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.26680484 Acc: 0.90122378\n",
      "Training complete in 0m 6s\n",
      "()\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump5/TSD/participant_0/best_state_0.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump5/TSD/participant_0/best_state_0.pt' (epoch 11)\n",
      "STARTING TRAINING\n",
      "Accuracy source 0.879395, main loss classifier 0.237798, source classification loss 0.323043, loss domain distinction 0.230103, accuracy domain distinction 0.496094\n",
      "VALIDATION Loss: 0.41674295 Acc: 0.8479021\n",
      "New best validation loss:  0.4167429506778717\n",
      "Epoch 1 of 500 took 0.239s\n",
      "Accuracy source 0.889648, main loss classifier 0.228846, source classification loss 0.311511, loss domain distinction 0.189839, accuracy domain distinction 0.499268\n",
      "VALIDATION Loss: 0.35619214 Acc: 0.86888112\n",
      "New best validation loss:  0.35619214177131653\n",
      "Epoch 2 of 500 took 0.239s\n",
      "Accuracy source 0.854980, main loss classifier 0.269250, source classification loss 0.393973, loss domain distinction 0.189173, accuracy domain distinction 0.500244\n",
      "VALIDATION Loss: 0.50162095 Acc: 0.81905594\n",
      "Epoch 3 of 500 took 0.240s\n",
      "Accuracy source 0.878906, main loss classifier 0.253791, source classification loss 0.365098, loss domain distinction 0.187211, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.53205335 Acc: 0.81031469\n",
      "Epoch 4 of 500 took 0.238s\n",
      "Accuracy source 0.879883, main loss classifier 0.243906, source classification loss 0.345348, loss domain distinction 0.187153, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.51171279 Acc: 0.82954545\n",
      "Epoch 5 of 500 took 0.240s\n",
      "Accuracy source 0.869629, main loss classifier 0.244930, source classification loss 0.349434, loss domain distinction 0.184766, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.41049957 Acc: 0.85839161\n",
      "Epoch    17: reducing learning rate of group 0 to 5.0300e-04.\n",
      "Epoch 6 of 500 took 0.237s\n",
      "Accuracy source 0.895020, main loss classifier 0.217399, source classification loss 0.299436, loss domain distinction 0.180221, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.41273695 Acc: 0.85839161\n",
      "Epoch 7 of 500 took 0.237s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy source 0.885742, main loss classifier 0.230688, source classification loss 0.325447, loss domain distinction 0.182406, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.41432518 Acc: 0.86363636\n",
      "Epoch 8 of 500 took 0.235s\n",
      "Accuracy source 0.907715, main loss classifier 0.202594, source classification loss 0.268868, loss domain distinction 0.181710, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.38151282 Acc: 0.86013986\n",
      "Epoch 9 of 500 took 0.235s\n",
      "Accuracy source 0.892090, main loss classifier 0.217118, source classification loss 0.298581, loss domain distinction 0.182306, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.31192154 Acc: 0.88636364\n",
      "New best validation loss:  0.31192153692245483\n",
      "Epoch 10 of 500 took 0.241s\n",
      "Accuracy source 0.907227, main loss classifier 0.208123, source classification loss 0.280923, loss domain distinction 0.181013, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.35221824 Acc: 0.8715035\n",
      "Epoch 11 of 500 took 0.235s\n",
      "Accuracy source 0.904297, main loss classifier 0.207907, source classification loss 0.280541, loss domain distinction 0.180779, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.31715253 Acc: 0.88374126\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0060e-04.\n",
      "Epoch 12 of 500 took 0.237s\n",
      "Accuracy source 0.906738, main loss classifier 0.202591, source classification loss 0.270824, loss domain distinction 0.183046, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.41177872 Acc: 0.85839161\n",
      "Epoch 13 of 500 took 0.235s\n",
      "Accuracy source 0.896484, main loss classifier 0.211338, source classification loss 0.288737, loss domain distinction 0.180112, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.27038389 Acc: 0.8951049\n",
      "New best validation loss:  0.27038389444351196\n",
      "Epoch 14 of 500 took 0.242s\n",
      "Accuracy source 0.905273, main loss classifier 0.199560, source classification loss 0.265564, loss domain distinction 0.182020, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.39692524 Acc: 0.85576923\n",
      "Epoch 15 of 500 took 0.234s\n",
      "Accuracy source 0.905762, main loss classifier 0.206043, source classification loss 0.277257, loss domain distinction 0.180832, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.31943932 Acc: 0.88374126\n",
      "Epoch 16 of 500 took 0.235s\n",
      "Accuracy source 0.903809, main loss classifier 0.202367, source classification loss 0.270727, loss domain distinction 0.178862, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.39332253 Acc: 0.86276224\n",
      "Epoch 17 of 500 took 0.236s\n",
      "Accuracy source 0.902832, main loss classifier 0.208687, source classification loss 0.282442, loss domain distinction 0.182164, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.32055190 Acc: 0.88111888\n",
      "Epoch 18 of 500 took 0.242s\n",
      "Accuracy source 0.899902, main loss classifier 0.206210, source classification loss 0.277449, loss domain distinction 0.181539, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.32947198 Acc: 0.88286713\n",
      "Epoch 19 of 500 took 0.234s\n",
      "Accuracy source 0.898926, main loss classifier 0.203142, source classification loss 0.271830, loss domain distinction 0.180591, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.40491202 Acc: 0.85926573\n",
      "Epoch 20 of 500 took 0.235s\n",
      "Accuracy source 0.914062, main loss classifier 0.194428, source classification loss 0.254417, loss domain distinction 0.181798, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.36680174 Acc: 0.86538462\n",
      "Epoch 21 of 500 took 0.234s\n",
      "Accuracy source 0.907715, main loss classifier 0.194478, source classification loss 0.254947, loss domain distinction 0.180332, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.43609938 Acc: 0.85314685\n",
      "Epoch 22 of 500 took 0.236s\n",
      "Accuracy source 0.909180, main loss classifier 0.195561, source classification loss 0.256895, loss domain distinction 0.181054, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.32268894 Acc: 0.88636364\n",
      "Epoch 23 of 500 took 0.237s\n",
      "Accuracy source 0.906738, main loss classifier 0.195712, source classification loss 0.256684, loss domain distinction 0.179240, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.37690294 Acc: 0.87237762\n",
      "Epoch 24 of 500 took 0.241s\n",
      "Accuracy source 0.907227, main loss classifier 0.202437, source classification loss 0.270635, loss domain distinction 0.181742, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.39157438 Acc: 0.86276224\n",
      "Training complete in 0m 6s\n",
      "()\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump5/TSD/participant_0/best_state_0.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump5/TSD/participant_0/best_state_0.pt' (epoch 11)\n",
      "STARTING TRAINING\n",
      "Accuracy source 0.882812, main loss classifier 0.250074, source classification loss 0.347068, loss domain distinction 0.231905, accuracy domain distinction 0.495117\n",
      "VALIDATION Loss: 0.25216243 Acc: 0.90821678\n",
      "New best validation loss:  0.2521624267101288\n",
      "Epoch 1 of 500 took 0.237s\n",
      "Accuracy source 0.886719, main loss classifier 0.234458, source classification loss 0.324518, loss domain distinction 0.186906, accuracy domain distinction 0.500244\n",
      "VALIDATION Loss: 0.33485097 Acc: 0.875\n",
      "Epoch 2 of 500 took 0.238s\n",
      "Accuracy source 0.866699, main loss classifier 0.265977, source classification loss 0.387524, loss domain distinction 0.189104, accuracy domain distinction 0.499756\n",
      "VALIDATION Loss: 0.34578565 Acc: 0.87762238\n",
      "Epoch 3 of 500 took 0.236s\n",
      "Accuracy source 0.883301, main loss classifier 0.235008, source classification loss 0.326615, loss domain distinction 0.186230, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.29723275 Acc: 0.8951049\n",
      "Epoch 4 of 500 took 0.236s\n",
      "Accuracy source 0.870117, main loss classifier 0.253949, source classification loss 0.365950, loss domain distinction 0.185830, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.41653326 Acc: 0.84877622\n",
      "Epoch 5 of 500 took 0.236s\n",
      "Accuracy source 0.880859, main loss classifier 0.238307, source classification loss 0.337068, loss domain distinction 0.183506, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.30829203 Acc: 0.88461538\n",
      "Epoch    17: reducing learning rate of group 0 to 5.0300e-04.\n",
      "Epoch 6 of 500 took 0.240s\n",
      "Accuracy source 0.888184, main loss classifier 0.220374, source classification loss 0.305477, loss domain distinction 0.179749, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.21555752 Acc: 0.90996503\n",
      "New best validation loss:  0.2155575156211853\n",
      "Epoch 7 of 500 took 0.235s\n",
      "Accuracy source 0.889160, main loss classifier 0.226092, source classification loss 0.316703, loss domain distinction 0.181420, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.31452134 Acc: 0.88024476\n",
      "Epoch 8 of 500 took 0.235s\n",
      "Accuracy source 0.894043, main loss classifier 0.215881, source classification loss 0.295913, loss domain distinction 0.181742, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.25223780 Acc: 0.89772727\n",
      "Epoch 9 of 500 took 0.235s\n",
      "Accuracy source 0.895508, main loss classifier 0.212478, source classification loss 0.289550, loss domain distinction 0.180968, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.23737162 Acc: 0.90646853\n",
      "Epoch 10 of 500 took 0.239s\n",
      "Accuracy source 0.900879, main loss classifier 0.209913, source classification loss 0.284380, loss domain distinction 0.180581, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.27919868 Acc: 0.88986014\n",
      "Epoch 11 of 500 took 0.235s\n",
      "Accuracy source 0.902832, main loss classifier 0.207594, source classification loss 0.279515, loss domain distinction 0.181675, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.34436125 Acc: 0.86713287\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0060e-04.\n",
      "Epoch 12 of 500 took 0.235s\n",
      "Accuracy source 0.907227, main loss classifier 0.206534, source classification loss 0.278782, loss domain distinction 0.181822, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.28880611 Acc: 0.88548951\n",
      "Epoch 13 of 500 took 0.236s\n",
      "Accuracy source 0.900879, main loss classifier 0.206975, source classification loss 0.280051, loss domain distinction 0.179280, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.21141008 Acc: 0.91258741\n",
      "New best validation loss:  0.21141007542610168\n",
      "Epoch 14 of 500 took 0.239s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy source 0.896484, main loss classifier 0.206149, source classification loss 0.278022, loss domain distinction 0.180834, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.27764004 Acc: 0.89423077\n",
      "Epoch 15 of 500 took 0.236s\n",
      "Accuracy source 0.897949, main loss classifier 0.221128, source classification loss 0.308165, loss domain distinction 0.180306, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.26292115 Acc: 0.8986014\n",
      "Epoch 16 of 500 took 0.235s\n",
      "Accuracy source 0.900879, main loss classifier 0.206258, source classification loss 0.278360, loss domain distinction 0.180694, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.29057533 Acc: 0.88461538\n",
      "Epoch 17 of 500 took 0.235s\n",
      "Accuracy source 0.921875, main loss classifier 0.185715, source classification loss 0.237535, loss domain distinction 0.180436, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.30418715 Acc: 0.87937063\n",
      "Epoch 18 of 500 took 0.234s\n",
      "Accuracy source 0.907715, main loss classifier 0.196933, source classification loss 0.260191, loss domain distinction 0.179539, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.27560261 Acc: 0.89160839\n",
      "Epoch 19 of 500 took 0.238s\n",
      "Accuracy source 0.912598, main loss classifier 0.196804, source classification loss 0.260043, loss domain distinction 0.177559, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.31518462 Acc: 0.88636364\n",
      "Epoch 20 of 500 took 0.238s\n",
      "Accuracy source 0.906738, main loss classifier 0.204063, source classification loss 0.274608, loss domain distinction 0.179408, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.26242486 Acc: 0.89335664\n",
      "Epoch 21 of 500 took 0.234s\n",
      "Accuracy source 0.911621, main loss classifier 0.197065, source classification loss 0.259459, loss domain distinction 0.182607, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.28104997 Acc: 0.88898601\n",
      "Epoch 22 of 500 took 0.235s\n",
      "Accuracy source 0.904297, main loss classifier 0.199605, source classification loss 0.264669, loss domain distinction 0.180436, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.29495427 Acc: 0.88636364\n",
      "Epoch 23 of 500 took 0.237s\n",
      "Accuracy source 0.909668, main loss classifier 0.202472, source classification loss 0.270617, loss domain distinction 0.180525, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.29147419 Acc: 0.88374126\n",
      "Epoch 24 of 500 took 0.235s\n",
      "Accuracy source 0.911133, main loss classifier 0.197380, source classification loss 0.260618, loss domain distinction 0.178622, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.28867561 Acc: 0.88898601\n",
      "Training complete in 0m 6s\n",
      "()\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump5/TSD/participant_0/best_state_0.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump5/TSD/participant_0/best_state_0.pt' (epoch 11)\n",
      "STARTING TRAINING\n",
      "Accuracy source 0.887207, main loss classifier 0.247460, source classification loss 0.339372, loss domain distinction 0.234704, accuracy domain distinction 0.494629\n",
      "VALIDATION Loss: 0.35821539 Acc: 0.86276224\n",
      "New best validation loss:  0.3582153916358948\n",
      "Epoch 1 of 500 took 0.241s\n",
      "Accuracy source 0.879883, main loss classifier 0.244639, source classification loss 0.342068, loss domain distinction 0.189455, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.33347660 Acc: 0.88024476\n",
      "New best validation loss:  0.33347660303115845\n",
      "Epoch 2 of 500 took 0.236s\n",
      "Accuracy source 0.875000, main loss classifier 0.248904, source classification loss 0.353086, loss domain distinction 0.189468, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.26996458 Acc: 0.89772727\n",
      "New best validation loss:  0.2699645757675171\n",
      "Epoch 3 of 500 took 0.235s\n",
      "Accuracy source 0.879883, main loss classifier 0.241413, source classification loss 0.340399, loss domain distinction 0.186769, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.31804392 Acc: 0.875\n",
      "Epoch 4 of 500 took 0.234s\n",
      "Accuracy source 0.889160, main loss classifier 0.231744, source classification loss 0.322557, loss domain distinction 0.186163, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.28087351 Acc: 0.89423077\n",
      "Epoch 5 of 500 took 0.283s\n",
      "Accuracy source 0.882324, main loss classifier 0.251547, source classification loss 0.362219, loss domain distinction 0.185760, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.34735301 Acc: 0.87325175\n",
      "Epoch    17: reducing learning rate of group 0 to 5.0300e-04.\n",
      "Epoch 6 of 500 took 0.239s\n",
      "Accuracy source 0.879883, main loss classifier 0.222718, source classification loss 0.309473, loss domain distinction 0.182112, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.20220019 Acc: 0.9222028\n",
      "New best validation loss:  0.20220018923282623\n",
      "Epoch 7 of 500 took 0.236s\n",
      "Accuracy source 0.911133, main loss classifier 0.192397, source classification loss 0.248203, loss domain distinction 0.182645, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.24802862 Acc: 0.89772727\n",
      "Epoch 8 of 500 took 0.236s\n",
      "Accuracy source 0.898438, main loss classifier 0.212364, source classification loss 0.288766, loss domain distinction 0.181787, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.22255392 Acc: 0.91258741\n",
      "Epoch 9 of 500 took 0.241s\n",
      "Accuracy source 0.908691, main loss classifier 0.201894, source classification loss 0.267792, loss domain distinction 0.182620, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.29525337 Acc: 0.88636364\n",
      "Epoch 10 of 500 took 0.246s\n",
      "Accuracy source 0.908203, main loss classifier 0.206773, source classification loss 0.277398, loss domain distinction 0.182907, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.25327900 Acc: 0.89685315\n",
      "Epoch 11 of 500 took 0.239s\n",
      "Accuracy source 0.915039, main loss classifier 0.196980, source classification loss 0.257830, loss domain distinction 0.182575, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.19805533 Acc: 0.92395105\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0060e-04.\n",
      "New best validation loss:  0.19805532693862915\n",
      "Epoch 12 of 500 took 0.243s\n",
      "Accuracy source 0.904785, main loss classifier 0.206105, source classification loss 0.277953, loss domain distinction 0.181496, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.24665613 Acc: 0.90559441\n",
      "Epoch 13 of 500 took 0.245s\n",
      "Accuracy source 0.898926, main loss classifier 0.208815, source classification loss 0.282358, loss domain distinction 0.181902, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.28846115 Acc: 0.88811189\n",
      "Epoch 14 of 500 took 0.234s\n",
      "Accuracy source 0.904297, main loss classifier 0.208930, source classification loss 0.283686, loss domain distinction 0.181130, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.26103267 Acc: 0.8986014\n",
      "Epoch 15 of 500 took 0.255s\n",
      "Accuracy source 0.895508, main loss classifier 0.212876, source classification loss 0.291080, loss domain distinction 0.181370, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.24980518 Acc: 0.90034965\n",
      "Epoch 16 of 500 took 0.253s\n",
      "Accuracy source 0.904785, main loss classifier 0.208842, source classification loss 0.282972, loss domain distinction 0.181337, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.29178610 Acc: 0.88548951\n",
      "Epoch 17 of 500 took 0.234s\n",
      "Accuracy source 0.908691, main loss classifier 0.198054, source classification loss 0.262000, loss domain distinction 0.180260, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.22179069 Acc: 0.91346154\n",
      "Epoch 18 of 500 took 0.235s\n",
      "Accuracy source 0.902344, main loss classifier 0.202693, source classification loss 0.270900, loss domain distinction 0.181243, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.27300605 Acc: 0.8951049\n",
      "Epoch 19 of 500 took 0.238s\n",
      "Accuracy source 0.905762, main loss classifier 0.206246, source classification loss 0.278263, loss domain distinction 0.181060, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.24931741 Acc: 0.90472028\n",
      "Epoch 20 of 500 took 0.235s\n",
      "Accuracy source 0.904785, main loss classifier 0.206451, source classification loss 0.278708, loss domain distinction 0.180438, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.24587516 Acc: 0.90034965\n",
      "Epoch 21 of 500 took 0.232s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy source 0.918457, main loss classifier 0.190056, source classification loss 0.244892, loss domain distinction 0.183704, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.24570388 Acc: 0.90821678\n",
      "Epoch 22 of 500 took 0.234s\n",
      "Accuracy source 0.896484, main loss classifier 0.206293, source classification loss 0.277597, loss domain distinction 0.180930, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.26109314 Acc: 0.90034965\n",
      "Training complete in 0m 6s\n"
     ]
    }
   ],
   "source": [
    "train_DANN(examples_datasets_train, labels_datasets_train, \n",
    "          num_kernels=num_kernels,\n",
    "          path_weights_fine_tuning=path_TSD,\n",
    "          number_of_classes=number_of_classes,\n",
    "          number_of_cycles_total = number_of_cycles_total,\n",
    "          number_of_cycle_for_first_training = number_of_cycle_for_first_training,\n",
    "          batch_size=batch_size,\n",
    "          feature_vector_input_length=feature_vector_input_length,\n",
    "          path_weights_to_save_to=path_DANN, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET one participant_examples  (6,)\n",
      "   GET one training_index_examples  (20, 572, 252)  at  0\n",
      "   GOT one group XY  (11440, 252)    (11440,)\n",
      "       one group XY test  (2860, 252)    (2860, 252)\n",
      "       one group XY train (10296, 252)    (10296,)\n",
      "       one group XY valid (1144, 252)    (1144, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  1\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  2\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  3\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  4\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  5\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "dataloaders: \n",
      "   train  (1, 6)\n",
      "   valid  (1, 6)\n",
      "   test  (1, 6)\n",
      "(6,)\n",
      "Participant ID:  0  Session ID:  0  Accuracy:  0.9157342657342658\n",
      "Participant ID:  0  Session ID:  1  Accuracy:  0.9003496503496503\n",
      "Participant ID:  0  Session ID:  2  Accuracy:  0.8356643356643356\n",
      "Participant ID:  0  Session ID:  3  Accuracy:  0.7709790209790209\n",
      "Participant ID:  0  Session ID:  4  Accuracy:  0.7115384615384616\n",
      "Participant ID:  0  Session ID:  5  Accuracy:  0.7097902097902098\n",
      "ACCURACY PARTICIPANT:  [0.9157342657342658, 0.9003496503496503, 0.8356643356643356, 0.7709790209790209, 0.7115384615384616, 0.7097902097902098]\n",
      "[[0.91573427 0.90034965 0.83566434 0.77097902 0.71153846 0.70979021]]\n",
      "[array([0.91573427, 0.90034965, 0.83566434, 0.77097902, 0.71153846,\n",
      "       0.70979021])]\n",
      "OVERALL ACCURACY: 0.8073426573426574\n"
     ]
    }
   ],
   "source": [
    "algo_name = \"DANN\"\n",
    "test_DANN_on_training_sessions(examples_datasets_train, labels_datasets_train,\n",
    "                              feature_vector_input_length=feature_vector_input_length,\n",
    "                              num_neurons=num_kernels, path_weights_DA=path_DANN,\n",
    "                              algo_name=algo_name, save_path = save_DANN, \n",
    "                              number_of_cycles_total=number_of_cycles_total,\n",
    "                              number_of_cycle_for_first_training = number_of_cycle_for_first_training,\n",
    "                              path_weights_normal=path_TSD, number_of_classes=number_of_classes,\n",
    "                              cycle_for_test=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Day_0~4</th>\n",
       "      <td>0.915734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_5</th>\n",
       "      <td>0.90035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_6</th>\n",
       "      <td>0.835664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_7</th>\n",
       "      <td>0.770979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_8</th>\n",
       "      <td>0.711538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_9</th>\n",
       "      <td>0.70979</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Participant_5\n",
       "Day_0~4      0.915734\n",
       "Day_5         0.90035\n",
       "Day_6        0.835664\n",
       "Day_7        0.770979\n",
       "Day_8        0.711538\n",
       "Day_9         0.70979"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_filename = save_DANN + '/predictions_' + algo_name + \".npy\"\n",
    "results = np.load(results_filename, allow_pickle=True)\n",
    "DANN_acc = results[0]\n",
    "DANN_acc_overall = np.mean(DANN_acc)\n",
    "DANN_df = pd.DataFrame(DANN_acc.transpose(), \n",
    "                       index = [f'Day_{i}' for i in index_participant_list],\n",
    "                        columns = ['Participant_5'])\n",
    "DANN_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "DANN_df.transpose().plot.bar(rot=0, figsize=(10,5))\n",
    "plt.title(\"DANN Accuracies\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ground_truths  =  (1, 6)\n",
      "predictions =  (1, 6)\n",
      "index_participant_list  ['0~4', 5, 6, 7, 8, 9]\n",
      "accuracies_gestures =  (22, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Loc1_Sub5_Day0~4-&gt;0~4</th>\n",
       "      <th>Loc1_Sub5_Day0~4-&gt;5</th>\n",
       "      <th>Loc1_Sub5_Day0~4-&gt;6</th>\n",
       "      <th>Loc1_Sub5_Day0~4-&gt;7</th>\n",
       "      <th>Loc1_Sub5_Day0~4-&gt;8</th>\n",
       "      <th>Loc1_Sub5_Day0~4-&gt;9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M1</td>\n",
       "      <td>0.969231</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.692308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M2</td>\n",
       "      <td>0.946154</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.653846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M3</td>\n",
       "      <td>0.876923</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.307692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M4</td>\n",
       "      <td>0.869231</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>M5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>M6</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>M7</td>\n",
       "      <td>0.992308</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.884615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>M8</td>\n",
       "      <td>0.915385</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>M9</td>\n",
       "      <td>0.892308</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.423077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>M10</td>\n",
       "      <td>0.892308</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>M11</td>\n",
       "      <td>0.838462</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>M12</td>\n",
       "      <td>0.738462</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>0.423077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>M13</td>\n",
       "      <td>0.830769</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>M14</td>\n",
       "      <td>0.823077</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.192308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>M15</td>\n",
       "      <td>0.823077</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.807692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>M16</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>M17</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.961538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>M18</td>\n",
       "      <td>0.992308</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>M19</td>\n",
       "      <td>0.992308</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.884615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>M20</td>\n",
       "      <td>0.876923</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.423077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>M21</td>\n",
       "      <td>0.876923</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Mean</td>\n",
       "      <td>0.915734</td>\n",
       "      <td>0.900350</td>\n",
       "      <td>0.835664</td>\n",
       "      <td>0.770979</td>\n",
       "      <td>0.711538</td>\n",
       "      <td>0.709790</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Loc1_Sub5_Day0~4->0~4  Loc1_Sub5_Day0~4->5  \\\n",
       "0          M0               1.000000             1.000000   \n",
       "1          M1               0.969231             0.961538   \n",
       "2          M2               0.946154             0.576923   \n",
       "3          M3               0.876923             1.000000   \n",
       "4          M4               0.869231             0.923077   \n",
       "5          M5               1.000000             1.000000   \n",
       "6          M6               1.000000             1.000000   \n",
       "7          M7               0.992308             0.846154   \n",
       "8          M8               0.915385             0.961538   \n",
       "9          M9               0.892308             0.769231   \n",
       "10        M10               0.892308             0.961538   \n",
       "11        M11               0.838462             0.884615   \n",
       "12        M12               0.738462             0.653846   \n",
       "13        M13               0.830769             1.000000   \n",
       "14        M14               0.823077             0.653846   \n",
       "15        M15               0.823077             0.923077   \n",
       "16        M16               1.000000             1.000000   \n",
       "17        M17               1.000000             1.000000   \n",
       "18        M18               0.992308             0.846154   \n",
       "19        M19               0.992308             1.000000   \n",
       "20        M20               0.876923             0.923077   \n",
       "21        M21               0.876923             0.923077   \n",
       "22       Mean               0.915734             0.900350   \n",
       "\n",
       "    Loc1_Sub5_Day0~4->6  Loc1_Sub5_Day0~4->7  Loc1_Sub5_Day0~4->8  \\\n",
       "0              1.000000             1.000000             1.000000   \n",
       "1              0.807692             0.615385             0.461538   \n",
       "2              0.730769             0.730769             0.692308   \n",
       "3              0.923077             0.769231             0.961538   \n",
       "4              0.230769             0.115385             0.269231   \n",
       "5              1.000000             0.692308             1.000000   \n",
       "6              0.615385             0.653846             0.500000   \n",
       "7              1.000000             1.000000             0.923077   \n",
       "8              0.846154             0.538462             0.807692   \n",
       "9              0.884615             0.692308             0.730769   \n",
       "10             1.000000             0.884615             1.000000   \n",
       "11             0.884615             1.000000             0.961538   \n",
       "12             0.692308             0.692308             0.269231   \n",
       "13             0.961538             1.000000             1.000000   \n",
       "14             0.269231             0.692308             0.000000   \n",
       "15             0.730769             0.576923             0.461538   \n",
       "16             1.000000             1.000000             1.000000   \n",
       "17             1.000000             1.000000             1.000000   \n",
       "18             0.961538             1.000000             0.423077   \n",
       "19             1.000000             0.538462             0.730769   \n",
       "20             0.846154             0.923077             0.461538   \n",
       "21             1.000000             0.846154             1.000000   \n",
       "22             0.835664             0.770979             0.711538   \n",
       "\n",
       "    Loc1_Sub5_Day0~4->9  \n",
       "0              1.000000  \n",
       "1              0.692308  \n",
       "2              0.653846  \n",
       "3              0.307692  \n",
       "4              0.000000  \n",
       "5              0.846154  \n",
       "6              1.000000  \n",
       "7              0.884615  \n",
       "8              0.500000  \n",
       "9              0.423077  \n",
       "10             0.923077  \n",
       "11             1.000000  \n",
       "12             0.423077  \n",
       "13             0.846154  \n",
       "14             0.192308  \n",
       "15             0.807692  \n",
       "16             1.000000  \n",
       "17             0.961538  \n",
       "18             1.000000  \n",
       "19             0.884615  \n",
       "20             0.423077  \n",
       "21             0.846154  \n",
       "22             0.709790  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truths = results[1]\n",
    "predictions = results[2]\n",
    "print(\"ground_truths  = \", np.shape(ground_truths))\n",
    "print(\"predictions = \", np.shape(predictions))\n",
    "df = get_gesture_accuracies(ground_truths, predictions, number_of_classes=number_of_classes, \n",
    "                            m_name=m_name, n_name=n_name, path=save_TSD, algo_name=algo_name,\n",
    "                           index_participant_list_customized=index_participant_list,\n",
    "                           lump_day_at_participant=5)\n",
    "df = pd.read_csv(save_DANN+'/'+algo_name+'.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. SCADANN\n",
    "\n",
    "* `run_SCADANN_training_sessions`: train SCADANN model. The first session uses TSD model_0 wegits; others use DANN weights\n",
    "    * specify `percentage_same_gesture_stable` based on the performance of most pseudo labels: \n",
    "        * print accuracies out and check what percentage will optimize `ACCURACY MODEL` and `ACCURACY PSEUDO` without cutting out too much data \n",
    "    * num_sessions-1 sets of training weights will be saved\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TrainingsAndEvaluations.ForTrainingSessions.train_tsd_dnn_SCADANN import \\\n",
    "    run_SCADANN_training_sessions, test_network_SCADANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET one participant_examples  (6,)\n",
      "   GET one training_index_examples  (20, 572, 252)  at  0\n",
      "   GOT one group XY  (11440, 252)    (11440,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (11440, 252)    (11440,)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  1\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2288, 252)    (2288,)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  2\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2288, 252)    (2288,)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  3\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2288, 252)    (2288,)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  4\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2288, 252)    (2288,)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  5\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2288, 252)    (2288,)\n",
      "dataloaders: \n",
      "   train  (1, 6)\n",
      "   valid  (0,)\n",
      "   test  (1, 0)\n",
      "participants_train =  1\n",
      "Optimizer =  <generator object Module.parameters at 0x7f585d34ef90>\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump5/DANN/participant_0/best_state_1.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump5/DANN/participant_0/best_state_1.pt' (epoch 13)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump5/TSD/participant_0/best_state_0.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump5/TSD/participant_0/best_state_0.pt' (epoch 11)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump5/DANN/participant_0/best_state_1.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump5/DANN/participant_0/best_state_1.pt' (epoch 13)\n",
      "==== models_array =  (2,)  @ session  1\n",
      "HANDLING NEW SESSION  0\n",
      "HANDLING NEW SESSION  1\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.0  len before:  26   len after:  17\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  1\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.0  len before:  26   len after:  9\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.0  len before:  26   len after:  15\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.6923076923076923  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.5769230769230769  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  7\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.8859265734265734   Accuracy pseudo: 0.9608835710998619  len pseudo:  2173    len predictions 2288\n",
      "STARTING TRAINING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy total 0.875579, main loss classifier 0.470333, source accuracy 0.884259 source classification loss 0.314442, target accuracy 0.866898 target loss 0.400255 accuracy domain distinction 0.500000 loss domain distinction 1.129842,\n",
      "VALIDATION Loss: 0.22494781 Acc: 0.91494253\n",
      "New best validation loss:  0.224947812301772\n",
      "Epoch 2 of 500 took 0.398s\n",
      "Accuracy total 0.888021, main loss classifier 0.419745, source accuracy 0.895833 source classification loss 0.289079, target accuracy 0.880208 target loss 0.338880 accuracy domain distinction 0.500000 loss domain distinction 1.057651,\n",
      "VALIDATION Loss: 0.22211971 Acc: 0.92413793\n",
      "New best validation loss:  0.22211970814636775\n",
      "Epoch 3 of 500 took 0.355s\n",
      "Accuracy total 0.882523, main loss classifier 0.441209, source accuracy 0.888889 source classification loss 0.308842, target accuracy 0.876157 target loss 0.364247 accuracy domain distinction 0.500000 loss domain distinction 1.046649,\n",
      "VALIDATION Loss: 0.22966437 Acc: 0.92183908\n",
      "Epoch 4 of 500 took 0.382s\n",
      "Accuracy total 0.887442, main loss classifier 0.422950, source accuracy 0.889468 source classification loss 0.305214, target accuracy 0.885417 target loss 0.333070 accuracy domain distinction 0.500000 loss domain distinction 1.038082,\n",
      "VALIDATION Loss: 0.22130410 Acc: 0.91264368\n",
      "New best validation loss:  0.22130409734589712\n",
      "Epoch 5 of 500 took 0.362s\n",
      "Accuracy total 0.891493, main loss classifier 0.421090, source accuracy 0.892940 source classification loss 0.314513, target accuracy 0.890046 target loss 0.319482 accuracy domain distinction 0.500000 loss domain distinction 1.040922,\n",
      "VALIDATION Loss: 0.20594340 Acc: 0.92643678\n",
      "New best validation loss:  0.20594340243509837\n",
      "Epoch 6 of 500 took 0.354s\n",
      "Accuracy total 0.897859, main loss classifier 0.403674, source accuracy 0.895833 source classification loss 0.298783, target accuracy 0.899884 target loss 0.301434 accuracy domain distinction 0.500000 loss domain distinction 1.035661,\n",
      "VALIDATION Loss: 0.18633357 Acc: 0.93103448\n",
      "New best validation loss:  0.18633356796843664\n",
      "Epoch 7 of 500 took 0.354s\n",
      "Accuracy total 0.892072, main loss classifier 0.405618, source accuracy 0.892361 source classification loss 0.294512, target accuracy 0.891782 target loss 0.309815 accuracy domain distinction 0.500000 loss domain distinction 1.034544,\n",
      "VALIDATION Loss: 0.23702752 Acc: 0.92643678\n",
      "Epoch 8 of 500 took 0.363s\n",
      "Accuracy total 0.896991, main loss classifier 0.406265, source accuracy 0.889468 source classification loss 0.314164, target accuracy 0.904514 target loss 0.290333 accuracy domain distinction 0.500000 loss domain distinction 1.040170,\n",
      "VALIDATION Loss: 0.19353666 Acc: 0.93793103\n",
      "Epoch 9 of 500 took 0.387s\n",
      "Accuracy total 0.892650, main loss classifier 0.405037, source accuracy 0.893519 source classification loss 0.299618, target accuracy 0.891782 target loss 0.305046 accuracy domain distinction 0.500000 loss domain distinction 1.027047,\n",
      "VALIDATION Loss: 0.18806650 Acc: 0.92643678\n",
      "Epoch 10 of 500 took 0.392s\n",
      "Accuracy total 0.896701, main loss classifier 0.404083, source accuracy 0.894676 source classification loss 0.299034, target accuracy 0.898727 target loss 0.301094 accuracy domain distinction 0.500000 loss domain distinction 1.040189,\n",
      "VALIDATION Loss: 0.16545826 Acc: 0.93333333\n",
      "New best validation loss:  0.16545826303107397\n",
      "Epoch 11 of 500 took 0.354s\n",
      "Accuracy total 0.900463, main loss classifier 0.396947, source accuracy 0.896991 source classification loss 0.298051, target accuracy 0.903935 target loss 0.288793 accuracy domain distinction 0.500000 loss domain distinction 1.035255,\n",
      "VALIDATION Loss: 0.20576042 Acc: 0.92413793\n",
      "Epoch 12 of 500 took 0.352s\n",
      "Accuracy total 0.900174, main loss classifier 0.394808, source accuracy 0.896991 source classification loss 0.297124, target accuracy 0.903356 target loss 0.285468 accuracy domain distinction 0.500000 loss domain distinction 1.035124,\n",
      "VALIDATION Loss: 0.18787913 Acc: 0.94252874\n",
      "Epoch 13 of 500 took 0.355s\n",
      "Accuracy total 0.904803, main loss classifier 0.370821, source accuracy 0.909144 source classification loss 0.255288, target accuracy 0.900463 target loss 0.280129 accuracy domain distinction 0.500000 loss domain distinction 1.031121,\n",
      "VALIDATION Loss: 0.19915336 Acc: 0.92643678\n",
      "Epoch 14 of 500 took 0.353s\n",
      "Accuracy total 0.899306, main loss classifier 0.391228, source accuracy 0.894676 source classification loss 0.295963, target accuracy 0.903935 target loss 0.279455 accuracy domain distinction 0.500000 loss domain distinction 1.035192,\n",
      "VALIDATION Loss: 0.19220943 Acc: 0.94022989\n",
      "Epoch 15 of 500 took 0.357s\n",
      "Accuracy total 0.896123, main loss classifier 0.391382, source accuracy 0.888889 source classification loss 0.306468, target accuracy 0.903356 target loss 0.270316 accuracy domain distinction 0.500000 loss domain distinction 1.029903,\n",
      "VALIDATION Loss: 0.17520764 Acc: 0.94252874\n",
      "Epoch 16 of 500 took 0.356s\n",
      "Accuracy total 0.903067, main loss classifier 0.380590, source accuracy 0.897569 source classification loss 0.279835, target accuracy 0.908565 target loss 0.274729 accuracy domain distinction 0.500000 loss domain distinction 1.033082,\n",
      "VALIDATION Loss: 0.19786573 Acc: 0.92643678\n",
      "Epoch    16: reducing learning rate of group 0 to 2.0120e-05.\n",
      "Epoch 17 of 500 took 0.353s\n",
      "Accuracy total 0.910301, main loss classifier 0.369043, source accuracy 0.912616 source classification loss 0.255868, target accuracy 0.907986 target loss 0.275351 accuracy domain distinction 0.500000 loss domain distinction 1.034333,\n",
      "VALIDATION Loss: 0.17683562 Acc: 0.93103448\n",
      "Epoch 18 of 500 took 0.354s\n",
      "Accuracy total 0.897859, main loss classifier 0.385526, source accuracy 0.895255 source classification loss 0.286538, target accuracy 0.900463 target loss 0.277772 accuracy domain distinction 0.500000 loss domain distinction 1.033712,\n",
      "VALIDATION Loss: 0.18932278 Acc: 0.92873563\n",
      "Epoch 19 of 500 took 0.353s\n",
      "Accuracy total 0.903646, main loss classifier 0.377937, source accuracy 0.898148 source classification loss 0.278975, target accuracy 0.909144 target loss 0.271143 accuracy domain distinction 0.500000 loss domain distinction 1.028779,\n",
      "VALIDATION Loss: 0.17584843 Acc: 0.93793103\n",
      "Epoch 20 of 500 took 0.353s\n",
      "Accuracy total 0.903646, main loss classifier 0.376731, source accuracy 0.896412 source classification loss 0.283678, target accuracy 0.910880 target loss 0.263646 accuracy domain distinction 0.500000 loss domain distinction 1.030685,\n",
      "VALIDATION Loss: 0.20026928 Acc: 0.93103448\n",
      "Epoch 21 of 500 took 0.356s\n",
      "Accuracy total 0.905961, main loss classifier 0.367652, source accuracy 0.905093 source classification loss 0.280398, target accuracy 0.906829 target loss 0.249082 accuracy domain distinction 0.500000 loss domain distinction 1.029123,\n",
      "VALIDATION Loss: 0.17866729 Acc: 0.94252874\n",
      "Epoch 22 of 500 took 0.359s\n",
      "Training complete in 0m 8s\n",
      "['participant_0']\n",
      "Optimizer =  <generator object Module.parameters at 0x7f585d34e510>\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump5/DANN/participant_0/best_state_2.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump5/DANN/participant_0/best_state_2.pt' (epoch 13)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump5/TSD/participant_0/best_state_0.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump5/TSD/participant_0/best_state_0.pt' (epoch 11)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump5/DANN/participant_0/best_state_1.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump5/DANN/participant_0/best_state_1.pt' (epoch 13)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump5/DANN/participant_0/best_state_2.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump5/DANN/participant_0/best_state_2.pt' (epoch 13)\n",
      "==== models_array =  (3,)  @ session  2\n",
      "HANDLING NEW SESSION  0\n",
      "HANDLING NEW SESSION  1\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.0  len before:  26   len after:  17\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  1\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.0  len before:  26   len after:  9\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.0  len before:  26   len after:  15\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.6923076923076923  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.5769230769230769  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  7\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.8859265734265734   Accuracy pseudo: 0.9608835710998619  len pseudo:  2173    len predictions 2288\n",
      "HANDLING NEW SESSION  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.8076923076923077   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  1.0  len before:  26   len after:  15\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  8\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  18\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.0  len before:  26   len after:  5\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.0  len before:  26   len after:  7\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.0  len before:  26   len after:  5\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.625  len before:  26   len after:  16\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  1.0  len before:  26   len after:  2\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.0  len before:  26   len after:  17\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  13\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  7\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.8247377622377622   Accuracy pseudo: 0.9131910766246363  len pseudo:  2062    len predictions 2288\n",
      "STARTING TRAINING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laiy/.local/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/laiy/.local/lib/python3.8/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy total 0.871250, main loss classifier 0.485914, source accuracy 0.891250 source classification loss 0.298798, target accuracy 0.851250 target loss 0.446797 accuracy domain distinction 0.500000 loss domain distinction 1.131163,\n",
      "VALIDATION Loss: 0.19235699 Acc: 0.92978208\n",
      "New best validation loss:  0.19235699304512568\n",
      "Epoch 2 of 500 took 0.423s\n",
      "Accuracy total 0.880938, main loss classifier 0.456274, source accuracy 0.903125 source classification loss 0.299204, target accuracy 0.858750 target loss 0.401229 accuracy domain distinction 0.500000 loss domain distinction 1.060569,\n",
      "VALIDATION Loss: 0.22858511 Acc: 0.92009685\n",
      "Epoch 3 of 500 took 0.447s\n",
      "Accuracy total 0.884375, main loss classifier 0.435394, source accuracy 0.881250 source classification loss 0.328169, target accuracy 0.887500 target loss 0.332752 accuracy domain distinction 0.500000 loss domain distinction 1.049339,\n",
      "VALIDATION Loss: 0.16435634 Acc: 0.95399516\n",
      "New best validation loss:  0.16435634344816208\n",
      "Epoch 4 of 500 took 0.459s\n",
      "Accuracy total 0.882812, main loss classifier 0.431635, source accuracy 0.891250 source classification loss 0.308423, target accuracy 0.874375 target loss 0.347206 accuracy domain distinction 0.500000 loss domain distinction 1.038201,\n",
      "VALIDATION Loss: 0.15658735 Acc: 0.94430993\n",
      "New best validation loss:  0.15658735164574214\n",
      "Epoch 5 of 500 took 0.475s\n",
      "Accuracy total 0.882812, main loss classifier 0.438271, source accuracy 0.894375 source classification loss 0.292460, target accuracy 0.871250 target loss 0.376463 accuracy domain distinction 0.500000 loss domain distinction 1.038095,\n",
      "VALIDATION Loss: 0.15815431 Acc: 0.95883777\n",
      "Epoch 6 of 500 took 0.422s\n",
      "Accuracy total 0.882500, main loss classifier 0.441265, source accuracy 0.893750 source classification loss 0.304573, target accuracy 0.871250 target loss 0.370928 accuracy domain distinction 0.500000 loss domain distinction 1.035149,\n",
      "VALIDATION Loss: 0.16468390 Acc: 0.95157385\n",
      "Epoch 7 of 500 took 0.396s\n",
      "Accuracy total 0.884687, main loss classifier 0.436412, source accuracy 0.886250 source classification loss 0.316659, target accuracy 0.883125 target loss 0.348316 accuracy domain distinction 0.500000 loss domain distinction 1.039242,\n",
      "VALIDATION Loss: 0.14169866 Acc: 0.95641646\n",
      "New best validation loss:  0.14169865953070776\n",
      "Epoch 8 of 500 took 0.332s\n",
      "Accuracy total 0.894375, main loss classifier 0.411758, source accuracy 0.895625 source classification loss 0.299828, target accuracy 0.893125 target loss 0.317865 accuracy domain distinction 0.500000 loss domain distinction 1.029118,\n",
      "VALIDATION Loss: 0.14308339 Acc: 0.94430993\n",
      "Epoch 9 of 500 took 0.330s\n",
      "Accuracy total 0.892813, main loss classifier 0.407545, source accuracy 0.892500 source classification loss 0.300042, target accuracy 0.893125 target loss 0.309213 accuracy domain distinction 0.500000 loss domain distinction 1.029176,\n",
      "VALIDATION Loss: 0.15629315 Acc: 0.95399516\n",
      "Epoch 10 of 500 took 0.331s\n",
      "Accuracy total 0.892500, main loss classifier 0.406660, source accuracy 0.893125 source classification loss 0.289040, target accuracy 0.891875 target loss 0.315795 accuracy domain distinction 0.500000 loss domain distinction 1.042427,\n",
      "VALIDATION Loss: 0.15599472 Acc: 0.96125908\n",
      "Epoch 11 of 500 took 0.328s\n",
      "Accuracy total 0.891875, main loss classifier 0.406771, source accuracy 0.890000 source classification loss 0.305068, target accuracy 0.893750 target loss 0.301629 accuracy domain distinction 0.500000 loss domain distinction 1.034228,\n",
      "VALIDATION Loss: 0.13698822 Acc: 0.96125908\n",
      "New best validation loss:  0.13698821834155492\n",
      "Epoch 12 of 500 took 0.330s\n",
      "Accuracy total 0.889062, main loss classifier 0.419055, source accuracy 0.880000 source classification loss 0.336104, target accuracy 0.898125 target loss 0.295065 accuracy domain distinction 0.500000 loss domain distinction 1.034700,\n",
      "VALIDATION Loss: 0.17598918 Acc: 0.9346247\n",
      "Epoch 13 of 500 took 0.365s\n",
      "Accuracy total 0.901563, main loss classifier 0.404081, source accuracy 0.899375 source classification loss 0.306851, target accuracy 0.903750 target loss 0.295022 accuracy domain distinction 0.500000 loss domain distinction 1.031444,\n",
      "VALIDATION Loss: 0.11256791 Acc: 0.968523\n",
      "New best validation loss:  0.11256791411765985\n",
      "Epoch 14 of 500 took 0.349s\n",
      "Accuracy total 0.896875, main loss classifier 0.400854, source accuracy 0.891250 source classification loss 0.304533, target accuracy 0.902500 target loss 0.290690 accuracy domain distinction 0.500000 loss domain distinction 1.032426,\n",
      "VALIDATION Loss: 0.13565750 Acc: 0.95641646\n",
      "Epoch 15 of 500 took 0.341s\n",
      "Accuracy total 0.896875, main loss classifier 0.412246, source accuracy 0.896875 source classification loss 0.300662, target accuracy 0.896875 target loss 0.317695 accuracy domain distinction 0.500000 loss domain distinction 1.030676,\n",
      "VALIDATION Loss: 0.16246937 Acc: 0.95399516\n",
      "Epoch 16 of 500 took 0.330s\n",
      "Accuracy total 0.894687, main loss classifier 0.419640, source accuracy 0.889375 source classification loss 0.331018, target accuracy 0.900000 target loss 0.301448 accuracy domain distinction 0.500000 loss domain distinction 1.034077,\n",
      "VALIDATION Loss: 0.14590621 Acc: 0.95883777\n",
      "Epoch 17 of 500 took 0.327s\n",
      "Accuracy total 0.903750, main loss classifier 0.393836, source accuracy 0.901875 source classification loss 0.297940, target accuracy 0.905625 target loss 0.284107 accuracy domain distinction 0.500000 loss domain distinction 1.028123,\n",
      "VALIDATION Loss: 0.14048800 Acc: 0.95399516\n",
      "Epoch 18 of 500 took 0.326s\n",
      "Accuracy total 0.902500, main loss classifier 0.399113, source accuracy 0.903750 source classification loss 0.286014, target accuracy 0.901250 target loss 0.305039 accuracy domain distinction 0.500000 loss domain distinction 1.035869,\n",
      "VALIDATION Loss: 0.16755500 Acc: 0.95157385\n",
      "Epoch 19 of 500 took 0.329s\n",
      "Accuracy total 0.895938, main loss classifier 0.401565, source accuracy 0.885625 source classification loss 0.322271, target accuracy 0.906250 target loss 0.274270 accuracy domain distinction 0.500000 loss domain distinction 1.032948,\n",
      "VALIDATION Loss: 0.13817096 Acc: 0.96125908\n",
      "Epoch    19: reducing learning rate of group 0 to 2.0120e-05.\n",
      "Epoch 20 of 500 took 0.331s\n",
      "Accuracy total 0.902500, main loss classifier 0.397809, source accuracy 0.896250 source classification loss 0.314031, target accuracy 0.908750 target loss 0.275350 accuracy domain distinction 0.500000 loss domain distinction 1.031181,\n",
      "VALIDATION Loss: 0.11582803 Acc: 0.97094431\n",
      "Epoch 21 of 500 took 0.327s\n",
      "Accuracy total 0.896250, main loss classifier 0.408651, source accuracy 0.885000 source classification loss 0.329323, target accuracy 0.907500 target loss 0.281999 accuracy domain distinction 0.500000 loss domain distinction 1.029904,\n",
      "VALIDATION Loss: 0.19037473 Acc: 0.94188862\n",
      "Epoch 22 of 500 took 0.331s\n",
      "Accuracy total 0.890312, main loss classifier 0.406580, source accuracy 0.884375 source classification loss 0.317271, target accuracy 0.896250 target loss 0.289408 accuracy domain distinction 0.500000 loss domain distinction 1.032401,\n",
      "VALIDATION Loss: 0.12710348 Acc: 0.968523\n",
      "Epoch 23 of 500 took 0.332s\n",
      "Accuracy total 0.901875, main loss classifier 0.378233, source accuracy 0.897500 source classification loss 0.280168, target accuracy 0.906250 target loss 0.269967 accuracy domain distinction 0.500000 loss domain distinction 1.031658,\n",
      "VALIDATION Loss: 0.13527163 Acc: 0.96610169\n",
      "Epoch 24 of 500 took 0.326s\n",
      "Accuracy total 0.892813, main loss classifier 0.405860, source accuracy 0.893125 source classification loss 0.316418, target accuracy 0.892500 target loss 0.289636 accuracy domain distinction 0.500000 loss domain distinction 1.028330,\n",
      "VALIDATION Loss: 0.15286430 Acc: 0.94673123\n",
      "Epoch 25 of 500 took 0.341s\n",
      "Training complete in 0m 9s\n",
      "['participant_0']\n",
      "Optimizer =  <generator object Module.parameters at 0x7f5855696d60>\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump5/DANN/participant_0/best_state_3.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump5/DANN/participant_0/best_state_3.pt' (epoch 14)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump5/TSD/participant_0/best_state_0.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump5/TSD/participant_0/best_state_0.pt' (epoch 11)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump5/DANN/participant_0/best_state_1.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump5/DANN/participant_0/best_state_1.pt' (epoch 13)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump5/DANN/participant_0/best_state_2.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump5/DANN/participant_0/best_state_2.pt' (epoch 13)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump5/DANN/participant_0/best_state_3.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump5/DANN/participant_0/best_state_3.pt' (epoch 14)\n",
      "==== models_array =  (4,)  @ session  3\n",
      "HANDLING NEW SESSION  0\n",
      "HANDLING NEW SESSION  1\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.0  len before:  26   len after:  17\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  1\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.0  len before:  26   len after:  9\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.0  len before:  26   len after:  15\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.6923076923076923  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.5769230769230769  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  7\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.8859265734265734   Accuracy pseudo: 0.9608835710998619  len pseudo:  2173    len predictions 2288\n",
      "HANDLING NEW SESSION  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.8076923076923077   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  1.0  len before:  26   len after:  15\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  8\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  18\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.0  len before:  26   len after:  5\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.0  len before:  26   len after:  7\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.0  len before:  26   len after:  5\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.625  len before:  26   len after:  16\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  1.0  len before:  26   len after:  2\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.0  len before:  26   len after:  17\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  13\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  7\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.8247377622377622   Accuracy pseudo: 0.9131910766246363  len pseudo:  2062    len predictions 2288\n",
      "HANDLING NEW SESSION  3\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.5384615384615384  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  9\n",
      "BEFORE:  0.11538461538461539   AFTER:  0.0  len before:  26   len after:  14\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.625  len before:  26   len after:  16\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  14\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.0  len before:  26   len after:  12\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.8  len before:  26   len after:  10\n",
      "BEFORE:  0.5   AFTER:  0.0  len before:  26   len after:  9\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.6666666666666666  len before:  26   len after:  9\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.6470588235294118  len before:  26   len after:  17\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.1111111111111111  len before:  26   len after:  9\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.5384615384615384  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.4  len before:  26   len after:  10\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.0  len before:  26   len after:  1\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.0  len before:  26   len after:  7\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.11538461538461539   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  8\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  9\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  9\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.9615384615384616  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  15\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.7548076923076923   Accuracy pseudo: 0.853585657370518  len pseudo:  2008    len predictions 2288\n",
      "STARTING TRAINING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy total 0.842500, main loss classifier 0.595721, source accuracy 0.886250 source classification loss 0.322834, target accuracy 0.798750 target loss 0.643148 accuracy domain distinction 0.500000 loss domain distinction 1.127298,\n",
      "VALIDATION Loss: 0.36989020 Acc: 0.8880597\n",
      "New best validation loss:  0.3698901959827968\n",
      "Epoch 2 of 500 took 0.350s\n",
      "Accuracy total 0.857812, main loss classifier 0.554053, source accuracy 0.872500 source classification loss 0.365260, target accuracy 0.843125 target loss 0.531102 accuracy domain distinction 0.500000 loss domain distinction 1.058723,\n",
      "VALIDATION Loss: 0.29698558 Acc: 0.90547264\n",
      "New best validation loss:  0.2969855751310076\n",
      "Epoch 3 of 500 took 0.331s\n",
      "Accuracy total 0.869375, main loss classifier 0.502631, source accuracy 0.890625 source classification loss 0.315651, target accuracy 0.848125 target loss 0.481714 accuracy domain distinction 0.500000 loss domain distinction 1.039488,\n",
      "VALIDATION Loss: 0.26354082 Acc: 0.91044776\n",
      "New best validation loss:  0.26354082354477476\n",
      "Epoch 4 of 500 took 0.335s\n",
      "Accuracy total 0.867188, main loss classifier 0.502331, source accuracy 0.863125 source classification loss 0.379906, target accuracy 0.871250 target loss 0.418399 accuracy domain distinction 0.500000 loss domain distinction 1.031778,\n",
      "VALIDATION Loss: 0.31618798 Acc: 0.90547264\n",
      "Epoch 5 of 500 took 0.336s\n",
      "Accuracy total 0.874062, main loss classifier 0.490063, source accuracy 0.895625 source classification loss 0.303653, target accuracy 0.852500 target loss 0.469105 accuracy domain distinction 0.500000 loss domain distinction 1.036840,\n",
      "VALIDATION Loss: 0.27654030 Acc: 0.89552239\n",
      "Epoch 6 of 500 took 0.327s\n",
      "Accuracy total 0.868125, main loss classifier 0.492584, source accuracy 0.882500 source classification loss 0.339028, target accuracy 0.853750 target loss 0.439163 accuracy domain distinction 0.500000 loss domain distinction 1.034880,\n",
      "VALIDATION Loss: 0.26280514 Acc: 0.91044776\n",
      "New best validation loss:  0.26280513618673595\n",
      "Epoch 7 of 500 took 0.332s\n",
      "Accuracy total 0.878125, main loss classifier 0.496667, source accuracy 0.888750 source classification loss 0.356320, target accuracy 0.867500 target loss 0.431115 accuracy domain distinction 0.500000 loss domain distinction 1.029495,\n",
      "VALIDATION Loss: 0.25032492 Acc: 0.92288557\n",
      "New best validation loss:  0.2503249230129378\n",
      "Epoch 8 of 500 took 0.334s\n",
      "Accuracy total 0.875938, main loss classifier 0.480876, source accuracy 0.886250 source classification loss 0.321015, target accuracy 0.865625 target loss 0.433904 accuracy domain distinction 0.500000 loss domain distinction 1.034159,\n",
      "VALIDATION Loss: 0.29977423 Acc: 0.89800995\n",
      "Epoch 9 of 500 took 0.328s\n",
      "Accuracy total 0.880938, main loss classifier 0.460389, source accuracy 0.893750 source classification loss 0.291468, target accuracy 0.868125 target loss 0.422362 accuracy domain distinction 0.500000 loss domain distinction 1.034735,\n",
      "VALIDATION Loss: 0.19981381 Acc: 0.93034826\n",
      "New best validation loss:  0.19981380552053452\n",
      "Epoch 10 of 500 took 0.334s\n",
      "Accuracy total 0.880000, main loss classifier 0.462785, source accuracy 0.886875 source classification loss 0.330898, target accuracy 0.873125 target loss 0.387404 accuracy domain distinction 0.500000 loss domain distinction 1.036342,\n",
      "VALIDATION Loss: 0.20917516 Acc: 0.93532338\n",
      "Epoch 11 of 500 took 0.335s\n",
      "Accuracy total 0.876563, main loss classifier 0.483378, source accuracy 0.888750 source classification loss 0.345771, target accuracy 0.864375 target loss 0.413912 accuracy domain distinction 0.500000 loss domain distinction 1.035360,\n",
      "VALIDATION Loss: 0.24211929 Acc: 0.90298507\n",
      "Epoch 12 of 500 took 0.327s\n",
      "Accuracy total 0.890000, main loss classifier 0.438935, source accuracy 0.890000 source classification loss 0.315672, target accuracy 0.890000 target loss 0.355955 accuracy domain distinction 0.500000 loss domain distinction 1.031219,\n",
      "VALIDATION Loss: 0.20880700 Acc: 0.92537313\n",
      "Epoch 13 of 500 took 0.329s\n",
      "Accuracy total 0.878750, main loss classifier 0.460980, source accuracy 0.889375 source classification loss 0.314965, target accuracy 0.868125 target loss 0.400597 accuracy domain distinction 0.500000 loss domain distinction 1.031992,\n",
      "VALIDATION Loss: 0.22518494 Acc: 0.92288557\n",
      "Epoch 14 of 500 took 0.329s\n",
      "Accuracy total 0.891563, main loss classifier 0.430015, source accuracy 0.903125 source classification loss 0.294448, target accuracy 0.880000 target loss 0.358678 accuracy domain distinction 0.500000 loss domain distinction 1.034519,\n",
      "VALIDATION Loss: 0.24020264 Acc: 0.91791045\n",
      "Epoch 15 of 500 took 0.327s\n",
      "Accuracy total 0.880313, main loss classifier 0.459458, source accuracy 0.880000 source classification loss 0.350111, target accuracy 0.880625 target loss 0.362357 accuracy domain distinction 0.500000 loss domain distinction 1.032239,\n",
      "VALIDATION Loss: 0.22748713 Acc: 0.92537313\n",
      "Epoch    15: reducing learning rate of group 0 to 2.0120e-05.\n",
      "Epoch 16 of 500 took 0.329s\n",
      "Accuracy total 0.882500, main loss classifier 0.463364, source accuracy 0.887500 source classification loss 0.331045, target accuracy 0.877500 target loss 0.389662 accuracy domain distinction 0.500000 loss domain distinction 1.030106,\n",
      "VALIDATION Loss: 0.19297117 Acc: 0.93283582\n",
      "New best validation loss:  0.19297117101294653\n",
      "Epoch 17 of 500 took 0.333s\n",
      "Accuracy total 0.892813, main loss classifier 0.433245, source accuracy 0.897500 source classification loss 0.306416, target accuracy 0.888125 target loss 0.354472 accuracy domain distinction 0.500000 loss domain distinction 1.028006,\n",
      "VALIDATION Loss: 0.23374021 Acc: 0.93532338\n",
      "Epoch 18 of 500 took 0.331s\n",
      "Accuracy total 0.886250, main loss classifier 0.442146, source accuracy 0.891875 source classification loss 0.298697, target accuracy 0.880625 target loss 0.379027 accuracy domain distinction 0.500000 loss domain distinction 1.032834,\n",
      "VALIDATION Loss: 0.19697531 Acc: 0.9278607\n",
      "Epoch 19 of 500 took 0.334s\n",
      "Accuracy total 0.885938, main loss classifier 0.441946, source accuracy 0.890000 source classification loss 0.320934, target accuracy 0.881875 target loss 0.357618 accuracy domain distinction 0.500000 loss domain distinction 1.026694,\n",
      "VALIDATION Loss: 0.15969683 Acc: 0.94776119\n",
      "New best validation loss:  0.15969683389578546\n",
      "Epoch 20 of 500 took 0.331s\n",
      "Accuracy total 0.874687, main loss classifier 0.478642, source accuracy 0.878125 source classification loss 0.364974, target accuracy 0.871250 target loss 0.386352 accuracy domain distinction 0.500000 loss domain distinction 1.029793,\n",
      "VALIDATION Loss: 0.32049059 Acc: 0.87313433\n",
      "Epoch 21 of 500 took 0.330s\n",
      "Accuracy total 0.890000, main loss classifier 0.447700, source accuracy 0.885000 source classification loss 0.343970, target accuracy 0.895000 target loss 0.345406 accuracy domain distinction 0.500000 loss domain distinction 1.030120,\n",
      "VALIDATION Loss: 0.23974624 Acc: 0.91293532\n",
      "Epoch 22 of 500 took 0.328s\n",
      "Accuracy total 0.894062, main loss classifier 0.429032, source accuracy 0.901250 source classification loss 0.313090, target accuracy 0.886875 target loss 0.339349 accuracy domain distinction 0.500000 loss domain distinction 1.028124,\n",
      "VALIDATION Loss: 0.17384897 Acc: 0.9278607\n",
      "Epoch 23 of 500 took 0.331s\n",
      "Accuracy total 0.887188, main loss classifier 0.437600, source accuracy 0.890000 source classification loss 0.317202, target accuracy 0.884375 target loss 0.352612 accuracy domain distinction 0.500000 loss domain distinction 1.026935,\n",
      "VALIDATION Loss: 0.22278446 Acc: 0.93034826\n",
      "Epoch 24 of 500 took 0.332s\n",
      "Accuracy total 0.890312, main loss classifier 0.440532, source accuracy 0.886250 source classification loss 0.330154, target accuracy 0.894375 target loss 0.344659 accuracy domain distinction 0.500000 loss domain distinction 1.031254,\n",
      "VALIDATION Loss: 0.24381111 Acc: 0.92537313\n",
      "Epoch 25 of 500 took 0.330s\n",
      "Accuracy total 0.888125, main loss classifier 0.436407, source accuracy 0.893125 source classification loss 0.309782, target accuracy 0.883125 target loss 0.357009 accuracy domain distinction 0.500000 loss domain distinction 1.030111,\n",
      "VALIDATION Loss: 0.20006886 Acc: 0.93034826\n",
      "Epoch    25: reducing learning rate of group 0 to 4.0240e-06.\n",
      "Epoch 26 of 500 took 0.333s\n",
      "Accuracy total 0.894687, main loss classifier 0.427737, source accuracy 0.900000 source classification loss 0.296800, target accuracy 0.889375 target loss 0.352314 accuracy domain distinction 0.500000 loss domain distinction 1.031801,\n",
      "VALIDATION Loss: 0.21781544 Acc: 0.92537313\n",
      "Epoch 27 of 500 took 0.333s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy total 0.894375, main loss classifier 0.428893, source accuracy 0.890625 source classification loss 0.309526, target accuracy 0.898125 target loss 0.342123 accuracy domain distinction 0.500000 loss domain distinction 1.030687,\n",
      "VALIDATION Loss: 0.15828739 Acc: 0.94527363\n",
      "New best validation loss:  0.15828738680907659\n",
      "Epoch 28 of 500 took 0.334s\n",
      "Accuracy total 0.888750, main loss classifier 0.436463, source accuracy 0.900625 source classification loss 0.289052, target accuracy 0.876875 target loss 0.377023 accuracy domain distinction 0.500000 loss domain distinction 1.034249,\n",
      "VALIDATION Loss: 0.30638359 Acc: 0.89303483\n",
      "Epoch 29 of 500 took 0.329s\n",
      "Accuracy total 0.888125, main loss classifier 0.434773, source accuracy 0.888750 source classification loss 0.301257, target accuracy 0.887500 target loss 0.362275 accuracy domain distinction 0.500000 loss domain distinction 1.030076,\n",
      "VALIDATION Loss: 0.23528246 Acc: 0.91542289\n",
      "Epoch 30 of 500 took 0.360s\n",
      "Accuracy total 0.884687, main loss classifier 0.450037, source accuracy 0.890625 source classification loss 0.333885, target accuracy 0.878750 target loss 0.360309 accuracy domain distinction 0.500000 loss domain distinction 1.029400,\n",
      "VALIDATION Loss: 0.21891278 Acc: 0.92039801\n",
      "Epoch 31 of 500 took 0.328s\n",
      "Accuracy total 0.888437, main loss classifier 0.444167, source accuracy 0.890000 source classification loss 0.317008, target accuracy 0.886875 target loss 0.366440 accuracy domain distinction 0.500000 loss domain distinction 1.024426,\n",
      "VALIDATION Loss: 0.17780168 Acc: 0.95522388\n",
      "Epoch 32 of 500 took 0.359s\n",
      "Accuracy total 0.889687, main loss classifier 0.440140, source accuracy 0.887500 source classification loss 0.332683, target accuracy 0.891875 target loss 0.341237 accuracy domain distinction 0.500000 loss domain distinction 1.031797,\n",
      "VALIDATION Loss: 0.19505657 Acc: 0.9278607\n",
      "Epoch 33 of 500 took 0.333s\n",
      "Accuracy total 0.889687, main loss classifier 0.441466, source accuracy 0.899375 source classification loss 0.308925, target accuracy 0.880000 target loss 0.369193 accuracy domain distinction 0.500000 loss domain distinction 1.024071,\n",
      "VALIDATION Loss: 0.23070926 Acc: 0.92288557\n",
      "Epoch    33: reducing learning rate of group 0 to 8.0480e-07.\n",
      "Epoch 34 of 500 took 0.354s\n",
      "Accuracy total 0.872500, main loss classifier 0.462216, source accuracy 0.868125 source classification loss 0.337077, target accuracy 0.876875 target loss 0.382111 accuracy domain distinction 0.500000 loss domain distinction 1.026219,\n",
      "VALIDATION Loss: 0.22076973 Acc: 0.9278607\n",
      "Epoch 35 of 500 took 0.331s\n",
      "Accuracy total 0.891875, main loss classifier 0.440700, source accuracy 0.888750 source classification loss 0.327623, target accuracy 0.895000 target loss 0.347503 accuracy domain distinction 0.500000 loss domain distinction 1.031368,\n",
      "VALIDATION Loss: 0.24169365 Acc: 0.91542289\n",
      "Epoch 36 of 500 took 0.333s\n",
      "Accuracy total 0.891250, main loss classifier 0.430619, source accuracy 0.888125 source classification loss 0.304886, target accuracy 0.894375 target loss 0.351216 accuracy domain distinction 0.500000 loss domain distinction 1.025681,\n",
      "VALIDATION Loss: 0.21386501 Acc: 0.93283582\n",
      "Epoch 37 of 500 took 0.329s\n",
      "Accuracy total 0.891563, main loss classifier 0.427185, source accuracy 0.896250 source classification loss 0.297445, target accuracy 0.886875 target loss 0.350723 accuracy domain distinction 0.500000 loss domain distinction 1.031008,\n",
      "VALIDATION Loss: 0.26720391 Acc: 0.91791045\n",
      "Epoch 38 of 500 took 0.330s\n",
      "Accuracy total 0.890625, main loss classifier 0.417850, source accuracy 0.896875 source classification loss 0.275938, target accuracy 0.884375 target loss 0.353639 accuracy domain distinction 0.500000 loss domain distinction 1.030617,\n",
      "VALIDATION Loss: 0.23774595 Acc: 0.92288557\n",
      "Epoch 39 of 500 took 0.334s\n",
      "Training complete in 0m 13s\n",
      "['participant_0']\n",
      "Optimizer =  <generator object Module.parameters at 0x7f58617bfac0>\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump5/DANN/participant_0/best_state_4.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump5/DANN/participant_0/best_state_4.pt' (epoch 14)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump5/TSD/participant_0/best_state_0.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump5/TSD/participant_0/best_state_0.pt' (epoch 11)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump5/DANN/participant_0/best_state_1.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump5/DANN/participant_0/best_state_1.pt' (epoch 13)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump5/DANN/participant_0/best_state_2.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump5/DANN/participant_0/best_state_2.pt' (epoch 13)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump5/DANN/participant_0/best_state_3.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump5/DANN/participant_0/best_state_3.pt' (epoch 14)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump5/DANN/participant_0/best_state_4.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump5/DANN/participant_0/best_state_4.pt' (epoch 14)\n",
      "==== models_array =  (5,)  @ session  4\n",
      "HANDLING NEW SESSION  0\n",
      "HANDLING NEW SESSION  1\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.0  len before:  26   len after:  17\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  1\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.0  len before:  26   len after:  9\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.0  len before:  26   len after:  15\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.6923076923076923  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.5769230769230769  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  7\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.8859265734265734   Accuracy pseudo: 0.9608835710998619  len pseudo:  2173    len predictions 2288\n",
      "HANDLING NEW SESSION  2\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.8076923076923077   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  1.0  len before:  26   len after:  15\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  8\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  18\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.0  len before:  26   len after:  5\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.0  len before:  26   len after:  7\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.0  len before:  26   len after:  5\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.625  len before:  26   len after:  16\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  1.0  len before:  26   len after:  2\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.0  len before:  26   len after:  17\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  13\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  7\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.8247377622377622   Accuracy pseudo: 0.9131910766246363  len pseudo:  2062    len predictions 2288\n",
      "HANDLING NEW SESSION  3\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.5384615384615384  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  9\n",
      "BEFORE:  0.11538461538461539   AFTER:  0.0  len before:  26   len after:  14\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.625  len before:  26   len after:  16\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  14\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.0  len before:  26   len after:  12\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.8  len before:  26   len after:  10\n",
      "BEFORE:  0.5   AFTER:  0.0  len before:  26   len after:  9\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.6666666666666666  len before:  26   len after:  9\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.6470588235294118  len before:  26   len after:  17\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.1111111111111111  len before:  26   len after:  9\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.5384615384615384  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.4  len before:  26   len after:  10\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.0  len before:  26   len after:  1\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.0  len before:  26   len after:  7\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.11538461538461539   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  8\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  9\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  9\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.9615384615384616  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  15\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.7548076923076923   Accuracy pseudo: 0.853585657370518  len pseudo:  2008    len predictions 2288\n",
      "HANDLING NEW SESSION  4\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  9\n",
      "BEFORE:  0.5   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.9230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  3\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  12\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  0.11538461538461539   AFTER:  0.0  len before:  26   len after:  10\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  8\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.46153846153846156  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.8846153846153846  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.0  len before:  26   len after:  18\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  7\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.0  len before:  26   len after:  12\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.7058823529411765  len before:  26   len after:  17\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.5769230769230769  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.7421328671328671   Accuracy pseudo: 0.8240115718418515  len pseudo:  2074    len predictions 2288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING TRAINING\n",
      "Accuracy total 0.841562, main loss classifier 0.623629, source accuracy 0.868125 source classification loss 0.399255, target accuracy 0.815000 target loss 0.622229 accuracy domain distinction 0.500000 loss domain distinction 1.128864,\n",
      "VALIDATION Loss: 0.34103993 Acc: 0.88674699\n",
      "New best validation loss:  0.3410399343286242\n",
      "Epoch 2 of 500 took 0.337s\n",
      "Accuracy total 0.857812, main loss classifier 0.552115, source accuracy 0.878750 source classification loss 0.343375, target accuracy 0.836875 target loss 0.550362 accuracy domain distinction 0.500000 loss domain distinction 1.052474,\n",
      "VALIDATION Loss: 0.33612141 Acc: 0.89638554\n",
      "New best validation loss:  0.33612141013145447\n",
      "Epoch 3 of 500 took 0.332s\n",
      "Accuracy total 0.856875, main loss classifier 0.533304, source accuracy 0.862500 source classification loss 0.377576, target accuracy 0.851250 target loss 0.480629 accuracy domain distinction 0.500000 loss domain distinction 1.042008,\n",
      "VALIDATION Loss: 0.35062395 Acc: 0.89638554\n",
      "Epoch 4 of 500 took 0.330s\n",
      "Accuracy total 0.866563, main loss classifier 0.502436, source accuracy 0.886875 source classification loss 0.324081, target accuracy 0.846250 target loss 0.473929 accuracy domain distinction 0.500000 loss domain distinction 1.034312,\n",
      "VALIDATION Loss: 0.36148022 Acc: 0.89879518\n",
      "Epoch 5 of 500 took 0.332s\n",
      "Accuracy total 0.874687, main loss classifier 0.497304, source accuracy 0.888125 source classification loss 0.337719, target accuracy 0.861250 target loss 0.450393 accuracy domain distinction 0.500000 loss domain distinction 1.032483,\n",
      "VALIDATION Loss: 0.29536541 Acc: 0.90843373\n",
      "New best validation loss:  0.2953654123204095\n",
      "Epoch 6 of 500 took 0.332s\n",
      "Accuracy total 0.865313, main loss classifier 0.527561, source accuracy 0.868125 source classification loss 0.391213, target accuracy 0.862500 target loss 0.456500 accuracy domain distinction 0.500000 loss domain distinction 1.037050,\n",
      "VALIDATION Loss: 0.41828902 Acc: 0.86987952\n",
      "Epoch 7 of 500 took 0.338s\n",
      "Accuracy total 0.874375, main loss classifier 0.492723, source accuracy 0.883125 source classification loss 0.345637, target accuracy 0.865625 target loss 0.433573 accuracy domain distinction 0.500000 loss domain distinction 1.031172,\n",
      "VALIDATION Loss: 0.39228848 Acc: 0.86746988\n",
      "Epoch 8 of 500 took 0.333s\n",
      "Accuracy total 0.873125, main loss classifier 0.496462, source accuracy 0.883125 source classification loss 0.361440, target accuracy 0.863125 target loss 0.425473 accuracy domain distinction 0.500000 loss domain distinction 1.030052,\n",
      "VALIDATION Loss: 0.30815158 Acc: 0.9060241\n",
      "Epoch 9 of 500 took 0.332s\n",
      "Accuracy total 0.866563, main loss classifier 0.513465, source accuracy 0.863125 source classification loss 0.393837, target accuracy 0.870000 target loss 0.426058 accuracy domain distinction 0.500000 loss domain distinction 1.035175,\n",
      "VALIDATION Loss: 0.34575848 Acc: 0.9060241\n",
      "Epoch 10 of 500 took 0.330s\n",
      "Accuracy total 0.885938, main loss classifier 0.476516, source accuracy 0.895000 source classification loss 0.332437, target accuracy 0.876875 target loss 0.413637 accuracy domain distinction 0.500000 loss domain distinction 1.034789,\n",
      "VALIDATION Loss: 0.32851809 Acc: 0.89638554\n",
      "Epoch 11 of 500 took 0.334s\n",
      "Accuracy total 0.875625, main loss classifier 0.483043, source accuracy 0.882500 source classification loss 0.363627, target accuracy 0.868750 target loss 0.395593 accuracy domain distinction 0.500000 loss domain distinction 1.034333,\n",
      "VALIDATION Loss: 0.32121968 Acc: 0.88915663\n",
      "Epoch    11: reducing learning rate of group 0 to 2.0120e-05.\n",
      "Epoch 12 of 500 took 0.334s\n",
      "Accuracy total 0.882188, main loss classifier 0.477131, source accuracy 0.892500 source classification loss 0.337487, target accuracy 0.871875 target loss 0.411058 accuracy domain distinction 0.500000 loss domain distinction 1.028586,\n",
      "VALIDATION Loss: 0.28314942 Acc: 0.91807229\n",
      "New best validation loss:  0.2831494169575827\n",
      "Epoch 13 of 500 took 0.331s\n",
      "Accuracy total 0.879375, main loss classifier 0.483979, source accuracy 0.885000 source classification loss 0.344636, target accuracy 0.873750 target loss 0.416450 accuracy domain distinction 0.499688 loss domain distinction 1.034362,\n",
      "VALIDATION Loss: 0.30644430 Acc: 0.91084337\n",
      "Epoch 14 of 500 took 0.331s\n",
      "Accuracy total 0.878125, main loss classifier 0.475043, source accuracy 0.887500 source classification loss 0.329547, target accuracy 0.868750 target loss 0.414774 accuracy domain distinction 0.500000 loss domain distinction 1.028833,\n",
      "VALIDATION Loss: 0.31615473 Acc: 0.90120482\n",
      "Epoch 15 of 500 took 0.331s\n",
      "Accuracy total 0.881563, main loss classifier 0.467854, source accuracy 0.885000 source classification loss 0.330534, target accuracy 0.878125 target loss 0.398380 accuracy domain distinction 0.500000 loss domain distinction 1.033977,\n",
      "VALIDATION Loss: 0.37637082 Acc: 0.87951807\n",
      "Epoch 16 of 500 took 0.332s\n",
      "Accuracy total 0.874062, main loss classifier 0.486356, source accuracy 0.888750 source classification loss 0.334224, target accuracy 0.859375 target loss 0.432832 accuracy domain distinction 0.500000 loss domain distinction 1.028282,\n",
      "VALIDATION Loss: 0.32111519 Acc: 0.90361446\n",
      "Epoch 17 of 500 took 0.334s\n",
      "Accuracy total 0.884062, main loss classifier 0.474296, source accuracy 0.875625 source classification loss 0.357264, target accuracy 0.892500 target loss 0.385105 accuracy domain distinction 0.500000 loss domain distinction 1.031110,\n",
      "VALIDATION Loss: 0.31876336 Acc: 0.91566265\n",
      "Epoch 18 of 500 took 0.330s\n",
      "Accuracy total 0.865938, main loss classifier 0.503061, source accuracy 0.874375 source classification loss 0.367444, target accuracy 0.857500 target loss 0.432141 accuracy domain distinction 0.500000 loss domain distinction 1.032679,\n",
      "VALIDATION Loss: 0.27531089 Acc: 0.92771084\n",
      "New best validation loss:  0.275310891015189\n",
      "Epoch 19 of 500 took 0.335s\n",
      "Accuracy total 0.880625, main loss classifier 0.472497, source accuracy 0.880625 source classification loss 0.373691, target accuracy 0.880625 target loss 0.364786 accuracy domain distinction 0.500000 loss domain distinction 1.032585,\n",
      "VALIDATION Loss: 0.32828068 Acc: 0.9060241\n",
      "Epoch 20 of 500 took 0.334s\n",
      "Accuracy total 0.892188, main loss classifier 0.452617, source accuracy 0.900000 source classification loss 0.308231, target accuracy 0.884375 target loss 0.391655 accuracy domain distinction 0.500000 loss domain distinction 1.026744,\n",
      "VALIDATION Loss: 0.27631096 Acc: 0.92048193\n",
      "Epoch 21 of 500 took 0.333s\n",
      "Accuracy total 0.865938, main loss classifier 0.509106, source accuracy 0.868750 source classification loss 0.401450, target accuracy 0.863125 target loss 0.410413 accuracy domain distinction 0.500000 loss domain distinction 1.031749,\n",
      "VALIDATION Loss: 0.39240793 Acc: 0.86506024\n",
      "Epoch 22 of 500 took 0.339s\n",
      "Accuracy total 0.877500, main loss classifier 0.468692, source accuracy 0.881875 source classification loss 0.333501, target accuracy 0.873125 target loss 0.398964 accuracy domain distinction 0.500000 loss domain distinction 1.024594,\n",
      "VALIDATION Loss: 0.30874631 Acc: 0.91325301\n",
      "Epoch 23 of 500 took 0.334s\n",
      "Accuracy total 0.871250, main loss classifier 0.497412, source accuracy 0.869375 source classification loss 0.395799, target accuracy 0.873125 target loss 0.392685 accuracy domain distinction 0.500000 loss domain distinction 1.031706,\n",
      "VALIDATION Loss: 0.35725027 Acc: 0.87710843\n",
      "Epoch 24 of 500 took 0.331s\n",
      "Accuracy total 0.879062, main loss classifier 0.484433, source accuracy 0.890000 source classification loss 0.348990, target accuracy 0.868125 target loss 0.413481 accuracy domain distinction 0.500000 loss domain distinction 1.031976,\n",
      "VALIDATION Loss: 0.29305908 Acc: 0.91566265\n",
      "Epoch    24: reducing learning rate of group 0 to 4.0240e-06.\n",
      "Epoch 25 of 500 took 0.332s\n",
      "Accuracy total 0.876563, main loss classifier 0.480151, source accuracy 0.885000 source classification loss 0.341995, target accuracy 0.868125 target loss 0.411412 accuracy domain distinction 0.500000 loss domain distinction 1.034474,\n",
      "VALIDATION Loss: 0.36344866 Acc: 0.8939759\n",
      "Epoch 26 of 500 took 0.333s\n",
      "Accuracy total 0.883437, main loss classifier 0.456310, source accuracy 0.891250 source classification loss 0.311988, target accuracy 0.875625 target loss 0.395583 accuracy domain distinction 0.500000 loss domain distinction 1.025247,\n",
      "VALIDATION Loss: 0.28262834 Acc: 0.91566265\n",
      "Epoch 27 of 500 took 0.334s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy total 0.878750, main loss classifier 0.462917, source accuracy 0.895625 source classification loss 0.313741, target accuracy 0.861875 target loss 0.405646 accuracy domain distinction 0.500000 loss domain distinction 1.032233,\n",
      "VALIDATION Loss: 0.29093083 Acc: 0.90120482\n",
      "Epoch 28 of 500 took 0.331s\n",
      "Accuracy total 0.880938, main loss classifier 0.473275, source accuracy 0.879375 source classification loss 0.363336, target accuracy 0.882500 target loss 0.378110 accuracy domain distinction 0.500000 loss domain distinction 1.025519,\n",
      "VALIDATION Loss: 0.27841633 Acc: 0.91807229\n",
      "Epoch 29 of 500 took 0.336s\n",
      "Accuracy total 0.882188, main loss classifier 0.479172, source accuracy 0.878125 source classification loss 0.365622, target accuracy 0.886250 target loss 0.386771 accuracy domain distinction 0.500000 loss domain distinction 1.029757,\n",
      "VALIDATION Loss: 0.34295819 Acc: 0.88192771\n",
      "Epoch 30 of 500 took 0.330s\n",
      "Training complete in 0m 10s\n",
      "['participant_0']\n",
      "Optimizer =  <generator object Module.parameters at 0x7f5855696d60>\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump5/DANN/participant_0/best_state_5.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump5/DANN/participant_0/best_state_5.pt' (epoch 12)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump5/TSD/participant_0/best_state_0.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump5/TSD/participant_0/best_state_0.pt' (epoch 11)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump5/DANN/participant_0/best_state_1.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump5/DANN/participant_0/best_state_1.pt' (epoch 13)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump5/DANN/participant_0/best_state_2.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump5/DANN/participant_0/best_state_2.pt' (epoch 13)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump5/DANN/participant_0/best_state_3.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump5/DANN/participant_0/best_state_3.pt' (epoch 14)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump5/DANN/participant_0/best_state_4.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump5/DANN/participant_0/best_state_4.pt' (epoch 14)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump5/DANN/participant_0/best_state_5.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump5/DANN/participant_0/best_state_5.pt' (epoch 12)\n",
      "==== models_array =  (6,)  @ session  5\n",
      "HANDLING NEW SESSION  0\n",
      "HANDLING NEW SESSION  1\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.0  len before:  26   len after:  17\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  1\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.0  len before:  26   len after:  9\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.0  len before:  26   len after:  15\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.6923076923076923  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.5769230769230769  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  7\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.8859265734265734   Accuracy pseudo: 0.9608835710998619  len pseudo:  2173    len predictions 2288\n",
      "HANDLING NEW SESSION  2\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.8076923076923077   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  1.0  len before:  26   len after:  15\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  8\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  18\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.0  len before:  26   len after:  5\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.0  len before:  26   len after:  7\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.0  len before:  26   len after:  5\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.625  len before:  26   len after:  16\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  1.0  len before:  26   len after:  2\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.0  len before:  26   len after:  17\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  13\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  7\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.8247377622377622   Accuracy pseudo: 0.9131910766246363  len pseudo:  2062    len predictions 2288\n",
      "HANDLING NEW SESSION  3\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.5384615384615384  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  9\n",
      "BEFORE:  0.11538461538461539   AFTER:  0.0  len before:  26   len after:  14\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.625  len before:  26   len after:  16\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  14\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.0  len before:  26   len after:  12\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE:  0.7307692307692307   AFTER:  0.8  len before:  26   len after:  10\n",
      "BEFORE:  0.5   AFTER:  0.0  len before:  26   len after:  9\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.6666666666666666  len before:  26   len after:  9\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.6470588235294118  len before:  26   len after:  17\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.1111111111111111  len before:  26   len after:  9\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.5384615384615384  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.4  len before:  26   len after:  10\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.0  len before:  26   len after:  1\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.0  len before:  26   len after:  7\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.11538461538461539   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  8\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  9\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  9\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.9615384615384616  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  15\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.7548076923076923   Accuracy pseudo: 0.853585657370518  len pseudo:  2008    len predictions 2288\n",
      "HANDLING NEW SESSION  4\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  9\n",
      "BEFORE:  0.5   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.9230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  3\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  12\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  0.11538461538461539   AFTER:  0.0  len before:  26   len after:  10\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  8\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.46153846153846156  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.8846153846153846  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.0  len before:  26   len after:  18\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  7\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.0  len before:  26   len after:  12\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.7058823529411765  len before:  26   len after:  17\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.5769230769230769  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.7421328671328671   Accuracy pseudo: 0.8240115718418515  len pseudo:  2074    len predictions 2288\n",
      "HANDLING NEW SESSION  5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.8461538461538461  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.6111111111111112  len before:  26   len after:  18\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  15\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.0  len before:  26   len after:  4\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.0  len before:  26   len after:  2\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.5384615384615384  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.8846153846153846  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  1.0  len before:  26   len after:  14\n",
      "BEFORE:  0.46153846153846156   AFTER:  1.0  len before:  26   len after:  1\n",
      "BEFORE:  0.038461538461538464   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  6\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  8\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.8846153846153846  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  3\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.5  len before:  26   len after:  10\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.6923076923076923  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.0  len before:  26   len after:  12\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  18\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  1.0  len before:  26   len after:  9\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.5882352941176471  len before:  26   len after:  17\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.4230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  9\n",
      "BEFORE:  0.8076923076923077   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.7268356643356644   Accuracy pseudo: 0.8615692153923038  len pseudo:  2001    len predictions 2288\n",
      "STARTING TRAINING\n",
      "Accuracy total 0.817187, main loss classifier 0.698314, source accuracy 0.857500 source classification loss 0.427855, target accuracy 0.776875 target loss 0.741766 accuracy domain distinction 0.500000 loss domain distinction 1.135035,\n",
      "VALIDATION Loss: 0.36055418 Acc: 0.86783042\n",
      "New best validation loss:  0.36055417571749004\n",
      "Epoch 2 of 500 took 0.337s\n",
      "Accuracy total 0.834063, main loss classifier 0.594974, source accuracy 0.853750 source classification loss 0.400575, target accuracy 0.814375 target loss 0.577911 accuracy domain distinction 0.500000 loss domain distinction 1.057308,\n",
      "VALIDATION Loss: 0.32743235 Acc: 0.86783042\n",
      "New best validation loss:  0.3274323514529637\n",
      "Epoch 3 of 500 took 0.332s\n",
      "Accuracy total 0.849375, main loss classifier 0.577136, source accuracy 0.872500 source classification loss 0.408925, target accuracy 0.826250 target loss 0.537488 accuracy domain distinction 0.500000 loss domain distinction 1.039295,\n",
      "VALIDATION Loss: 0.30625501 Acc: 0.87281796\n",
      "New best validation loss:  0.30625500636441366\n",
      "Epoch 4 of 500 took 0.334s\n",
      "Accuracy total 0.865313, main loss classifier 0.534880, source accuracy 0.885625 source classification loss 0.351392, target accuracy 0.845000 target loss 0.510484 accuracy domain distinction 0.500000 loss domain distinction 1.039421,\n",
      "VALIDATION Loss: 0.27355967 Acc: 0.90024938\n",
      "New best validation loss:  0.2735596682344164\n",
      "Epoch 5 of 500 took 0.332s\n",
      "Accuracy total 0.862187, main loss classifier 0.536010, source accuracy 0.876250 source classification loss 0.357014, target accuracy 0.848125 target loss 0.507722 accuracy domain distinction 0.500000 loss domain distinction 1.036425,\n",
      "VALIDATION Loss: 0.27144039 Acc: 0.88528678\n",
      "New best validation loss:  0.2714403931583677\n",
      "Epoch 6 of 500 took 0.339s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy total 0.862812, main loss classifier 0.525860, source accuracy 0.879375 source classification loss 0.362294, target accuracy 0.846250 target loss 0.482097 accuracy domain distinction 0.500000 loss domain distinction 1.036645,\n",
      "VALIDATION Loss: 0.32523530 Acc: 0.87780549\n",
      "Epoch 7 of 500 took 0.333s\n",
      "Accuracy total 0.856563, main loss classifier 0.532159, source accuracy 0.876250 source classification loss 0.366144, target accuracy 0.836875 target loss 0.489621 accuracy domain distinction 0.500000 loss domain distinction 1.042766,\n",
      "VALIDATION Loss: 0.26956988 Acc: 0.90523691\n",
      "New best validation loss:  0.2695698823247637\n",
      "Epoch 8 of 500 took 0.332s\n",
      "Accuracy total 0.864062, main loss classifier 0.523679, source accuracy 0.885000 source classification loss 0.347862, target accuracy 0.843125 target loss 0.492223 accuracy domain distinction 0.500000 loss domain distinction 1.036366,\n",
      "VALIDATION Loss: 0.25835092 Acc: 0.90024938\n",
      "New best validation loss:  0.25835091939994265\n",
      "Epoch 9 of 500 took 0.331s\n",
      "Accuracy total 0.859375, main loss classifier 0.519331, source accuracy 0.878125 source classification loss 0.365756, target accuracy 0.840625 target loss 0.467109 accuracy domain distinction 0.500000 loss domain distinction 1.028991,\n",
      "VALIDATION Loss: 0.28578185 Acc: 0.88778055\n",
      "Epoch 10 of 500 took 0.334s\n",
      "Accuracy total 0.866875, main loss classifier 0.510152, source accuracy 0.884375 source classification loss 0.366041, target accuracy 0.849375 target loss 0.447037 accuracy domain distinction 0.500000 loss domain distinction 1.036128,\n",
      "VALIDATION Loss: 0.26482653 Acc: 0.90274314\n",
      "Epoch 11 of 500 took 0.329s\n",
      "Accuracy total 0.863437, main loss classifier 0.525259, source accuracy 0.877500 source classification loss 0.382795, target accuracy 0.849375 target loss 0.461073 accuracy domain distinction 0.500000 loss domain distinction 1.033252,\n",
      "VALIDATION Loss: 0.34102523 Acc: 0.88029925\n",
      "Epoch 12 of 500 took 0.330s\n",
      "Accuracy total 0.858750, main loss classifier 0.508313, source accuracy 0.868750 source classification loss 0.349576, target accuracy 0.848750 target loss 0.460542 accuracy domain distinction 0.500000 loss domain distinction 1.032541,\n",
      "VALIDATION Loss: 0.25360872 Acc: 0.89526185\n",
      "New best validation loss:  0.2536087153213365\n",
      "Epoch 13 of 500 took 0.335s\n",
      "Accuracy total 0.870000, main loss classifier 0.520339, source accuracy 0.873750 source classification loss 0.389997, target accuracy 0.866250 target loss 0.444133 accuracy domain distinction 0.500000 loss domain distinction 1.032738,\n",
      "VALIDATION Loss: 0.31367922 Acc: 0.88029925\n",
      "Epoch 14 of 500 took 0.333s\n",
      "Accuracy total 0.874375, main loss classifier 0.492247, source accuracy 0.882500 source classification loss 0.366034, target accuracy 0.866250 target loss 0.412028 accuracy domain distinction 0.500000 loss domain distinction 1.032162,\n",
      "VALIDATION Loss: 0.29277220 Acc: 0.88528678\n",
      "Epoch 15 of 500 took 0.332s\n",
      "Accuracy total 0.872500, main loss classifier 0.499402, source accuracy 0.875000 source classification loss 0.362724, target accuracy 0.870000 target loss 0.429642 accuracy domain distinction 0.500000 loss domain distinction 1.032195,\n",
      "VALIDATION Loss: 0.24654306 Acc: 0.90773067\n",
      "New best validation loss:  0.24654306152037211\n",
      "Epoch 16 of 500 took 0.338s\n",
      "Accuracy total 0.874062, main loss classifier 0.469571, source accuracy 0.885625 source classification loss 0.329357, target accuracy 0.862500 target loss 0.403739 accuracy domain distinction 0.500000 loss domain distinction 1.030232,\n",
      "VALIDATION Loss: 0.24714638 Acc: 0.90024938\n",
      "Epoch 17 of 500 took 0.329s\n",
      "Accuracy total 0.863437, main loss classifier 0.525496, source accuracy 0.864375 source classification loss 0.423732, target accuracy 0.862500 target loss 0.420905 accuracy domain distinction 0.500000 loss domain distinction 1.031780,\n",
      "VALIDATION Loss: 0.29739537 Acc: 0.87281796\n",
      "Epoch 18 of 500 took 0.329s\n",
      "Accuracy total 0.873437, main loss classifier 0.477839, source accuracy 0.878125 source classification loss 0.347361, target accuracy 0.868750 target loss 0.402335 accuracy domain distinction 0.500000 loss domain distinction 1.029911,\n",
      "VALIDATION Loss: 0.22215668 Acc: 0.9127182\n",
      "New best validation loss:  0.222156680056027\n",
      "Epoch 19 of 500 took 0.335s\n",
      "Accuracy total 0.885000, main loss classifier 0.470464, source accuracy 0.892500 source classification loss 0.337699, target accuracy 0.877500 target loss 0.396383 accuracy domain distinction 0.500000 loss domain distinction 1.034229,\n",
      "VALIDATION Loss: 0.28512998 Acc: 0.90274314\n",
      "Epoch 20 of 500 took 0.330s\n",
      "Accuracy total 0.860000, main loss classifier 0.510533, source accuracy 0.865000 source classification loss 0.400146, target accuracy 0.855000 target loss 0.414398 accuracy domain distinction 0.500000 loss domain distinction 1.032607,\n",
      "VALIDATION Loss: 0.25381539 Acc: 0.9127182\n",
      "Epoch 21 of 500 took 0.339s\n",
      "Accuracy total 0.871563, main loss classifier 0.478269, source accuracy 0.877500 source classification loss 0.336169, target accuracy 0.865625 target loss 0.413531 accuracy domain distinction 0.500000 loss domain distinction 1.034195,\n",
      "VALIDATION Loss: 0.24497496 Acc: 0.9127182\n",
      "Epoch 22 of 500 took 0.336s\n",
      "Accuracy total 0.871250, main loss classifier 0.477913, source accuracy 0.892500 source classification loss 0.326353, target accuracy 0.850000 target loss 0.422658 accuracy domain distinction 0.500000 loss domain distinction 1.034074,\n",
      "VALIDATION Loss: 0.26874755 Acc: 0.90274314\n",
      "Epoch 23 of 500 took 0.329s\n",
      "Accuracy total 0.870000, main loss classifier 0.479055, source accuracy 0.868125 source classification loss 0.358126, target accuracy 0.871875 target loss 0.392952 accuracy domain distinction 0.500000 loss domain distinction 1.035160,\n",
      "VALIDATION Loss: 0.26010938 Acc: 0.89276808\n",
      "Epoch 24 of 500 took 0.332s\n",
      "Accuracy total 0.874687, main loss classifier 0.479378, source accuracy 0.869375 source classification loss 0.371057, target accuracy 0.880000 target loss 0.380606 accuracy domain distinction 0.500000 loss domain distinction 1.035466,\n",
      "VALIDATION Loss: 0.30206631 Acc: 0.89276808\n",
      "Epoch    24: reducing learning rate of group 0 to 2.0120e-05.\n",
      "Epoch 25 of 500 took 0.334s\n",
      "Accuracy total 0.874375, main loss classifier 0.472297, source accuracy 0.878125 source classification loss 0.350471, target accuracy 0.870625 target loss 0.387198 accuracy domain distinction 0.500000 loss domain distinction 1.034633,\n",
      "VALIDATION Loss: 0.20560392 Acc: 0.9127182\n",
      "New best validation loss:  0.20560392098767416\n",
      "Epoch 26 of 500 took 0.332s\n",
      "Accuracy total 0.876563, main loss classifier 0.468317, source accuracy 0.881875 source classification loss 0.347453, target accuracy 0.871250 target loss 0.383145 accuracy domain distinction 0.500000 loss domain distinction 1.030183,\n",
      "VALIDATION Loss: 0.21644449 Acc: 0.92269327\n",
      "Epoch 27 of 500 took 0.331s\n",
      "Accuracy total 0.884375, main loss classifier 0.458939, source accuracy 0.893750 source classification loss 0.323900, target accuracy 0.875000 target loss 0.387744 accuracy domain distinction 0.500000 loss domain distinction 1.031175,\n",
      "VALIDATION Loss: 0.26670594 Acc: 0.89526185\n",
      "Epoch 28 of 500 took 0.336s\n",
      "Accuracy total 0.870625, main loss classifier 0.470147, source accuracy 0.873750 source classification loss 0.363481, target accuracy 0.867500 target loss 0.371202 accuracy domain distinction 0.500000 loss domain distinction 1.028057,\n",
      "VALIDATION Loss: 0.23658880 Acc: 0.9127182\n",
      "Epoch 29 of 500 took 0.330s\n",
      "Accuracy total 0.880313, main loss classifier 0.473674, source accuracy 0.884375 source classification loss 0.355469, target accuracy 0.876250 target loss 0.385607 accuracy domain distinction 0.500000 loss domain distinction 1.031361,\n",
      "VALIDATION Loss: 0.27485625 Acc: 0.88528678\n",
      "Epoch 30 of 500 took 0.330s\n",
      "Accuracy total 0.875938, main loss classifier 0.480808, source accuracy 0.881875 source classification loss 0.357250, target accuracy 0.870000 target loss 0.398576 accuracy domain distinction 0.500000 loss domain distinction 1.028947,\n",
      "VALIDATION Loss: 0.25413982 Acc: 0.90274314\n",
      "Epoch 31 of 500 took 0.334s\n",
      "Accuracy total 0.878437, main loss classifier 0.464223, source accuracy 0.887500 source classification loss 0.325638, target accuracy 0.869375 target loss 0.397138 accuracy domain distinction 0.500000 loss domain distinction 1.028345,\n",
      "VALIDATION Loss: 0.26605779 Acc: 0.90274314\n",
      "Epoch    31: reducing learning rate of group 0 to 4.0240e-06.\n",
      "Epoch 32 of 500 took 0.332s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy total 0.879687, main loss classifier 0.449883, source accuracy 0.886250 source classification loss 0.332606, target accuracy 0.873125 target loss 0.361834 accuracy domain distinction 0.500000 loss domain distinction 1.026627,\n",
      "VALIDATION Loss: 0.25463642 Acc: 0.90773067\n",
      "Epoch 33 of 500 took 0.329s\n",
      "Accuracy total 0.882188, main loss classifier 0.470555, source accuracy 0.876875 source classification loss 0.362735, target accuracy 0.887500 target loss 0.372431 accuracy domain distinction 0.500000 loss domain distinction 1.029722,\n",
      "VALIDATION Loss: 0.22646954 Acc: 0.9127182\n",
      "Epoch 34 of 500 took 0.332s\n",
      "Accuracy total 0.877812, main loss classifier 0.475287, source accuracy 0.881250 source classification loss 0.355874, target accuracy 0.874375 target loss 0.388767 accuracy domain distinction 0.500000 loss domain distinction 1.029669,\n",
      "VALIDATION Loss: 0.28064326 Acc: 0.90024938\n",
      "Epoch 35 of 500 took 0.330s\n",
      "Accuracy total 0.868437, main loss classifier 0.487409, source accuracy 0.861875 source classification loss 0.389081, target accuracy 0.875000 target loss 0.378820 accuracy domain distinction 0.500000 loss domain distinction 1.034589,\n",
      "VALIDATION Loss: 0.22900472 Acc: 0.91521197\n",
      "Epoch 36 of 500 took 0.334s\n",
      "Accuracy total 0.885000, main loss classifier 0.442959, source accuracy 0.888750 source classification loss 0.308015, target accuracy 0.881250 target loss 0.371646 accuracy domain distinction 0.500000 loss domain distinction 1.031285,\n",
      "VALIDATION Loss: 0.25420432 Acc: 0.91770574\n",
      "Epoch 37 of 500 took 0.335s\n",
      "Training complete in 0m 12s\n",
      "['participant_0']\n"
     ]
    }
   ],
   "source": [
    "percentage_same_gesture_stable = 0.75 \n",
    "run_SCADANN_training_sessions(examples_datasets=examples_datasets_train, labels_datasets=labels_datasets_train,\n",
    "                              num_kernels=num_kernels, feature_vector_input_length=feature_vector_input_length,\n",
    "                              path_weights_to_save_to=path_SCADANN,\n",
    "                              path_weights_Adversarial_training=path_DANN,\n",
    "                              path_weights_Normal_training=path_TSD,\n",
    "                              number_of_cycles_total = number_of_cycles_total, \n",
    "                              number_of_cycle_for_first_training = number_of_cycle_for_first_training,\n",
    "                              number_of_classes=number_of_classes,\n",
    "                              learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET one participant_examples  (6,)\n",
      "   GET one training_index_examples  (20, 572, 252)  at  0\n",
      "   GOT one group XY  (11440, 252)    (11440,)\n",
      "       one group XY test  (2860, 252)    (2860, 252)\n",
      "       one group XY train (10296, 252)    (10296,)\n",
      "       one group XY valid (1144, 252)    (1144, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  1\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  2\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  3\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  4\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  5\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "dataloaders: \n",
      "   train  (1, 6)\n",
      "   valid  (1, 6)\n",
      "   test  (1, 6)\n",
      "Participant:  0  Accuracy:  0.9157342657342658\n",
      "Participant:  0  Accuracy:  0.9143356643356644\n",
      "Participant:  0  Accuracy:  0.8531468531468531\n",
      "Participant:  0  Accuracy:  0.8234265734265734\n",
      "Participant:  0  Accuracy:  0.7307692307692307\n",
      "Participant:  0  Accuracy:  0.7954545454545454\n",
      "ACCURACY PARTICIPANT:  [0.9157342657342658, 0.9143356643356644, 0.8531468531468531, 0.8234265734265734, 0.7307692307692307, 0.7954545454545454]\n",
      "[[0.91573427 0.91433566 0.85314685 0.82342657 0.73076923 0.79545455]]\n",
      "[array([0.91573427, 0.91433566, 0.85314685, 0.82342657, 0.73076923,\n",
      "       0.79545455])]\n",
      "OVERALL ACCURACY: 0.8388111888111887\n"
     ]
    }
   ],
   "source": [
    "algo_name = \"SCADANN\"\n",
    "test_network_SCADANN(examples_datasets_train=examples_datasets_train, labels_datasets_train=labels_datasets_train,\n",
    "                     num_neurons=num_kernels, feature_vector_input_length=feature_vector_input_length,\n",
    "                     path_weights_SCADANN =path_SCADANN, path_weights_normal=path_TSD,\n",
    "                     algo_name=algo_name, cycle_test=3, number_of_cycles_total=number_of_cycles_total,\n",
    "                     number_of_cycle_for_first_training = number_of_cycle_for_first_training,\n",
    "                     number_of_classes=number_of_classes, save_path = save_SCADANN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Day_0~4</th>\n",
       "      <td>0.915734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_5</th>\n",
       "      <td>0.914336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_6</th>\n",
       "      <td>0.853147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_7</th>\n",
       "      <td>0.823427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_8</th>\n",
       "      <td>0.730769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_9</th>\n",
       "      <td>0.795455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Participant_5\n",
       "Day_0~4      0.915734\n",
       "Day_5        0.914336\n",
       "Day_6        0.853147\n",
       "Day_7        0.823427\n",
       "Day_8        0.730769\n",
       "Day_9        0.795455"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_filename = save_SCADANN + '/predictions_' + algo_name + \".npy\"\n",
    "results = np.load(results_filename, allow_pickle=True)\n",
    "SCADANN_acc = results[0]\n",
    "SCADANN_acc_overall = np.mean(SCADANN_acc)\n",
    "SCADANN_df = pd.DataFrame(SCADANN_acc.transpose(), \n",
    "                       index = [f'Day_{i}' for i in index_participant_list],\n",
    "                        columns = ['Participant_5'])\n",
    "SCADANN_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "SCADANN_df.transpose().plot.bar(rot=0, figsize=(10,5))\n",
    "plt.title(\"SCADANN Accuracies\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ground_truths  =  (1, 6)\n",
      "predictions =  (1, 6)\n",
      "index_participant_list  ['0~4', 5, 6, 7, 8, 9]\n",
      "accuracies_gestures =  (22, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Loc1_Sub5_Day0~4-&gt;0~4</th>\n",
       "      <th>Loc1_Sub5_Day0~4-&gt;5</th>\n",
       "      <th>Loc1_Sub5_Day0~4-&gt;6</th>\n",
       "      <th>Loc1_Sub5_Day0~4-&gt;7</th>\n",
       "      <th>Loc1_Sub5_Day0~4-&gt;8</th>\n",
       "      <th>Loc1_Sub5_Day0~4-&gt;9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M1</td>\n",
       "      <td>0.969231</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.884615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M2</td>\n",
       "      <td>0.946154</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.807692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M3</td>\n",
       "      <td>0.876923</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M4</td>\n",
       "      <td>0.869231</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>M5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.961538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>M6</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>M7</td>\n",
       "      <td>0.992308</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>M8</td>\n",
       "      <td>0.915385</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.884615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>M9</td>\n",
       "      <td>0.892308</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.730769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>M10</td>\n",
       "      <td>0.892308</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>M11</td>\n",
       "      <td>0.838462</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>M12</td>\n",
       "      <td>0.738462</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>0.307692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>M13</td>\n",
       "      <td>0.830769</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>M14</td>\n",
       "      <td>0.823077</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.153846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>M15</td>\n",
       "      <td>0.823077</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.846154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>M16</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>M17</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>M18</td>\n",
       "      <td>0.992308</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>M19</td>\n",
       "      <td>0.992308</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.961538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>M20</td>\n",
       "      <td>0.876923</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.538462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>M21</td>\n",
       "      <td>0.876923</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Mean</td>\n",
       "      <td>0.915734</td>\n",
       "      <td>0.914336</td>\n",
       "      <td>0.853147</td>\n",
       "      <td>0.823427</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.795455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Loc1_Sub5_Day0~4->0~4  Loc1_Sub5_Day0~4->5  \\\n",
       "0          M0               1.000000             1.000000   \n",
       "1          M1               0.969231             0.961538   \n",
       "2          M2               0.946154             0.615385   \n",
       "3          M3               0.876923             1.000000   \n",
       "4          M4               0.869231             0.961538   \n",
       "5          M5               1.000000             1.000000   \n",
       "6          M6               1.000000             1.000000   \n",
       "7          M7               0.992308             0.884615   \n",
       "8          M8               0.915385             0.961538   \n",
       "9          M9               0.892308             0.769231   \n",
       "10        M10               0.892308             0.923077   \n",
       "11        M11               0.838462             1.000000   \n",
       "12        M12               0.738462             0.730769   \n",
       "13        M13               0.830769             1.000000   \n",
       "14        M14               0.823077             0.615385   \n",
       "15        M15               0.823077             0.846154   \n",
       "16        M16               1.000000             1.000000   \n",
       "17        M17               1.000000             1.000000   \n",
       "18        M18               0.992308             0.923077   \n",
       "19        M19               0.992308             1.000000   \n",
       "20        M20               0.876923             0.923077   \n",
       "21        M21               0.876923             1.000000   \n",
       "22       Mean               0.915734             0.914336   \n",
       "\n",
       "    Loc1_Sub5_Day0~4->6  Loc1_Sub5_Day0~4->7  Loc1_Sub5_Day0~4->8  \\\n",
       "0              1.000000             1.000000             1.000000   \n",
       "1              0.923077             0.653846             0.461538   \n",
       "2              0.730769             0.730769             0.730769   \n",
       "3              0.923077             0.769231             1.000000   \n",
       "4              0.230769             0.000000             0.076923   \n",
       "5              1.000000             0.961538             0.923077   \n",
       "6              0.653846             0.884615             0.576923   \n",
       "7              1.000000             1.000000             1.000000   \n",
       "8              0.884615             0.500000             0.807692   \n",
       "9              0.846154             0.769231             0.769231   \n",
       "10             1.000000             0.923077             1.000000   \n",
       "11             0.884615             1.000000             1.000000   \n",
       "12             0.769231             0.692308             0.269231   \n",
       "13             0.961538             1.000000             1.000000   \n",
       "14             0.269231             0.730769             0.000000   \n",
       "15             0.807692             0.576923             0.500000   \n",
       "16             1.000000             1.000000             1.000000   \n",
       "17             1.000000             1.000000             1.000000   \n",
       "18             1.000000             1.000000             0.384615   \n",
       "19             1.000000             0.923077             0.961538   \n",
       "20             0.884615             1.000000             0.615385   \n",
       "21             1.000000             1.000000             1.000000   \n",
       "22             0.853147             0.823427             0.730769   \n",
       "\n",
       "    Loc1_Sub5_Day0~4->9  \n",
       "0              1.000000  \n",
       "1              0.884615  \n",
       "2              0.807692  \n",
       "3              0.500000  \n",
       "4              0.000000  \n",
       "5              0.961538  \n",
       "6              1.000000  \n",
       "7              1.000000  \n",
       "8              0.884615  \n",
       "9              0.730769  \n",
       "10             1.000000  \n",
       "11             1.000000  \n",
       "12             0.307692  \n",
       "13             0.923077  \n",
       "14             0.153846  \n",
       "15             0.846154  \n",
       "16             1.000000  \n",
       "17             1.000000  \n",
       "18             1.000000  \n",
       "19             0.961538  \n",
       "20             0.538462  \n",
       "21             1.000000  \n",
       "22             0.795455  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truths = results[1]\n",
    "predictions = results[2]\n",
    "print(\"ground_truths  = \", np.shape(ground_truths))\n",
    "print(\"predictions = \", np.shape(predictions))\n",
    "df = get_gesture_accuracies(ground_truths, predictions, number_of_classes=number_of_classes, \n",
    "                            m_name=m_name, n_name=n_name, path=save_TSD, algo_name=algo_name,\n",
    "                           index_participant_list_customized=index_participant_list,\n",
    "                           lump_day_at_participant=5)\n",
    "df = pd.read_csv(save_SCADANN+'/'+algo_name+'.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "Suppose there is a ndarray of NxM dataloaders, then N group of models will be trained, and each group will have M model. Each group is independent of the other, and each model within a group is dependent on its previous training weights.\n",
    "\n",
    "In general, overall accuracies of SCADANN are better than DANN, and DANN is better than TSD.\n",
    "Occasionally accuracies of SCADANN end up a little smaller than DANN, reasons may be lack of datasets put into training model (fixed) and non-optimal percentage_same_gesture_sable (fixed). Code should be reproducible if processed dataset sticks to the shape defined above.  \n",
    "\n",
    "The amount of increase in accuracies from DANN to SCADANN looks random. But if the base model is better at classifying one session, then its corresponding SCADANN is also better at classifying the same session. Given such result, to obtain the best performance from SCADANN, a good model trained with good data should be the starting point.\n",
    "\n",
    "* What to check if sth goes wrong:\n",
    "    * percentage_same_gesture_sable\n",
    "    * number of cycles or sessions\n",
    "    * shape of dataloaders (combination of train, test, valid should include all dataset)\n",
    "    * shape of procssed datasets\n",
    "    * directory paths of weights and results\n",
    "    * if weights are stored or loaded correcltyTSD_acc_overall_one = np.mean(TSD_acc, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TSD\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Day_0~4</th>\n",
       "      <td>0.915734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_5</th>\n",
       "      <td>0.814685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_6</th>\n",
       "      <td>0.804196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_7</th>\n",
       "      <td>0.729021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_8</th>\n",
       "      <td>0.653846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_9</th>\n",
       "      <td>0.673077</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Participant_5\n",
       "Day_0~4      0.915734\n",
       "Day_5        0.814685\n",
       "Day_6        0.804196\n",
       "Day_7        0.729021\n",
       "Day_8        0.653846\n",
       "Day_9        0.673077"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DANN\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Day_0~4</th>\n",
       "      <td>0.915734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_5</th>\n",
       "      <td>0.90035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_6</th>\n",
       "      <td>0.835664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_7</th>\n",
       "      <td>0.770979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_8</th>\n",
       "      <td>0.711538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_9</th>\n",
       "      <td>0.70979</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Participant_5\n",
       "Day_0~4      0.915734\n",
       "Day_5         0.90035\n",
       "Day_6        0.835664\n",
       "Day_7        0.770979\n",
       "Day_8        0.711538\n",
       "Day_9         0.70979"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCADANN\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Day_0~4</th>\n",
       "      <td>0.915734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_5</th>\n",
       "      <td>0.914336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_6</th>\n",
       "      <td>0.853147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_7</th>\n",
       "      <td>0.823427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_8</th>\n",
       "      <td>0.730769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_9</th>\n",
       "      <td>0.795455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Participant_5\n",
       "Day_0~4      0.915734\n",
       "Day_5        0.914336\n",
       "Day_6        0.853147\n",
       "Day_7        0.823427\n",
       "Day_8        0.730769\n",
       "Day_9        0.795455"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"TSD\")\n",
    "display(TSD_df)\n",
    "print(\"DANN\")\n",
    "display(DANN_df)\n",
    "print(\"SCADANN\")\n",
    "display(SCADANN_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Day_5</th>\n",
       "      <td>0.09965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_6</th>\n",
       "      <td>0.048951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_7</th>\n",
       "      <td>0.094406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_8</th>\n",
       "      <td>0.076923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_9</th>\n",
       "      <td>0.122378</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Participant_5\n",
       "Day_5       0.09965\n",
       "Day_6      0.048951\n",
       "Day_7      0.094406\n",
       "Day_8      0.076923\n",
       "Day_9      0.122378"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "diff_df = SCADANN_df-TSD_df\n",
    "diff_df = diff_df.drop('Day_'+index_participant_list[0])\n",
    "display(diff_df)\n",
    "diff_df.to_csv(save_TSD+'/diff_results/across_day_loc1_lump5_diff.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Overall_Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TSD</th>\n",
       "      <td>0.765093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DANN</th>\n",
       "      <td>0.807343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SCADANN</th>\n",
       "      <td>0.838811</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Overall_Acc\n",
       "TSD         0.765093\n",
       "DANN        0.807343\n",
       "SCADANN     0.838811"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_acc_df = pd.DataFrame([TSD_acc_overall, DANN_acc_overall, SCADANN_acc_overall],\n",
    "                             index = [\"TSD\", \"DANN\", \"SCADANN\"],\n",
    "                             columns = [\"Overall_Acc\"])\n",
    "overall_acc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1800x1800 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(25,25))\n",
    "acc_list = [TSD_df, DANN_df, SCADANN_df, overall_acc_df]\n",
    "title_list = [\"TSD\", \"DANN\", \"SCADANN\", \"Overall\"]\n",
    "for idx, ax in enumerate(axes.reshape(-1)): \n",
    "    acc_list[idx].transpose().plot.bar(ax = ax, rot=0)\n",
    "    ax.set_title(title_list[idx])\n",
    "    ax.set_ylim([0, 1.0])\n",
    "    for p in ax.patches:\n",
    "        ax.annotate(str(np.round(p.get_height(),2)), (p.get_x()+p.get_width()/2., p.get_height()),\n",
    "                    ha='center', va='center', xytext=(0, 8),textcoords='offset points')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
