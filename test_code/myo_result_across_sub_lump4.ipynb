{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results of TSD, DANN, SCADANN models across 5 subjects starting at  subject_0+1+2+3 while wearing at neutral location\n",
    "\n",
    "\n",
    "Library used can be downloaded from https://github.com/aonai/long_term_EMG_myo   \n",
    "&emsp; Original by UlysseCoteAllard https://github.com/UlysseCoteAllard/LongTermEMG   \n",
    "Dataset recorded by https://github.com/Suguru55/Wearable_Sensor_Long-term_sEMG_Dataset   \n",
    "Extended robot project can be found in https://github.com/aonai/myo_robot_arm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "* weights for TSD are total of 15 training models, 5 for each wearing location\n",
    "* weights for DANN and SCADANN are total of 12 trianing models, 4 for each wearing location\n",
    "\n",
    "\n",
    "* training examples should have shape (1, 2,)\n",
    "* first session has shape (160, 572, 252)\n",
    "* the following sessions have shape (40, 572, 252)\n",
    "* training labels should have shape (1, 2, )\n",
    "\n",
    "\n",
    "* location 0, 1, and 2 corresponds to neutral position, inward rotation, and outward rotation respectively\n",
    "* session mentioned below are subjects, so number of sessions is 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_dir = \"/home/laiy/gitrepos/msr_final/LongTermEMG_myo\"\n",
    "os.chdir(code_dir)\n",
    "from PrepareAndLoadData.process_data import read_data_training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Prepare Data\n",
    "use `switch=1` to train across subjects and individually on each wearing location\n",
    "\n",
    "### specify the directories used for running the code:\n",
    "* `code_diar`: path to long_term_EMG_myo library\n",
    "* `data_dir`: where raw dataset is loaded; raw data is in csv format\n",
    "* `processed_data_dir`: where processed dataset is loaded; processed data is in npy pickle format\n",
    "    * processed data should be a ndarray of shape   \n",
    "    (controlling_factor_1 x controlling_factor_2 x num_sessions_per_gesture x #examples_window*#mov(26*22=572) x processed_channel_shape(252 for TSD, (4,8,10) for ConvNet)\n",
    "* `path_<model_name>`: where model weights are saved\n",
    "    * weights should be saved in folder `/Weights/<model_name>`. Each folder has subfolders containing weights for the first controlling factor.\n",
    "    * weights for base model (TSD or ConvNet) contain m set of training model\n",
    "    * weights for DANN and SCADANN contain m-1 set of trianing model (these models are trianed based on TSD, so they do not have a best_state_0.pt model). \n",
    "* `save_<model_name>`: where model results are saved\n",
    "    * each result for testing a model on a group of dataset is saved in folder `results`. Each result has corresponding \n",
    "        * `<model_name>.txt` includes predictions, ground truths, array of accuracies for each participant and each session, and overall accuracy\n",
    "        * `predictions_<model_name>.npy` includes array of accuracies, ground truths, predictions, and model outputs (probability array for each prediction)\n",
    "        * remember to make blank files in these names before saving\n",
    "\n",
    "\n",
    "\n",
    "* use `read_data_training` to process raw dataset\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/home/laiy/gitrepos/msr_final/Wearable_Sensor_Long-term_sEMG_Dataset/data\"\n",
    "processed_data_dir = \"/home/laiy/gitrepos/msr_final/LongTermEMG_myo/Processed_datasets_all_across_sub_lump4\"\n",
    "code_dir = \"/home/laiy/gitrepos/msr_final/LongTermEMG_myo\"\n",
    "save_dir = \"/home/laiy/gitrepos/msr_final/LongTermEMG_myo/Results\"\n",
    "\n",
    "path_TSD =\"/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_sub_lump4/TSD\"\n",
    "save_TSD = \"/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/results\"\n",
    "\n",
    "path_DANN =\"/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_sub_lump4/DANN\"\n",
    "save_DANN = \"/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/results\"\n",
    "\n",
    "path_SCADANN =\"/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_sub_lump4/SCADANN\"\n",
    "save_SCADANN = \"/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing Training datasets...\n",
      "index_participant_list  [1, 2, 3, 4, 5]\n",
      "session  0  --- process data in days  [1, 4, 9, 10, 12, 13, 19, 26, 27, 29]\n",
      "READ  Sub 1 _Loc 0 _Day 1\n",
      "READ  Sub 1 _Loc 0 _Day 4\n",
      "READ  Sub 1 _Loc 0 _Day 9\n",
      "READ  Sub 1 _Loc 0 _Day 10\n",
      "READ  Sub 1 _Loc 0 _Day 12\n",
      "READ  Sub 1 _Loc 0 _Day 13\n",
      "READ  Sub 1 _Loc 0 _Day 19\n",
      "READ  Sub 1 _Loc 0 _Day 26\n",
      "READ  Sub 1 _Loc 0 _Day 27\n",
      "READ  Sub 1 _Loc 0 _Day 29\n",
      "@ traning sessions =  (1, 40, 572, 252)\n",
      "session  0  --- process data in days  [1, 4, 9, 10, 12, 13, 19, 26, 27, 29]\n",
      "READ  Sub 2 _Loc 0 _Day 1\n",
      "READ  Sub 2 _Loc 0 _Day 4\n",
      "READ  Sub 2 _Loc 0 _Day 9\n",
      "READ  Sub 2 _Loc 0 _Day 10\n",
      "READ  Sub 2 _Loc 0 _Day 12\n",
      "READ  Sub 2 _Loc 0 _Day 13\n",
      "READ  Sub 2 _Loc 0 _Day 19\n",
      "READ  Sub 2 _Loc 0 _Day 26\n",
      "READ  Sub 2 _Loc 0 _Day 27\n",
      "READ  Sub 2 _Loc 0 _Day 29\n",
      "Include sub  2  in first dataset  (40, 572, 252)\n",
      "examples of first session =  (80, 572)\n",
      "@ traning sessions =  (1, 80, 572, 252)\n",
      "session  0  --- process data in days  [1, 4, 9, 10, 12, 13, 19, 26, 27, 29]\n",
      "READ  Sub 3 _Loc 0 _Day 1\n",
      "READ  Sub 3 _Loc 0 _Day 4\n",
      "READ  Sub 3 _Loc 0 _Day 9\n",
      "READ  Sub 3 _Loc 0 _Day 10\n",
      "READ  Sub 3 _Loc 0 _Day 12\n",
      "READ  Sub 3 _Loc 0 _Day 13\n",
      "READ  Sub 3 _Loc 0 _Day 19\n",
      "READ  Sub 3 _Loc 0 _Day 26\n",
      "READ  Sub 3 _Loc 0 _Day 27\n",
      "READ  Sub 3 _Loc 0 _Day 29\n",
      "Include sub  3  in first dataset  (80, 572, 252)\n",
      "examples of first session =  (120, 572)\n",
      "@ traning sessions =  (1, 120, 572, 252)\n",
      "session  0  --- process data in days  [1, 4, 9, 10, 12, 13, 19, 26, 27, 29]\n",
      "READ  Sub 4 _Loc 0 _Day 1\n",
      "READ  Sub 4 _Loc 0 _Day 4\n",
      "READ  Sub 4 _Loc 0 _Day 9\n",
      "READ  Sub 4 _Loc 0 _Day 10\n",
      "READ  Sub 4 _Loc 0 _Day 12\n",
      "READ  Sub 4 _Loc 0 _Day 13\n",
      "READ  Sub 4 _Loc 0 _Day 19\n",
      "READ  Sub 4 _Loc 0 _Day 26\n",
      "READ  Sub 4 _Loc 0 _Day 27\n",
      "READ  Sub 4 _Loc 0 _Day 29\n",
      "Include sub  4  in first dataset  (120, 572, 252)\n",
      "examples of first session =  (160, 572)\n",
      "@ traning sessions =  (1, 160, 572, 252)\n",
      "session  0  --- process data in days  [1, 4, 9, 10, 12, 13, 19, 26, 27, 29]\n",
      "READ  Sub 5 _Loc 0 _Day 1\n",
      "READ  Sub 5 _Loc 0 _Day 4\n",
      "READ  Sub 5 _Loc 0 _Day 9\n",
      "READ  Sub 5 _Loc 0 _Day 10\n",
      "READ  Sub 5 _Loc 0 _Day 12\n",
      "READ  Sub 5 _Loc 0 _Day 13\n",
      "READ  Sub 5 _Loc 0 _Day 19\n",
      "READ  Sub 5 _Loc 0 _Day 26\n",
      "READ  Sub 5 _Loc 0 _Day 27\n",
      "READ  Sub 5 _Loc 0 _Day 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laiy/.local/lib/python3.8/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@ traning sessions =  (2,)\n",
      "traning examples  (2,)\n",
      "traning labels  (2,)\n",
      "all traning examples  (1, 2)\n",
      "all traning labels  (1, 2)\n"
     ]
    }
   ],
   "source": [
    "# read_data_training(path=data_dir, store_path = processed_data_dir, switch=1, \\\n",
    "#                    include_in_first=4, sessions_to_include =[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traning examples  (1, 2)\n",
      "traning labels  (1, 2)\n"
     ]
    }
   ],
   "source": [
    "# check stored pickle \n",
    "with open(processed_data_dir + \"/training_session.pickle\", 'rb') as f:\n",
    "    dataset_training = pickle.load(file=f)\n",
    "\n",
    "examples_datasets_train = dataset_training['examples_training']\n",
    "print('traning examples ', np.shape(examples_datasets_train))\n",
    "labels_datasets_train = dataset_training['labels_training']\n",
    "print('traning labels ', np.shape(labels_datasets_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  examples_per_session =  (160, 572, 252)\n",
      "0  labels_per_session =  (160, 572)\n",
      "1  examples_per_session =  (40, 572, 252)\n",
      "1  labels_per_session =  (40, 572)\n"
     ]
    }
   ],
   "source": [
    "for idx, examples_per_session in enumerate (examples_datasets_train[0]):\n",
    "    print(idx, \" examples_per_session = \", np.shape(examples_per_session))\n",
    "    print(idx, \" labels_per_session = \", np.shape(labels_datasets_train[0][idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify params used for training and testing\n",
    "\n",
    "During training and testing, processed datasets are first put into pytorch dataloders, then feed to the model trainer; following are params for TSD model and dataloaders\n",
    "\n",
    "* `num_kernels`: list of integers defining number of neurons used in each linear layer (linear block has `dropout`=0.5)\n",
    "* `number_of_cycles_total`: number of trails performed for each session (assuming that all session have the same trail size)\n",
    "    * 40 for myo\n",
    "* `number_of_classes`: total number of gestures performed in dataset\n",
    "    * 22 for myo\n",
    "* `batch_size`: number of examples stored in each batch\n",
    "* `feature_vector_input_length`: length of input array or each processed signal; i.e. size of one training example \n",
    "    * 252 for TSD\n",
    "* `learning_rate`= 0.002515\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number_of_cycle_for_first_training  160\n",
      "number_of_cycles_total  40\n"
     ]
    }
   ],
   "source": [
    "num_kernels=[200, 200, 200]                                \n",
    "number_of_cycle_for_first_training = np.shape(examples_datasets_train[0][0])[0]               \n",
    "number_of_cycles_total=np.shape(examples_datasets_train[-1][-1])[0]               \n",
    "print(\"number_of_cycle_for_first_training \", number_of_cycle_for_first_training)\n",
    "print(\"number_of_cycles_total \", number_of_cycles_total)\n",
    "number_of_classes=22\n",
    "batch_size=128          \n",
    "feature_vector_input_length=252                     \n",
    "learning_rate=0.002515"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. TSD_DNN\n",
    "* `train_fine_tuning`: used to train data using a base model (TSD or ConvNet)\n",
    "    * running this function will save num_sessions sets of TSD model weights (each is fine tuned based on the previous training)  \n",
    "    \n",
    "* `test_standard_model_on_training_sessions`: test model result\n",
    "\n",
    "\n",
    "### check if dataloaders are loaded correctly:\n",
    "* each participant has shape (num_session x 40 x 572 x 252)\n",
    "* each session has shape (40 x 572 x 252)\n",
    "* put these data into on group ends up with shape (40*572=22880, 252)\n",
    "    * shuffle on group of data and put into dataloaders\n",
    "    * each participant should have num_sessions sets of dataloaders, each correspond to one session\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TrainingsAndEvaluations.ForTrainingSessions.train_tsd_dnn_standard import \\\n",
    "            test_standard_model_on_training_sessions, train_fine_tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET one participant_examples  (2,)\n",
      "   GET one training_index_examples  (160, 572, 252)  at  0\n",
      "   GOT one group XY  (91520, 252)    (91520,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (82368, 252)    (82368,)\n",
      "       one group XY valid (9152, 252)    (9152, 252)\n",
      "   GET one training_index_examples  (40, 572, 252)  at  1\n",
      "   GOT one group XY  (22880, 252)    (22880,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (20592, 252)    (20592,)\n",
      "       one group XY valid (2288, 252)    (2288, 252)\n",
      "dataloaders: \n",
      "   train  (1, 2)\n",
      "   valid  (1, 2)\n",
      "   test  (1, 0)\n",
      "START TRAINING\n",
      "Participant:  0\n",
      "Session:  0\n",
      "<generator object Module.parameters at 0x7f7cfc03e5f0>\n",
      "Epoch 0/499\n",
      "----------\n",
      "train Loss: 0.00798791 Acc: 0.63690708\n",
      "val Loss: 0.00007144 Acc: 0.76660839\n",
      "New best validation loss: 7.144057589185822e-05\n",
      "Epoch 1 of 500 took 4.201s\n",
      "Epoch 1/499\n",
      "----------\n",
      "train Loss: 0.00612848 Acc: 0.71550593\n",
      "val Loss: 0.00006608 Acc: 0.78496503\n",
      "Epoch 2 of 500 took 4.180s\n",
      "Epoch 2/499\n",
      "----------\n",
      "train Loss: 0.00567421 Acc: 0.73682932\n",
      "val Loss: 0.00006102 Acc: 0.79621941\n",
      "Epoch 3 of 500 took 4.261s\n",
      "Epoch 3/499\n",
      "----------\n",
      "train Loss: 0.00536686 Acc: 0.75161596\n",
      "val Loss: 0.00005590 Acc: 0.81741696\n",
      "Epoch 4 of 500 took 4.183s\n",
      "Epoch 4/499\n",
      "----------\n",
      "train Loss: 0.00522127 Acc: 0.7585172\n",
      "val Loss: 0.00006052 Acc: 0.79458042\n",
      "Epoch 5 of 500 took 4.266s\n",
      "Epoch 5/499\n",
      "----------\n",
      "train Loss: 0.00505545 Acc: 0.76730171\n",
      "val Loss: 0.00005523 Acc: 0.82069493\n",
      "Epoch 6 of 500 took 4.573s\n",
      "Epoch 6/499\n",
      "----------\n",
      "train Loss: 0.00495714 Acc: 0.77213744\n",
      "val Loss: 0.00005406 Acc: 0.82309878\n",
      "Epoch 7 of 500 took 4.537s\n",
      "Epoch 7/499\n",
      "----------\n",
      "train Loss: 0.00482717 Acc: 0.77786013\n",
      "val Loss: 0.00005180 Acc: 0.82648601\n",
      "Epoch 8 of 500 took 4.271s\n",
      "Epoch 8/499\n",
      "----------\n",
      "train Loss: 0.00474799 Acc: 0.78094625\n",
      "val Loss: 0.00004947 Acc: 0.83774038\n",
      "Epoch 9 of 500 took 4.193s\n",
      "Epoch 9/499\n",
      "----------\n",
      "train Loss: 0.00468250 Acc: 0.78416602\n",
      "val Loss: 0.00005086 Acc: 0.83271416\n",
      "Epoch 10 of 500 took 4.347s\n",
      "Epoch 10/499\n",
      "----------\n",
      "train Loss: 0.00464992 Acc: 0.78648668\n",
      "val Loss: 0.00004892 Acc: 0.84287587\n",
      "Epoch 11 of 500 took 4.512s\n",
      "Epoch 11/499\n",
      "----------\n",
      "train Loss: 0.00453949 Acc: 0.79086071\n",
      "val Loss: 0.00004756 Acc: 0.84156469\n",
      "Epoch 12 of 500 took 4.215s\n",
      "\n",
      "Training complete in 0m 52s\n",
      "Best val loss: 0.000071\n",
      "Session:  1\n",
      "<generator object Module.parameters at 0x7f7cfa78e9e0>\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_sub_lump4/TSD/participant_0/best_state_0.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_sub_lump4/TSD/participant_0/best_state_0.pt' (epoch 1)\n",
      "Epoch 0/499\n",
      "----------\n",
      "train Loss: 0.00560241 Acc: 0.74716797\n",
      "val Loss: 0.00019061 Acc: 0.83916084\n",
      "New best validation loss: 0.00019061192870140076\n",
      "Epoch 1 of 500 took 1.263s\n",
      "Epoch 1/499\n",
      "----------\n",
      "train Loss: 0.00450133 Acc: 0.79438477\n",
      "val Loss: 0.00015531 Acc: 0.86800699\n",
      "Epoch 2 of 500 took 1.336s\n",
      "Epoch 2/499\n",
      "----------\n",
      "train Loss: 0.00411373 Acc: 0.81362305\n",
      "val Loss: 0.00014771 Acc: 0.87412587\n",
      "Epoch 3 of 500 took 1.140s\n",
      "Epoch 3/499\n",
      "----------\n",
      "train Loss: 0.00385580 Acc: 0.82270508\n",
      "val Loss: 0.00012950 Acc: 0.89073427\n",
      "Epoch 4 of 500 took 1.065s\n",
      "Epoch 4/499\n",
      "----------\n",
      "train Loss: 0.00374033 Acc: 0.82773438\n",
      "val Loss: 0.00013958 Acc: 0.87893357\n",
      "Epoch 5 of 500 took 1.062s\n",
      "Epoch 5/499\n",
      "----------\n",
      "train Loss: 0.00357978 Acc: 0.83955078\n",
      "val Loss: 0.00013399 Acc: 0.88723776\n",
      "Epoch 6 of 500 took 1.061s\n",
      "Epoch 6/499\n",
      "----------\n",
      "train Loss: 0.00344370 Acc: 0.84453125\n",
      "val Loss: 0.00012071 Acc: 0.90078671\n",
      "Epoch 7 of 500 took 1.069s\n",
      "Epoch 7/499\n",
      "----------\n",
      "train Loss: 0.00333786 Acc: 0.85078125\n",
      "val Loss: 0.00011139 Acc: 0.90603147\n",
      "Epoch 8 of 500 took 1.061s\n",
      "Epoch 8/499\n",
      "----------\n",
      "train Loss: 0.00324392 Acc: 0.85332031\n",
      "val Loss: 0.00011853 Acc: 0.89641608\n",
      "Epoch 9 of 500 took 1.064s\n",
      "Epoch 9/499\n",
      "----------\n",
      "train Loss: 0.00319191 Acc: 0.8550293\n",
      "val Loss: 0.00011667 Acc: 0.90122378\n",
      "Epoch 10 of 500 took 1.074s\n",
      "Epoch 10/499\n",
      "----------\n",
      "train Loss: 0.00310545 Acc: 0.85737305\n",
      "val Loss: 0.00012620 Acc: 0.88636364\n",
      "Epoch 11 of 500 took 1.065s\n",
      "Epoch 11/499\n",
      "----------\n",
      "train Loss: 0.00305054 Acc: 0.86123047\n",
      "val Loss: 0.00014665 Acc: 0.87106643\n",
      "Epoch 12 of 500 took 1.063s\n",
      "\n",
      "Training complete in 0m 13s\n",
      "Best val loss: 0.000191\n"
     ]
    }
   ],
   "source": [
    "# train_fine_tuning(examples_datasets_train, labels_datasets_train,\n",
    "#                   num_kernels=num_kernels, path_weight_to_save_to=path_TSD,\n",
    "#                   number_of_classes=number_of_classes, \n",
    "#                   number_of_cycles_total=number_of_cycles_total,\n",
    "#                   number_of_cycle_for_first_training = number_of_cycle_for_first_training,\n",
    "#                   batch_size=batch_size,\n",
    "#                   feature_vector_input_length=feature_vector_input_length,\n",
    "#                   learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET one participant_examples  (2,)\n",
      "   GET one training_index_examples  (160, 572, 252)  at  0\n",
      "   GOT one group XY  (91520, 252)    (91520,)\n",
      "       one group XY test  (22880, 252)    (22880, 252)\n",
      "       one group XY train (82368, 252)    (82368,)\n",
      "       one group XY valid (9152, 252)    (9152, 252)\n",
      "   GET one training_index_examples  (40, 572, 252)  at  1\n",
      "   GOT one group XY  (22880, 252)    (22880,)\n",
      "       one group XY test  (5720, 252)    (5720, 252)\n",
      "       one group XY train (20592, 252)    (20592,)\n",
      "       one group XY valid (2288, 252)    (2288, 252)\n",
      "dataloaders: \n",
      "   train  (1, 2)\n",
      "   valid  (1, 2)\n",
      "   test  (1, 2)\n",
      "0  SESSION   data =  22880\n",
      "Participant:  0  Accuracy:  0.7699737762237763\n",
      "1  SESSION   data =  5720\n",
      "Participant:  0  Accuracy:  0.3814685314685315\n",
      "ACCURACY PARTICIPANT  0 :  [0.7699737762237763, 0.3814685314685315]\n",
      "[array([0.76997378, 0.38146853])]\n",
      "OVERALL ACCURACY: 0.5757211538461539\n"
     ]
    }
   ],
   "source": [
    "algo_name = \"standard_TSD\"\n",
    "test_standard_model_on_training_sessions(examples_datasets_train, labels_datasets_train,\n",
    "                                  num_neurons=num_kernels, use_only_first_training=True,\n",
    "                                  path_weights=path_TSD,\n",
    "                                  feature_vector_input_length=feature_vector_input_length,\n",
    "                                  save_path = save_TSD, algo_name=algo_name,\n",
    "                                  number_of_cycles_total=number_of_cycles_total,\n",
    "                                  number_of_cycle_for_first_training = number_of_cycle_for_first_training,\n",
    "                                  number_of_classes=number_of_classes, cycle_for_test=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Loc_0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Participant_0~3</th>\n",
       "      <td>0.769974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Participant_4</th>\n",
       "      <td>0.381469</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Loc_0\n",
       "Participant_0~3  0.769974\n",
       "Participant_4    0.381469"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_filename = save_TSD + '/predictions_' + algo_name + \"_no_retraining.npy\"\n",
    "results = np.load(results_filename, allow_pickle=True)\n",
    "TSD_acc = results[0]\n",
    "TSD_acc_overall = np.mean(TSD_acc)\n",
    "index_participant_list = ['0~3', 4]\n",
    "TSD_df = pd.DataFrame(TSD_acc.transpose(), \n",
    "                       index = [f'Participant_{i}' for i in index_participant_list],\n",
    "                        columns = [f'Loc_{j}' for j in range(TSD_acc.shape[0])])\n",
    "TSD_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "TSD_df.transpose().plot.bar(rot=0, figsize=(10,5))\n",
    "plt.title(\"TSD Accuracies\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TrainingsAndEvaluations.ForTrainingSessions.utils import get_gesture_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ground_truths  =  (1, 2)\n",
      "predictions =  (1, 2)\n",
      "index_participant_list  ['0~3', 4]\n",
      "accuracies_gestures =  (22, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Loc0_Sub0~3-&gt;0~3</th>\n",
       "      <th>Loc0_Sub0~3-&gt;4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M0</td>\n",
       "      <td>0.981731</td>\n",
       "      <td>0.988462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M1</td>\n",
       "      <td>0.592308</td>\n",
       "      <td>0.650000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M2</td>\n",
       "      <td>0.926923</td>\n",
       "      <td>0.219231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M3</td>\n",
       "      <td>0.881731</td>\n",
       "      <td>0.615385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M4</td>\n",
       "      <td>0.719231</td>\n",
       "      <td>0.011538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>M5</td>\n",
       "      <td>0.879808</td>\n",
       "      <td>0.646154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>M6</td>\n",
       "      <td>0.927885</td>\n",
       "      <td>0.811538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>M7</td>\n",
       "      <td>0.642308</td>\n",
       "      <td>0.080769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>M8</td>\n",
       "      <td>0.873077</td>\n",
       "      <td>0.269231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>M9</td>\n",
       "      <td>0.672115</td>\n",
       "      <td>0.092308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>M10</td>\n",
       "      <td>0.721154</td>\n",
       "      <td>0.707692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>M11</td>\n",
       "      <td>0.782692</td>\n",
       "      <td>0.484615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>M12</td>\n",
       "      <td>0.725962</td>\n",
       "      <td>0.565385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>M13</td>\n",
       "      <td>0.564423</td>\n",
       "      <td>0.542308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>M14</td>\n",
       "      <td>0.430769</td>\n",
       "      <td>0.119231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>M15</td>\n",
       "      <td>0.533654</td>\n",
       "      <td>0.030769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>M16</td>\n",
       "      <td>0.870192</td>\n",
       "      <td>0.461538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>M17</td>\n",
       "      <td>0.957692</td>\n",
       "      <td>0.042308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>M18</td>\n",
       "      <td>0.897115</td>\n",
       "      <td>0.523077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>M19</td>\n",
       "      <td>0.783654</td>\n",
       "      <td>0.123077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>M20</td>\n",
       "      <td>0.912500</td>\n",
       "      <td>0.234615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>M21</td>\n",
       "      <td>0.662500</td>\n",
       "      <td>0.173077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Mean</td>\n",
       "      <td>0.769974</td>\n",
       "      <td>0.381469</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Loc0_Sub0~3->0~3  Loc0_Sub0~3->4\n",
       "0          M0          0.981731        0.988462\n",
       "1          M1          0.592308        0.650000\n",
       "2          M2          0.926923        0.219231\n",
       "3          M3          0.881731        0.615385\n",
       "4          M4          0.719231        0.011538\n",
       "5          M5          0.879808        0.646154\n",
       "6          M6          0.927885        0.811538\n",
       "7          M7          0.642308        0.080769\n",
       "8          M8          0.873077        0.269231\n",
       "9          M9          0.672115        0.092308\n",
       "10        M10          0.721154        0.707692\n",
       "11        M11          0.782692        0.484615\n",
       "12        M12          0.725962        0.565385\n",
       "13        M13          0.564423        0.542308\n",
       "14        M14          0.430769        0.119231\n",
       "15        M15          0.533654        0.030769\n",
       "16        M16          0.870192        0.461538\n",
       "17        M17          0.957692        0.042308\n",
       "18        M18          0.897115        0.523077\n",
       "19        M19          0.783654        0.123077\n",
       "20        M20          0.912500        0.234615\n",
       "21        M21          0.662500        0.173077\n",
       "22       Mean          0.769974        0.381469"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truths = results[1]\n",
    "predictions = results[2]\n",
    "print(\"ground_truths  = \", np.shape(ground_truths))\n",
    "print(\"predictions = \", np.shape(predictions))\n",
    "m_name = \"Loc\"\n",
    "n_name = \"Sub0~3->\"\n",
    "df = get_gesture_accuracies(ground_truths, predictions, number_of_classes=number_of_classes, \n",
    "                            m_name=m_name, n_name=n_name, path=save_TSD, algo_name=algo_name,\n",
    "                           index_participant_list_customized=index_participant_list)\n",
    "df = pd.read_csv(save_TSD+'/'+algo_name+'.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. DANN\n",
    "* `train_DANN`: train DANN model using the first set of training weights from base model\n",
    "    * num_sessions-1 sets of training weights will be saved\n",
    "* `test_DANN_on_training_sessions`: test DANN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TrainingsAndEvaluations.ForTrainingSessions.train_tsd_dnn_DA import train_DANN, test_DANN_on_training_sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET one participant_examples  (2,)\n",
      "   GET one training_index_examples  (160, 572, 252)  at  0\n",
      "   GOT one group XY  (91520, 252)    (91520,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (82368, 252)    (82368,)\n",
      "       one group XY valid (9152, 252)    (9152, 252)\n",
      "   GET one training_index_examples  (40, 572, 252)  at  1\n",
      "   GOT one group XY  (22880, 252)    (22880,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (20592, 252)    (20592,)\n",
      "       one group XY valid (2288, 252)    (2288, 252)\n",
      "dataloaders: \n",
      "   train  (1, 2)\n",
      "   valid  (1, 2)\n",
      "   test  (1, 0)\n",
      "SHAPE SESSIONS:  (2,)\n",
      "()\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_sub_lump4/TSD/participant_0/best_state_0.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_sub_lump4/TSD/participant_0/best_state_0.pt' (epoch 1)\n",
      "STARTING TRAINING\n",
      "Accuracy source 0.667041, main loss classifier 0.529465, source classification loss 0.916982, loss domain distinction 0.189996, accuracy domain distinction 0.499707\n",
      "VALIDATION Loss: 1.16516221 Acc: 0.62740385\n",
      "New best validation loss:  1.165162205696106\n",
      "Epoch 1 of 500 took 2.376s\n",
      "Accuracy source 0.687646, main loss classifier 0.496291, source classification loss 0.853322, loss domain distinction 0.185283, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 1.78860927 Acc: 0.52425699\n",
      "Epoch 2 of 500 took 2.392s\n",
      "Accuracy source 0.704883, main loss classifier 0.481611, source classification loss 0.824503, loss domain distinction 0.185204, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 1.39776373 Acc: 0.59211101\n",
      "Epoch 3 of 500 took 2.337s\n",
      "Accuracy source 0.714209, main loss classifier 0.462114, source classification loss 0.786430, loss domain distinction 0.183716, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 1.54910254 Acc: 0.56184441\n",
      "Epoch 4 of 500 took 2.371s\n",
      "Accuracy source 0.719189, main loss classifier 0.456139, source classification loss 0.774937, loss domain distinction 0.183530, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 1.16295505 Acc: 0.61363636\n",
      "New best validation loss:  1.1629550457000732\n",
      "Epoch 5 of 500 took 2.311s\n",
      "Accuracy source 0.722070, main loss classifier 0.447988, source classification loss 0.759544, loss domain distinction 0.181279, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 1.26151597 Acc: 0.62751311\n",
      "Epoch     7: reducing learning rate of group 0 to 5.0300e-04.\n",
      "Epoch 6 of 500 took 2.360s\n",
      "Accuracy source 0.748389, main loss classifier 0.416639, source classification loss 0.699483, loss domain distinction 0.179305, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 1.26157713 Acc: 0.61844406\n",
      "Epoch 7 of 500 took 2.320s\n",
      "Accuracy source 0.753857, main loss classifier 0.408130, source classification loss 0.682656, loss domain distinction 0.178480, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 1.34908950 Acc: 0.60533217\n",
      "Epoch 8 of 500 took 2.301s\n",
      "Accuracy source 0.765869, main loss classifier 0.392440, source classification loss 0.651164, loss domain distinction 0.178757, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 1.26684368 Acc: 0.61975524\n",
      "Epoch 9 of 500 took 2.368s\n",
      "Accuracy source 0.766162, main loss classifier 0.396147, source classification loss 0.658754, loss domain distinction 0.178349, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 1.08334804 Acc: 0.64281031\n",
      "New best validation loss:  1.083348035812378\n",
      "Epoch 10 of 500 took 2.308s\n",
      "Accuracy source 0.768408, main loss classifier 0.389509, source classification loss 0.645361, loss domain distinction 0.178444, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 1.09910655 Acc: 0.64379371\n",
      "Epoch 11 of 500 took 2.308s\n",
      "Accuracy source 0.771240, main loss classifier 0.380907, source classification loss 0.628319, loss domain distinction 0.177813, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 1.36446106 Acc: 0.6048951\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0060e-04.\n",
      "Epoch 12 of 500 took 2.385s\n",
      "Accuracy source 0.779639, main loss classifier 0.372581, source classification loss 0.612059, loss domain distinction 0.177666, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 1.11408067 Acc: 0.65351836\n",
      "Epoch 13 of 500 took 2.302s\n",
      "Accuracy source 0.776270, main loss classifier 0.378227, source classification loss 0.623248, loss domain distinction 0.177950, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 1.29217708 Acc: 0.62248689\n",
      "Epoch 14 of 500 took 2.360s\n",
      "Accuracy source 0.773096, main loss classifier 0.379310, source classification loss 0.625677, loss domain distinction 0.177586, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 1.09012449 Acc: 0.65865385\n",
      "Epoch 15 of 500 took 2.299s\n",
      "Accuracy source 0.778955, main loss classifier 0.372029, source classification loss 0.611050, loss domain distinction 0.177591, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.99629551 Acc: 0.67285839\n",
      "New best validation loss:  0.9962955117225647\n",
      "Epoch 16 of 500 took 2.314s\n",
      "Accuracy source 0.781104, main loss classifier 0.373208, source classification loss 0.613519, loss domain distinction 0.177925, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 1.46587110 Acc: 0.61024913\n",
      "Epoch 17 of 500 took 2.354s\n",
      "Accuracy source 0.781592, main loss classifier 0.368028, source classification loss 0.603093, loss domain distinction 0.177574, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 1.11500013 Acc: 0.64925699\n",
      "Epoch 18 of 500 took 2.312s\n",
      "Accuracy source 0.779687, main loss classifier 0.373599, source classification loss 0.614237, loss domain distinction 0.177515, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 1.22361541 Acc: 0.62969843\n",
      "Epoch 19 of 500 took 2.357s\n",
      "Accuracy source 0.782813, main loss classifier 0.369375, source classification loss 0.605856, loss domain distinction 0.177436, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 1.09088469 Acc: 0.65712413\n",
      "Epoch 20 of 500 took 2.310s\n",
      "Accuracy source 0.786377, main loss classifier 0.367413, source classification loss 0.601943, loss domain distinction 0.177511, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 1.02084172 Acc: 0.66630245\n",
      "Epoch 21 of 500 took 2.319s\n",
      "Accuracy source 0.782715, main loss classifier 0.372077, source classification loss 0.611266, loss domain distinction 0.177423, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 1.12875414 Acc: 0.64739948\n",
      "Epoch 22 of 500 took 2.362s\n",
      "Accuracy source 0.787939, main loss classifier 0.363339, source classification loss 0.593934, loss domain distinction 0.176707, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 1.09527123 Acc: 0.66378934\n",
      "Epoch 23 of 500 took 2.314s\n",
      "Accuracy source 0.786768, main loss classifier 0.365961, source classification loss 0.598966, loss domain distinction 0.177388, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 1.13154018 Acc: 0.6520979\n",
      "Epoch 24 of 500 took 2.400s\n",
      "Accuracy source 0.790918, main loss classifier 0.358169, source classification loss 0.583370, loss domain distinction 0.177517, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.95765150 Acc: 0.68640734\n",
      "New best validation loss:  0.9576514959335327\n",
      "Epoch 25 of 500 took 2.312s\n",
      "Accuracy source 0.788477, main loss classifier 0.362451, source classification loss 0.591895, loss domain distinction 0.177310, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 1.09197211 Acc: 0.65756119\n",
      "Epoch 26 of 500 took 2.309s\n",
      "Accuracy source 0.786816, main loss classifier 0.364663, source classification loss 0.596346, loss domain distinction 0.177643, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 1.48824537 Acc: 0.61112325\n",
      "Epoch 27 of 500 took 2.477s\n",
      "Accuracy source 0.785449, main loss classifier 0.365607, source classification loss 0.598349, loss domain distinction 0.177673, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 1.12915289 Acc: 0.6517701\n",
      "Epoch 28 of 500 took 2.517s\n",
      "Accuracy source 0.785156, main loss classifier 0.365051, source classification loss 0.597195, loss domain distinction 0.177115, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 1.46781111 Acc: 0.60063374\n",
      "Epoch 29 of 500 took 2.486s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy source 0.787891, main loss classifier 0.362860, source classification loss 0.592858, loss domain distinction 0.177329, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 1.13557208 Acc: 0.64936626\n",
      "Epoch 30 of 500 took 2.414s\n",
      "Accuracy source 0.791846, main loss classifier 0.357186, source classification loss 0.581409, loss domain distinction 0.177425, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 1.20751858 Acc: 0.65472028\n",
      "Epoch 31 of 500 took 2.422s\n",
      "Accuracy source 0.786621, main loss classifier 0.366418, source classification loss 0.600048, loss domain distinction 0.176961, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 1.10316300 Acc: 0.66586538\n",
      "Epoch 32 of 500 took 2.367s\n",
      "Accuracy source 0.785693, main loss classifier 0.365195, source classification loss 0.597563, loss domain distinction 0.177136, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 1.50396550 Acc: 0.60576923\n",
      "Epoch 33 of 500 took 2.309s\n",
      "Accuracy source 0.789844, main loss classifier 0.357692, source classification loss 0.582605, loss domain distinction 0.176975, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 1.29620779 Acc: 0.63527098\n",
      "Epoch 34 of 500 took 2.309s\n",
      "Accuracy source 0.789307, main loss classifier 0.362435, source classification loss 0.591908, loss domain distinction 0.177359, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 1.34282959 Acc: 0.6215035\n",
      "Epoch 35 of 500 took 2.375s\n",
      "Accuracy source 0.783154, main loss classifier 0.365513, source classification loss 0.598072, loss domain distinction 0.177566, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 1.21734679 Acc: 0.63756556\n",
      "Training complete in 1m 25s\n"
     ]
    }
   ],
   "source": [
    "# train_DANN(examples_datasets_train, labels_datasets_train, \n",
    "#           num_kernels=num_kernels,\n",
    "#           path_weights_fine_tuning=path_TSD,\n",
    "#           number_of_classes=number_of_classes,\n",
    "#           number_of_cycles_total = number_of_cycles_total,\n",
    "#           number_of_cycle_for_first_training = number_of_cycle_for_first_training,\n",
    "#           batch_size=batch_size,\n",
    "#           feature_vector_input_length=feature_vector_input_length,\n",
    "#           path_weights_to_save_to=path_DANN, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET one participant_examples  (2,)\n",
      "   GET one training_index_examples  (160, 572, 252)  at  0\n",
      "   GOT one group XY  (91520, 252)    (91520,)\n",
      "       one group XY test  (22880, 252)    (22880, 252)\n",
      "       one group XY train (82368, 252)    (82368,)\n",
      "       one group XY valid (9152, 252)    (9152, 252)\n",
      "   GET one training_index_examples  (40, 572, 252)  at  1\n",
      "   GOT one group XY  (22880, 252)    (22880,)\n",
      "       one group XY test  (5720, 252)    (5720, 252)\n",
      "       one group XY train (20592, 252)    (20592,)\n",
      "       one group XY valid (2288, 252)    (2288, 252)\n",
      "dataloaders: \n",
      "   train  (1, 2)\n",
      "   valid  (1, 2)\n",
      "   test  (1, 2)\n",
      "(2,)\n",
      "Participant ID:  0  Session ID:  0  Accuracy:  0.7699737762237763\n",
      "Participant ID:  0  Session ID:  1  Accuracy:  0.5907342657342657\n",
      "ACCURACY PARTICIPANT:  [0.7699737762237763, 0.5907342657342657]\n",
      "[[0.76997378 0.59073427]]\n",
      "[array([0.76997378, 0.59073427])]\n",
      "OVERALL ACCURACY: 0.680354020979021\n"
     ]
    }
   ],
   "source": [
    "algo_name = \"DANN\"\n",
    "test_DANN_on_training_sessions(examples_datasets_train, labels_datasets_train,\n",
    "                              feature_vector_input_length=feature_vector_input_length,\n",
    "                              num_neurons=num_kernels, path_weights_DA=path_DANN,\n",
    "                              algo_name=algo_name, save_path = save_DANN, \n",
    "                              number_of_cycles_total=number_of_cycles_total,\n",
    "                              number_of_cycle_for_first_training = number_of_cycle_for_first_training,\n",
    "                              path_weights_normal=path_TSD, number_of_classes=number_of_classes,\n",
    "                              cycle_for_test=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Loc_0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Participant_0~3</th>\n",
       "      <td>0.769974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Participant_4</th>\n",
       "      <td>0.590734</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Loc_0\n",
       "Participant_0~3  0.769974\n",
       "Participant_4    0.590734"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_filename = save_DANN + '/predictions_' + algo_name + \".npy\"\n",
    "results = np.load(results_filename, allow_pickle=True)\n",
    "DANN_acc = results[0]\n",
    "DANN_acc_overall = np.mean(DANN_acc)\n",
    "DANN_df = pd.DataFrame(DANN_acc.transpose(), \n",
    "                       index = [f'Participant_{i}' for i in index_participant_list],\n",
    "                        columns = [f'Loc_{j}' for j in range(DANN_acc.shape[0])])\n",
    "DANN_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "DANN_df.transpose().plot.bar(rot=0, figsize=(10,5))\n",
    "plt.title(\"DANN Accuracies\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ground_truths  =  (1, 2)\n",
      "predictions =  (1, 2)\n",
      "index_participant_list  ['0~3', 4]\n",
      "accuracies_gestures =  (22, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Loc0_Sub0~3-&gt;0~3</th>\n",
       "      <th>Loc0_Sub0~3-&gt;4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M0</td>\n",
       "      <td>0.981731</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M1</td>\n",
       "      <td>0.592308</td>\n",
       "      <td>0.823077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M2</td>\n",
       "      <td>0.926923</td>\n",
       "      <td>0.646154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M3</td>\n",
       "      <td>0.881731</td>\n",
       "      <td>0.719231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M4</td>\n",
       "      <td>0.719231</td>\n",
       "      <td>0.342308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>M5</td>\n",
       "      <td>0.879808</td>\n",
       "      <td>0.773077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>M6</td>\n",
       "      <td>0.927885</td>\n",
       "      <td>0.503846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>M7</td>\n",
       "      <td>0.642308</td>\n",
       "      <td>0.661538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>M8</td>\n",
       "      <td>0.873077</td>\n",
       "      <td>0.430769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>M9</td>\n",
       "      <td>0.672115</td>\n",
       "      <td>0.534615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>M10</td>\n",
       "      <td>0.721154</td>\n",
       "      <td>0.711538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>M11</td>\n",
       "      <td>0.782692</td>\n",
       "      <td>0.703846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>M12</td>\n",
       "      <td>0.725962</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>M13</td>\n",
       "      <td>0.564423</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>M14</td>\n",
       "      <td>0.430769</td>\n",
       "      <td>0.165385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>M15</td>\n",
       "      <td>0.533654</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>M16</td>\n",
       "      <td>0.870192</td>\n",
       "      <td>0.719231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>M17</td>\n",
       "      <td>0.957692</td>\n",
       "      <td>0.473077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>M18</td>\n",
       "      <td>0.897115</td>\n",
       "      <td>0.888462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>M19</td>\n",
       "      <td>0.783654</td>\n",
       "      <td>0.388462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>M20</td>\n",
       "      <td>0.912500</td>\n",
       "      <td>0.707692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>M21</td>\n",
       "      <td>0.662500</td>\n",
       "      <td>0.353846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Mean</td>\n",
       "      <td>0.769974</td>\n",
       "      <td>0.590734</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Loc0_Sub0~3->0~3  Loc0_Sub0~3->4\n",
       "0          M0          0.981731        1.000000\n",
       "1          M1          0.592308        0.823077\n",
       "2          M2          0.926923        0.646154\n",
       "3          M3          0.881731        0.719231\n",
       "4          M4          0.719231        0.342308\n",
       "5          M5          0.879808        0.773077\n",
       "6          M6          0.927885        0.503846\n",
       "7          M7          0.642308        0.661538\n",
       "8          M8          0.873077        0.430769\n",
       "9          M9          0.672115        0.534615\n",
       "10        M10          0.721154        0.711538\n",
       "11        M11          0.782692        0.703846\n",
       "12        M12          0.725962        0.500000\n",
       "13        M13          0.564423        0.700000\n",
       "14        M14          0.430769        0.165385\n",
       "15        M15          0.533654        0.250000\n",
       "16        M16          0.870192        0.719231\n",
       "17        M17          0.957692        0.473077\n",
       "18        M18          0.897115        0.888462\n",
       "19        M19          0.783654        0.388462\n",
       "20        M20          0.912500        0.707692\n",
       "21        M21          0.662500        0.353846\n",
       "22       Mean          0.769974        0.590734"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truths = results[1]\n",
    "predictions = results[2]\n",
    "print(\"ground_truths  = \", np.shape(ground_truths))\n",
    "print(\"predictions = \", np.shape(predictions))\n",
    "df = get_gesture_accuracies(ground_truths, predictions, number_of_classes=number_of_classes, \n",
    "                            m_name=m_name, n_name=n_name, path=save_DANN, algo_name=algo_name,\n",
    "                           index_participant_list_customized=index_participant_list)\n",
    "df = pd.read_csv(save_DANN+'/'+algo_name+'.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. SCADANN\n",
    "\n",
    "* `run_SCADANN_training_sessions`: train SCADANN model. The first session uses TSD model_0 wegits; others use DANN weights\n",
    "    * specify `percentage_same_gesture_stable` based on the performance of most pseudo labels: \n",
    "        * print accuracies out and check what percentage will optimize `ACCURACY MODEL` and `ACCURACY PSEUDO` without cutting out too much data \n",
    "    * num_sessions-1 sets of training weights will be saved\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TrainingsAndEvaluations.ForTrainingSessions.train_tsd_dnn_SCADANN import \\\n",
    "    run_SCADANN_training_sessions, test_network_SCADANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET one participant_examples  (2,)\n",
      "   GET one training_index_examples  (160, 572, 252)  at  0\n",
      "   GOT one group XY  (91520, 252)    (91520,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (91520, 252)    (91520,)\n",
      "   GET one training_index_examples  (40, 572, 252)  at  1\n",
      "   GOT one group XY  (22880, 252)    (22880,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (22880, 252)    (22880,)\n",
      "dataloaders: \n",
      "   train  (1, 2)\n",
      "   valid  (0,)\n",
      "   test  (1, 0)\n",
      "participants_train =  1\n",
      "Optimizer =  <generator object Module.parameters at 0x7f7cfb69e900>\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_sub_lump4/DANN/participant_0/best_state_1.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_sub_lump4/DANN/participant_0/best_state_1.pt' (epoch 25)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_sub_lump4/TSD/participant_0/best_state_0.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_sub_lump4/TSD/participant_0/best_state_0.pt' (epoch 1)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_sub_lump4/DANN/participant_0/best_state_1.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_sub_lump4/DANN/participant_0/best_state_1.pt' (epoch 25)\n",
      "==== models_array =  (2,)  @ session  1\n",
      "HANDLING NEW SESSION  0\n",
      "HANDLING NEW SESSION  1\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  1.0  len before:  26   len after:  9\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.0  len before:  26   len after:  5\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  8\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  13\n",
      "BEFORE:  0.5   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.11538461538461539   AFTER:  0.0  len before:  26   len after:  2\n",
      "BEFORE:  0.11538461538461539   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  14\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  5\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  12\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.0  len before:  26   len after:  7\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.0  len before:  26   len after:  6\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  9\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.23076923076923078  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  6\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  17\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  15\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  1.0  len before:  26   len after:  6\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.0  len before:  26   len after:  12\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.5769230769230769  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.0  len before:  26   len after:  7\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.0  len before:  26   len after:  10\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  1.0  len before:  26   len after:  11\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.6923076923076923  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  1.0  len before:  26   len after:  14\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.34615384615384615  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.6111111111111112  len before:  26   len after:  18\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  5\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  9\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  7\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  2\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  12\n",
      "BEFORE:  0.46153846153846156   AFTER:  1.0  len before:  26   len after:  12\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.11538461538461539   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.0  len before:  26   len after:  6\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.6923076923076923  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  9\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  0.5   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.0  len before:  26   len after:  12\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  15\n",
      "BEFORE:  0.5   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.6538461538461539  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  0.8461538461538461  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.0  len before:  26   len after:  4\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.0  len before:  26   len after:  7\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.11538461538461539   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.0  len before:  26   len after:  7\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.0  len before:  26   len after:  13\n",
      "BEFORE:  0.15384615384615385   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.6538461538461539  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  13\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.5882352941176471  len before:  26   len after:  17\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.23076923076923078  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.5  len before:  26   len after:  4\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.11538461538461539   AFTER:  0.0  len before:  26   len after:  17\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.0  len before:  26   len after:  3\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.0  len before:  26   len after:  3\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.2  len before:  26   len after:  10\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.4230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.5769230769230769  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.0  len before:  26   len after:  16\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  9\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.8846153846153846  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.0  len before:  26   len after:  7\n",
      "BEFORE:  0.5   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  13\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  2\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  4\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.5769230769230769  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  16\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  14\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.5384615384615384  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  14\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  6\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  3\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.0  len before:  26   len after:  1\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  13\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.11538461538461539   AFTER:  0.0  len before:  26   len after:  4\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  5\n",
      "BEFORE:  0.34615384615384615   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.5   AFTER:  0.0  len before:  26   len after:  6\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  8\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  2\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  8\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  1.0  len before:  26   len after:  7\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  8\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  10\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  15\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  9\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.46153846153846156  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  5\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  7\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  1\n",
      "BEFORE:  0.3076923076923077   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.6875  len before:  26   len after:  16\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  6\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.9615384615384616  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.0  len before:  26   len after:  9\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.11538461538461539   AFTER:  0.0  len before:  26   len after:  4\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  6\n",
      "BEFORE:  0.07692307692307693   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  11\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.0  len before:  26   len after:  2\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  13\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  8\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  15\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  7\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.0  len before:  26   len after:  6\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.0  len before:  26   len after:  8\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  15\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  7\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.0  len before:  26   len after:  11\n",
      "BEFORE:  0.5   AFTER:  0.0  len before:  26   len after:  3\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.6666666666666666  len before:  26   len after:  18\n",
      "BEFORE:  0.5   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.4230769230769231   AFTER:  1.0  len before:  26   len after:  16\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  8\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  15\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  15\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  0.0  len before:  26   len after:  5\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  5\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.0  len before:  26   len after:  9\n",
      "BEFORE:  0.5   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.5   AFTER:  0.6538461538461539  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  16\n",
      "BEFORE:  0.7307692307692307   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  15\n",
      "BEFORE:  0.8076923076923077   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  13\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  14\n",
      "BEFORE:  0.46153846153846156   AFTER:  1.0  len before:  26   len after:  13\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.6  len before:  26   len after:  10\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  6\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  18\n",
      "BEFORE:  0.4230769230769231   AFTER:  1.0  len before:  26   len after:  8\n",
      "BEFORE:  0.5384615384615384   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.34615384615384615   AFTER:  1.0  len before:  26   len after:  1\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laiy/.local/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/laiy/.local/lib/python3.8/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE:  0.5384615384615384   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.11538461538461539   AFTER:  0.0  len before:  26   len after:  17\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.4230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.15384615384615385  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  1.0  len before:  26   len after:  8\n",
      "BEFORE:  0.4230769230769231   AFTER:  1.0  len before:  26   len after:  2\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  6\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  16\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.0  len before:  26   len after:  1\n",
      "BEFORE:  0.46153846153846156   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  5\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  8\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.8846153846153846  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.11538461538461539   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  15\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  12\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.0  len before:  26   len after:  3\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.0  len before:  26   len after:  6\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.11538461538461539   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.0  len before:  26   len after:  5\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.11538461538461539   AFTER:  0.0  len before:  26   len after:  5\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.0  len before:  26   len after:  10\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  13\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.34615384615384615  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  15\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  5\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.11538461538461539   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  0.7692307692307693   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.6923076923076923  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.5769230769230769  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.14285714285714285  len before:  26   len after:  7\n",
      "BEFORE:  0.5   AFTER:  1.0  len before:  26   len after:  15\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.0  len before:  26   len after:  2\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.6538461538461539  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  3\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.0  len before:  26   len after:  2\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.038461538461538464  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.3076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  15\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  11\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.0  len before:  26   len after:  1\n",
      "BEFORE:  0.23076923076923078   AFTER:  1.0  len before:  26   len after:  6\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  7\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.0  len before:  26   len after:  4\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.4230769230769231   AFTER:  1.0  len before:  26   len after:  12\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.0  len before:  26   len after:  17\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  12\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  1\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  12\n",
      "BEFORE:  0.6923076923076923   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.0  len before:  26   len after:  6\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  14\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  7\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  2\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  5\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.0  len before:  26   len after:  1\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.5769230769230769   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.3076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.5   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.6923076923076923  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.0  len before:  26   len after:  10\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  5\n",
      "BEFORE:  0.46153846153846156   AFTER:  1.0  len before:  26   len after:  6\n",
      "BEFORE:  0.5   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  7\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.6470588235294118  len before:  26   len after:  17\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  6\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.2692307692307692  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.0  len before:  26   len after:  13\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  11\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  16\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  5\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  2\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  14\n",
      "BEFORE:  0.19230769230769232   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.038461538461538464   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.07692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  5\n",
      "BEFORE:  0.11538461538461539   AFTER:  0.0  len before:  26   len after:  11\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  5\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  13\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.5384615384615384  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.11538461538461539   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.15384615384615385  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  6\n",
      "BEFORE:  0.11538461538461539   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  12\n",
      "BEFORE:  0.38461538461538464   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.0  len before:  26   len after:  13\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  9\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.6923076923076923  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.0  len before:  26   len after:  17\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.8571428571428571  len before:  26   len after:  7\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  13\n",
      "BEFORE:  0.0   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.0  len before:  26   len after:  17\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  6\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  12\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  1\n",
      "BEFORE:  0.5   AFTER:  0.0  len before:  26   len after:  1\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  7\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  9\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.5769230769230769  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  17\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.11538461538461539   AFTER:  0.0  len before:  26   len after:  8\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.5384615384615384   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  18\n",
      "BEFORE:  0.46153846153846156   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  8\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.46153846153846156  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.11538461538461539   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  4\n",
      "BEFORE:  0.0   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.2692307692307692  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  1.0  len before:  26   len after:  16\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  0.46153846153846156   AFTER:  1.0  len before:  26   len after:  12\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  8\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  15\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  4\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.0  len before:  26   len after:  15\n",
      "BEFORE:  0.11538461538461539   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.23076923076923078  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  4\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  1\n",
      "BEFORE:  0.0   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.23076923076923078  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  nan  len before:  26   len after:  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  9\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.4444444444444444  len before:  26   len after:  9\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.11538461538461539   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.8888888888888888  len before:  26   len after:  9\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  5\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.5769230769230769  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.6538461538461539  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  1\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  5\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.5769230769230769  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.11538461538461539   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  15\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  9\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  17\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.0  len before:  26   len after:  7\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  4\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  10\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.0  len before:  26   len after:  15\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  14\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.0  len before:  26   len after:  7\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.6538461538461539  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  8\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  9\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.0  len before:  26   len after:  4\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.34615384615384615  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.11538461538461539   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  15\n",
      "BEFORE:  0.0   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.0  len before:  26   len after:  7\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  8\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  14\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  14\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.625  len before:  26   len after:  16\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  1.0  len before:  26   len after:  8\n",
      "BEFORE:  0.19230769230769232   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  18\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.4230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.0  len before:  26   len after:  4\n",
      "BEFORE:  0.11538461538461539   AFTER:  0.0  len before:  26   len after:  3\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.9230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  1.0  len before:  26   len after:  8\n",
      "BEFORE:  0.11538461538461539   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.5769230769230769  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  1.0  len before:  26   len after:  14\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.0   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  1.0  len before:  26   len after:  11\n",
      "BEFORE:  0.8076923076923077   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  11\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.46153846153846156  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.6923076923076923  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.7222222222222222  len before:  26   len after:  18\n",
      "BEFORE:  0.5   AFTER:  0.5384615384615384  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.16666666666666666  len before:  26   len after:  18\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.5882352941176471  len before:  26   len after:  17\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.16666666666666666  len before:  26   len after:  6\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  1\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.34615384615384615  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.0  len before:  26   len after:  5\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  8\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  11\n",
      "BEFORE:  0.38461538461538464   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.0  len before:  26   len after:  5\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.23076923076923078  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.5  len before:  26   len after:  10\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  16\n",
      "BEFORE:  0.7692307692307693   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.5   AFTER:  0.34615384615384615  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  15\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.0  len before:  26   len after:  5\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  9\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.0   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.8076923076923077   AFTER:  0.8846153846153846  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  4\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.6538461538461539  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  16\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.11538461538461539   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  8\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  7\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.0  len before:  26   len after:  18\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  16\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.2222222222222222  len before:  26   len after:  9\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.4230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  12\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  13\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  17\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.0  len before:  26   len after:  10\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  13\n",
      "BEFORE:  0.6923076923076923   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.0  len before:  26   len after:  15\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  16\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.23076923076923078  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.4230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  9\n",
      "BEFORE:  0.19230769230769232   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  17\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  2\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  8\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  10\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.5555555555555556  len before:  26   len after:  9\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  13\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  10\n",
      "ACCURACY MODEL:  0.5945367132867133   Accuracy pseudo: 0.7185827643476657  len pseudo:  16257    len predictions 22880\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING TRAINING\n",
      "Accuracy total 0.784367, main loss classifier 0.755536, source accuracy 0.775400 source classification loss 0.631697, target accuracy 0.793334 target loss 0.669598 accuracy domain distinction 0.500000 loss domain distinction 1.048884,\n",
      "VALIDATION Loss: 0.41728364 Acc: 0.87423124\n",
      "New best validation loss:  0.41728364457102385\n",
      "Epoch 2 of 500 took 2.696s\n",
      "Accuracy total 0.794681, main loss classifier 0.723666, source accuracy 0.775400 source classification loss 0.638775, target accuracy 0.813962 target loss 0.603706 accuracy domain distinction 0.500000 loss domain distinction 1.024257,\n",
      "VALIDATION Loss: 0.55148096 Acc: 0.8299508\n",
      "Epoch 3 of 500 took 2.692s\n",
      "Accuracy total 0.796144, main loss classifier 0.711445, source accuracy 0.774323 source classification loss 0.636196, target accuracy 0.817965 target loss 0.581793 accuracy domain distinction 0.500000 loss domain distinction 1.024503,\n",
      "VALIDATION Loss: 0.40639625 Acc: 0.89298893\n",
      "New best validation loss:  0.40639625431275833\n",
      "Epoch 4 of 500 took 2.726s\n",
      "Accuracy total 0.797452, main loss classifier 0.715323, source accuracy 0.770628 source classification loss 0.651247, target accuracy 0.824276 target loss 0.574770 accuracy domain distinction 0.500000 loss domain distinction 1.023148,\n",
      "VALIDATION Loss: 0.39290912 Acc: 0.88591636\n",
      "New best validation loss:  0.3929091166047489\n",
      "Epoch 5 of 500 took 2.718s\n",
      "Accuracy total 0.796490, main loss classifier 0.707378, source accuracy 0.769627 source classification loss 0.641865, target accuracy 0.823353 target loss 0.567987 accuracy domain distinction 0.500000 loss domain distinction 1.024522,\n",
      "VALIDATION Loss: 0.55475015 Acc: 0.83148831\n",
      "Epoch 6 of 500 took 2.700s\n",
      "Accuracy total 0.800185, main loss classifier 0.706070, source accuracy 0.772014 source classification loss 0.650127, target accuracy 0.828356 target loss 0.557497 accuracy domain distinction 0.500000 loss domain distinction 1.022581,\n",
      "VALIDATION Loss: 0.47047995 Acc: 0.85455105\n",
      "Epoch 7 of 500 took 2.686s\n",
      "Accuracy total 0.802147, main loss classifier 0.697657, source accuracy 0.774477 source classification loss 0.640502, target accuracy 0.829818 target loss 0.550155 accuracy domain distinction 0.500000 loss domain distinction 1.023281,\n",
      "VALIDATION Loss: 0.47060550 Acc: 0.85947109\n",
      "Epoch 8 of 500 took 2.732s\n",
      "Accuracy total 0.803494, main loss classifier 0.695910, source accuracy 0.775939 source classification loss 0.643157, target accuracy 0.831050 target loss 0.544041 accuracy domain distinction 0.500000 loss domain distinction 1.023111,\n",
      "VALIDATION Loss: 0.41535834 Acc: 0.86838868\n",
      "Epoch 9 of 500 took 2.709s\n",
      "Accuracy total 0.803995, main loss classifier 0.692470, source accuracy 0.770397 source classification loss 0.649161, target accuracy 0.837592 target loss 0.531333 accuracy domain distinction 0.500000 loss domain distinction 1.022226,\n",
      "VALIDATION Loss: 0.36822651 Acc: 0.89760148\n",
      "New best validation loss:  0.3682265129743838\n",
      "Epoch 10 of 500 took 2.759s\n",
      "Accuracy total 0.801686, main loss classifier 0.692814, source accuracy 0.768165 source classification loss 0.637420, target accuracy 0.835206 target loss 0.543790 accuracy domain distinction 0.500000 loss domain distinction 1.022085,\n",
      "VALIDATION Loss: 0.41879609 Acc: 0.86808118\n",
      "Epoch 11 of 500 took 2.692s\n",
      "Accuracy total 0.800493, main loss classifier 0.689846, source accuracy 0.768473 source classification loss 0.647568, target accuracy 0.832512 target loss 0.527475 accuracy domain distinction 0.500000 loss domain distinction 1.023247,\n",
      "VALIDATION Loss: 0.37867068 Acc: 0.88591636\n",
      "Epoch 12 of 500 took 2.700s\n",
      "Accuracy total 0.804918, main loss classifier 0.688547, source accuracy 0.773861 source classification loss 0.638970, target accuracy 0.835976 target loss 0.533534 accuracy domain distinction 0.500000 loss domain distinction 1.022950,\n",
      "VALIDATION Loss: 0.36186971 Acc: 0.89821648\n",
      "New best validation loss:  0.3618697115019256\n",
      "Epoch 13 of 500 took 2.692s\n",
      "Accuracy total 0.805457, main loss classifier 0.683155, source accuracy 0.771167 source classification loss 0.639030, target accuracy 0.839748 target loss 0.522760 accuracy domain distinction 0.500000 loss domain distinction 1.022602,\n",
      "VALIDATION Loss: 0.39807628 Acc: 0.88530135\n",
      "Epoch 14 of 500 took 2.692s\n",
      "Accuracy total 0.805226, main loss classifier 0.676968, source accuracy 0.770859 source classification loss 0.641085, target accuracy 0.839594 target loss 0.508250 accuracy domain distinction 0.500000 loss domain distinction 1.023000,\n",
      "VALIDATION Loss: 0.47528166 Acc: 0.84563346\n",
      "Epoch 15 of 500 took 2.723s\n",
      "Accuracy total 0.806689, main loss classifier 0.679188, source accuracy 0.774323 source classification loss 0.635801, target accuracy 0.839055 target loss 0.518014 accuracy domain distinction 0.500000 loss domain distinction 1.022808,\n",
      "VALIDATION Loss: 0.34388809 Acc: 0.90436654\n",
      "New best validation loss:  0.3438880875998852\n",
      "Epoch 16 of 500 took 2.707s\n",
      "Accuracy total 0.809267, main loss classifier 0.674663, source accuracy 0.775554 source classification loss 0.630694, target accuracy 0.842980 target loss 0.514226 accuracy domain distinction 0.500000 loss domain distinction 1.022027,\n",
      "VALIDATION Loss: 0.36787781 Acc: 0.89268143\n",
      "Epoch 17 of 500 took 2.687s\n",
      "Accuracy total 0.810422, main loss classifier 0.675371, source accuracy 0.776401 source classification loss 0.636756, target accuracy 0.844443 target loss 0.509592 accuracy domain distinction 0.500000 loss domain distinction 1.021972,\n",
      "VALIDATION Loss: 0.39205049 Acc: 0.88806888\n",
      "Epoch 18 of 500 took 2.679s\n",
      "Accuracy total 0.808882, main loss classifier 0.677006, source accuracy 0.774631 source classification loss 0.637589, target accuracy 0.843134 target loss 0.511747 accuracy domain distinction 0.500000 loss domain distinction 1.023387,\n",
      "VALIDATION Loss: 0.44689661 Acc: 0.85824108\n",
      "Epoch 19 of 500 took 2.691s\n",
      "Accuracy total 0.806419, main loss classifier 0.677106, source accuracy 0.771706 source classification loss 0.635947, target accuracy 0.841133 target loss 0.513781 accuracy domain distinction 0.500000 loss domain distinction 1.022427,\n",
      "VALIDATION Loss: 0.37255343 Acc: 0.89483395\n",
      "Epoch 20 of 500 took 2.692s\n",
      "Accuracy total 0.809421, main loss classifier 0.667553, source accuracy 0.771398 source classification loss 0.637853, target accuracy 0.847445 target loss 0.492554 accuracy domain distinction 0.500000 loss domain distinction 1.023490,\n",
      "VALIDATION Loss: 0.43040248 Acc: 0.87546125\n",
      "Epoch 21 of 500 took 2.685s\n",
      "Accuracy total 0.812269, main loss classifier 0.671423, source accuracy 0.779172 source classification loss 0.634973, target accuracy 0.845366 target loss 0.503351 accuracy domain distinction 0.500000 loss domain distinction 1.022614,\n",
      "VALIDATION Loss: 0.37964851 Acc: 0.89360394\n",
      "Epoch    21: reducing learning rate of group 0 to 2.0120e-05.\n",
      "Epoch 22 of 500 took 2.677s\n",
      "Accuracy total 0.809768, main loss classifier 0.670837, source accuracy 0.772398 source classification loss 0.643082, target accuracy 0.847137 target loss 0.494373 accuracy domain distinction 0.500000 loss domain distinction 1.021096,\n",
      "VALIDATION Loss: 0.35137352 Acc: 0.90129151\n",
      "Epoch 23 of 500 took 2.698s\n",
      "Accuracy total 0.813847, main loss classifier 0.664077, source accuracy 0.780249 source classification loss 0.632875, target accuracy 0.847445 target loss 0.491096 accuracy domain distinction 0.500000 loss domain distinction 1.020913,\n",
      "VALIDATION Loss: 0.37214908 Acc: 0.89421894\n",
      "Epoch 24 of 500 took 2.688s\n",
      "Accuracy total 0.811461, main loss classifier 0.671409, source accuracy 0.774246 source classification loss 0.639307, target accuracy 0.848676 target loss 0.499094 accuracy domain distinction 0.500000 loss domain distinction 1.022083,\n",
      "VALIDATION Loss: 0.36070992 Acc: 0.89421894\n",
      "Epoch 25 of 500 took 2.677s\n",
      "Accuracy total 0.814155, main loss classifier 0.666142, source accuracy 0.777709 source classification loss 0.623629, target accuracy 0.850600 target loss 0.504276 accuracy domain distinction 0.500000 loss domain distinction 1.021893,\n",
      "VALIDATION Loss: 0.39328132 Acc: 0.87761378\n",
      "Epoch 26 of 500 took 2.680s\n",
      "Accuracy total 0.813578, main loss classifier 0.655180, source accuracy 0.777401 source classification loss 0.613431, target accuracy 0.849754 target loss 0.492661 accuracy domain distinction 0.500000 loss domain distinction 1.021338,\n",
      "VALIDATION Loss: 0.35429425 Acc: 0.90098401\n",
      "Epoch 27 of 500 took 2.824s\n",
      "Training complete in 1m 10s\n",
      "['participant_0']\n"
     ]
    }
   ],
   "source": [
    "# percentage_same_gesture_stable = 0.75 \n",
    "# run_SCADANN_training_sessions(examples_datasets=examples_datasets_train, labels_datasets=labels_datasets_train,\n",
    "#                               num_kernels=num_kernels, feature_vector_input_length=feature_vector_input_length,\n",
    "#                               path_weights_to_save_to=path_SCADANN,\n",
    "#                               path_weights_Adversarial_training=path_DANN,\n",
    "#                               path_weights_Normal_training=path_TSD,\n",
    "#                               number_of_cycles_total = number_of_cycles_total, \n",
    "#                               number_of_cycle_for_first_training = number_of_cycle_for_first_training,\n",
    "#                               number_of_classes=number_of_classes,\n",
    "#                               learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET one participant_examples  (2,)\n",
      "   GET one training_index_examples  (160, 572, 252)  at  0\n",
      "   GOT one group XY  (91520, 252)    (91520,)\n",
      "       one group XY test  (22880, 252)    (22880, 252)\n",
      "       one group XY train (82368, 252)    (82368,)\n",
      "       one group XY valid (9152, 252)    (9152, 252)\n",
      "   GET one training_index_examples  (40, 572, 252)  at  1\n",
      "   GOT one group XY  (22880, 252)    (22880,)\n",
      "       one group XY test  (5720, 252)    (5720, 252)\n",
      "       one group XY train (20592, 252)    (20592,)\n",
      "       one group XY valid (2288, 252)    (2288, 252)\n",
      "dataloaders: \n",
      "   train  (1, 2)\n",
      "   valid  (1, 2)\n",
      "   test  (1, 2)\n",
      "Participant:  0  Accuracy:  0.7699737762237763\n",
      "Participant:  0  Accuracy:  0.6260489510489511\n",
      "ACCURACY PARTICIPANT:  [0.7699737762237763, 0.6260489510489511]\n",
      "[[0.76997378 0.62604895]]\n",
      "[array([0.76997378, 0.62604895])]\n",
      "OVERALL ACCURACY: 0.6980113636363636\n"
     ]
    }
   ],
   "source": [
    "algo_name = \"SCADANN\"\n",
    "test_network_SCADANN(examples_datasets_train=examples_datasets_train, labels_datasets_train=labels_datasets_train,\n",
    "                     num_neurons=num_kernels, feature_vector_input_length=feature_vector_input_length,\n",
    "                     path_weights_SCADANN =path_SCADANN, path_weights_normal=path_TSD,\n",
    "                     algo_name=algo_name, cycle_test=3, number_of_cycles_total=number_of_cycles_total,\n",
    "                     number_of_cycle_for_first_training = number_of_cycle_for_first_training,\n",
    "                     number_of_classes=number_of_classes, save_path = save_SCADANN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Loc_0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Participant_0~3</th>\n",
       "      <td>0.769974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Participant_4</th>\n",
       "      <td>0.626049</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Loc_0\n",
       "Participant_0~3  0.769974\n",
       "Participant_4    0.626049"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_filename = save_SCADANN + '/predictions_' + algo_name + \".npy\"\n",
    "results = np.load(results_filename, allow_pickle=True)\n",
    "SCADANN_acc = results[0]\n",
    "SCADANN_acc_overall = np.mean(SCADANN_acc)\n",
    "SCADANN_df = pd.DataFrame(SCADANN_acc.transpose(), \n",
    "                       index = [f'Participant_{i}' for i in index_participant_list],\n",
    "                        columns = [f'Loc_{j}' for j in range(SCADANN_acc.shape[0])])\n",
    "SCADANN_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "SCADANN_df.transpose().plot.bar(rot=0, figsize=(10,5))\n",
    "plt.title(\"SCADANN Accuracies\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ground_truths  =  (1, 2)\n",
      "predictions =  (1, 2)\n",
      "index_participant_list  ['0~3', 4]\n",
      "accuracies_gestures =  (22, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Loc0_Sub0~3-&gt;0~3</th>\n",
       "      <th>Loc0_Sub0~3-&gt;4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M0</td>\n",
       "      <td>0.981731</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M1</td>\n",
       "      <td>0.592308</td>\n",
       "      <td>0.830769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M2</td>\n",
       "      <td>0.926923</td>\n",
       "      <td>0.788462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M3</td>\n",
       "      <td>0.881731</td>\n",
       "      <td>0.776923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M4</td>\n",
       "      <td>0.719231</td>\n",
       "      <td>0.369231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>M5</td>\n",
       "      <td>0.879808</td>\n",
       "      <td>0.823077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>M6</td>\n",
       "      <td>0.927885</td>\n",
       "      <td>0.603846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>M7</td>\n",
       "      <td>0.642308</td>\n",
       "      <td>0.707692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>M8</td>\n",
       "      <td>0.873077</td>\n",
       "      <td>0.423077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>M9</td>\n",
       "      <td>0.672115</td>\n",
       "      <td>0.569231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>M10</td>\n",
       "      <td>0.721154</td>\n",
       "      <td>0.842308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>M11</td>\n",
       "      <td>0.782692</td>\n",
       "      <td>0.738462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>M12</td>\n",
       "      <td>0.725962</td>\n",
       "      <td>0.557692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>M13</td>\n",
       "      <td>0.564423</td>\n",
       "      <td>0.715385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>M14</td>\n",
       "      <td>0.430769</td>\n",
       "      <td>0.103846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>M15</td>\n",
       "      <td>0.533654</td>\n",
       "      <td>0.146154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>M16</td>\n",
       "      <td>0.870192</td>\n",
       "      <td>0.680769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>M17</td>\n",
       "      <td>0.957692</td>\n",
       "      <td>0.607692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>M18</td>\n",
       "      <td>0.897115</td>\n",
       "      <td>0.907692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>M19</td>\n",
       "      <td>0.783654</td>\n",
       "      <td>0.334615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>M20</td>\n",
       "      <td>0.912500</td>\n",
       "      <td>0.834615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>M21</td>\n",
       "      <td>0.662500</td>\n",
       "      <td>0.411538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Mean</td>\n",
       "      <td>0.769974</td>\n",
       "      <td>0.626049</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Loc0_Sub0~3->0~3  Loc0_Sub0~3->4\n",
       "0          M0          0.981731        1.000000\n",
       "1          M1          0.592308        0.830769\n",
       "2          M2          0.926923        0.788462\n",
       "3          M3          0.881731        0.776923\n",
       "4          M4          0.719231        0.369231\n",
       "5          M5          0.879808        0.823077\n",
       "6          M6          0.927885        0.603846\n",
       "7          M7          0.642308        0.707692\n",
       "8          M8          0.873077        0.423077\n",
       "9          M9          0.672115        0.569231\n",
       "10        M10          0.721154        0.842308\n",
       "11        M11          0.782692        0.738462\n",
       "12        M12          0.725962        0.557692\n",
       "13        M13          0.564423        0.715385\n",
       "14        M14          0.430769        0.103846\n",
       "15        M15          0.533654        0.146154\n",
       "16        M16          0.870192        0.680769\n",
       "17        M17          0.957692        0.607692\n",
       "18        M18          0.897115        0.907692\n",
       "19        M19          0.783654        0.334615\n",
       "20        M20          0.912500        0.834615\n",
       "21        M21          0.662500        0.411538\n",
       "22       Mean          0.769974        0.626049"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truths = results[1]\n",
    "predictions = results[2]\n",
    "print(\"ground_truths  = \", np.shape(ground_truths))\n",
    "print(\"predictions = \", np.shape(predictions))\n",
    "df = get_gesture_accuracies(ground_truths, predictions, number_of_classes=number_of_classes, \n",
    "                            m_name=m_name, n_name=n_name, path=save_SCADANN, algo_name=algo_name,\n",
    "                           index_participant_list_customized=index_participant_list)\n",
    "df = pd.read_csv(save_SCADANN+'/'+algo_name+'.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "Suppose there is a ndarray of NxM dataloaders, then N group of models will be trained, and each group will have M model. Each group is independent of the other, and each model within a group is dependent on its previous training weights.\n",
    "\n",
    "In general, overall accuracies of SCADANN are better than DANN, and DANN is better than TSD.\n",
    "Occasionally accuracies of SCADANN end up a little smaller than DANN, reasons may be lack of datasets put into training model (fixed) and non-optimal percentage_same_gesture_sable (fixed). Code should be reproducible if processed dataset sticks to the shape defined above.  \n",
    "\n",
    "The amount of increase in accuracies from DANN to SCADANN looks random. But if the base model is better at classifying one session, then its corresponding SCADANN is also better at classifying the same session. Given such result, to obtain the best performance from SCADANN, a good model trained with good data should be the starting point.\n",
    "\n",
    "* What to check if sth goes wrong:\n",
    "    * percentage_same_gesture_sable\n",
    "    * number of cycles or sessions\n",
    "    * shape of dataloaders (combination of train, test, valid should include all dataset)\n",
    "    * shape of procssed datasets\n",
    "    * directory paths of weights and results\n",
    "    * if weights are stored or loaded correclty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the accumulative accuracies of individual participant over all sessions. In general, SCADANN increase performance. Accuracy over all participants is dependant on how good participant_0 model is when testing on other participants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant_0~3</th>\n",
       "      <th>Participant_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TSD</th>\n",
       "      <td>0.769974</td>\n",
       "      <td>0.575721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DANN</th>\n",
       "      <td>0.769974</td>\n",
       "      <td>0.680354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SCADANN</th>\n",
       "      <td>0.769974</td>\n",
       "      <td>0.698011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Participant_0~3  Participant_4\n",
       "TSD             0.769974       0.575721\n",
       "DANN            0.769974       0.680354\n",
       "SCADANN         0.769974       0.698011"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accu_acc_TSD = []\n",
    "accu_acc_DANN = []\n",
    "accu_acc_SCADANN = []\n",
    "\n",
    "for i in range(1,DANN_acc.shape[1]+1):\n",
    "    accu_acc_TSD.append(np.mean(TSD_acc[:,:i]))\n",
    "    accu_acc_DANN.append(np.mean(DANN_acc[:,:i]))\n",
    "    accu_acc_SCADANN.append(np.mean(SCADANN_acc[:,:i]))\n",
    "\n",
    "accu_df = pd.DataFrame([accu_acc_TSD, accu_acc_DANN, accu_acc_SCADANN],\n",
    "                      index = [\"TSD\", \"DANN\", \"SCADANN\"],\n",
    "                     columns =[f'Participant_{i}' for i in index_participant_list])\n",
    "accu_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TSD\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Loc_0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Participant_0~3</th>\n",
       "      <td>0.769974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Participant_4</th>\n",
       "      <td>0.381469</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Loc_0\n",
       "Participant_0~3  0.769974\n",
       "Participant_4    0.381469"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DANN\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Loc_0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Participant_0~3</th>\n",
       "      <td>0.769974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Participant_4</th>\n",
       "      <td>0.590734</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Loc_0\n",
       "Participant_0~3  0.769974\n",
       "Participant_4    0.590734"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCADANN\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Loc_0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Participant_0~3</th>\n",
       "      <td>0.769974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Participant_4</th>\n",
       "      <td>0.626049</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Loc_0\n",
       "Participant_0~3  0.769974\n",
       "Participant_4    0.626049"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"TSD\")\n",
    "display(TSD_df)\n",
    "print(\"DANN\")\n",
    "display(DANN_df)\n",
    "print(\"SCADANN\")\n",
    "display(SCADANN_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Loc_0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Participant_4</th>\n",
       "      <td>0.24458</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Loc_0\n",
       "Participant_4  0.24458"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "diff_df = SCADANN_df-TSD_df\n",
    "diff_df = diff_df.drop('Participant_'+index_participant_list[0])\n",
    "display(diff_df)\n",
    "diff_df.to_csv(save_TSD+'/diff_results/across_sub0_lump4_diff.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Overall_Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TSD</th>\n",
       "      <td>0.575721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DANN</th>\n",
       "      <td>0.680354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SCADANN</th>\n",
       "      <td>0.698011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Overall_Acc\n",
       "TSD         0.575721\n",
       "DANN        0.680354\n",
       "SCADANN     0.698011"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_acc_df = pd.DataFrame([TSD_acc_overall, DANN_acc_overall, SCADANN_acc_overall],\n",
    "                             index = [\"TSD\", \"DANN\", \"SCADANN\"],\n",
    "                             columns = [\"Overall_Acc\"])\n",
    "overall_acc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1440 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(20,20))\n",
    "acc_list = [TSD_df, DANN_df, SCADANN_df, overall_acc_df]\n",
    "title_list = [\"TSD\", \"DANN\", \"SCADANN\", \"Overall\"]\n",
    "for idx, ax in enumerate(axes.reshape(-1)): \n",
    "    acc_list[idx].transpose().plot.bar(ax = ax, rot=0)\n",
    "    ax.set_title(title_list[idx])\n",
    "    ax.set_ylim([0, 1.0])\n",
    "    for p in ax.patches:\n",
    "        ax.annotate(str(np.round(p.get_height(),2)), (p.get_x()+p.get_width()/2., p.get_height()),\n",
    "                ha='center', va='center', xytext=(0, 8),textcoords='offset points')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
