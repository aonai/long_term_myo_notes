{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results of TSD, DANN, SCADANN models across 10 days of inward rotation starting at Day_0~5 for Subject_4\n",
    "\n",
    "Library used can be downloaded from https://github.com/aonai/long_term_EMG_myo   \n",
    "&emsp; Original by UlysseCoteAllard https://github.com/UlysseCoteAllard/LongTermEMG   \n",
    "Dataset recorded by https://github.com/Suguru55/Wearable_Sensor_Long-term_sEMG_Dataset   \n",
    "Extended robot project can be found in https://github.com/aonai/myo_robot_arm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "* weights for TSD are total of 50 training models, 10 for each day\n",
    "* weights for DANN and SCADANN are total of 45 trianing models, 9 for each day\n",
    "\n",
    "\n",
    "* training examples should have shape (1, 5,)\n",
    "* first session has shape (24, 572, 252)\n",
    "* the following sessions have shape (4, 572, 252)\n",
    "* training labels should have shape (1, 5,)\n",
    "\n",
    "\n",
    "* location 0, 1, and 2 corresponds to neutral position, inward rotation, and outward rotation respectively\n",
    "* session mentioned below are days, so number of sessions is 10\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "import torch\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_dir = \"/home/laiy/gitrepos/msr_final/LongTermEMG_myo\"\n",
    "os.chdir(code_dir)\n",
    "from PrepareAndLoadData.process_data import read_data_training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Prepare Data\n",
    "use `switch=2` to train across days and individually on wearing location 0 (`session_in_include=[0]`)\n",
    "\n",
    "### specify the directories used for running the code:\n",
    "* `code_diar`: path to long_term_EMG_myo library\n",
    "* `data_dir`: where raw dataset is loaded; raw data is in csv format\n",
    "* `processed_data_dir`: where processed dataset is loaded; processed data is in npy pickle format\n",
    "    * processed data should be a ndarray of shape   \n",
    "    (controlling_factor_1 x controlling_factor_2 x num_sessions_per_gesture x #examples_window*#mov(26*22=572) x processed_channel_shape(252 for TSD, (4,8,10) for ConvNet)\n",
    "* `path_<model_name>`: where model weights are saved\n",
    "    * weights should be saved in folder `/Weights/<model_name>`. Each folder has subfolders containing weights for the first controlling factor.\n",
    "    * weights for base model (TSD or ConvNet) contain m set of training model\n",
    "    * weights for DANN and SCADANN contain m-1 set of trianing model (these models are trianed based on TSD, so they do not have a best_state_0.pt model). \n",
    "* `save_<model_name>`: where model results are saved\n",
    "    * each result for testing a model on a group of dataset is saved in folder `results`. Each result has corresponding \n",
    "        * `<model_name>.txt` includes predictions, ground truths, array of accuracies for each participant and each session, and overall accuracy\n",
    "        * `predictions_<model_name>.npy` includes array of accuracies, ground truths, predictions, and model outputs (probability array for each prediction)\n",
    "        * remember to make blank files in these names before saving\n",
    "\n",
    "\n",
    "\n",
    "* use `read_data_training` to process raw dataset\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/home/laiy/gitrepos/msr_final/Wearable_Sensor_Long-term_sEMG_Dataset/data\"\n",
    "processed_data_dir = \"/home/laiy/gitrepos/msr_final/LongTermEMG_myo/Processed_datasets_all_across_day_loc_1_lump6\"\n",
    "code_dir = \"/home/laiy/gitrepos/msr_final/LongTermEMG_myo\"\n",
    "save_dir = \"/home/laiy/gitrepos/msr_final/LongTermEMG_myo/Results\"\n",
    "\n",
    "path_TSD =\"/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump6/TSD\"\n",
    "save_TSD = \"/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/results\"\n",
    "\n",
    "path_DANN =\"/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump6/DANN\"\n",
    "save_DANN = \"/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/results\"\n",
    "\n",
    "path_SCADANN =\"/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump6/SCADANN\"\n",
    "save_SCADANN = \"/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing Training datasets...\n",
      "session  1  --- process data in days  [2, 5, 6, 16, 17, 18, 22, 24, 25, 28]\n",
      "index_participant_list  [5]\n",
      "READ  Sub 5 _Loc 1 _Day 2\n",
      "examples_per_session =  (1, 4, 572, 252)\n",
      "READ  Sub 5 _Loc 1 _Day 5\n",
      "Include day  5  in first dataset  (4, 572, 252)\n",
      "examples of first session =  (8, 572, 252)\n",
      "examples_per_session =  (1, 8, 572, 252)\n",
      "READ  Sub 5 _Loc 1 _Day 6\n",
      "Include day  6  in first dataset  (8, 572, 252)\n",
      "examples of first session =  (12, 572, 252)\n",
      "examples_per_session =  (1, 12, 572, 252)\n",
      "READ  Sub 5 _Loc 1 _Day 16\n",
      "Include day  16  in first dataset  (12, 572, 252)\n",
      "examples of first session =  (16, 572, 252)\n",
      "examples_per_session =  (1, 16, 572, 252)\n",
      "READ  Sub 5 _Loc 1 _Day 17\n",
      "Include day  17  in first dataset  (16, 572, 252)\n",
      "examples of first session =  (20, 572, 252)\n",
      "examples_per_session =  (1, 20, 572, 252)\n",
      "READ  Sub 5 _Loc 1 _Day 18\n",
      "Include day  18  in first dataset  (20, 572, 252)\n",
      "examples of first session =  (24, 572, 252)\n",
      "examples_per_session =  (1, 24, 572, 252)\n",
      "READ  Sub 5 _Loc 1 _Day 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laiy/.local/lib/python3.8/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "examples_per_session =  (2,)\n",
      "READ  Sub 5 _Loc 1 _Day 24\n",
      "examples_per_session =  (3,)\n",
      "READ  Sub 5 _Loc 1 _Day 25\n",
      "examples_per_session =  (4,)\n",
      "READ  Sub 5 _Loc 1 _Day 28\n",
      "examples_per_session =  (5,)\n",
      "@ traning sessions =  (1, 5)\n",
      "traning examples  (1, 5)\n",
      "traning labels  (1, 5)\n",
      "all traning examples  (1, 5)\n",
      "all traning labels  (1, 5)\n"
     ]
    }
   ],
   "source": [
    "read_data_training(path=data_dir, store_path = processed_data_dir,  \n",
    "                   sessions_to_include =[1], switch=2, include_in_first=6,\n",
    "                   start_at_participant=5, num_participant=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traning examples  (1, 5)\n",
      "traning labels  (1, 5)\n"
     ]
    }
   ],
   "source": [
    "# check stored pickle \n",
    "with open(processed_data_dir + \"/training_session.pickle\", 'rb') as f:\n",
    "    dataset_training = pickle.load(file=f)\n",
    "\n",
    "examples_datasets_train = dataset_training['examples_training']\n",
    "print('traning examples ', np.shape(examples_datasets_train))\n",
    "labels_datasets_train = dataset_training['labels_training']\n",
    "print('traning labels ', np.shape(labels_datasets_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  examples_per_session =  (24, 572, 252)\n",
      "0  labels_per_session =  (24, 572)\n",
      "1  examples_per_session =  (4, 572, 252)\n",
      "1  labels_per_session =  (4, 572)\n",
      "2  examples_per_session =  (4, 572, 252)\n",
      "2  labels_per_session =  (4, 572)\n",
      "3  examples_per_session =  (4, 572, 252)\n",
      "3  labels_per_session =  (4, 572)\n",
      "4  examples_per_session =  (4, 572, 252)\n",
      "4  labels_per_session =  (4, 572)\n"
     ]
    }
   ],
   "source": [
    "for idx, examples_per_session in enumerate (examples_datasets_train[0]):\n",
    "    print(idx, \" examples_per_session = \", np.shape(examples_per_session))\n",
    "    print(idx, \" labels_per_session = \", np.shape(labels_datasets_train[0][idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify params used for training and testing\n",
    "\n",
    "During training and testing, processed datasets are first put into pytorch dataloders, then feed to the model trainer; following are params for TSD model and dataloaders\n",
    "\n",
    "* `num_kernels`: list of integers defining number of neurons used in each linear layer (linear block has `dropout`=0.5)\n",
    "* `number_of_cycles_total`: number of trails performed for each session (assuming that all session have the same trail size)\n",
    "    * 4 for myo across day training\n",
    "* `number_of_classes`: total number of gestures performed in dataset\n",
    "    * 22 for myo\n",
    "* `batch_size`: number of examples stored in each batch\n",
    "* `feature_vector_input_length`: length of input array or each processed signal; i.e. size of one training example \n",
    "    * 252 for TSD\n",
    "* `learning_rate`= 0.002515\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number_of_cycle_for_first_training  24\n",
      "number_of_cycles_total  4\n"
     ]
    }
   ],
   "source": [
    "num_kernels=[200, 200, 200]                                \n",
    "number_of_cycle_for_first_training = np.shape(examples_datasets_train[0][0])[0]               \n",
    "number_of_cycles_total=np.shape(examples_datasets_train[-1][-1])[0]               \n",
    "print(\"number_of_cycle_for_first_training \", number_of_cycle_for_first_training)\n",
    "print(\"number_of_cycles_total \", number_of_cycles_total)\n",
    "number_of_classes=22\n",
    "batch_size=128          \n",
    "feature_vector_input_length=252                     \n",
    "learning_rate=0.002515"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. TSD_DNN\n",
    "* `train_fine_tuning`: used to train data using a base model (TSD or ConvNet)\n",
    "    * running this function will save num_sessions sets of TSD model weights (each is fine tuned based on the previous training)  \n",
    "    \n",
    "* `test_standard_model_on_training_sessions`: test model result\n",
    "\n",
    "\n",
    "### check if dataloaders are loaded correctly:\n",
    "* each participant has shape (num_session x 40 x 572 x 252)\n",
    "* each session has shape (40 x 572 x 252)\n",
    "* put these data into on group ends up with shape (40*572=22880, 252)\n",
    "    * shuffle on group of data and put into dataloaders\n",
    "    * each participant should have num_sessions sets of dataloaders, each correspond to one session\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TrainingsAndEvaluations.ForTrainingSessions.train_tsd_dnn_standard import \\\n",
    "            test_standard_model_on_training_sessions, train_fine_tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET one participant_examples  (5,)\n",
      "   GET one training_index_examples  (24, 572, 252)  at  0\n",
      "   GOT one group XY  (13728, 252)    (13728,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (12355, 252)    (12355,)\n",
      "       one group XY valid (1373, 252)    (1373, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  1\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  2\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  3\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  4\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "dataloaders: \n",
      "   train  (1, 5)\n",
      "   valid  (1, 5)\n",
      "   test  (1, 0)\n",
      "START TRAINING\n",
      "Participant:  0\n",
      "Session:  0\n",
      "<generator object Module.parameters at 0x7f63785afc10>\n",
      "Epoch 0/499\n",
      "----------\n",
      "train Loss: 0.00885067 Acc: 0.65698242\n",
      "val Loss: 0.00030669 Acc: 0.86380189\n",
      "New best validation loss: 0.00030669248720545535\n",
      "Epoch 1 of 500 took 0.633s\n",
      "Epoch 1/499\n",
      "----------\n",
      "train Loss: 0.00463840 Acc: 0.79679362\n",
      "val Loss: 0.00026127 Acc: 0.86962855\n",
      "Epoch 2 of 500 took 0.626s\n",
      "Epoch 2/499\n",
      "----------\n",
      "train Loss: 0.00392195 Acc: 0.82747396\n",
      "val Loss: 0.00027328 Acc: 0.86016023\n",
      "Epoch 3 of 500 took 0.624s\n",
      "Epoch 3/499\n",
      "----------\n",
      "train Loss: 0.00362763 Acc: 0.84090169\n",
      "val Loss: 0.00021098 Acc: 0.89876184\n",
      "Epoch 4 of 500 took 0.628s\n",
      "Epoch 4/499\n",
      "----------\n",
      "train Loss: 0.00340507 Acc: 0.84716797\n",
      "val Loss: 0.00019959 Acc: 0.90094683\n",
      "New best validation loss: 0.00019959342453658625\n",
      "Epoch 5 of 500 took 0.628s\n",
      "Epoch 5/499\n",
      "----------\n",
      "train Loss: 0.00313044 Acc: 0.86222331\n",
      "val Loss: 0.00023264 Acc: 0.88856519\n",
      "Epoch 6 of 500 took 0.624s\n",
      "Epoch 6/499\n",
      "----------\n",
      "train Loss: 0.00301378 Acc: 0.86694336\n",
      "val Loss: 0.00019996 Acc: 0.89439184\n",
      "Epoch 7 of 500 took 0.637s\n",
      "Epoch 7/499\n",
      "----------\n",
      "train Loss: 0.00295063 Acc: 0.8659668\n",
      "val Loss: 0.00021188 Acc: 0.88638019\n",
      "Epoch 8 of 500 took 0.631s\n",
      "Epoch 8/499\n",
      "----------\n",
      "train Loss: 0.00284505 Acc: 0.87166341\n",
      "val Loss: 0.00016480 Acc: 0.91114348\n",
      "Epoch 9 of 500 took 0.649s\n",
      "Epoch 9/499\n",
      "----------\n",
      "train Loss: 0.00270394 Acc: 0.87613932\n",
      "val Loss: 0.00018252 Acc: 0.90458849\n",
      "Epoch 10 of 500 took 0.624s\n",
      "Epoch 10/499\n",
      "----------\n",
      "train Loss: 0.00262463 Acc: 0.88167318\n",
      "val Loss: 0.00018602 Acc: 0.90531682\n",
      "Epoch 11 of 500 took 0.622s\n",
      "Epoch 11/499\n",
      "----------\n",
      "train Loss: 0.00253158 Acc: 0.88842773\n",
      "val Loss: 0.00019889 Acc: 0.9024035\n",
      "Epoch 12 of 500 took 0.630s\n",
      "Epoch 12/499\n",
      "----------\n",
      "train Loss: 0.00245718 Acc: 0.88964844\n",
      "val Loss: 0.00015859 Acc: 0.91988347\n",
      "Epoch 13 of 500 took 0.638s\n",
      "Epoch 13/499\n",
      "----------\n",
      "train Loss: 0.00242516 Acc: 0.88867188\n",
      "val Loss: 0.00015099 Acc: 0.91769847\n",
      "Epoch 14 of 500 took 0.622s\n",
      "Epoch 14/499\n",
      "----------\n",
      "train Loss: 0.00240259 Acc: 0.89152018\n",
      "val Loss: 0.00016150 Acc: 0.91697014\n",
      "Epoch 15 of 500 took 0.630s\n",
      "Epoch 15/499\n",
      "----------\n",
      "train Loss: 0.00233837 Acc: 0.89485677\n",
      "val Loss: 0.00016091 Acc: 0.91551347\n",
      "Epoch 16 of 500 took 0.628s\n",
      "\n",
      "Training complete in 0m 10s\n",
      "Best val loss: 0.000200\n",
      "Session:  1\n",
      "<generator object Module.parameters at 0x7f63785afc10>\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump6/TSD/participant_0/best_state_0.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump6/TSD/participant_0/best_state_0.pt' (epoch 5)\n",
      "Epoch 0/499\n",
      "----------\n",
      "train Loss: 0.00381161 Acc: 0.83251953\n",
      "val Loss: 0.00114801 Acc: 0.89956332\n",
      "New best validation loss: 0.001148009664627142\n",
      "Epoch 1 of 500 took 0.106s\n",
      "Epoch 1/499\n",
      "----------\n",
      "train Loss: 0.00304914 Acc: 0.86474609\n",
      "val Loss: 0.00118926 Acc: 0.89956332\n",
      "Epoch 2 of 500 took 0.109s\n",
      "Epoch 2/499\n",
      "----------\n",
      "train Loss: 0.00249838 Acc: 0.89306641\n",
      "val Loss: 0.00112234 Acc: 0.91703057\n",
      "Epoch 3 of 500 took 0.105s\n",
      "Epoch 3/499\n",
      "----------\n",
      "train Loss: 0.00248170 Acc: 0.88964844\n",
      "val Loss: 0.00085113 Acc: 0.91266376\n",
      "New best validation loss: 0.0008511270628225334\n",
      "Epoch 4 of 500 took 0.114s\n",
      "Epoch 4/499\n",
      "----------\n",
      "train Loss: 0.00214294 Acc: 0.91210938\n",
      "val Loss: 0.00095176 Acc: 0.92576419\n",
      "Epoch 5 of 500 took 0.114s\n",
      "Epoch 5/499\n",
      "----------\n",
      "train Loss: 0.00218881 Acc: 0.90185547\n",
      "val Loss: 0.00095796 Acc: 0.92576419\n",
      "Epoch 6 of 500 took 0.120s\n",
      "Epoch 6/499\n",
      "----------\n",
      "train Loss: 0.00216902 Acc: 0.90283203\n",
      "val Loss: 0.00077475 Acc: 0.930131\n",
      "Epoch 7 of 500 took 0.104s\n",
      "Epoch 7/499\n",
      "----------\n",
      "train Loss: 0.00194322 Acc: 0.91308594\n",
      "val Loss: 0.00067385 Acc: 0.96069869\n",
      "New best validation loss: 0.0006738513968396916\n",
      "Epoch 8 of 500 took 0.111s\n",
      "Epoch 8/499\n",
      "----------\n",
      "train Loss: 0.00176341 Acc: 0.91503906\n",
      "val Loss: 0.00068663 Acc: 0.93886463\n",
      "Epoch 9 of 500 took 0.105s\n",
      "Epoch 9/499\n",
      "----------\n",
      "train Loss: 0.00182838 Acc: 0.91699219\n",
      "val Loss: 0.00048506 Acc: 0.95196507\n",
      "New best validation loss: 0.0004850649742580397\n",
      "Epoch 10 of 500 took 0.110s\n",
      "Epoch 10/499\n",
      "----------\n",
      "train Loss: 0.00161348 Acc: 0.92333984\n",
      "val Loss: 0.00076600 Acc: 0.94323144\n",
      "Epoch 11 of 500 took 0.105s\n",
      "Epoch 11/499\n",
      "----------\n",
      "train Loss: 0.00162214 Acc: 0.92236328\n",
      "val Loss: 0.00071625 Acc: 0.94759825\n",
      "Epoch 12 of 500 took 0.109s\n",
      "Epoch 12/499\n",
      "----------\n",
      "train Loss: 0.00164313 Acc: 0.92382812\n",
      "val Loss: 0.00060602 Acc: 0.94759825\n",
      "Epoch 13 of 500 took 0.118s\n",
      "Epoch 13/499\n",
      "----------\n",
      "train Loss: 0.00178668 Acc: 0.91894531\n",
      "val Loss: 0.00099971 Acc: 0.92576419\n",
      "Epoch 14 of 500 took 0.108s\n",
      "Epoch 14/499\n",
      "----------\n",
      "train Loss: 0.00157553 Acc: 0.9296875\n",
      "val Loss: 0.00066090 Acc: 0.94323144\n",
      "Epoch 15 of 500 took 0.106s\n",
      "Epoch 15/499\n",
      "----------\n",
      "train Loss: 0.00165850 Acc: 0.91943359\n",
      "val Loss: 0.00049419 Acc: 0.9650655\n",
      "Epoch    16: reducing learning rate of group 0 to 5.0300e-04.\n",
      "Epoch 16 of 500 took 0.109s\n",
      "Epoch 16/499\n",
      "----------\n",
      "train Loss: 0.00134410 Acc: 0.94384766\n",
      "val Loss: 0.00039281 Acc: 0.9650655\n",
      "Epoch 17 of 500 took 0.107s\n",
      "Epoch 17/499\n",
      "----------\n",
      "train Loss: 0.00123927 Acc: 0.94677734\n",
      "val Loss: 0.00044878 Acc: 0.9650655\n",
      "Epoch 18 of 500 took 0.109s\n",
      "Epoch 18/499\n",
      "----------\n",
      "train Loss: 0.00116006 Acc: 0.95019531\n",
      "val Loss: 0.00039735 Acc: 0.9650655\n",
      "Epoch 19 of 500 took 0.106s\n",
      "Epoch 19/499\n",
      "----------\n",
      "train Loss: 0.00112284 Acc: 0.95166016\n",
      "val Loss: 0.00039091 Acc: 0.96943231\n",
      "Epoch 20 of 500 took 0.109s\n",
      "Epoch 20/499\n",
      "----------\n",
      "train Loss: 0.00113865 Acc: 0.94824219\n",
      "val Loss: 0.00038568 Acc: 0.97379913\n",
      "Epoch 21 of 500 took 0.105s\n",
      "\n",
      "Training complete in 0m 2s\n",
      "Best val loss: 0.000485\n",
      "Session:  2\n",
      "<generator object Module.parameters at 0x7f6377ecb0b0>\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump6/TSD/participant_0/best_state_1.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump6/TSD/participant_0/best_state_1.pt' (epoch 10)\n",
      "Epoch 0/499\n",
      "----------\n",
      "train Loss: 0.00382848 Acc: 0.83837891\n",
      "val Loss: 0.00150529 Acc: 0.87336245\n",
      "New best validation loss: 0.0015052937003722877\n",
      "Epoch 1 of 500 took 0.115s\n",
      "Epoch 1/499\n",
      "----------\n",
      "train Loss: 0.00295007 Acc: 0.87402344\n",
      "val Loss: 0.00110621 Acc: 0.89082969\n",
      "New best validation loss: 0.0011062118424078262\n",
      "Epoch 2 of 500 took 0.107s\n",
      "Epoch 2/499\n",
      "----------\n",
      "train Loss: 0.00236331 Acc: 0.88964844\n",
      "val Loss: 0.00097009 Acc: 0.91266376\n",
      "New best validation loss: 0.0009700924027955168\n",
      "Epoch 3 of 500 took 0.110s\n",
      "Epoch 3/499\n",
      "----------\n",
      "train Loss: 0.00258667 Acc: 0.8828125\n",
      "val Loss: 0.00102670 Acc: 0.89519651\n",
      "Epoch 4 of 500 took 0.105s\n",
      "Epoch 4/499\n",
      "----------\n",
      "train Loss: 0.00245139 Acc: 0.88867188\n",
      "val Loss: 0.00090961 Acc: 0.90829694\n",
      "Epoch 5 of 500 took 0.113s\n",
      "Epoch 5/499\n",
      "----------\n",
      "train Loss: 0.00233880 Acc: 0.89990234\n",
      "val Loss: 0.00081317 Acc: 0.930131\n",
      "New best validation loss: 0.0008131735449795119\n",
      "Epoch 6 of 500 took 0.109s\n",
      "Epoch 6/499\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.00219992 Acc: 0.90136719\n",
      "val Loss: 0.00106811 Acc: 0.92576419\n",
      "Epoch 7 of 500 took 0.108s\n",
      "Epoch 7/499\n",
      "----------\n",
      "train Loss: 0.00223141 Acc: 0.89941406\n",
      "val Loss: 0.00068689 Acc: 0.93886463\n",
      "New best validation loss: 0.0006868906937311831\n",
      "Epoch 8 of 500 took 0.108s\n",
      "Epoch 8/499\n",
      "----------\n",
      "train Loss: 0.00193286 Acc: 0.91601562\n",
      "val Loss: 0.00070329 Acc: 0.92576419\n",
      "Epoch 9 of 500 took 0.112s\n",
      "Epoch 9/499\n",
      "----------\n",
      "train Loss: 0.00213306 Acc: 0.90087891\n",
      "val Loss: 0.00121157 Acc: 0.90829694\n",
      "Epoch 10 of 500 took 0.112s\n",
      "Epoch 10/499\n",
      "----------\n",
      "train Loss: 0.00209554 Acc: 0.90380859\n",
      "val Loss: 0.00071695 Acc: 0.930131\n",
      "Epoch 11 of 500 took 0.109s\n",
      "Epoch 11/499\n",
      "----------\n",
      "train Loss: 0.00184711 Acc: 0.92236328\n",
      "val Loss: 0.00061706 Acc: 0.96069869\n",
      "Epoch 12 of 500 took 0.105s\n",
      "Epoch 12/499\n",
      "----------\n",
      "train Loss: 0.00174996 Acc: 0.91650391\n",
      "val Loss: 0.00096747 Acc: 0.92576419\n",
      "Epoch 13 of 500 took 0.108s\n",
      "Epoch 13/499\n",
      "----------\n",
      "train Loss: 0.00173199 Acc: 0.92138672\n",
      "val Loss: 0.00075956 Acc: 0.93886463\n",
      "Epoch 14 of 500 took 0.108s\n",
      "Epoch 14/499\n",
      "----------\n",
      "train Loss: 0.00170476 Acc: 0.92089844\n",
      "val Loss: 0.00069990 Acc: 0.94323144\n",
      "Epoch 15 of 500 took 0.109s\n",
      "Epoch 15/499\n",
      "----------\n",
      "train Loss: 0.00158245 Acc: 0.92724609\n",
      "val Loss: 0.00100074 Acc: 0.91266376\n",
      "Epoch 16 of 500 took 0.107s\n",
      "Epoch 16/499\n",
      "----------\n",
      "train Loss: 0.00154024 Acc: 0.92724609\n",
      "val Loss: 0.00051507 Acc: 0.94323144\n",
      "New best validation loss: 0.0005150656317519309\n",
      "Epoch 17 of 500 took 0.110s\n",
      "Epoch 17/499\n",
      "----------\n",
      "train Loss: 0.00154337 Acc: 0.93408203\n",
      "val Loss: 0.00062830 Acc: 0.94323144\n",
      "Epoch 18 of 500 took 0.105s\n",
      "Epoch 18/499\n",
      "----------\n",
      "train Loss: 0.00147408 Acc: 0.93505859\n",
      "val Loss: 0.00057810 Acc: 0.95633188\n",
      "Epoch 19 of 500 took 0.110s\n",
      "Epoch 19/499\n",
      "----------\n",
      "train Loss: 0.00155084 Acc: 0.92529297\n",
      "val Loss: 0.00047072 Acc: 0.95196507\n",
      "Epoch 20 of 500 took 0.109s\n",
      "Epoch 20/499\n",
      "----------\n",
      "train Loss: 0.00136133 Acc: 0.93505859\n",
      "val Loss: 0.00130583 Acc: 0.86462882\n",
      "Epoch 21 of 500 took 0.110s\n",
      "Epoch 21/499\n",
      "----------\n",
      "train Loss: 0.00139082 Acc: 0.93652344\n",
      "val Loss: 0.00043109 Acc: 0.96943231\n",
      "Epoch 22 of 500 took 0.107s\n",
      "Epoch 22/499\n",
      "----------\n",
      "train Loss: 0.00133250 Acc: 0.93847656\n",
      "val Loss: 0.00049832 Acc: 0.95633188\n",
      "Epoch 23 of 500 took 0.110s\n",
      "Epoch 23/499\n",
      "----------\n",
      "train Loss: 0.00146843 Acc: 0.93457031\n",
      "val Loss: 0.00036173 Acc: 0.96943231\n",
      "New best validation loss: 0.0003617279654507033\n",
      "Epoch 24 of 500 took 0.107s\n",
      "Epoch 24/499\n",
      "----------\n",
      "train Loss: 0.00151843 Acc: 0.93457031\n",
      "val Loss: 0.00170980 Acc: 0.86026201\n",
      "Epoch 25 of 500 took 0.109s\n",
      "Epoch 25/499\n",
      "----------\n",
      "train Loss: 0.00154777 Acc: 0.93261719\n",
      "val Loss: 0.00079415 Acc: 0.92139738\n",
      "Epoch 26 of 500 took 0.105s\n",
      "Epoch 26/499\n",
      "----------\n",
      "train Loss: 0.00146281 Acc: 0.93994141\n",
      "val Loss: 0.00076193 Acc: 0.93886463\n",
      "Epoch 27 of 500 took 0.108s\n",
      "Epoch 27/499\n",
      "----------\n",
      "train Loss: 0.00133616 Acc: 0.94335938\n",
      "val Loss: 0.00075887 Acc: 0.930131\n",
      "Epoch 28 of 500 took 0.106s\n",
      "Epoch 28/499\n",
      "----------\n",
      "train Loss: 0.00123080 Acc: 0.9453125\n",
      "val Loss: 0.00066598 Acc: 0.93449782\n",
      "Epoch 29 of 500 took 0.111s\n",
      "Epoch 29/499\n",
      "----------\n",
      "train Loss: 0.00123921 Acc: 0.94091797\n",
      "val Loss: 0.00042690 Acc: 0.96069869\n",
      "Epoch    30: reducing learning rate of group 0 to 5.0300e-04.\n",
      "Epoch 30 of 500 took 0.106s\n",
      "Epoch 30/499\n",
      "----------\n",
      "train Loss: 0.00109663 Acc: 0.95214844\n",
      "val Loss: 0.00029882 Acc: 0.98253275\n",
      "Epoch 31 of 500 took 0.109s\n",
      "Epoch 31/499\n",
      "----------\n",
      "train Loss: 0.00086589 Acc: 0.96240234\n",
      "val Loss: 0.00028628 Acc: 0.98253275\n",
      "Epoch 32 of 500 took 0.106s\n",
      "Epoch 32/499\n",
      "----------\n",
      "train Loss: 0.00086836 Acc: 0.96533203\n",
      "val Loss: 0.00034084 Acc: 0.96069869\n",
      "Epoch 33 of 500 took 0.109s\n",
      "Epoch 33/499\n",
      "----------\n",
      "train Loss: 0.00095317 Acc: 0.96337891\n",
      "val Loss: 0.00024890 Acc: 0.97816594\n",
      "New best validation loss: 0.00024890201776308783\n",
      "Epoch 34 of 500 took 0.108s\n",
      "Epoch 34/499\n",
      "----------\n",
      "train Loss: 0.00078606 Acc: 0.96533203\n",
      "val Loss: 0.00024950 Acc: 0.98253275\n",
      "Epoch 35 of 500 took 0.109s\n",
      "Epoch 35/499\n",
      "----------\n",
      "train Loss: 0.00077415 Acc: 0.96435547\n",
      "val Loss: 0.00036812 Acc: 0.96943231\n",
      "Epoch 36 of 500 took 0.107s\n",
      "Epoch 36/499\n",
      "----------\n",
      "train Loss: 0.00077178 Acc: 0.96679688\n",
      "val Loss: 0.00028017 Acc: 0.97379913\n",
      "Epoch 37 of 500 took 0.109s\n",
      "Epoch 37/499\n",
      "----------\n",
      "train Loss: 0.00083479 Acc: 0.96240234\n",
      "val Loss: 0.00033710 Acc: 0.9650655\n",
      "Epoch 38 of 500 took 0.110s\n",
      "Epoch 38/499\n",
      "----------\n",
      "train Loss: 0.00070200 Acc: 0.97021484\n",
      "val Loss: 0.00023311 Acc: 0.98689956\n",
      "Epoch 39 of 500 took 0.109s\n",
      "Epoch 39/499\n",
      "----------\n",
      "train Loss: 0.00077436 Acc: 0.96484375\n",
      "val Loss: 0.00025400 Acc: 0.97816594\n",
      "Epoch 40 of 500 took 0.107s\n",
      "Epoch 40/499\n",
      "----------\n",
      "train Loss: 0.00071475 Acc: 0.97119141\n",
      "val Loss: 0.00025380 Acc: 0.98253275\n",
      "Epoch 41 of 500 took 0.109s\n",
      "Epoch 41/499\n",
      "----------\n",
      "train Loss: 0.00075522 Acc: 0.96826172\n",
      "val Loss: 0.00027552 Acc: 0.97816594\n",
      "Epoch 42 of 500 took 0.106s\n",
      "Epoch 42/499\n",
      "----------\n",
      "train Loss: 0.00069993 Acc: 0.96972656\n",
      "val Loss: 0.00027419 Acc: 0.97816594\n",
      "Epoch 43 of 500 took 0.109s\n",
      "Epoch 43/499\n",
      "----------\n",
      "train Loss: 0.00072132 Acc: 0.96875\n",
      "val Loss: 0.00028495 Acc: 0.97379913\n",
      "Epoch 44 of 500 took 0.106s\n",
      "Epoch 44/499\n",
      "----------\n",
      "train Loss: 0.00070918 Acc: 0.96630859\n",
      "val Loss: 0.00024334 Acc: 0.97379913\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0060e-04.\n",
      "Epoch 45 of 500 took 0.108s\n",
      "\n",
      "Training complete in 0m 5s\n",
      "Best val loss: 0.000249\n",
      "Session:  3\n",
      "<generator object Module.parameters at 0x7f6377ecb660>\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump6/TSD/participant_0/best_state_2.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump6/TSD/participant_0/best_state_2.pt' (epoch 34)\n",
      "Epoch 0/499\n",
      "----------\n",
      "train Loss: 0.00374833 Acc: 0.85644531\n",
      "val Loss: 0.00107045 Acc: 0.89519651\n",
      "New best validation loss: 0.0010704512679420705\n",
      "Epoch 1 of 500 took 0.108s\n",
      "Epoch 1/499\n",
      "----------\n",
      "train Loss: 0.00261175 Acc: 0.88183594\n",
      "val Loss: 0.00091589 Acc: 0.91703057\n",
      "New best validation loss: 0.0009158871840181309\n",
      "Epoch 2 of 500 took 0.111s\n",
      "Epoch 2/499\n",
      "----------\n",
      "train Loss: 0.00236505 Acc: 0.88867188\n",
      "val Loss: 0.00092670 Acc: 0.92139738\n",
      "Epoch 3 of 500 took 0.111s\n",
      "Epoch 3/499\n",
      "----------\n",
      "train Loss: 0.00228859 Acc: 0.89111328\n",
      "val Loss: 0.00088693 Acc: 0.930131\n",
      "Epoch 4 of 500 took 0.110s\n",
      "Epoch 4/499\n",
      "----------\n",
      "train Loss: 0.00215481 Acc: 0.90234375\n",
      "val Loss: 0.00074685 Acc: 0.94323144\n",
      "New best validation loss: 0.0007468465894590819\n",
      "Epoch 5 of 500 took 0.107s\n",
      "Epoch 5/499\n",
      "----------\n",
      "train Loss: 0.00187824 Acc: 0.91455078\n",
      "val Loss: 0.00080225 Acc: 0.94323144\n",
      "Epoch 6 of 500 took 0.109s\n",
      "Epoch 6/499\n",
      "----------\n",
      "train Loss: 0.00179064 Acc: 0.91845703\n",
      "val Loss: 0.00083584 Acc: 0.930131\n",
      "Epoch 7 of 500 took 0.105s\n",
      "Epoch 7/499\n",
      "----------\n",
      "train Loss: 0.00180563 Acc: 0.91601562\n",
      "val Loss: 0.00071735 Acc: 0.92139738\n",
      "Epoch 8 of 500 took 0.108s\n",
      "Epoch 8/499\n",
      "----------\n",
      "train Loss: 0.00175365 Acc: 0.91894531\n",
      "val Loss: 0.00061967 Acc: 0.95633188\n",
      "New best validation loss: 0.0006196731823500587\n",
      "Epoch 9 of 500 took 0.107s\n",
      "Epoch 9/499\n",
      "----------\n",
      "train Loss: 0.00161738 Acc: 0.92919922\n",
      "val Loss: 0.00071107 Acc: 0.93886463\n",
      "Epoch 10 of 500 took 0.112s\n",
      "Epoch 10/499\n",
      "----------\n",
      "train Loss: 0.00157178 Acc: 0.92480469\n",
      "val Loss: 0.00061443 Acc: 0.95196507\n",
      "Epoch 11 of 500 took 0.122s\n",
      "Epoch 11/499\n",
      "----------\n",
      "train Loss: 0.00165286 Acc: 0.92578125\n",
      "val Loss: 0.00058616 Acc: 0.95196507\n",
      "Epoch 12 of 500 took 0.112s\n",
      "Epoch 12/499\n",
      "----------\n",
      "train Loss: 0.00147680 Acc: 0.93310547\n",
      "val Loss: 0.00058883 Acc: 0.94759825\n",
      "Epoch 13 of 500 took 0.106s\n",
      "Epoch 13/499\n",
      "----------\n",
      "train Loss: 0.00155614 Acc: 0.93066406\n",
      "val Loss: 0.00076734 Acc: 0.94323144\n",
      "Epoch 14 of 500 took 0.106s\n",
      "Epoch 14/499\n",
      "----------\n",
      "train Loss: 0.00167859 Acc: 0.92285156\n",
      "val Loss: 0.00057838 Acc: 0.94323144\n",
      "Epoch 15 of 500 took 0.107s\n",
      "Epoch 15/499\n",
      "----------\n",
      "train Loss: 0.00142456 Acc: 0.93457031\n",
      "val Loss: 0.00068311 Acc: 0.95196507\n",
      "Epoch 16 of 500 took 0.108s\n",
      "Epoch 16/499\n",
      "----------\n",
      "train Loss: 0.00119683 Acc: 0.94433594\n",
      "val Loss: 0.00040929 Acc: 0.96069869\n",
      "New best validation loss: 0.00040929292747547534\n",
      "Epoch 17 of 500 took 0.108s\n",
      "Epoch 17/499\n",
      "----------\n",
      "train Loss: 0.00134760 Acc: 0.93408203\n",
      "val Loss: 0.00048190 Acc: 0.9650655\n",
      "Epoch 18 of 500 took 0.108s\n",
      "Epoch 18/499\n",
      "----------\n",
      "train Loss: 0.00114091 Acc: 0.95117188\n",
      "val Loss: 0.00061519 Acc: 0.96069869\n",
      "Epoch 19 of 500 took 0.107s\n",
      "Epoch 19/499\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.00126332 Acc: 0.93798828\n",
      "val Loss: 0.00066226 Acc: 0.94759825\n",
      "Epoch 20 of 500 took 0.114s\n",
      "Epoch 20/499\n",
      "----------\n",
      "train Loss: 0.00128528 Acc: 0.94042969\n",
      "val Loss: 0.00063503 Acc: 0.95633188\n",
      "Epoch 21 of 500 took 0.108s\n",
      "Epoch 21/499\n",
      "----------\n",
      "train Loss: 0.00116863 Acc: 0.94287109\n",
      "val Loss: 0.00043594 Acc: 0.96069869\n",
      "Epoch 22 of 500 took 0.108s\n",
      "Epoch 22/499\n",
      "----------\n",
      "train Loss: 0.00125915 Acc: 0.94287109\n",
      "val Loss: 0.00056482 Acc: 0.95196507\n",
      "Epoch    23: reducing learning rate of group 0 to 5.0300e-04.\n",
      "Epoch 23 of 500 took 0.105s\n",
      "Epoch 23/499\n",
      "----------\n",
      "train Loss: 0.00114378 Acc: 0.94580078\n",
      "val Loss: 0.00036737 Acc: 0.97816594\n",
      "Epoch 24 of 500 took 0.110s\n",
      "Epoch 24/499\n",
      "----------\n",
      "train Loss: 0.00090189 Acc: 0.95507812\n",
      "val Loss: 0.00043304 Acc: 0.96943231\n",
      "Epoch 25 of 500 took 0.106s\n",
      "Epoch 25/499\n",
      "----------\n",
      "train Loss: 0.00093333 Acc: 0.95800781\n",
      "val Loss: 0.00042610 Acc: 0.9650655\n",
      "Epoch 26 of 500 took 0.107s\n",
      "Epoch 26/499\n",
      "----------\n",
      "train Loss: 0.00089995 Acc: 0.9609375\n",
      "val Loss: 0.00037109 Acc: 0.9650655\n",
      "Epoch 27 of 500 took 0.106s\n",
      "Epoch 27/499\n",
      "----------\n",
      "train Loss: 0.00080118 Acc: 0.96240234\n",
      "val Loss: 0.00040429 Acc: 0.9650655\n",
      "Epoch 28 of 500 took 0.108s\n",
      "\n",
      "Training complete in 0m 3s\n",
      "Best val loss: 0.000409\n",
      "Session:  4\n",
      "<generator object Module.parameters at 0x7f6377ecb0b0>\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump6/TSD/participant_0/best_state_3.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump6/TSD/participant_0/best_state_3.pt' (epoch 17)\n",
      "Epoch 0/499\n",
      "----------\n",
      "train Loss: 0.00362939 Acc: 0.8515625\n",
      "val Loss: 0.00106786 Acc: 0.90393013\n",
      "New best validation loss: 0.001067861654352413\n",
      "Epoch 1 of 500 took 0.108s\n",
      "Epoch 1/499\n",
      "----------\n",
      "train Loss: 0.00255882 Acc: 0.88623047\n",
      "val Loss: 0.00094902 Acc: 0.91703057\n",
      "New best validation loss: 0.0009490188422681984\n",
      "Epoch 2 of 500 took 0.113s\n",
      "Epoch 2/499\n",
      "----------\n",
      "train Loss: 0.00233737 Acc: 0.88769531\n",
      "val Loss: 0.00103905 Acc: 0.90393013\n",
      "Epoch 3 of 500 took 0.105s\n",
      "Epoch 3/499\n",
      "----------\n",
      "train Loss: 0.00217943 Acc: 0.89746094\n",
      "val Loss: 0.00075472 Acc: 0.930131\n",
      "New best validation loss: 0.000754722471320473\n",
      "Epoch 4 of 500 took 0.112s\n",
      "Epoch 4/499\n",
      "----------\n",
      "train Loss: 0.00203251 Acc: 0.90136719\n",
      "val Loss: 0.00121076 Acc: 0.89082969\n",
      "Epoch 5 of 500 took 0.107s\n",
      "Epoch 5/499\n",
      "----------\n",
      "train Loss: 0.00192240 Acc: 0.91259766\n",
      "val Loss: 0.00077398 Acc: 0.94759825\n",
      "Epoch 6 of 500 took 0.108s\n",
      "Epoch 6/499\n",
      "----------\n",
      "train Loss: 0.00210371 Acc: 0.90087891\n",
      "val Loss: 0.00085964 Acc: 0.92576419\n",
      "Epoch 7 of 500 took 0.106s\n",
      "Epoch 7/499\n",
      "----------\n",
      "train Loss: 0.00194448 Acc: 0.91113281\n",
      "val Loss: 0.00073062 Acc: 0.930131\n",
      "Epoch 8 of 500 took 0.108s\n",
      "Epoch 8/499\n",
      "----------\n",
      "train Loss: 0.00165720 Acc: 0.91943359\n",
      "val Loss: 0.00078564 Acc: 0.930131\n",
      "Epoch 9 of 500 took 0.105s\n",
      "Epoch 9/499\n",
      "----------\n",
      "train Loss: 0.00170094 Acc: 0.92236328\n",
      "val Loss: 0.00068310 Acc: 0.94323144\n",
      "Epoch 10 of 500 took 0.111s\n",
      "Epoch 10/499\n",
      "----------\n",
      "train Loss: 0.00158401 Acc: 0.91894531\n",
      "val Loss: 0.00076210 Acc: 0.930131\n",
      "Epoch 11 of 500 took 0.106s\n",
      "Epoch 11/499\n",
      "----------\n",
      "train Loss: 0.00146707 Acc: 0.93310547\n",
      "val Loss: 0.00068053 Acc: 0.93449782\n",
      "Epoch 12 of 500 took 0.112s\n",
      "Epoch 12/499\n",
      "----------\n",
      "train Loss: 0.00144262 Acc: 0.93701172\n",
      "val Loss: 0.00088773 Acc: 0.930131\n",
      "Epoch 13 of 500 took 0.106s\n",
      "Epoch 13/499\n",
      "----------\n",
      "train Loss: 0.00150296 Acc: 0.92626953\n",
      "val Loss: 0.00055277 Acc: 0.94759825\n",
      "New best validation loss: 0.0005527659925310893\n",
      "Epoch 14 of 500 took 0.110s\n",
      "Epoch 14/499\n",
      "----------\n",
      "train Loss: 0.00145587 Acc: 0.9375\n",
      "val Loss: 0.00077408 Acc: 0.93449782\n",
      "Epoch 15 of 500 took 0.108s\n",
      "Epoch 15/499\n",
      "----------\n",
      "train Loss: 0.00122253 Acc: 0.94042969\n",
      "val Loss: 0.00067852 Acc: 0.93886463\n",
      "Epoch 16 of 500 took 0.108s\n",
      "Epoch 16/499\n",
      "----------\n",
      "train Loss: 0.00129950 Acc: 0.93603516\n",
      "val Loss: 0.00088445 Acc: 0.92576419\n",
      "Epoch 17 of 500 took 0.106s\n",
      "Epoch 17/499\n",
      "----------\n",
      "train Loss: 0.00138222 Acc: 0.93310547\n",
      "val Loss: 0.00058806 Acc: 0.94323144\n",
      "Epoch 18 of 500 took 0.108s\n",
      "Epoch 18/499\n",
      "----------\n",
      "train Loss: 0.00130239 Acc: 0.93798828\n",
      "val Loss: 0.00060370 Acc: 0.94323144\n",
      "Epoch 19 of 500 took 0.106s\n",
      "Epoch 19/499\n",
      "----------\n",
      "train Loss: 0.00122605 Acc: 0.94287109\n",
      "val Loss: 0.00084872 Acc: 0.92576419\n",
      "Epoch    20: reducing learning rate of group 0 to 5.0300e-04.\n",
      "Epoch 20 of 500 took 0.108s\n",
      "Epoch 20/499\n",
      "----------\n",
      "train Loss: 0.00110916 Acc: 0.94970703\n",
      "val Loss: 0.00050908 Acc: 0.94323144\n",
      "Epoch 21 of 500 took 0.110s\n",
      "Epoch 21/499\n",
      "----------\n",
      "train Loss: 0.00105238 Acc: 0.94921875\n",
      "val Loss: 0.00048605 Acc: 0.95633188\n",
      "Epoch 22 of 500 took 0.110s\n",
      "Epoch 22/499\n",
      "----------\n",
      "train Loss: 0.00096505 Acc: 0.95458984\n",
      "val Loss: 0.00043801 Acc: 0.95633188\n",
      "New best validation loss: 0.00043800850622518616\n",
      "Epoch 23 of 500 took 0.107s\n",
      "Epoch 23/499\n",
      "----------\n",
      "train Loss: 0.00085244 Acc: 0.96386719\n",
      "val Loss: 0.00042940 Acc: 0.95196507\n",
      "Epoch 24 of 500 took 0.109s\n",
      "Epoch 24/499\n",
      "----------\n",
      "train Loss: 0.00083341 Acc: 0.96240234\n",
      "val Loss: 0.00053772 Acc: 0.94323144\n",
      "Epoch 25 of 500 took 0.106s\n",
      "Epoch 25/499\n",
      "----------\n",
      "train Loss: 0.00095998 Acc: 0.95654297\n",
      "val Loss: 0.00049297 Acc: 0.94323144\n",
      "Epoch 26 of 500 took 0.108s\n",
      "Epoch 26/499\n",
      "----------\n",
      "train Loss: 0.00085803 Acc: 0.96289062\n",
      "val Loss: 0.00045898 Acc: 0.96069869\n",
      "Epoch 27 of 500 took 0.105s\n",
      "Epoch 27/499\n",
      "----------\n",
      "train Loss: 0.00086030 Acc: 0.96191406\n",
      "val Loss: 0.00043558 Acc: 0.94759825\n",
      "Epoch 28 of 500 took 0.109s\n",
      "Epoch 28/499\n",
      "----------\n",
      "train Loss: 0.00088030 Acc: 0.96044922\n",
      "val Loss: 0.00042875 Acc: 0.96069869\n",
      "Epoch 29 of 500 took 0.106s\n",
      "Epoch 29/499\n",
      "----------\n",
      "train Loss: 0.00087544 Acc: 0.96240234\n",
      "val Loss: 0.00046214 Acc: 0.94759825\n",
      "Epoch 30 of 500 took 0.109s\n",
      "Epoch 30/499\n",
      "----------\n",
      "train Loss: 0.00073752 Acc: 0.96679688\n",
      "val Loss: 0.00042306 Acc: 0.95196507\n",
      "Epoch 31 of 500 took 0.108s\n",
      "Epoch 31/499\n",
      "----------\n",
      "train Loss: 0.00082517 Acc: 0.9609375\n",
      "val Loss: 0.00041091 Acc: 0.95633188\n",
      "Epoch 32 of 500 took 0.109s\n",
      "Epoch 32/499\n",
      "----------\n",
      "train Loss: 0.00079043 Acc: 0.95996094\n",
      "val Loss: 0.00053384 Acc: 0.95633188\n",
      "Epoch 33 of 500 took 0.108s\n",
      "Epoch 33/499\n",
      "----------\n",
      "train Loss: 0.00084125 Acc: 0.96386719\n",
      "val Loss: 0.00039861 Acc: 0.96069869\n",
      "Epoch 34 of 500 took 0.184s\n",
      "\n",
      "Training complete in 0m 4s\n",
      "Best val loss: 0.000438\n"
     ]
    }
   ],
   "source": [
    "train_fine_tuning(examples_datasets_train, labels_datasets_train,\n",
    "                  num_kernels=num_kernels, path_weight_to_save_to=path_TSD,\n",
    "                  number_of_classes=number_of_classes, \n",
    "                  number_of_cycles_total=number_of_cycles_total,\n",
    "                  number_of_cycle_for_first_training = number_of_cycle_for_first_training,\n",
    "                  batch_size=batch_size,\n",
    "                  feature_vector_input_length=feature_vector_input_length,\n",
    "                  learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET one participant_examples  (5,)\n",
      "   GET one training_index_examples  (24, 572, 252)  at  0\n",
      "   GOT one group XY  (13728, 252)    (13728,)\n",
      "       one group XY test  (3432, 252)    (3432, 252)\n",
      "       one group XY train (12355, 252)    (12355,)\n",
      "       one group XY valid (1373, 252)    (1373, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  1\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  2\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  3\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  4\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "dataloaders: \n",
      "   train  (1, 5)\n",
      "   valid  (1, 5)\n",
      "   test  (1, 5)\n",
      "0  SESSION   data =  3432\n",
      "Participant:  0  Accuracy:  0.8875291375291375\n",
      "1  SESSION   data =  572\n",
      "Participant:  0  Accuracy:  0.798951048951049\n",
      "2  SESSION   data =  572\n",
      "Participant:  0  Accuracy:  0.7342657342657343\n",
      "3  SESSION   data =  572\n",
      "Participant:  0  Accuracy:  0.7255244755244755\n",
      "4  SESSION   data =  572\n",
      "Participant:  0  Accuracy:  0.7132867132867133\n",
      "ACCURACY PARTICIPANT  0 :  [0.8875291375291375, 0.798951048951049, 0.7342657342657343, 0.7255244755244755, 0.7132867132867133]\n",
      "[array([0.88752914, 0.79895105, 0.73426573, 0.72552448, 0.71328671])]\n",
      "OVERALL ACCURACY: 0.7719114219114218\n"
     ]
    }
   ],
   "source": [
    "algo_name = \"standard_TSD\"\n",
    "test_standard_model_on_training_sessions(examples_datasets_train, labels_datasets_train,\n",
    "                                  num_neurons=num_kernels, use_only_first_training=True,\n",
    "                                  path_weights=path_TSD,\n",
    "                                  feature_vector_input_length=feature_vector_input_length,\n",
    "                                  save_path = save_TSD, algo_name=algo_name,\n",
    "                                  number_of_cycles_total=number_of_cycles_total,\n",
    "                                  number_of_cycle_for_first_training = number_of_cycle_for_first_training,\n",
    "                                  number_of_classes=number_of_classes, cycle_for_test=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Day_0~5</th>\n",
       "      <td>0.887529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_6</th>\n",
       "      <td>0.798951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_7</th>\n",
       "      <td>0.734266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_8</th>\n",
       "      <td>0.725524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_9</th>\n",
       "      <td>0.713287</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Participant_5\n",
       "Day_0~5      0.887529\n",
       "Day_6        0.798951\n",
       "Day_7        0.734266\n",
       "Day_8        0.725524\n",
       "Day_9        0.713287"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_filename = save_TSD + '/predictions_' + algo_name + \"_no_retraining.npy\"\n",
    "results = np.load(results_filename, allow_pickle=True)\n",
    "TSD_acc = results[0]\n",
    "TSD_acc_overall = np.mean(TSD_acc)\n",
    "index_participant_list = ['0~5', 6, 7, 8, 9]\n",
    "TSD_df = pd.DataFrame(TSD_acc.transpose(), \n",
    "                       index = [f'Day_{i}' for i in index_participant_list],\n",
    "                        columns = ['Participant_5'])\n",
    "TSD_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "TSD_df.transpose().plot.bar(rot=0, figsize=(10,5))\n",
    "plt.title(\"TSD Accuracies\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TrainingsAndEvaluations.ForTrainingSessions.utils import get_gesture_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ground_truths  =  (1, 5)\n",
      "predictions =  (1, 5)\n",
      "index_participant_list  ['0~5', 6, 7, 8, 9]\n",
      "accuracies_gestures =  (22, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Loc1_Sub5_Day0~5-&gt;0~5</th>\n",
       "      <th>Loc1_Sub5_Day0~5-&gt;6</th>\n",
       "      <th>Loc1_Sub5_Day0~5-&gt;7</th>\n",
       "      <th>Loc1_Sub5_Day0~5-&gt;8</th>\n",
       "      <th>Loc1_Sub5_Day0~5-&gt;9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M1</td>\n",
       "      <td>0.955128</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.384615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M2</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.692308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M3</td>\n",
       "      <td>0.942308</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.192308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M4</td>\n",
       "      <td>0.852564</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>M5</td>\n",
       "      <td>0.993590</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.653846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>M6</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>M7</td>\n",
       "      <td>0.980769</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.961538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>M8</td>\n",
       "      <td>0.935897</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>M9</td>\n",
       "      <td>0.935897</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.884615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>M10</td>\n",
       "      <td>0.852564</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.807692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>M11</td>\n",
       "      <td>0.756410</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.961538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>M12</td>\n",
       "      <td>0.724359</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.307692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>M13</td>\n",
       "      <td>0.737179</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.692308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>M14</td>\n",
       "      <td>0.673077</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>M15</td>\n",
       "      <td>0.782051</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.192308</td>\n",
       "      <td>0.576923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>M16</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>M17</td>\n",
       "      <td>0.993590</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>M18</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.192308</td>\n",
       "      <td>0.961538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>M19</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.961538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>M20</td>\n",
       "      <td>0.852564</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.769231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>M21</td>\n",
       "      <td>0.814103</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.884615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Mean</td>\n",
       "      <td>0.887529</td>\n",
       "      <td>0.798951</td>\n",
       "      <td>0.734266</td>\n",
       "      <td>0.725524</td>\n",
       "      <td>0.713287</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Loc1_Sub5_Day0~5->0~5  Loc1_Sub5_Day0~5->6  \\\n",
       "0          M0               1.000000             1.000000   \n",
       "1          M1               0.955128             0.730769   \n",
       "2          M2               0.846154             0.576923   \n",
       "3          M3               0.942308             0.846154   \n",
       "4          M4               0.852564             0.000000   \n",
       "5          M5               0.993590             1.000000   \n",
       "6          M6               0.961538             0.423077   \n",
       "7          M7               0.980769             1.000000   \n",
       "8          M8               0.935897             1.000000   \n",
       "9          M9               0.935897             1.000000   \n",
       "10        M10               0.852564             1.000000   \n",
       "11        M11               0.756410             0.923077   \n",
       "12        M12               0.724359             0.576923   \n",
       "13        M13               0.737179             0.846154   \n",
       "14        M14               0.673077             0.269231   \n",
       "15        M15               0.782051             0.653846   \n",
       "16        M16               0.961538             1.000000   \n",
       "17        M17               0.993590             1.000000   \n",
       "18        M18               0.974359             0.884615   \n",
       "19        M19               1.000000             1.000000   \n",
       "20        M20               0.852564             0.923077   \n",
       "21        M21               0.814103             0.923077   \n",
       "22       Mean               0.887529             0.798951   \n",
       "\n",
       "    Loc1_Sub5_Day0~5->7  Loc1_Sub5_Day0~5->8  Loc1_Sub5_Day0~5->9  \n",
       "0              1.000000             1.000000             1.000000  \n",
       "1              0.500000             0.500000             0.384615  \n",
       "2              0.653846             0.769231             0.692308  \n",
       "3              0.076923             0.846154             0.192308  \n",
       "4              0.000000             0.115385             0.000000  \n",
       "5              0.384615             0.961538             0.653846  \n",
       "6              0.615385             0.384615             1.000000  \n",
       "7              1.000000             1.000000             0.961538  \n",
       "8              1.000000             1.000000             1.000000  \n",
       "9              1.000000             1.000000             0.884615  \n",
       "10             0.769231             0.961538             0.807692  \n",
       "11             0.884615             1.000000             0.961538  \n",
       "12             0.730769             0.576923             0.307692  \n",
       "13             0.807692             1.000000             0.692308  \n",
       "14             0.653846             0.000000             0.000000  \n",
       "15             0.423077             0.192308             0.576923  \n",
       "16             1.000000             1.000000             1.000000  \n",
       "17             1.000000             1.000000             1.000000  \n",
       "18             0.884615             0.192308             0.961538  \n",
       "19             0.846154             0.730769             0.961538  \n",
       "20             0.961538             0.730769             0.769231  \n",
       "21             0.961538             1.000000             0.884615  \n",
       "22             0.734266             0.725524             0.713287  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truths = results[1]\n",
    "predictions = results[2]\n",
    "print(\"ground_truths  = \", np.shape(ground_truths))\n",
    "print(\"predictions = \", np.shape(predictions))\n",
    "m_name = \"Loc1_Sub\"\n",
    "n_name = \"Day0~5->\"\n",
    "df = get_gesture_accuracies(ground_truths, predictions, number_of_classes=number_of_classes, \n",
    "                            m_name=m_name, n_name=n_name, path=save_TSD, algo_name=algo_name,\n",
    "                           index_participant_list_customized=index_participant_list,\n",
    "                           lump_day_at_participant=5)\n",
    "df = pd.read_csv(save_TSD+'/'+algo_name+'.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. DANN\n",
    "* `train_DANN`: train DANN model using the first set of training weights from base model\n",
    "    * num_sessions-1 sets of training weights will be saved\n",
    "* `test_DANN_on_training_sessions`: test DANN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TrainingsAndEvaluations.ForTrainingSessions.train_tsd_dnn_DA import train_DANN, test_DANN_on_training_sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET one participant_examples  (5,)\n",
      "   GET one training_index_examples  (24, 572, 252)  at  0\n",
      "   GOT one group XY  (13728, 252)    (13728,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (12355, 252)    (12355,)\n",
      "       one group XY valid (1373, 252)    (1373, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  1\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  2\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  3\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  4\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "dataloaders: \n",
      "   train  (1, 5)\n",
      "   valid  (1, 5)\n",
      "   test  (1, 0)\n",
      "SHAPE SESSIONS:  (5,)\n",
      "()\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump6/TSD/participant_0/best_state_0.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump6/TSD/participant_0/best_state_0.pt' (epoch 5)\n",
      "STARTING TRAINING\n",
      "Accuracy source 0.861328, main loss classifier 0.282376, source classification loss 0.410490, loss domain distinction 0.274652, accuracy domain distinction 0.491455\n",
      "VALIDATION Loss: 0.34302232 Acc: 0.87035688\n",
      "New best validation loss:  0.34302231669425964\n",
      "Epoch 1 of 500 took 0.267s\n",
      "Accuracy source 0.840332, main loss classifier 0.306472, source classification loss 0.465039, loss domain distinction 0.193988, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.37712196 Acc: 0.85287691\n",
      "Epoch 2 of 500 took 0.245s\n",
      "Accuracy source 0.852539, main loss classifier 0.280199, source classification loss 0.415657, loss domain distinction 0.188429, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.33664224 Acc: 0.87035688\n",
      "New best validation loss:  0.33664223551750183\n",
      "Epoch 3 of 500 took 0.242s\n",
      "Accuracy source 0.853027, main loss classifier 0.272089, source classification loss 0.399694, loss domain distinction 0.189324, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.29930484 Acc: 0.88346686\n",
      "New best validation loss:  0.2993048429489136\n",
      "Epoch 4 of 500 took 0.241s\n",
      "Accuracy source 0.835938, main loss classifier 0.296022, source classification loss 0.449227, loss domain distinction 0.189634, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.39782625 Acc: 0.85214858\n",
      "Epoch 5 of 500 took 0.237s\n",
      "Accuracy source 0.846191, main loss classifier 0.281436, source classification loss 0.420923, loss domain distinction 0.187763, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.40776086 Acc: 0.84996358\n",
      "Epoch    11: reducing learning rate of group 0 to 5.0300e-04.\n",
      "Epoch 6 of 500 took 0.235s\n",
      "Accuracy source 0.871094, main loss classifier 0.259439, source classification loss 0.382029, loss domain distinction 0.184157, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.35494328 Acc: 0.87472688\n",
      "Epoch 7 of 500 took 0.236s\n",
      "Accuracy source 0.890137, main loss classifier 0.227234, source classification loss 0.316596, loss domain distinction 0.184521, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.37841734 Acc: 0.8616169\n",
      "Epoch 8 of 500 took 0.240s\n",
      "Accuracy source 0.891602, main loss classifier 0.240125, source classification loss 0.343001, loss domain distinction 0.184024, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.34116957 Acc: 0.87327021\n",
      "Epoch 9 of 500 took 0.235s\n",
      "Accuracy source 0.872559, main loss classifier 0.242982, source classification loss 0.349230, loss domain distinction 0.183192, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.25804725 Acc: 0.90386016\n",
      "New best validation loss:  0.2580472528934479\n",
      "Epoch 10 of 500 took 0.238s\n",
      "Accuracy source 0.876953, main loss classifier 0.239210, source classification loss 0.342055, loss domain distinction 0.181351, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.34236389 Acc: 0.87327021\n",
      "Epoch 11 of 500 took 0.235s\n",
      "Accuracy source 0.871094, main loss classifier 0.251382, source classification loss 0.365573, loss domain distinction 0.184388, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.32003495 Acc: 0.87472688\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0060e-04.\n",
      "Epoch 12 of 500 took 0.235s\n",
      "Accuracy source 0.891602, main loss classifier 0.227836, source classification loss 0.318953, loss domain distinction 0.184028, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.25528923 Acc: 0.89657684\n",
      "New best validation loss:  0.255289226770401\n",
      "Epoch 13 of 500 took 0.236s\n",
      "Accuracy source 0.886719, main loss classifier 0.231445, source classification loss 0.328103, loss domain distinction 0.181156, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.30270472 Acc: 0.88638019\n",
      "Epoch 14 of 500 took 0.235s\n",
      "Accuracy source 0.878906, main loss classifier 0.240865, source classification loss 0.346202, loss domain distinction 0.182741, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.27350470 Acc: 0.89002185\n",
      "Epoch 15 of 500 took 0.236s\n",
      "Accuracy source 0.889648, main loss classifier 0.226879, source classification loss 0.318014, loss domain distinction 0.181785, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.24642093 Acc: 0.89949017\n",
      "New best validation loss:  0.2464209347963333\n",
      "Epoch 16 of 500 took 0.237s\n",
      "Accuracy source 0.879883, main loss classifier 0.233911, source classification loss 0.331967, loss domain distinction 0.183451, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.24938609 Acc: 0.90677349\n",
      "Epoch 17 of 500 took 0.239s\n",
      "Accuracy source 0.897461, main loss classifier 0.224055, source classification loss 0.311970, loss domain distinction 0.182330, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.38140664 Acc: 0.8616169\n",
      "Epoch 18 of 500 took 0.237s\n",
      "Accuracy source 0.870117, main loss classifier 0.257113, source classification loss 0.378606, loss domain distinction 0.183519, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.29223737 Acc: 0.88419519\n",
      "Epoch 19 of 500 took 0.234s\n",
      "Accuracy source 0.884766, main loss classifier 0.239811, source classification loss 0.343814, loss domain distinction 0.184194, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.30877838 Acc: 0.89075018\n",
      "Epoch 20 of 500 took 0.238s\n",
      "Accuracy source 0.882812, main loss classifier 0.240246, source classification loss 0.344551, loss domain distinction 0.183394, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.30423722 Acc: 0.88055353\n",
      "Epoch 21 of 500 took 0.286s\n",
      "Accuracy source 0.892578, main loss classifier 0.225874, source classification loss 0.315304, loss domain distinction 0.184579, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.29488003 Acc: 0.88710852\n",
      "Epoch 22 of 500 took 0.241s\n",
      "Accuracy source 0.891113, main loss classifier 0.228878, source classification loss 0.321874, loss domain distinction 0.183257, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.28954268 Acc: 0.88856519\n",
      "Epoch 23 of 500 took 0.234s\n",
      "Accuracy source 0.889648, main loss classifier 0.229540, source classification loss 0.323497, loss domain distinction 0.183274, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.38034227 Acc: 0.86380189\n",
      "Epoch 24 of 500 took 0.235s\n",
      "Accuracy source 0.895996, main loss classifier 0.219002, source classification loss 0.303124, loss domain distinction 0.182034, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.25822231 Acc: 0.89584851\n",
      "Epoch 25 of 500 took 0.234s\n",
      "Accuracy source 0.881348, main loss classifier 0.232329, source classification loss 0.329302, loss domain distinction 0.183562, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.33554548 Acc: 0.8776402\n",
      "Epoch 26 of 500 took 0.236s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy source 0.886719, main loss classifier 0.229451, source classification loss 0.324141, loss domain distinction 0.181072, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.28464296 Acc: 0.88929352\n",
      "Training complete in 0m 6s\n",
      "()\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump6/TSD/participant_0/best_state_0.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump6/TSD/participant_0/best_state_0.pt' (epoch 5)\n",
      "STARTING TRAINING\n",
      "Accuracy source 0.834961, main loss classifier 0.309515, source classification loss 0.462662, loss domain distinction 0.276731, accuracy domain distinction 0.487793\n",
      "VALIDATION Loss: 0.39522079 Acc: 0.84996358\n",
      "New best validation loss:  0.3952207863330841\n",
      "Epoch 1 of 500 took 0.239s\n",
      "Accuracy source 0.837402, main loss classifier 0.310665, source classification loss 0.474160, loss domain distinction 0.191214, accuracy domain distinction 0.499512\n",
      "VALIDATION Loss: 0.49045110 Acc: 0.83102695\n",
      "Epoch 2 of 500 took 0.236s\n",
      "Accuracy source 0.831055, main loss classifier 0.323392, source classification loss 0.502460, loss domain distinction 0.189468, accuracy domain distinction 0.499512\n",
      "VALIDATION Loss: 0.49840945 Acc: 0.81573197\n",
      "Epoch 3 of 500 took 0.236s\n",
      "Accuracy source 0.819824, main loss classifier 0.322165, source classification loss 0.501587, loss domain distinction 0.188468, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.50366616 Acc: 0.82665696\n",
      "Epoch 4 of 500 took 0.235s\n",
      "Accuracy source 0.846191, main loss classifier 0.288560, source classification loss 0.435173, loss domain distinction 0.185560, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.37373337 Acc: 0.85651857\n",
      "New best validation loss:  0.37373337149620056\n",
      "Epoch 5 of 500 took 0.237s\n",
      "Accuracy source 0.843750, main loss classifier 0.285960, source classification loss 0.431430, loss domain distinction 0.186045, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.42243093 Acc: 0.84996358\n",
      "Epoch    11: reducing learning rate of group 0 to 5.0300e-04.\n",
      "Epoch 6 of 500 took 0.236s\n",
      "Accuracy source 0.869141, main loss classifier 0.261508, source classification loss 0.387329, loss domain distinction 0.182139, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.39178479 Acc: 0.85651857\n",
      "Epoch 7 of 500 took 0.238s\n",
      "Accuracy source 0.865723, main loss classifier 0.257989, source classification loss 0.379003, loss domain distinction 0.183436, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.45338768 Acc: 0.84195193\n",
      "Epoch 8 of 500 took 0.238s\n",
      "Accuracy source 0.872070, main loss classifier 0.256209, source classification loss 0.375851, loss domain distinction 0.182782, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.45740932 Acc: 0.83831027\n",
      "Epoch 9 of 500 took 0.237s\n",
      "Accuracy source 0.873047, main loss classifier 0.249341, source classification loss 0.362248, loss domain distinction 0.182089, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.56489164 Acc: 0.80335033\n",
      "Epoch 10 of 500 took 0.235s\n",
      "Accuracy source 0.881836, main loss classifier 0.238492, source classification loss 0.340437, loss domain distinction 0.183888, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.41012722 Acc: 0.85214858\n",
      "Epoch 11 of 500 took 0.239s\n",
      "Accuracy source 0.878906, main loss classifier 0.242864, source classification loss 0.348717, loss domain distinction 0.182925, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.38400817 Acc: 0.86016023\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0060e-04.\n",
      "Epoch 12 of 500 took 0.266s\n",
      "Accuracy source 0.875977, main loss classifier 0.245646, source classification loss 0.356921, loss domain distinction 0.183340, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.35624859 Acc: 0.87035688\n",
      "New best validation loss:  0.3562485873699188\n",
      "Epoch 13 of 500 took 0.236s\n",
      "Accuracy source 0.869629, main loss classifier 0.250288, source classification loss 0.365508, loss domain distinction 0.182102, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.35032690 Acc: 0.87254188\n",
      "New best validation loss:  0.35032689571380615\n",
      "Epoch 14 of 500 took 0.238s\n",
      "Accuracy source 0.877441, main loss classifier 0.240093, source classification loss 0.344550, loss domain distinction 0.182429, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.45385697 Acc: 0.84268026\n",
      "Epoch 15 of 500 took 0.237s\n",
      "Accuracy source 0.888672, main loss classifier 0.232918, source classification loss 0.330063, loss domain distinction 0.182165, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.35432076 Acc: 0.86671522\n",
      "Epoch 16 of 500 took 0.238s\n",
      "Accuracy source 0.873047, main loss classifier 0.247298, source classification loss 0.358691, loss domain distinction 0.183310, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.34307462 Acc: 0.87181355\n",
      "New best validation loss:  0.34307461977005005\n",
      "Epoch 17 of 500 took 0.287s\n",
      "Accuracy source 0.896484, main loss classifier 0.231537, source classification loss 0.328168, loss domain distinction 0.182037, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.33641598 Acc: 0.87399854\n",
      "New best validation loss:  0.33641597628593445\n",
      "Epoch 18 of 500 took 0.236s\n",
      "Accuracy source 0.877930, main loss classifier 0.241053, source classification loss 0.347633, loss domain distinction 0.181294, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.32071173 Acc: 0.87691187\n",
      "New best validation loss:  0.32071173191070557\n",
      "Epoch 19 of 500 took 0.246s\n",
      "Accuracy source 0.885254, main loss classifier 0.232993, source classification loss 0.330673, loss domain distinction 0.181413, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.39793593 Acc: 0.85579024\n",
      "Epoch 20 of 500 took 0.237s\n",
      "Accuracy source 0.887207, main loss classifier 0.226513, source classification loss 0.317422, loss domain distinction 0.182230, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.39123353 Acc: 0.8594319\n",
      "Epoch 21 of 500 took 0.238s\n",
      "Accuracy source 0.888184, main loss classifier 0.227192, source classification loss 0.319302, loss domain distinction 0.181541, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.41271505 Acc: 0.85651857\n",
      "Epoch 22 of 500 took 0.240s\n",
      "Accuracy source 0.890625, main loss classifier 0.226914, source classification loss 0.318976, loss domain distinction 0.181304, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.34232864 Acc: 0.87035688\n",
      "Epoch 23 of 500 took 0.248s\n",
      "Accuracy source 0.885254, main loss classifier 0.229482, source classification loss 0.323556, loss domain distinction 0.180525, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.38265896 Acc: 0.85870357\n",
      "Epoch 24 of 500 took 0.241s\n",
      "Accuracy source 0.904297, main loss classifier 0.219568, source classification loss 0.304172, loss domain distinction 0.181161, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.39187852 Acc: 0.85870357\n",
      "Epoch 25 of 500 took 0.235s\n",
      "Accuracy source 0.893555, main loss classifier 0.217328, source classification loss 0.299517, loss domain distinction 0.181290, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.39329445 Acc: 0.8572469\n",
      "Epoch 26 of 500 took 0.237s\n",
      "Accuracy source 0.884766, main loss classifier 0.233089, source classification loss 0.331315, loss domain distinction 0.182464, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.42034209 Acc: 0.84850692\n",
      "Epoch 27 of 500 took 0.237s\n",
      "Accuracy source 0.892578, main loss classifier 0.217521, source classification loss 0.300045, loss domain distinction 0.180616, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.35567781 Acc: 0.87181355\n",
      "Epoch 28 of 500 took 0.235s\n",
      "Accuracy source 0.881348, main loss classifier 0.235753, source classification loss 0.336159, loss domain distinction 0.182718, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.39761692 Acc: 0.85870357\n",
      "Epoch 29 of 500 took 0.234s\n",
      "Accuracy source 0.880371, main loss classifier 0.237068, source classification loss 0.339887, loss domain distinction 0.180480, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.38943359 Acc: 0.86598689\n",
      "Training complete in 0m 7s\n",
      "()\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump6/TSD/participant_0/best_state_0.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump6/TSD/participant_0/best_state_0.pt' (epoch 5)\n",
      "STARTING TRAINING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy source 0.880371, main loss classifier 0.252671, source classification loss 0.350039, loss domain distinction 0.273564, accuracy domain distinction 0.488525\n",
      "VALIDATION Loss: 0.28944197 Acc: 0.88419519\n",
      "New best validation loss:  0.2894419729709625\n",
      "Epoch 1 of 500 took 0.238s\n",
      "Accuracy source 0.837891, main loss classifier 0.301200, source classification loss 0.455956, loss domain distinction 0.191848, accuracy domain distinction 0.500244\n",
      "VALIDATION Loss: 0.36086997 Acc: 0.8616169\n",
      "Epoch 2 of 500 took 0.237s\n",
      "Accuracy source 0.829590, main loss classifier 0.314095, source classification loss 0.483357, loss domain distinction 0.189004, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.29636106 Acc: 0.89147851\n",
      "Epoch 3 of 500 took 0.238s\n",
      "Accuracy source 0.844238, main loss classifier 0.300801, source classification loss 0.458404, loss domain distinction 0.187665, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.43677828 Acc: 0.8390386\n",
      "Epoch 4 of 500 took 0.238s\n",
      "Accuracy source 0.842773, main loss classifier 0.281360, source classification loss 0.420623, loss domain distinction 0.187880, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.43503419 Acc: 0.84049527\n",
      "Epoch 5 of 500 took 0.237s\n",
      "Accuracy source 0.859863, main loss classifier 0.269766, source classification loss 0.397558, loss domain distinction 0.186574, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.44589335 Acc: 0.84268026\n",
      "Epoch    11: reducing learning rate of group 0 to 5.0300e-04.\n",
      "Epoch 6 of 500 took 0.243s\n",
      "Accuracy source 0.874023, main loss classifier 0.249883, source classification loss 0.362415, loss domain distinction 0.184422, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.31891111 Acc: 0.88055353\n",
      "Epoch 7 of 500 took 0.235s\n",
      "Accuracy source 0.871094, main loss classifier 0.254791, source classification loss 0.373440, loss domain distinction 0.182470, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.32398629 Acc: 0.88419519\n",
      "Epoch 8 of 500 took 0.235s\n",
      "Accuracy source 0.877930, main loss classifier 0.247040, source classification loss 0.357416, loss domain distinction 0.182346, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.26853830 Acc: 0.89147851\n",
      "New best validation loss:  0.26853829622268677\n",
      "Epoch 9 of 500 took 0.238s\n",
      "Accuracy source 0.868164, main loss classifier 0.253455, source classification loss 0.370508, loss domain distinction 0.183985, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.31384122 Acc: 0.87836854\n",
      "Epoch 10 of 500 took 0.237s\n",
      "Accuracy source 0.891602, main loss classifier 0.230252, source classification loss 0.324943, loss domain distinction 0.182038, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.31515428 Acc: 0.8820102\n",
      "Epoch 11 of 500 took 0.235s\n",
      "Accuracy source 0.882324, main loss classifier 0.242069, source classification loss 0.348250, loss domain distinction 0.181709, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.27518809 Acc: 0.89293518\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0060e-04.\n",
      "Epoch 12 of 500 took 0.234s\n",
      "Accuracy source 0.892578, main loss classifier 0.228218, source classification loss 0.321783, loss domain distinction 0.180216, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.26707888 Acc: 0.90094683\n",
      "New best validation loss:  0.26707887649536133\n",
      "Epoch 13 of 500 took 0.241s\n",
      "Accuracy source 0.880859, main loss classifier 0.247763, source classification loss 0.360835, loss domain distinction 0.182002, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.31196350 Acc: 0.88929352\n",
      "Epoch 14 of 500 took 0.285s\n",
      "Accuracy source 0.892090, main loss classifier 0.230469, source classification loss 0.325242, loss domain distinction 0.181995, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.30967256 Acc: 0.88710852\n",
      "Epoch 15 of 500 took 0.236s\n",
      "Accuracy source 0.882812, main loss classifier 0.238881, source classification loss 0.342822, loss domain distinction 0.182174, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.37359765 Acc: 0.86744355\n",
      "Epoch 16 of 500 took 0.234s\n",
      "Accuracy source 0.895508, main loss classifier 0.218188, source classification loss 0.301164, loss domain distinction 0.182094, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.35029331 Acc: 0.87254188\n",
      "Epoch 17 of 500 took 0.235s\n",
      "Accuracy source 0.877930, main loss classifier 0.241069, source classification loss 0.346432, loss domain distinction 0.180958, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.31172872 Acc: 0.88346686\n",
      "Epoch 18 of 500 took 0.233s\n",
      "Accuracy source 0.880371, main loss classifier 0.238637, source classification loss 0.341856, loss domain distinction 0.184147, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.36358052 Acc: 0.87254188\n",
      "Epoch 19 of 500 took 0.235s\n",
      "Accuracy source 0.888672, main loss classifier 0.231548, source classification loss 0.326824, loss domain distinction 0.184848, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.29984534 Acc: 0.88638019\n",
      "Epoch 20 of 500 took 0.235s\n",
      "Accuracy source 0.889160, main loss classifier 0.230622, source classification loss 0.325831, loss domain distinction 0.182214, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.40061805 Acc: 0.85506191\n",
      "Epoch 21 of 500 took 0.236s\n",
      "Accuracy source 0.885742, main loss classifier 0.228884, source classification loss 0.322933, loss domain distinction 0.180339, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.27934107 Acc: 0.89220685\n",
      "Epoch 22 of 500 took 0.234s\n",
      "Accuracy source 0.886230, main loss classifier 0.231025, source classification loss 0.327479, loss domain distinction 0.180250, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.31125355 Acc: 0.87909687\n",
      "Epoch 23 of 500 took 0.233s\n",
      "Accuracy source 0.888184, main loss classifier 0.228563, source classification loss 0.321988, loss domain distinction 0.181386, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.33420649 Acc: 0.87472688\n",
      "Training complete in 0m 6s\n",
      "()\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump6/TSD/participant_0/best_state_0.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump6/TSD/participant_0/best_state_0.pt' (epoch 5)\n",
      "STARTING TRAINING\n",
      "Accuracy source 0.839355, main loss classifier 0.311929, source classification loss 0.466985, loss domain distinction 0.279919, accuracy domain distinction 0.487061\n",
      "VALIDATION Loss: 0.38301426 Acc: 0.86453023\n",
      "New best validation loss:  0.3830142617225647\n",
      "Epoch 1 of 500 took 0.236s\n",
      "Accuracy source 0.860840, main loss classifier 0.271737, source classification loss 0.396831, loss domain distinction 0.190893, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.29593775 Acc: 0.88273853\n",
      "New best validation loss:  0.29593774676322937\n",
      "Epoch 2 of 500 took 0.236s\n",
      "Accuracy source 0.848633, main loss classifier 0.293700, source classification loss 0.442912, loss domain distinction 0.188635, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.31466612 Acc: 0.8798252\n",
      "Epoch 3 of 500 took 0.239s\n",
      "Accuracy source 0.868652, main loss classifier 0.265235, source classification loss 0.386900, loss domain distinction 0.189039, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.31191137 Acc: 0.88856519\n",
      "Epoch 4 of 500 took 0.237s\n",
      "Accuracy source 0.851562, main loss classifier 0.284532, source classification loss 0.427092, loss domain distinction 0.187495, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.27711216 Acc: 0.90094683\n",
      "New best validation loss:  0.2771121561527252\n",
      "Epoch 5 of 500 took 0.236s\n",
      "Accuracy source 0.854004, main loss classifier 0.279612, source classification loss 0.417864, loss domain distinction 0.187137, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.40190580 Acc: 0.8616169\n",
      "Epoch    11: reducing learning rate of group 0 to 5.0300e-04.\n",
      "Epoch 6 of 500 took 0.237s\n",
      "Accuracy source 0.865234, main loss classifier 0.264874, source classification loss 0.392778, loss domain distinction 0.184938, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.26043269 Acc: 0.89876184\n",
      "New best validation loss:  0.2604326903820038\n",
      "Epoch 7 of 500 took 0.235s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy source 0.870605, main loss classifier 0.260934, source classification loss 0.384344, loss domain distinction 0.183658, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.36247942 Acc: 0.86962855\n",
      "Epoch 8 of 500 took 0.237s\n",
      "Accuracy source 0.871582, main loss classifier 0.252393, source classification loss 0.366868, loss domain distinction 0.185903, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.30358154 Acc: 0.88273853\n",
      "Epoch 9 of 500 took 0.236s\n",
      "Accuracy source 0.886230, main loss classifier 0.228671, source classification loss 0.320788, loss domain distinction 0.182212, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.33752844 Acc: 0.8776402\n",
      "Epoch 10 of 500 took 0.234s\n",
      "Accuracy source 0.882324, main loss classifier 0.235784, source classification loss 0.335103, loss domain distinction 0.182685, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.31242889 Acc: 0.88419519\n",
      "Epoch 11 of 500 took 0.235s\n",
      "Accuracy source 0.895996, main loss classifier 0.221724, source classification loss 0.306711, loss domain distinction 0.183467, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.28312328 Acc: 0.88856519\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0060e-04.\n",
      "Epoch 12 of 500 took 0.236s\n",
      "Accuracy source 0.875977, main loss classifier 0.246735, source classification loss 0.358023, loss domain distinction 0.182540, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.33775821 Acc: 0.87254188\n",
      "Epoch 13 of 500 took 0.237s\n",
      "Accuracy source 0.890625, main loss classifier 0.231328, source classification loss 0.327202, loss domain distinction 0.181874, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.27513897 Acc: 0.88929352\n",
      "Epoch 14 of 500 took 0.240s\n",
      "Accuracy source 0.878418, main loss classifier 0.237952, source classification loss 0.339914, loss domain distinction 0.182993, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.31340271 Acc: 0.88128186\n",
      "Epoch 15 of 500 took 0.234s\n",
      "Accuracy source 0.885254, main loss classifier 0.234357, source classification loss 0.333855, loss domain distinction 0.183515, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.24939445 Acc: 0.89657684\n",
      "New best validation loss:  0.24939444661140442\n",
      "Epoch 16 of 500 took 0.240s\n",
      "Accuracy source 0.901367, main loss classifier 0.224083, source classification loss 0.312290, loss domain distinction 0.181287, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.28517902 Acc: 0.88710852\n",
      "Epoch 17 of 500 took 0.287s\n",
      "Accuracy source 0.887695, main loss classifier 0.230181, source classification loss 0.324858, loss domain distinction 0.182759, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.39742923 Acc: 0.8594319\n",
      "Epoch 18 of 500 took 0.235s\n",
      "Accuracy source 0.889648, main loss classifier 0.224736, source classification loss 0.314052, loss domain distinction 0.182493, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.25059369 Acc: 0.89949017\n",
      "Epoch 19 of 500 took 0.235s\n",
      "Accuracy source 0.878418, main loss classifier 0.231918, source classification loss 0.328621, loss domain distinction 0.182566, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.27882299 Acc: 0.8980335\n",
      "Epoch 20 of 500 took 0.237s\n",
      "Accuracy source 0.885742, main loss classifier 0.233424, source classification loss 0.331819, loss domain distinction 0.182700, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.23426589 Acc: 0.91260015\n",
      "New best validation loss:  0.23426589369773865\n",
      "Epoch 21 of 500 took 0.238s\n",
      "Accuracy source 0.875488, main loss classifier 0.243389, source classification loss 0.351445, loss domain distinction 0.182267, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.28221163 Acc: 0.89220685\n",
      "Epoch 22 of 500 took 0.239s\n",
      "Accuracy source 0.879395, main loss classifier 0.244492, source classification loss 0.352981, loss domain distinction 0.183397, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.29197329 Acc: 0.88419519\n",
      "Epoch 23 of 500 took 0.234s\n",
      "Accuracy source 0.879883, main loss classifier 0.243777, source classification loss 0.352734, loss domain distinction 0.181632, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.32850862 Acc: 0.8776402\n",
      "Epoch 24 of 500 took 0.240s\n",
      "Accuracy source 0.883301, main loss classifier 0.224944, source classification loss 0.314423, loss domain distinction 0.183102, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.29688841 Acc: 0.88492353\n",
      "Epoch 25 of 500 took 0.234s\n",
      "Accuracy source 0.873047, main loss classifier 0.247913, source classification loss 0.360471, loss domain distinction 0.182937, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.22383225 Acc: 0.91187181\n",
      "New best validation loss:  0.22383224964141846\n",
      "Epoch 26 of 500 took 0.236s\n",
      "Accuracy source 0.885254, main loss classifier 0.226063, source classification loss 0.316885, loss domain distinction 0.182698, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.26823786 Acc: 0.8980335\n",
      "Epoch 27 of 500 took 0.236s\n",
      "Accuracy source 0.875977, main loss classifier 0.233825, source classification loss 0.332075, loss domain distinction 0.181679, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.32125241 Acc: 0.88055353\n",
      "Epoch 28 of 500 took 0.234s\n",
      "Accuracy source 0.886230, main loss classifier 0.239274, source classification loss 0.343239, loss domain distinction 0.181785, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.26429155 Acc: 0.89657684\n",
      "Epoch 29 of 500 took 0.235s\n",
      "Accuracy source 0.876953, main loss classifier 0.234011, source classification loss 0.332489, loss domain distinction 0.181664, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.28092012 Acc: 0.89075018\n",
      "Epoch 30 of 500 took 0.234s\n",
      "Accuracy source 0.891113, main loss classifier 0.221469, source classification loss 0.307947, loss domain distinction 0.181787, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.28402457 Acc: 0.89147851\n",
      "Epoch 31 of 500 took 0.239s\n",
      "Accuracy source 0.892578, main loss classifier 0.220258, source classification loss 0.304836, loss domain distinction 0.182955, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.30573317 Acc: 0.88710852\n",
      "Epoch 32 of 500 took 0.251s\n",
      "Accuracy source 0.883301, main loss classifier 0.225285, source classification loss 0.315798, loss domain distinction 0.182052, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.27942270 Acc: 0.89657684\n",
      "Epoch 33 of 500 took 0.235s\n",
      "Accuracy source 0.878906, main loss classifier 0.247651, source classification loss 0.359867, loss domain distinction 0.183924, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.28814551 Acc: 0.88710852\n",
      "Epoch 34 of 500 took 0.235s\n",
      "Accuracy source 0.888672, main loss classifier 0.221512, source classification loss 0.308175, loss domain distinction 0.181935, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.28195319 Acc: 0.88856519\n",
      "Epoch 35 of 500 took 0.234s\n",
      "Accuracy source 0.887695, main loss classifier 0.222949, source classification loss 0.311001, loss domain distinction 0.182396, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.26661271 Acc: 0.89147851\n",
      "Epoch 36 of 500 took 0.235s\n",
      "Accuracy source 0.895508, main loss classifier 0.218810, source classification loss 0.302564, loss domain distinction 0.181517, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.29567462 Acc: 0.89002185\n",
      "Training complete in 0m 9s\n"
     ]
    }
   ],
   "source": [
    "train_DANN(examples_datasets_train, labels_datasets_train, \n",
    "          num_kernels=num_kernels,\n",
    "          path_weights_fine_tuning=path_TSD,\n",
    "          number_of_classes=number_of_classes,\n",
    "          number_of_cycles_total = number_of_cycles_total,\n",
    "          number_of_cycle_for_first_training = number_of_cycle_for_first_training,\n",
    "          batch_size=batch_size,\n",
    "          feature_vector_input_length=feature_vector_input_length,\n",
    "          path_weights_to_save_to=path_DANN, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET one participant_examples  (5,)\n",
      "   GET one training_index_examples  (24, 572, 252)  at  0\n",
      "   GOT one group XY  (13728, 252)    (13728,)\n",
      "       one group XY test  (3432, 252)    (3432, 252)\n",
      "       one group XY train (12355, 252)    (12355,)\n",
      "       one group XY valid (1373, 252)    (1373, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  1\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  2\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  3\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  4\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "dataloaders: \n",
      "   train  (1, 5)\n",
      "   valid  (1, 5)\n",
      "   test  (1, 5)\n",
      "(5,)\n",
      "Participant ID:  0  Session ID:  0  Accuracy:  0.8875291375291375\n",
      "Participant ID:  0  Session ID:  1  Accuracy:  0.8601398601398601\n",
      "Participant ID:  0  Session ID:  2  Accuracy:  0.8286713286713286\n",
      "Participant ID:  0  Session ID:  3  Accuracy:  0.763986013986014\n",
      "Participant ID:  0  Session ID:  4  Accuracy:  0.7342657342657343\n",
      "ACCURACY PARTICIPANT:  [0.8875291375291375, 0.8601398601398601, 0.8286713286713286, 0.763986013986014, 0.7342657342657343]\n",
      "[[0.88752914 0.86013986 0.82867133 0.76398601 0.73426573]]\n",
      "[array([0.88752914, 0.86013986, 0.82867133, 0.76398601, 0.73426573])]\n",
      "OVERALL ACCURACY: 0.814918414918415\n"
     ]
    }
   ],
   "source": [
    "algo_name = \"DANN\"\n",
    "test_DANN_on_training_sessions(examples_datasets_train, labels_datasets_train,\n",
    "                              feature_vector_input_length=feature_vector_input_length,\n",
    "                              num_neurons=num_kernels, path_weights_DA=path_DANN,\n",
    "                              algo_name=algo_name, save_path = save_DANN, \n",
    "                              number_of_cycles_total=number_of_cycles_total,\n",
    "                              number_of_cycle_for_first_training = number_of_cycle_for_first_training,\n",
    "                              path_weights_normal=path_TSD, number_of_classes=number_of_classes,\n",
    "                              cycle_for_test=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Day_0~5</th>\n",
       "      <td>0.887529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_6</th>\n",
       "      <td>0.86014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_7</th>\n",
       "      <td>0.828671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_8</th>\n",
       "      <td>0.763986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_9</th>\n",
       "      <td>0.734266</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Participant_5\n",
       "Day_0~5      0.887529\n",
       "Day_6         0.86014\n",
       "Day_7        0.828671\n",
       "Day_8        0.763986\n",
       "Day_9        0.734266"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_filename = save_DANN + '/predictions_' + algo_name + \".npy\"\n",
    "results = np.load(results_filename, allow_pickle=True)\n",
    "DANN_acc = results[0]\n",
    "DANN_acc_overall = np.mean(DANN_acc)\n",
    "DANN_df = pd.DataFrame(DANN_acc.transpose(), \n",
    "                       index = [f'Day_{i}' for i in index_participant_list],\n",
    "                        columns = ['Participant_5'])\n",
    "DANN_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "DANN_df.transpose().plot.bar(rot=0, figsize=(10,5))\n",
    "plt.title(\"DANN Accuracies\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ground_truths  =  (1, 5)\n",
      "predictions =  (1, 5)\n",
      "index_participant_list  ['0~5', 6, 7, 8, 9]\n",
      "accuracies_gestures =  (22, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Loc1_Sub5_Day0~5-&gt;0~5</th>\n",
       "      <th>Loc1_Sub5_Day0~5-&gt;6</th>\n",
       "      <th>Loc1_Sub5_Day0~5-&gt;7</th>\n",
       "      <th>Loc1_Sub5_Day0~5-&gt;8</th>\n",
       "      <th>Loc1_Sub5_Day0~5-&gt;9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M1</td>\n",
       "      <td>0.955128</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.423077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M2</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.769231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M3</td>\n",
       "      <td>0.942308</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.346154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M4</td>\n",
       "      <td>0.852564</td>\n",
       "      <td>0.346154</td>\n",
       "      <td>0.346154</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>M5</td>\n",
       "      <td>0.993590</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.884615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>M6</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>M7</td>\n",
       "      <td>0.980769</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.884615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>M8</td>\n",
       "      <td>0.935897</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.884615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>M9</td>\n",
       "      <td>0.935897</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.576923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>M10</td>\n",
       "      <td>0.852564</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>M11</td>\n",
       "      <td>0.756410</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>M12</td>\n",
       "      <td>0.724359</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.307692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>M13</td>\n",
       "      <td>0.737179</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.769231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>M14</td>\n",
       "      <td>0.673077</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.230769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>M15</td>\n",
       "      <td>0.782051</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.807692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>M16</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>M17</td>\n",
       "      <td>0.993590</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>M18</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.961538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>M19</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>M20</td>\n",
       "      <td>0.852564</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.576923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>M21</td>\n",
       "      <td>0.814103</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.807692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Mean</td>\n",
       "      <td>0.887529</td>\n",
       "      <td>0.860140</td>\n",
       "      <td>0.828671</td>\n",
       "      <td>0.763986</td>\n",
       "      <td>0.734266</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Loc1_Sub5_Day0~5->0~5  Loc1_Sub5_Day0~5->6  \\\n",
       "0          M0               1.000000             1.000000   \n",
       "1          M1               0.955128             0.807692   \n",
       "2          M2               0.846154             0.730769   \n",
       "3          M3               0.942308             0.884615   \n",
       "4          M4               0.852564             0.346154   \n",
       "5          M5               0.993590             1.000000   \n",
       "6          M6               0.961538             0.653846   \n",
       "7          M7               0.980769             1.000000   \n",
       "8          M8               0.935897             0.884615   \n",
       "9          M9               0.935897             0.884615   \n",
       "10        M10               0.852564             1.000000   \n",
       "11        M11               0.756410             0.884615   \n",
       "12        M12               0.724359             0.653846   \n",
       "13        M13               0.737179             0.961538   \n",
       "14        M14               0.673077             0.576923   \n",
       "15        M15               0.782051             0.807692   \n",
       "16        M16               0.961538             1.000000   \n",
       "17        M17               0.993590             1.000000   \n",
       "18        M18               0.974359             1.000000   \n",
       "19        M19               1.000000             1.000000   \n",
       "20        M20               0.852564             0.884615   \n",
       "21        M21               0.814103             0.961538   \n",
       "22       Mean               0.887529             0.860140   \n",
       "\n",
       "    Loc1_Sub5_Day0~5->7  Loc1_Sub5_Day0~5->8  Loc1_Sub5_Day0~5->9  \n",
       "0              1.000000             1.000000             1.000000  \n",
       "1              0.615385             0.423077             0.423077  \n",
       "2              0.884615             0.884615             0.769231  \n",
       "3              0.730769             1.000000             0.346154  \n",
       "4              0.346154             0.384615             0.000000  \n",
       "5              0.615385             0.923077             0.884615  \n",
       "6              1.000000             0.653846             1.000000  \n",
       "7              1.000000             1.000000             0.884615  \n",
       "8              0.692308             0.923077             0.884615  \n",
       "9              0.730769             0.692308             0.576923  \n",
       "10             0.846154             0.961538             0.923077  \n",
       "11             0.961538             1.000000             1.000000  \n",
       "12             0.769231             0.538462             0.307692  \n",
       "13             1.000000             1.000000             0.769231  \n",
       "14             0.730769             0.000000             0.230769  \n",
       "15             0.576923             0.538462             0.807692  \n",
       "16             1.000000             1.000000             1.000000  \n",
       "17             1.000000             1.000000             1.000000  \n",
       "18             1.000000             0.423077             0.961538  \n",
       "19             0.923077             0.769231             1.000000  \n",
       "20             0.846154             0.692308             0.576923  \n",
       "21             0.961538             1.000000             0.807692  \n",
       "22             0.828671             0.763986             0.734266  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truths = results[1]\n",
    "predictions = results[2]\n",
    "print(\"ground_truths  = \", np.shape(ground_truths))\n",
    "print(\"predictions = \", np.shape(predictions))\n",
    "df = get_gesture_accuracies(ground_truths, predictions, number_of_classes=number_of_classes, \n",
    "                            m_name=m_name, n_name=n_name, path=save_TSD, algo_name=algo_name,\n",
    "                           index_participant_list_customized=index_participant_list,\n",
    "                           lump_day_at_participant=5)\n",
    "df = pd.read_csv(save_DANN+'/'+algo_name+'.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. SCADANN\n",
    "\n",
    "* `run_SCADANN_training_sessions`: train SCADANN model. The first session uses TSD model_0 wegits; others use DANN weights\n",
    "    * specify `percentage_same_gesture_stable` based on the performance of most pseudo labels: \n",
    "        * print accuracies out and check what percentage will optimize `ACCURACY MODEL` and `ACCURACY PSEUDO` without cutting out too much data \n",
    "    * num_sessions-1 sets of training weights will be saved\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TrainingsAndEvaluations.ForTrainingSessions.train_tsd_dnn_SCADANN import \\\n",
    "    run_SCADANN_training_sessions, test_network_SCADANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET one participant_examples  (5,)\n",
      "   GET one training_index_examples  (24, 572, 252)  at  0\n",
      "   GOT one group XY  (13728, 252)    (13728,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (13728, 252)    (13728,)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  1\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2288, 252)    (2288,)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  2\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2288, 252)    (2288,)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  3\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2288, 252)    (2288,)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  4\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2288, 252)    (2288,)\n",
      "dataloaders: \n",
      "   train  (1, 5)\n",
      "   valid  (0,)\n",
      "   test  (1, 0)\n",
      "participants_train =  1\n",
      "Optimizer =  <generator object Module.parameters at 0x7f63740adba0>\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump6/DANN/participant_0/best_state_1.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump6/DANN/participant_0/best_state_1.pt' (epoch 16)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump6/TSD/participant_0/best_state_0.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump6/TSD/participant_0/best_state_0.pt' (epoch 5)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump6/DANN/participant_0/best_state_1.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump6/DANN/participant_0/best_state_1.pt' (epoch 16)\n",
      "==== models_array =  (2,)  @ session  1\n",
      "HANDLING NEW SESSION  0\n",
      "HANDLING NEW SESSION  1\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.5384615384615384  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  15\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.0  len before:  26   len after:  9\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.0  len before:  26   len after:  5\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.625  len before:  26   len after:  16\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.16666666666666666  len before:  26   len after:  6\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  17\n",
      "ACCURACY MODEL:  0.8548951048951049   Accuracy pseudo: 0.9382948657560056  len pseudo:  2123    len predictions 2288\n",
      "STARTING TRAINING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laiy/.local/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/laiy/.local/lib/python3.8/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy total 0.878005, main loss classifier 0.504287, source accuracy 0.884615 source classification loss 0.351019, target accuracy 0.871394 target loss 0.429698 accuracy domain distinction 0.500000 loss domain distinction 1.139283,\n",
      "VALIDATION Loss: 0.26080588 Acc: 0.89647059\n",
      "New best validation loss:  0.26080588144915445\n",
      "Epoch 2 of 500 took 0.349s\n",
      "Accuracy total 0.882212, main loss classifier 0.462078, source accuracy 0.885817 source classification loss 0.333930, target accuracy 0.878606 target loss 0.377883 accuracy domain distinction 0.500000 loss domain distinction 1.061713,\n",
      "VALIDATION Loss: 0.16428584 Acc: 0.95058824\n",
      "New best validation loss:  0.16428584073271071\n",
      "Epoch 3 of 500 took 0.341s\n",
      "Accuracy total 0.875601, main loss classifier 0.471174, source accuracy 0.877404 source classification loss 0.348966, target accuracy 0.873798 target loss 0.385012 accuracy domain distinction 0.500000 loss domain distinction 1.041851,\n",
      "VALIDATION Loss: 0.17722240 Acc: 0.93176471\n",
      "Epoch 4 of 500 took 0.341s\n",
      "Accuracy total 0.879507, main loss classifier 0.476080, source accuracy 0.871995 source classification loss 0.369018, target accuracy 0.887019 target loss 0.375415 accuracy domain distinction 0.500000 loss domain distinction 1.038631,\n",
      "VALIDATION Loss: 0.15935034 Acc: 0.95058824\n",
      "New best validation loss:  0.15935034304857254\n",
      "Epoch 5 of 500 took 0.347s\n",
      "Accuracy total 0.877103, main loss classifier 0.460329, source accuracy 0.871394 source classification loss 0.366872, target accuracy 0.882812 target loss 0.346364 accuracy domain distinction 0.500000 loss domain distinction 1.037109,\n",
      "VALIDATION Loss: 0.16393835 Acc: 0.94588235\n",
      "Epoch 6 of 500 took 0.338s\n",
      "Accuracy total 0.870493, main loss classifier 0.473317, source accuracy 0.867188 source classification loss 0.360345, target accuracy 0.873798 target loss 0.379034 accuracy domain distinction 0.500000 loss domain distinction 1.036278,\n",
      "VALIDATION Loss: 0.18230580 Acc: 0.94117647\n",
      "Epoch 7 of 500 took 0.339s\n",
      "Accuracy total 0.878606, main loss classifier 0.467475, source accuracy 0.862981 source classification loss 0.382746, target accuracy 0.894231 target loss 0.345236 accuracy domain distinction 0.500000 loss domain distinction 1.034841,\n",
      "VALIDATION Loss: 0.19462739 Acc: 0.92941176\n",
      "Epoch 8 of 500 took 0.336s\n",
      "Accuracy total 0.887921, main loss classifier 0.444402, source accuracy 0.877404 source classification loss 0.350023, target accuracy 0.898438 target loss 0.330828 accuracy domain distinction 0.500000 loss domain distinction 1.039765,\n",
      "VALIDATION Loss: 0.15514509 Acc: 0.96\n",
      "New best validation loss:  0.1551450863480568\n",
      "Epoch 9 of 500 took 0.340s\n",
      "Accuracy total 0.876803, main loss classifier 0.471974, source accuracy 0.868990 source classification loss 0.382650, target accuracy 0.884615 target loss 0.353808 accuracy domain distinction 0.500000 loss domain distinction 1.037446,\n",
      "VALIDATION Loss: 0.16053727 Acc: 0.95058824\n",
      "Epoch 10 of 500 took 0.339s\n",
      "Accuracy total 0.892428, main loss classifier 0.441814, source accuracy 0.893029 source classification loss 0.328366, target accuracy 0.891827 target loss 0.347992 accuracy domain distinction 0.500000 loss domain distinction 1.036351,\n",
      "VALIDATION Loss: 0.15939978 Acc: 0.94588235\n",
      "Epoch 11 of 500 took 0.338s\n",
      "Accuracy total 0.883714, main loss classifier 0.449610, source accuracy 0.875000 source classification loss 0.360635, target accuracy 0.892428 target loss 0.331409 accuracy domain distinction 0.500000 loss domain distinction 1.035876,\n",
      "VALIDATION Loss: 0.24013278 Acc: 0.90588235\n",
      "Epoch 12 of 500 took 0.341s\n",
      "Accuracy total 0.887320, main loss classifier 0.438125, source accuracy 0.880409 source classification loss 0.334695, target accuracy 0.894231 target loss 0.333504 accuracy domain distinction 0.500000 loss domain distinction 1.040256,\n",
      "VALIDATION Loss: 0.16007286 Acc: 0.95294118\n",
      "Epoch 13 of 500 took 0.338s\n",
      "Accuracy total 0.892428, main loss classifier 0.422876, source accuracy 0.890625 source classification loss 0.311133, target accuracy 0.894231 target loss 0.328308 accuracy domain distinction 0.500000 loss domain distinction 1.031552,\n",
      "VALIDATION Loss: 0.15826117 Acc: 0.94352941\n",
      "Epoch 14 of 500 took 0.339s\n",
      "Accuracy total 0.890325, main loss classifier 0.438057, source accuracy 0.880409 source classification loss 0.343809, target accuracy 0.900240 target loss 0.325388 accuracy domain distinction 0.500000 loss domain distinction 1.034580,\n",
      "VALIDATION Loss: 0.16112130 Acc: 0.94352941\n",
      "Epoch    14: reducing learning rate of group 0 to 2.0120e-05.\n",
      "Epoch 15 of 500 took 0.344s\n",
      "Accuracy total 0.893329, main loss classifier 0.434847, source accuracy 0.885817 source classification loss 0.335116, target accuracy 0.900841 target loss 0.327758 accuracy domain distinction 0.500000 loss domain distinction 1.034091,\n",
      "VALIDATION Loss: 0.15241695 Acc: 0.94823529\n",
      "New best validation loss:  0.15241694982562745\n",
      "Epoch 16 of 500 took 0.341s\n",
      "Accuracy total 0.889123, main loss classifier 0.438002, source accuracy 0.882812 source classification loss 0.338958, target accuracy 0.895433 target loss 0.330276 accuracy domain distinction 0.500000 loss domain distinction 1.033850,\n",
      "VALIDATION Loss: 0.19717340 Acc: 0.93411765\n",
      "Epoch 17 of 500 took 0.339s\n",
      "Accuracy total 0.893029, main loss classifier 0.417233, source accuracy 0.884014 source classification loss 0.334007, target accuracy 0.902043 target loss 0.295213 accuracy domain distinction 0.500000 loss domain distinction 1.026230,\n",
      "VALIDATION Loss: 0.14820850 Acc: 0.96\n",
      "New best validation loss:  0.1482085000191416\n",
      "Epoch 18 of 500 took 0.343s\n",
      "Accuracy total 0.893630, main loss classifier 0.423225, source accuracy 0.884014 source classification loss 0.330762, target accuracy 0.903245 target loss 0.308308 accuracy domain distinction 0.500000 loss domain distinction 1.036897,\n",
      "VALIDATION Loss: 0.17342701 Acc: 0.92705882\n",
      "Epoch 19 of 500 took 0.342s\n",
      "Accuracy total 0.893930, main loss classifier 0.423825, source accuracy 0.879808 source classification loss 0.334598, target accuracy 0.908053 target loss 0.306262 accuracy domain distinction 0.500000 loss domain distinction 1.033955,\n",
      "VALIDATION Loss: 0.14319027 Acc: 0.95294118\n",
      "New best validation loss:  0.14319027002368653\n",
      "Epoch 20 of 500 took 0.340s\n",
      "Accuracy total 0.891827, main loss classifier 0.421531, source accuracy 0.885216 source classification loss 0.323780, target accuracy 0.898438 target loss 0.312057 accuracy domain distinction 0.500000 loss domain distinction 1.036125,\n",
      "VALIDATION Loss: 0.15145233 Acc: 0.95058824\n",
      "Epoch 21 of 500 took 0.344s\n",
      "Accuracy total 0.894231, main loss classifier 0.431381, source accuracy 0.892428 source classification loss 0.328234, target accuracy 0.896034 target loss 0.328872 accuracy domain distinction 0.500000 loss domain distinction 1.028282,\n",
      "VALIDATION Loss: 0.13133362 Acc: 0.95529412\n",
      "New best validation loss:  0.13133362361363002\n",
      "Epoch 22 of 500 took 0.341s\n",
      "Accuracy total 0.896334, main loss classifier 0.418528, source accuracy 0.890625 source classification loss 0.323942, target accuracy 0.902043 target loss 0.305373 accuracy domain distinction 0.500000 loss domain distinction 1.038701,\n",
      "VALIDATION Loss: 0.13945975 Acc: 0.95764706\n",
      "Epoch 23 of 500 took 0.338s\n",
      "Accuracy total 0.890024, main loss classifier 0.428726, source accuracy 0.876803 source classification loss 0.349786, target accuracy 0.903245 target loss 0.300455 accuracy domain distinction 0.500000 loss domain distinction 1.036060,\n",
      "VALIDATION Loss: 0.17190640 Acc: 0.93411765\n",
      "Epoch 24 of 500 took 0.342s\n",
      "Accuracy total 0.887320, main loss classifier 0.446385, source accuracy 0.879808 source classification loss 0.361597, target accuracy 0.894832 target loss 0.324947 accuracy domain distinction 0.500000 loss domain distinction 1.031132,\n",
      "VALIDATION Loss: 0.15048285 Acc: 0.94823529\n",
      "Epoch 25 of 500 took 0.340s\n",
      "Accuracy total 0.893630, main loss classifier 0.428311, source accuracy 0.883413 source classification loss 0.336547, target accuracy 0.903846 target loss 0.312478 accuracy domain distinction 0.500000 loss domain distinction 1.037982,\n",
      "VALIDATION Loss: 0.16370113 Acc: 0.94588235\n",
      "Epoch 26 of 500 took 0.341s\n",
      "Accuracy total 0.894531, main loss classifier 0.422087, source accuracy 0.884014 source classification loss 0.331514, target accuracy 0.905048 target loss 0.305964 accuracy domain distinction 0.500000 loss domain distinction 1.033479,\n",
      "VALIDATION Loss: 0.19915284 Acc: 0.93176471\n",
      "Epoch 27 of 500 took 0.342s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy total 0.892428, main loss classifier 0.425112, source accuracy 0.890024 source classification loss 0.328270, target accuracy 0.894832 target loss 0.315026 accuracy domain distinction 0.500000 loss domain distinction 1.034643,\n",
      "VALIDATION Loss: 0.14186172 Acc: 0.96\n",
      "Epoch    27: reducing learning rate of group 0 to 4.0240e-06.\n",
      "Epoch 28 of 500 took 0.338s\n",
      "Accuracy total 0.895433, main loss classifier 0.420448, source accuracy 0.890625 source classification loss 0.313277, target accuracy 0.900240 target loss 0.321137 accuracy domain distinction 0.500000 loss domain distinction 1.032408,\n",
      "VALIDATION Loss: 0.13990670 Acc: 0.95764706\n",
      "Epoch 29 of 500 took 0.339s\n",
      "Accuracy total 0.883413, main loss classifier 0.444636, source accuracy 0.868389 source classification loss 0.359675, target accuracy 0.898438 target loss 0.323224 accuracy domain distinction 0.500000 loss domain distinction 1.031864,\n",
      "VALIDATION Loss: 0.17016440 Acc: 0.93647059\n",
      "Epoch 30 of 500 took 0.345s\n",
      "Accuracy total 0.890925, main loss classifier 0.432107, source accuracy 0.877404 source classification loss 0.354720, target accuracy 0.904447 target loss 0.303587 accuracy domain distinction 0.500000 loss domain distinction 1.029540,\n",
      "VALIDATION Loss: 0.14587952 Acc: 0.96235294\n",
      "Epoch 31 of 500 took 0.339s\n",
      "Accuracy total 0.894531, main loss classifier 0.431027, source accuracy 0.882212 source classification loss 0.349076, target accuracy 0.906851 target loss 0.306342 accuracy domain distinction 0.500000 loss domain distinction 1.033178,\n",
      "VALIDATION Loss: 0.13604164 Acc: 0.96470588\n",
      "Epoch 32 of 500 took 0.357s\n",
      "Accuracy total 0.895433, main loss classifier 0.418227, source accuracy 0.894231 source classification loss 0.310541, target accuracy 0.896635 target loss 0.318983 accuracy domain distinction 0.500000 loss domain distinction 1.034645,\n",
      "VALIDATION Loss: 0.14915822 Acc: 0.94823529\n",
      "Epoch 33 of 500 took 0.359s\n",
      "Training complete in 0m 11s\n",
      "['participant_0']\n",
      "Optimizer =  <generator object Module.parameters at 0x7f636d3a8970>\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump6/DANN/participant_0/best_state_2.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump6/DANN/participant_0/best_state_2.pt' (epoch 19)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump6/TSD/participant_0/best_state_0.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump6/TSD/participant_0/best_state_0.pt' (epoch 5)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump6/DANN/participant_0/best_state_1.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump6/DANN/participant_0/best_state_1.pt' (epoch 16)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump6/DANN/participant_0/best_state_2.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump6/DANN/participant_0/best_state_2.pt' (epoch 19)\n",
      "==== models_array =  (3,)  @ session  2\n",
      "HANDLING NEW SESSION  0\n",
      "HANDLING NEW SESSION  1\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.5384615384615384  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  15\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.0  len before:  26   len after:  9\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.0  len before:  26   len after:  5\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.625  len before:  26   len after:  16\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.16666666666666666  len before:  26   len after:  6\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  17\n",
      "ACCURACY MODEL:  0.8548951048951049   Accuracy pseudo: 0.9382948657560056  len pseudo:  2123    len predictions 2288\n",
      "HANDLING NEW SESSION  2\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.5384615384615384  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  5\n",
      "BEFORE:  0.11538461538461539   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  0.5   AFTER:  0.6923076923076923  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.625  len before:  26   len after:  16\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  14\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  8\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  9\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.8846153846153846  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.7058823529411765  len before:  26   len after:  17\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.8461538461538461  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.46153846153846156  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  2\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.0  len before:  26   len after:  2\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.0  len before:  26   len after:  6\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.8823529411764706  len before:  26   len after:  17\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.0  len before:  26   len after:  16\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.5769230769230769  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.0  len before:  26   len after:  16\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.8846153846153846  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.75  len before:  26   len after:  8\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  9\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.791958041958042   Accuracy pseudo: 0.8935116394254582  len pseudo:  2019    len predictions 2288\n",
      "STARTING TRAINING\n",
      "Accuracy total 0.840313, main loss classifier 0.587588, source accuracy 0.870625 source classification loss 0.363984, target accuracy 0.810000 target loss 0.582802 accuracy domain distinction 0.500000 loss domain distinction 1.141954,\n",
      "VALIDATION Loss: 0.32057236 Acc: 0.89851485\n",
      "New best validation loss:  0.3205723613500595\n",
      "Epoch 2 of 500 took 0.330s\n",
      "Accuracy total 0.870000, main loss classifier 0.506130, source accuracy 0.883750 source classification loss 0.367487, target accuracy 0.856250 target loss 0.432550 accuracy domain distinction 0.500000 loss domain distinction 1.061112,\n",
      "VALIDATION Loss: 0.30016014 Acc: 0.91584158\n",
      "New best validation loss:  0.300160142992224\n",
      "Epoch 3 of 500 took 0.333s\n",
      "Accuracy total 0.865625, main loss classifier 0.496736, source accuracy 0.875625 source classification loss 0.358213, target accuracy 0.855625 target loss 0.426422 accuracy domain distinction 0.500000 loss domain distinction 1.044182,\n",
      "VALIDATION Loss: 0.26054982 Acc: 0.90594059\n",
      "New best validation loss:  0.260549820959568\n",
      "Epoch 4 of 500 took 0.332s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy total 0.865313, main loss classifier 0.496695, source accuracy 0.871875 source classification loss 0.359154, target accuracy 0.858750 target loss 0.426605 accuracy domain distinction 0.500000 loss domain distinction 1.038152,\n",
      "VALIDATION Loss: 0.24997185 Acc: 0.90346535\n",
      "New best validation loss:  0.24997184638466155\n",
      "Epoch 5 of 500 took 0.346s\n",
      "Accuracy total 0.875000, main loss classifier 0.487253, source accuracy 0.889375 source classification loss 0.335889, target accuracy 0.860625 target loss 0.430757 accuracy domain distinction 0.500000 loss domain distinction 1.039304,\n",
      "VALIDATION Loss: 0.25281492 Acc: 0.91336634\n",
      "Epoch 6 of 500 took 0.335s\n",
      "Accuracy total 0.877812, main loss classifier 0.486684, source accuracy 0.878125 source classification loss 0.376380, target accuracy 0.877500 target loss 0.388334 accuracy domain distinction 0.500000 loss domain distinction 1.043271,\n",
      "VALIDATION Loss: 0.24326661 Acc: 0.91584158\n",
      "New best validation loss:  0.24326660909823009\n",
      "Epoch 7 of 500 took 0.330s\n",
      "Accuracy total 0.870938, main loss classifier 0.477220, source accuracy 0.880000 source classification loss 0.344960, target accuracy 0.861875 target loss 0.402979 accuracy domain distinction 0.500000 loss domain distinction 1.032511,\n",
      "VALIDATION Loss: 0.25984350 Acc: 0.91089109\n",
      "Epoch 8 of 500 took 0.330s\n",
      "Accuracy total 0.874375, main loss classifier 0.480236, source accuracy 0.880625 source classification loss 0.349648, target accuracy 0.868125 target loss 0.402812 accuracy domain distinction 0.500000 loss domain distinction 1.040058,\n",
      "VALIDATION Loss: 0.24435033 Acc: 0.92326733\n",
      "Epoch 9 of 500 took 0.332s\n",
      "Accuracy total 0.877812, main loss classifier 0.460003, source accuracy 0.875000 source classification loss 0.350606, target accuracy 0.880625 target loss 0.362334 accuracy domain distinction 0.500000 loss domain distinction 1.035328,\n",
      "VALIDATION Loss: 0.25700873 Acc: 0.91584158\n",
      "Epoch 10 of 500 took 0.332s\n",
      "Accuracy total 0.870938, main loss classifier 0.469699, source accuracy 0.872500 source classification loss 0.372834, target accuracy 0.869375 target loss 0.360309 accuracy domain distinction 0.500000 loss domain distinction 1.031270,\n",
      "VALIDATION Loss: 0.27157337 Acc: 0.91089109\n",
      "Epoch 11 of 500 took 0.329s\n",
      "Accuracy total 0.877812, main loss classifier 0.456230, source accuracy 0.871250 source classification loss 0.354098, target accuracy 0.884375 target loss 0.350470 accuracy domain distinction 0.500000 loss domain distinction 1.039457,\n",
      "VALIDATION Loss: 0.23421507 Acc: 0.92326733\n",
      "New best validation loss:  0.23421506583690643\n",
      "Epoch 12 of 500 took 0.335s\n",
      "Accuracy total 0.878437, main loss classifier 0.450074, source accuracy 0.876875 source classification loss 0.334008, target accuracy 0.880000 target loss 0.359997 accuracy domain distinction 0.500000 loss domain distinction 1.030713,\n",
      "VALIDATION Loss: 0.21021689 Acc: 0.92574257\n",
      "New best validation loss:  0.21021688623087748\n",
      "Epoch 13 of 500 took 0.330s\n",
      "Accuracy total 0.866250, main loss classifier 0.484858, source accuracy 0.864375 source classification loss 0.373095, target accuracy 0.868125 target loss 0.388923 accuracy domain distinction 0.500000 loss domain distinction 1.038494,\n",
      "VALIDATION Loss: 0.23291182 Acc: 0.91336634\n",
      "Epoch 14 of 500 took 0.330s\n",
      "Accuracy total 0.872500, main loss classifier 0.468678, source accuracy 0.873125 source classification loss 0.367887, target accuracy 0.871875 target loss 0.362861 accuracy domain distinction 0.500000 loss domain distinction 1.033041,\n",
      "VALIDATION Loss: 0.25555978 Acc: 0.91089109\n",
      "Epoch 15 of 500 took 0.331s\n",
      "Accuracy total 0.886250, main loss classifier 0.457793, source accuracy 0.887500 source classification loss 0.344680, target accuracy 0.885000 target loss 0.364671 accuracy domain distinction 0.500000 loss domain distinction 1.031172,\n",
      "VALIDATION Loss: 0.21532753 Acc: 0.92821782\n",
      "Epoch 16 of 500 took 0.333s\n",
      "Accuracy total 0.881875, main loss classifier 0.449656, source accuracy 0.881875 source classification loss 0.350436, target accuracy 0.881875 target loss 0.341415 accuracy domain distinction 0.500000 loss domain distinction 1.037305,\n",
      "VALIDATION Loss: 0.24495015 Acc: 0.91089109\n",
      "Epoch 17 of 500 took 0.330s\n",
      "Accuracy total 0.885000, main loss classifier 0.441823, source accuracy 0.880000 source classification loss 0.344474, target accuracy 0.890000 target loss 0.332575 accuracy domain distinction 0.500000 loss domain distinction 1.032989,\n",
      "VALIDATION Loss: 0.29544882 Acc: 0.8960396\n",
      "Epoch 18 of 500 took 0.331s\n",
      "Accuracy total 0.878437, main loss classifier 0.449107, source accuracy 0.873750 source classification loss 0.344218, target accuracy 0.883125 target loss 0.347846 accuracy domain distinction 0.500000 loss domain distinction 1.030751,\n",
      "VALIDATION Loss: 0.25956115 Acc: 0.93316832\n",
      "Epoch    18: reducing learning rate of group 0 to 2.0120e-05.\n",
      "Epoch 19 of 500 took 0.327s\n",
      "Accuracy total 0.882500, main loss classifier 0.437323, source accuracy 0.880000 source classification loss 0.334465, target accuracy 0.885000 target loss 0.334253 accuracy domain distinction 0.500000 loss domain distinction 1.029642,\n",
      "VALIDATION Loss: 0.22560267 Acc: 0.93564356\n",
      "Epoch 20 of 500 took 0.330s\n",
      "Accuracy total 0.889062, main loss classifier 0.443961, source accuracy 0.882500 source classification loss 0.370305, target accuracy 0.895625 target loss 0.311138 accuracy domain distinction 0.500000 loss domain distinction 1.032389,\n",
      "VALIDATION Loss: 0.25634893 Acc: 0.91584158\n",
      "Epoch 21 of 500 took 0.328s\n",
      "Accuracy total 0.891250, main loss classifier 0.430477, source accuracy 0.893750 source classification loss 0.327240, target accuracy 0.888750 target loss 0.327176 accuracy domain distinction 0.500000 loss domain distinction 1.032695,\n",
      "VALIDATION Loss: 0.23408614 Acc: 0.93069307\n",
      "Epoch 22 of 500 took 0.329s\n",
      "Accuracy total 0.886875, main loss classifier 0.451960, source accuracy 0.883750 source classification loss 0.352660, target accuracy 0.890000 target loss 0.345414 accuracy domain distinction 0.500000 loss domain distinction 1.029229,\n",
      "VALIDATION Loss: 0.24744672 Acc: 0.92326733\n",
      "Epoch 23 of 500 took 0.331s\n",
      "Accuracy total 0.882188, main loss classifier 0.440199, source accuracy 0.883750 source classification loss 0.340168, target accuracy 0.880625 target loss 0.333739 accuracy domain distinction 0.500000 loss domain distinction 1.032455,\n",
      "VALIDATION Loss: 0.23218282 Acc: 0.92326733\n",
      "Epoch 24 of 500 took 0.331s\n",
      "Training complete in 0m 8s\n",
      "['participant_0']\n",
      "Optimizer =  <generator object Module.parameters at 0x7f637425cb30>\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump6/DANN/participant_0/best_state_3.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump6/DANN/participant_0/best_state_3.pt' (epoch 13)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump6/TSD/participant_0/best_state_0.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump6/TSD/participant_0/best_state_0.pt' (epoch 5)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump6/DANN/participant_0/best_state_1.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump6/DANN/participant_0/best_state_1.pt' (epoch 16)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump6/DANN/participant_0/best_state_2.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump6/DANN/participant_0/best_state_2.pt' (epoch 19)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump6/DANN/participant_0/best_state_3.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump6/DANN/participant_0/best_state_3.pt' (epoch 13)\n",
      "==== models_array =  (4,)  @ session  3\n",
      "HANDLING NEW SESSION  0\n",
      "HANDLING NEW SESSION  1\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.5384615384615384  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  15\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.0  len before:  26   len after:  9\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.0  len before:  26   len after:  5\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.625  len before:  26   len after:  16\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.16666666666666666  len before:  26   len after:  6\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  17\n",
      "ACCURACY MODEL:  0.8548951048951049   Accuracy pseudo: 0.9382948657560056  len pseudo:  2123    len predictions 2288\n",
      "HANDLING NEW SESSION  2\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.5384615384615384  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  5\n",
      "BEFORE:  0.11538461538461539   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  0.5   AFTER:  0.6923076923076923  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.625  len before:  26   len after:  16\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  14\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  8\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  9\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.8846153846153846  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.7058823529411765  len before:  26   len after:  17\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.8461538461538461  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.46153846153846156  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  2\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.0  len before:  26   len after:  2\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.0  len before:  26   len after:  6\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.8823529411764706  len before:  26   len after:  17\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.0  len before:  26   len after:  16\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.5769230769230769  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.0  len before:  26   len after:  16\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.8846153846153846  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.75  len before:  26   len after:  8\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  9\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.791958041958042   Accuracy pseudo: 0.8935116394254582  len pseudo:  2019    len predictions 2288\n",
      "HANDLING NEW SESSION  3\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.9  len before:  26   len after:  10\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.6923076923076923  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.6923076923076923  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  8\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.8846153846153846  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  2\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.5769230769230769  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  16\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  5\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.0  len before:  26   len after:  18\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.4230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  14\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.5769230769230769  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.7058823529411765  len before:  26   len after:  17\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.625  len before:  26   len after:  16\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.7915209790209791   Accuracy pseudo: 0.8526912181303116  len pseudo:  2118    len predictions 2288\n",
      "STARTING TRAINING\n",
      "Accuracy total 0.844952, main loss classifier 0.567507, source accuracy 0.864784 source classification loss 0.397043, target accuracy 0.825120 target loss 0.511605 accuracy domain distinction 0.500000 loss domain distinction 1.131825,\n",
      "VALIDATION Loss: 0.26522593 Acc: 0.91745283\n",
      "New best validation loss:  0.26522593413080486\n",
      "Epoch 2 of 500 took 0.348s\n",
      "Accuracy total 0.851262, main loss classifier 0.544748, source accuracy 0.853966 source classification loss 0.413701, target accuracy 0.848558 target loss 0.463427 accuracy domain distinction 0.500000 loss domain distinction 1.061838,\n",
      "VALIDATION Loss: 0.22827420 Acc: 0.94339623\n",
      "New best validation loss:  0.22827420383691788\n",
      "Epoch 3 of 500 took 0.349s\n",
      "Accuracy total 0.856971, main loss classifier 0.534521, source accuracy 0.858173 source classification loss 0.398639, target accuracy 0.855769 target loss 0.461635 accuracy domain distinction 0.500000 loss domain distinction 1.043839,\n",
      "VALIDATION Loss: 0.26038097 Acc: 0.92688679\n",
      "Epoch 4 of 500 took 0.343s\n",
      "Accuracy total 0.850962, main loss classifier 0.537907, source accuracy 0.863582 source classification loss 0.407707, target accuracy 0.838341 target loss 0.460324 accuracy domain distinction 0.500000 loss domain distinction 1.038914,\n",
      "VALIDATION Loss: 0.22617540 Acc: 0.92688679\n",
      "New best validation loss:  0.22617539976324355\n",
      "Epoch 5 of 500 took 0.345s\n",
      "Accuracy total 0.851562, main loss classifier 0.525518, source accuracy 0.855168 source classification loss 0.414323, target accuracy 0.847957 target loss 0.429595 accuracy domain distinction 0.500000 loss domain distinction 1.035591,\n",
      "VALIDATION Loss: 0.23308338 Acc: 0.91981132\n",
      "Epoch 6 of 500 took 0.349s\n",
      "Accuracy total 0.868990, main loss classifier 0.496021, source accuracy 0.864183 source classification loss 0.394017, target accuracy 0.873798 target loss 0.390396 accuracy domain distinction 0.500000 loss domain distinction 1.038142,\n",
      "VALIDATION Loss: 0.27512463 Acc: 0.90566038\n",
      "Epoch 7 of 500 took 0.339s\n",
      "Accuracy total 0.865084, main loss classifier 0.503904, source accuracy 0.868389 source classification loss 0.397062, target accuracy 0.861779 target loss 0.404932 accuracy domain distinction 0.500000 loss domain distinction 1.029067,\n",
      "VALIDATION Loss: 0.22401283 Acc: 0.94103774\n",
      "New best validation loss:  0.22401283468518937\n",
      "Epoch 8 of 500 took 0.348s\n",
      "Accuracy total 0.873798, main loss classifier 0.481146, source accuracy 0.866587 source classification loss 0.389020, target accuracy 0.881010 target loss 0.366268 accuracy domain distinction 0.500000 loss domain distinction 1.035020,\n",
      "VALIDATION Loss: 0.19286071 Acc: 0.93632075\n",
      "New best validation loss:  0.19286071402685984\n",
      "Epoch 9 of 500 took 0.342s\n",
      "Accuracy total 0.868089, main loss classifier 0.487004, source accuracy 0.869591 source classification loss 0.377150, target accuracy 0.866587 target loss 0.389974 accuracy domain distinction 0.500000 loss domain distinction 1.034420,\n",
      "VALIDATION Loss: 0.26004723 Acc: 0.90330189\n",
      "Epoch 10 of 500 took 0.343s\n",
      "Accuracy total 0.876502, main loss classifier 0.472946, source accuracy 0.874399 source classification loss 0.381980, target accuracy 0.878606 target loss 0.356386 accuracy domain distinction 0.500000 loss domain distinction 1.037634,\n",
      "VALIDATION Loss: 0.19790064 Acc: 0.94103774\n",
      "Epoch 11 of 500 took 0.343s\n",
      "Accuracy total 0.869591, main loss classifier 0.490478, source accuracy 0.864784 source classification loss 0.407869, target accuracy 0.874399 target loss 0.365579 accuracy domain distinction 0.500000 loss domain distinction 1.037542,\n",
      "VALIDATION Loss: 0.20144680 Acc: 0.94103774\n",
      "Epoch 12 of 500 took 0.341s\n",
      "Accuracy total 0.872596, main loss classifier 0.470683, source accuracy 0.865986 source classification loss 0.375369, target accuracy 0.879207 target loss 0.358152 accuracy domain distinction 0.500000 loss domain distinction 1.039223,\n",
      "VALIDATION Loss: 0.19513104 Acc: 0.94339623\n",
      "Epoch 13 of 500 took 0.341s\n",
      "Accuracy total 0.871695, main loss classifier 0.489452, source accuracy 0.870793 source classification loss 0.393289, target accuracy 0.872596 target loss 0.378217 accuracy domain distinction 0.500000 loss domain distinction 1.036993,\n",
      "VALIDATION Loss: 0.19487834 Acc: 0.93160377\n",
      "Epoch 14 of 500 took 0.344s\n",
      "Accuracy total 0.875300, main loss classifier 0.475762, source accuracy 0.872596 source classification loss 0.387013, target accuracy 0.878005 target loss 0.357730 accuracy domain distinction 0.500000 loss domain distinction 1.033902,\n",
      "VALIDATION Loss: 0.18192771 Acc: 0.94575472\n",
      "New best validation loss:  0.18192770651408605\n",
      "Epoch 15 of 500 took 0.344s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy total 0.877103, main loss classifier 0.476896, source accuracy 0.876202 source classification loss 0.378866, target accuracy 0.878005 target loss 0.367304 accuracy domain distinction 0.500000 loss domain distinction 1.038103,\n",
      "VALIDATION Loss: 0.22005428 Acc: 0.93632075\n",
      "Epoch 16 of 500 took 0.358s\n",
      "Accuracy total 0.871094, main loss classifier 0.471554, source accuracy 0.862380 source classification loss 0.389964, target accuracy 0.879808 target loss 0.347219 accuracy domain distinction 0.500000 loss domain distinction 1.029626,\n",
      "VALIDATION Loss: 0.19215949 Acc: 0.93867925\n",
      "Epoch 17 of 500 took 0.343s\n",
      "Accuracy total 0.880409, main loss classifier 0.456861, source accuracy 0.881010 source classification loss 0.356465, target accuracy 0.879808 target loss 0.350087 accuracy domain distinction 0.500000 loss domain distinction 1.035857,\n",
      "VALIDATION Loss: 0.23780597 Acc: 0.92924528\n",
      "Epoch 18 of 500 took 0.341s\n",
      "Accuracy total 0.877404, main loss classifier 0.467078, source accuracy 0.879808 source classification loss 0.372816, target accuracy 0.875000 target loss 0.355055 accuracy domain distinction 0.500000 loss domain distinction 1.031425,\n",
      "VALIDATION Loss: 0.20787952 Acc: 0.93867925\n",
      "Epoch 19 of 500 took 0.346s\n",
      "Accuracy total 0.871995, main loss classifier 0.482854, source accuracy 0.859976 source classification loss 0.399968, target accuracy 0.884014 target loss 0.359199 accuracy domain distinction 0.500000 loss domain distinction 1.032706,\n",
      "VALIDATION Loss: 0.24951632 Acc: 0.92216981\n",
      "Epoch 20 of 500 took 0.345s\n",
      "Accuracy total 0.880409, main loss classifier 0.450289, source accuracy 0.869591 source classification loss 0.370358, target accuracy 0.891226 target loss 0.322925 accuracy domain distinction 0.500000 loss domain distinction 1.036483,\n",
      "VALIDATION Loss: 0.19076305 Acc: 0.93632075\n",
      "Epoch    20: reducing learning rate of group 0 to 2.0120e-05.\n",
      "Epoch 21 of 500 took 0.345s\n",
      "Accuracy total 0.885517, main loss classifier 0.448723, source accuracy 0.878606 source classification loss 0.369308, target accuracy 0.892428 target loss 0.322052 accuracy domain distinction 0.500000 loss domain distinction 1.030433,\n",
      "VALIDATION Loss: 0.17409414 Acc: 0.93632075\n",
      "New best validation loss:  0.17409414052963257\n",
      "Epoch 22 of 500 took 0.341s\n",
      "Accuracy total 0.890325, main loss classifier 0.436745, source accuracy 0.881010 source classification loss 0.356686, target accuracy 0.899639 target loss 0.311019 accuracy domain distinction 0.500000 loss domain distinction 1.028928,\n",
      "VALIDATION Loss: 0.17211951 Acc: 0.94811321\n",
      "New best validation loss:  0.17211950996092387\n",
      "Epoch 23 of 500 took 0.344s\n",
      "Accuracy total 0.874700, main loss classifier 0.459702, source accuracy 0.860577 source classification loss 0.385699, target accuracy 0.888822 target loss 0.326308 accuracy domain distinction 0.500000 loss domain distinction 1.036984,\n",
      "VALIDATION Loss: 0.22376356 Acc: 0.92924528\n",
      "Epoch 24 of 500 took 0.341s\n",
      "Accuracy total 0.883113, main loss classifier 0.459998, source accuracy 0.874399 source classification loss 0.377889, target accuracy 0.891827 target loss 0.335634 accuracy domain distinction 0.500000 loss domain distinction 1.032368,\n",
      "VALIDATION Loss: 0.25949428 Acc: 0.90801887\n",
      "Epoch 25 of 500 took 0.340s\n",
      "Accuracy total 0.880108, main loss classifier 0.447774, source accuracy 0.871394 source classification loss 0.361010, target accuracy 0.888822 target loss 0.328509 accuracy domain distinction 0.500000 loss domain distinction 1.030141,\n",
      "VALIDATION Loss: 0.19651205 Acc: 0.93632075\n",
      "Epoch 26 of 500 took 0.343s\n",
      "Accuracy total 0.876803, main loss classifier 0.455593, source accuracy 0.862380 source classification loss 0.377684, target accuracy 0.891226 target loss 0.326624 accuracy domain distinction 0.500000 loss domain distinction 1.034389,\n",
      "VALIDATION Loss: 0.20388352 Acc: 0.92924528\n",
      "Epoch 27 of 500 took 0.342s\n",
      "Accuracy total 0.882512, main loss classifier 0.441214, source accuracy 0.873798 source classification loss 0.354476, target accuracy 0.891226 target loss 0.322196 accuracy domain distinction 0.500000 loss domain distinction 1.028785,\n",
      "VALIDATION Loss: 0.23250450 Acc: 0.93396226\n",
      "Epoch 28 of 500 took 0.341s\n",
      "Accuracy total 0.889123, main loss classifier 0.430280, source accuracy 0.886418 source classification loss 0.335315, target accuracy 0.891827 target loss 0.318596 accuracy domain distinction 0.500000 loss domain distinction 1.033243,\n",
      "VALIDATION Loss: 0.22656989 Acc: 0.93160377\n",
      "Epoch    28: reducing learning rate of group 0 to 4.0240e-06.\n",
      "Epoch 29 of 500 took 0.343s\n",
      "Accuracy total 0.885817, main loss classifier 0.434888, source accuracy 0.873197 source classification loss 0.357890, target accuracy 0.898438 target loss 0.305436 accuracy domain distinction 0.500000 loss domain distinction 1.032251,\n",
      "VALIDATION Loss: 0.17143221 Acc: 0.94575472\n",
      "New best validation loss:  0.17143220880201884\n",
      "Epoch 30 of 500 took 0.342s\n",
      "Accuracy total 0.881911, main loss classifier 0.465704, source accuracy 0.879808 source classification loss 0.380442, target accuracy 0.884014 target loss 0.344910 accuracy domain distinction 0.500000 loss domain distinction 1.030277,\n",
      "VALIDATION Loss: 0.26235472 Acc: 0.9245283\n",
      "Epoch 31 of 500 took 0.341s\n",
      "Accuracy total 0.881310, main loss classifier 0.452321, source accuracy 0.879808 source classification loss 0.360578, target accuracy 0.882812 target loss 0.337286 accuracy domain distinction 0.500000 loss domain distinction 1.033893,\n",
      "VALIDATION Loss: 0.22535435 Acc: 0.9245283\n",
      "Epoch 32 of 500 took 0.344s\n",
      "Accuracy total 0.880709, main loss classifier 0.464188, source accuracy 0.862981 source classification loss 0.408533, target accuracy 0.898438 target loss 0.313391 accuracy domain distinction 0.500000 loss domain distinction 1.032264,\n",
      "VALIDATION Loss: 0.21186460 Acc: 0.93867925\n",
      "Epoch 33 of 500 took 0.345s\n",
      "Accuracy total 0.883714, main loss classifier 0.449609, source accuracy 0.875000 source classification loss 0.368557, target accuracy 0.892428 target loss 0.324407 accuracy domain distinction 0.500000 loss domain distinction 1.031271,\n",
      "VALIDATION Loss: 0.16406058 Acc: 0.95990566\n",
      "New best validation loss:  0.16406057881457464\n",
      "Epoch 34 of 500 took 0.344s\n",
      "Accuracy total 0.885216, main loss classifier 0.451739, source accuracy 0.878606 source classification loss 0.382816, target accuracy 0.891827 target loss 0.314685 accuracy domain distinction 0.500000 loss domain distinction 1.029884,\n",
      "VALIDATION Loss: 0.16507166 Acc: 0.94575472\n",
      "Epoch 35 of 500 took 0.349s\n",
      "Accuracy total 0.879507, main loss classifier 0.451096, source accuracy 0.867188 source classification loss 0.370173, target accuracy 0.891827 target loss 0.326136 accuracy domain distinction 0.500000 loss domain distinction 1.029416,\n",
      "VALIDATION Loss: 0.24462129 Acc: 0.93396226\n",
      "Epoch 36 of 500 took 0.340s\n",
      "Accuracy total 0.886719, main loss classifier 0.441167, source accuracy 0.861779 source classification loss 0.392890, target accuracy 0.911659 target loss 0.282670 accuracy domain distinction 0.500000 loss domain distinction 1.033866,\n",
      "VALIDATION Loss: 0.20944811 Acc: 0.93160377\n",
      "Epoch 37 of 500 took 0.342s\n",
      "Accuracy total 0.885517, main loss classifier 0.438317, source accuracy 0.884014 source classification loss 0.355107, target accuracy 0.887019 target loss 0.314191 accuracy domain distinction 0.500000 loss domain distinction 1.036678,\n",
      "VALIDATION Loss: 0.20854009 Acc: 0.93867925\n",
      "Epoch 38 of 500 took 0.342s\n",
      "Accuracy total 0.887921, main loss classifier 0.445609, source accuracy 0.871995 source classification loss 0.372978, target accuracy 0.903846 target loss 0.313006 accuracy domain distinction 0.500000 loss domain distinction 1.026175,\n",
      "VALIDATION Loss: 0.16737821 Acc: 0.9504717\n",
      "Epoch 39 of 500 took 0.340s\n",
      "Accuracy total 0.886418, main loss classifier 0.449863, source accuracy 0.882812 source classification loss 0.357670, target accuracy 0.890024 target loss 0.336336 accuracy domain distinction 0.500000 loss domain distinction 1.028600,\n",
      "VALIDATION Loss: 0.21272052 Acc: 0.94103774\n",
      "Epoch    39: reducing learning rate of group 0 to 8.0480e-07.\n",
      "Epoch 40 of 500 took 0.341s\n",
      "Accuracy total 0.879207, main loss classifier 0.467390, source accuracy 0.866587 source classification loss 0.407669, target accuracy 0.891827 target loss 0.320828 accuracy domain distinction 0.500000 loss domain distinction 1.031410,\n",
      "VALIDATION Loss: 0.22308457 Acc: 0.9245283\n",
      "Epoch 41 of 500 took 0.342s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy total 0.891526, main loss classifier 0.452834, source accuracy 0.870793 source classification loss 0.400436, target accuracy 0.912260 target loss 0.300010 accuracy domain distinction 0.500000 loss domain distinction 1.026105,\n",
      "VALIDATION Loss: 0.23849113 Acc: 0.92688679\n",
      "Epoch 42 of 500 took 0.340s\n",
      "Accuracy total 0.885216, main loss classifier 0.445521, source accuracy 0.873798 source classification loss 0.371303, target accuracy 0.896635 target loss 0.314263 accuracy domain distinction 0.500000 loss domain distinction 1.027379,\n",
      "VALIDATION Loss: 0.19748313 Acc: 0.94103774\n",
      "Epoch 43 of 500 took 0.342s\n",
      "Accuracy total 0.878005, main loss classifier 0.452170, source accuracy 0.874399 source classification loss 0.366789, target accuracy 0.881611 target loss 0.330763 accuracy domain distinction 0.500000 loss domain distinction 1.033943,\n",
      "VALIDATION Loss: 0.23130191 Acc: 0.92216981\n",
      "Epoch 44 of 500 took 0.344s\n",
      "Accuracy total 0.876202, main loss classifier 0.473516, source accuracy 0.852163 source classification loss 0.415799, target accuracy 0.900240 target loss 0.324630 accuracy domain distinction 0.500000 loss domain distinction 1.033011,\n",
      "VALIDATION Loss: 0.25298615 Acc: 0.92688679\n",
      "Epoch 45 of 500 took 0.340s\n",
      "Training complete in 0m 15s\n",
      "['participant_0']\n",
      "Optimizer =  <generator object Module.parameters at 0x7f636d3a8970>\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump6/DANN/participant_0/best_state_4.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump6/DANN/participant_0/best_state_4.pt' (epoch 26)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump6/TSD/participant_0/best_state_0.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump6/TSD/participant_0/best_state_0.pt' (epoch 5)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump6/DANN/participant_0/best_state_1.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump6/DANN/participant_0/best_state_1.pt' (epoch 16)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump6/DANN/participant_0/best_state_2.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump6/DANN/participant_0/best_state_2.pt' (epoch 19)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump6/DANN/participant_0/best_state_3.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump6/DANN/participant_0/best_state_3.pt' (epoch 13)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump6/DANN/participant_0/best_state_4.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump6/DANN/participant_0/best_state_4.pt' (epoch 26)\n",
      "==== models_array =  (5,)  @ session  4\n",
      "HANDLING NEW SESSION  0\n",
      "HANDLING NEW SESSION  1\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.5384615384615384  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  15\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.0  len before:  26   len after:  9\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.0  len before:  26   len after:  5\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.625  len before:  26   len after:  16\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.16666666666666666  len before:  26   len after:  6\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  17\n",
      "ACCURACY MODEL:  0.8548951048951049   Accuracy pseudo: 0.9382948657560056  len pseudo:  2123    len predictions 2288\n",
      "HANDLING NEW SESSION  2\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.5384615384615384  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  5\n",
      "BEFORE:  0.11538461538461539   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  0.5   AFTER:  0.6923076923076923  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.625  len before:  26   len after:  16\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  14\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  8\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  9\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.8846153846153846  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.7058823529411765  len before:  26   len after:  17\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.8461538461538461  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.46153846153846156  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  2\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.0  len before:  26   len after:  2\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.0  len before:  26   len after:  6\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.8823529411764706  len before:  26   len after:  17\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.0  len before:  26   len after:  16\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.5769230769230769  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.0  len before:  26   len after:  16\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.8846153846153846  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.75  len before:  26   len after:  8\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  9\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.791958041958042   Accuracy pseudo: 0.8935116394254582  len pseudo:  2019    len predictions 2288\n",
      "HANDLING NEW SESSION  3\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.9  len before:  26   len after:  10\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.6923076923076923  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.6923076923076923  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  8\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.8846153846153846  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  2\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.5769230769230769  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  16\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  5\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.0  len before:  26   len after:  18\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.4230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  14\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.5769230769230769  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.7058823529411765  len before:  26   len after:  17\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.625  len before:  26   len after:  16\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.7915209790209791   Accuracy pseudo: 0.8526912181303116  len pseudo:  2118    len predictions 2288\n",
      "HANDLING NEW SESSION  4\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.8461538461538461  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.0  len before:  26   len after:  8\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  15\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  9\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  14\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.0  len before:  26   len after:  10\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  15\n",
      "BEFORE:  0.6538461538461539   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.11538461538461539   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  6\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  13\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  10\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.0  len before:  26   len after:  4\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  8\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.6666666666666666  len before:  26   len after:  6\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.8461538461538461  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  17\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  9\n",
      "BEFORE:  0.8076923076923077   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.7661713286713286   Accuracy pseudo: 0.8947368421052632  len pseudo:  1938    len predictions 2288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING TRAINING\n",
      "Accuracy total 0.839193, main loss classifier 0.586925, source accuracy 0.857422 source classification loss 0.419775, target accuracy 0.820964 target loss 0.525727 accuracy domain distinction 0.500000 loss domain distinction 1.141739,\n",
      "VALIDATION Loss: 0.35782465 Acc: 0.87113402\n",
      "New best validation loss:  0.3578246533870697\n",
      "Epoch 2 of 500 took 0.317s\n",
      "Accuracy total 0.845703, main loss classifier 0.556554, source accuracy 0.850911 source classification loss 0.430677, target accuracy 0.840495 target loss 0.470632 accuracy domain distinction 0.500000 loss domain distinction 1.058996,\n",
      "VALIDATION Loss: 0.30730999 Acc: 0.88659794\n",
      "New best validation loss:  0.30730998516082764\n",
      "Epoch 3 of 500 took 0.318s\n",
      "Accuracy total 0.847982, main loss classifier 0.542824, source accuracy 0.847656 source classification loss 0.438468, target accuracy 0.848307 target loss 0.438210 accuracy domain distinction 0.500000 loss domain distinction 1.044852,\n",
      "VALIDATION Loss: 0.22940833 Acc: 0.91237113\n",
      "New best validation loss:  0.22940833121538162\n",
      "Epoch 4 of 500 took 0.332s\n",
      "Accuracy total 0.852865, main loss classifier 0.560227, source accuracy 0.843750 source classification loss 0.491655, target accuracy 0.861979 target loss 0.419961 accuracy domain distinction 0.500000 loss domain distinction 1.044190,\n",
      "VALIDATION Loss: 0.25506580 Acc: 0.92525773\n",
      "Epoch 5 of 500 took 0.317s\n",
      "Accuracy total 0.855794, main loss classifier 0.509628, source accuracy 0.860026 source classification loss 0.394303, target accuracy 0.851562 target loss 0.416924 accuracy domain distinction 0.500000 loss domain distinction 1.040142,\n",
      "VALIDATION Loss: 0.25489297 Acc: 0.92783505\n",
      "Epoch 6 of 500 took 0.318s\n",
      "Accuracy total 0.868815, main loss classifier 0.488101, source accuracy 0.882161 source classification loss 0.368653, target accuracy 0.855469 target loss 0.400896 accuracy domain distinction 0.500000 loss domain distinction 1.033267,\n",
      "VALIDATION Loss: 0.21358005 Acc: 0.92268041\n",
      "New best validation loss:  0.2135800485100065\n",
      "Epoch 7 of 500 took 0.322s\n",
      "Accuracy total 0.869792, main loss classifier 0.501026, source accuracy 0.871745 source classification loss 0.387865, target accuracy 0.867839 target loss 0.405859 accuracy domain distinction 0.500000 loss domain distinction 1.041640,\n",
      "VALIDATION Loss: 0.30969695 Acc: 0.86340206\n",
      "Epoch 8 of 500 took 0.316s\n",
      "Accuracy total 0.868490, main loss classifier 0.493188, source accuracy 0.869792 source classification loss 0.397047, target accuracy 0.867188 target loss 0.382387 accuracy domain distinction 0.500000 loss domain distinction 1.034709,\n",
      "VALIDATION Loss: 0.29846320 Acc: 0.93041237\n",
      "Epoch 9 of 500 took 0.317s\n",
      "Accuracy total 0.875651, main loss classifier 0.478808, source accuracy 0.874349 source classification loss 0.382391, target accuracy 0.876953 target loss 0.367674 accuracy domain distinction 0.500000 loss domain distinction 1.037750,\n",
      "VALIDATION Loss: 0.21916753 Acc: 0.92010309\n",
      "Epoch 10 of 500 took 0.318s\n",
      "Accuracy total 0.866211, main loss classifier 0.500880, source accuracy 0.856120 source classification loss 0.425208, target accuracy 0.876302 target loss 0.369753 accuracy domain distinction 0.500000 loss domain distinction 1.033993,\n",
      "VALIDATION Loss: 0.25153883 Acc: 0.92525773\n",
      "Epoch 11 of 500 took 0.317s\n",
      "Accuracy total 0.873698, main loss classifier 0.471970, source accuracy 0.875000 source classification loss 0.374950, target accuracy 0.872396 target loss 0.361956 accuracy domain distinction 0.500000 loss domain distinction 1.035173,\n",
      "VALIDATION Loss: 0.30354875 Acc: 0.88917526\n",
      "Epoch 12 of 500 took 0.318s\n",
      "Accuracy total 0.880859, main loss classifier 0.467071, source accuracy 0.882161 source classification loss 0.373235, target accuracy 0.879557 target loss 0.353577 accuracy domain distinction 0.500000 loss domain distinction 1.036645,\n",
      "VALIDATION Loss: 0.18137611 Acc: 0.93298969\n",
      "New best validation loss:  0.18137611235891068\n",
      "Epoch 13 of 500 took 0.319s\n",
      "Accuracy total 0.871094, main loss classifier 0.477292, source accuracy 0.868490 source classification loss 0.385493, target accuracy 0.873698 target loss 0.361853 accuracy domain distinction 0.500000 loss domain distinction 1.036188,\n",
      "VALIDATION Loss: 0.23998169 Acc: 0.92783505\n",
      "Epoch 14 of 500 took 0.321s\n",
      "Accuracy total 0.874349, main loss classifier 0.473173, source accuracy 0.868490 source classification loss 0.378003, target accuracy 0.880208 target loss 0.361246 accuracy domain distinction 0.500000 loss domain distinction 1.035489,\n",
      "VALIDATION Loss: 0.16469930 Acc: 0.94072165\n",
      "New best validation loss:  0.16469930431672505\n",
      "Epoch 15 of 500 took 0.319s\n",
      "Accuracy total 0.875977, main loss classifier 0.452587, source accuracy 0.865885 source classification loss 0.366705, target accuracy 0.886068 target loss 0.332264 accuracy domain distinction 0.500000 loss domain distinction 1.031030,\n",
      "VALIDATION Loss: 0.17972533 Acc: 0.92525773\n",
      "Epoch 16 of 500 took 0.317s\n",
      "Accuracy total 0.874349, main loss classifier 0.473259, source accuracy 0.863932 source classification loss 0.404465, target accuracy 0.884766 target loss 0.334411 accuracy domain distinction 0.500000 loss domain distinction 1.038209,\n",
      "VALIDATION Loss: 0.20332395 Acc: 0.93814433\n",
      "Epoch 17 of 500 took 0.321s\n",
      "Accuracy total 0.875651, main loss classifier 0.463369, source accuracy 0.867188 source classification loss 0.378974, target accuracy 0.884115 target loss 0.340698 accuracy domain distinction 0.500000 loss domain distinction 1.035330,\n",
      "VALIDATION Loss: 0.20020407 Acc: 0.92268041\n",
      "Epoch 18 of 500 took 0.318s\n",
      "Accuracy total 0.874349, main loss classifier 0.481705, source accuracy 0.867839 source classification loss 0.416942, target accuracy 0.880859 target loss 0.339668 accuracy domain distinction 0.500000 loss domain distinction 1.033996,\n",
      "VALIDATION Loss: 0.17299483 Acc: 0.93556701\n",
      "Epoch 19 of 500 took 0.318s\n",
      "Accuracy total 0.877930, main loss classifier 0.449683, source accuracy 0.866536 source classification loss 0.383033, target accuracy 0.889323 target loss 0.309733 accuracy domain distinction 0.500000 loss domain distinction 1.033001,\n",
      "VALIDATION Loss: 0.18500318 Acc: 0.93041237\n",
      "Epoch 20 of 500 took 0.327s\n",
      "Accuracy total 0.878906, main loss classifier 0.456979, source accuracy 0.868490 source classification loss 0.365235, target accuracy 0.889323 target loss 0.342061 accuracy domain distinction 0.500000 loss domain distinction 1.033313,\n",
      "VALIDATION Loss: 0.18869332 Acc: 0.92010309\n",
      "Epoch    20: reducing learning rate of group 0 to 2.0120e-05.\n",
      "Epoch 21 of 500 took 0.321s\n",
      "Accuracy total 0.883789, main loss classifier 0.452607, source accuracy 0.870443 source classification loss 0.376441, target accuracy 0.897135 target loss 0.320866 accuracy domain distinction 0.500000 loss domain distinction 1.039531,\n",
      "VALIDATION Loss: 0.23591569 Acc: 0.92525773\n",
      "Epoch 22 of 500 took 0.320s\n",
      "Accuracy total 0.887695, main loss classifier 0.445875, source accuracy 0.879557 source classification loss 0.375304, target accuracy 0.895833 target loss 0.309561 accuracy domain distinction 0.500000 loss domain distinction 1.034419,\n",
      "VALIDATION Loss: 0.15254016 Acc: 0.94072165\n",
      "New best validation loss:  0.1525401630039726\n",
      "Epoch 23 of 500 took 0.350s\n",
      "Accuracy total 0.880859, main loss classifier 0.447841, source accuracy 0.865234 source classification loss 0.369326, target accuracy 0.896484 target loss 0.319512 accuracy domain distinction 0.500000 loss domain distinction 1.034217,\n",
      "VALIDATION Loss: 0.20278645 Acc: 0.92268041\n",
      "Epoch 24 of 500 took 0.318s\n",
      "Accuracy total 0.882161, main loss classifier 0.453332, source accuracy 0.860677 source classification loss 0.396026, target accuracy 0.903646 target loss 0.305047 accuracy domain distinction 0.500000 loss domain distinction 1.027950,\n",
      "VALIDATION Loss: 0.16179824 Acc: 0.93041237\n",
      "Epoch 25 of 500 took 0.316s\n",
      "Accuracy total 0.890951, main loss classifier 0.438155, source accuracy 0.884115 source classification loss 0.345292, target accuracy 0.897786 target loss 0.323769 accuracy domain distinction 0.500000 loss domain distinction 1.036242,\n",
      "VALIDATION Loss: 0.20737171 Acc: 0.93814433\n",
      "Epoch 26 of 500 took 0.322s\n",
      "Accuracy total 0.879557, main loss classifier 0.458408, source accuracy 0.875000 source classification loss 0.372133, target accuracy 0.884115 target loss 0.338102 accuracy domain distinction 0.500000 loss domain distinction 1.032907,\n",
      "VALIDATION Loss: 0.15563849 Acc: 0.93298969\n",
      "Epoch 27 of 500 took 0.320s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy total 0.880859, main loss classifier 0.451632, source accuracy 0.873698 source classification loss 0.374059, target accuracy 0.888021 target loss 0.321589 accuracy domain distinction 0.500000 loss domain distinction 1.038076,\n",
      "VALIDATION Loss: 0.16878182 Acc: 0.92525773\n",
      "Epoch 28 of 500 took 0.316s\n",
      "Accuracy total 0.877930, main loss classifier 0.442462, source accuracy 0.859375 source classification loss 0.374210, target accuracy 0.896484 target loss 0.304494 accuracy domain distinction 0.500000 loss domain distinction 1.031098,\n",
      "VALIDATION Loss: 0.14830086 Acc: 0.94845361\n",
      "New best validation loss:  0.14830085635185242\n",
      "Epoch 29 of 500 took 0.332s\n",
      "Accuracy total 0.879232, main loss classifier 0.450789, source accuracy 0.876302 source classification loss 0.366519, target accuracy 0.882161 target loss 0.327800 accuracy domain distinction 0.500000 loss domain distinction 1.036294,\n",
      "VALIDATION Loss: 0.15994987 Acc: 0.94072165\n",
      "Epoch 30 of 500 took 0.329s\n",
      "Accuracy total 0.887044, main loss classifier 0.449626, source accuracy 0.871094 source classification loss 0.382810, target accuracy 0.902995 target loss 0.308839 accuracy domain distinction 0.500000 loss domain distinction 1.038017,\n",
      "VALIDATION Loss: 0.28853217 Acc: 0.88659794\n",
      "Epoch 31 of 500 took 0.317s\n",
      "Accuracy total 0.877604, main loss classifier 0.460561, source accuracy 0.877604 source classification loss 0.357424, target accuracy 0.877604 target loss 0.357295 accuracy domain distinction 0.500000 loss domain distinction 1.032021,\n",
      "VALIDATION Loss: 0.18933938 Acc: 0.92525773\n",
      "Epoch 32 of 500 took 0.322s\n",
      "Accuracy total 0.884115, main loss classifier 0.450952, source accuracy 0.875651 source classification loss 0.368901, target accuracy 0.892578 target loss 0.326508 accuracy domain distinction 0.500000 loss domain distinction 1.032473,\n",
      "VALIDATION Loss: 0.25036037 Acc: 0.93814433\n",
      "Epoch 33 of 500 took 0.320s\n",
      "Accuracy total 0.879232, main loss classifier 0.452785, source accuracy 0.873698 source classification loss 0.364398, target accuracy 0.884766 target loss 0.335074 accuracy domain distinction 0.500000 loss domain distinction 1.030489,\n",
      "VALIDATION Loss: 0.25304804 Acc: 0.94329897\n",
      "Epoch 34 of 500 took 0.320s\n",
      "Accuracy total 0.882487, main loss classifier 0.458730, source accuracy 0.865885 source classification loss 0.399138, target accuracy 0.899089 target loss 0.312254 accuracy domain distinction 0.500000 loss domain distinction 1.030335,\n",
      "VALIDATION Loss: 0.17475779 Acc: 0.93814433\n",
      "Epoch    34: reducing learning rate of group 0 to 4.0240e-06.\n",
      "Epoch 35 of 500 took 0.323s\n",
      "Accuracy total 0.879883, main loss classifier 0.439136, source accuracy 0.872396 source classification loss 0.360653, target accuracy 0.887370 target loss 0.311372 accuracy domain distinction 0.500000 loss domain distinction 1.031236,\n",
      "VALIDATION Loss: 0.15496844 Acc: 0.92783505\n",
      "Epoch 36 of 500 took 0.322s\n",
      "Accuracy total 0.878255, main loss classifier 0.452090, source accuracy 0.871745 source classification loss 0.367324, target accuracy 0.884766 target loss 0.329813 accuracy domain distinction 0.500000 loss domain distinction 1.035217,\n",
      "VALIDATION Loss: 0.16495926 Acc: 0.93556701\n",
      "Epoch 37 of 500 took 0.318s\n",
      "Accuracy total 0.881836, main loss classifier 0.460788, source accuracy 0.876302 source classification loss 0.386776, target accuracy 0.887370 target loss 0.328992 accuracy domain distinction 0.500000 loss domain distinction 1.029036,\n",
      "VALIDATION Loss: 0.22975579 Acc: 0.95103093\n",
      "Epoch 38 of 500 took 0.318s\n",
      "Accuracy total 0.881510, main loss classifier 0.460289, source accuracy 0.869141 source classification loss 0.389882, target accuracy 0.893880 target loss 0.324199 accuracy domain distinction 0.500000 loss domain distinction 1.032485,\n",
      "VALIDATION Loss: 0.40057135 Acc: 0.90206186\n",
      "Epoch 39 of 500 took 0.322s\n",
      "Accuracy total 0.889323, main loss classifier 0.444753, source accuracy 0.881510 source classification loss 0.376981, target accuracy 0.897135 target loss 0.306365 accuracy domain distinction 0.500000 loss domain distinction 1.030795,\n",
      "VALIDATION Loss: 0.25254191 Acc: 0.90206186\n",
      "Epoch 40 of 500 took 0.316s\n",
      "Training complete in 0m 13s\n",
      "['participant_0']\n"
     ]
    }
   ],
   "source": [
    "percentage_same_gesture_stable = 0.75 \n",
    "run_SCADANN_training_sessions(examples_datasets=examples_datasets_train, labels_datasets=labels_datasets_train,\n",
    "                              num_kernels=num_kernels, feature_vector_input_length=feature_vector_input_length,\n",
    "                              path_weights_to_save_to=path_SCADANN,\n",
    "                              path_weights_Adversarial_training=path_DANN,\n",
    "                              path_weights_Normal_training=path_TSD,\n",
    "                              number_of_cycles_total = number_of_cycles_total, \n",
    "                              number_of_cycle_for_first_training = number_of_cycle_for_first_training,\n",
    "                              number_of_classes=number_of_classes,\n",
    "                              learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET one participant_examples  (5,)\n",
      "   GET one training_index_examples  (24, 572, 252)  at  0\n",
      "   GOT one group XY  (13728, 252)    (13728,)\n",
      "       one group XY test  (3432, 252)    (3432, 252)\n",
      "       one group XY train (12355, 252)    (12355,)\n",
      "       one group XY valid (1373, 252)    (1373, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  1\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  2\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  3\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  4\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "dataloaders: \n",
      "   train  (1, 5)\n",
      "   valid  (1, 5)\n",
      "   test  (1, 5)\n",
      "Participant:  0  Accuracy:  0.8875291375291375\n",
      "Participant:  0  Accuracy:  0.8793706293706294\n",
      "Participant:  0  Accuracy:  0.8479020979020979\n",
      "Participant:  0  Accuracy:  0.7937062937062938\n",
      "Participant:  0  Accuracy:  0.8006993006993007\n",
      "ACCURACY PARTICIPANT:  [0.8875291375291375, 0.8793706293706294, 0.8479020979020979, 0.7937062937062938, 0.8006993006993007]\n",
      "[[0.88752914 0.87937063 0.8479021  0.79370629 0.8006993 ]]\n",
      "[array([0.88752914, 0.87937063, 0.8479021 , 0.79370629, 0.8006993 ])]\n",
      "OVERALL ACCURACY: 0.8418414918414918\n"
     ]
    }
   ],
   "source": [
    "algo_name = \"SCADANN\"\n",
    "test_network_SCADANN(examples_datasets_train=examples_datasets_train, labels_datasets_train=labels_datasets_train,\n",
    "                     num_neurons=num_kernels, feature_vector_input_length=feature_vector_input_length,\n",
    "                     path_weights_SCADANN =path_SCADANN, path_weights_normal=path_TSD,\n",
    "                     algo_name=algo_name, cycle_test=3, number_of_cycles_total=number_of_cycles_total,\n",
    "                     number_of_cycle_for_first_training = number_of_cycle_for_first_training,\n",
    "                     number_of_classes=number_of_classes, save_path = save_SCADANN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Day_0~5</th>\n",
       "      <td>0.887529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_6</th>\n",
       "      <td>0.879371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_7</th>\n",
       "      <td>0.847902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_8</th>\n",
       "      <td>0.793706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_9</th>\n",
       "      <td>0.800699</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Participant_5\n",
       "Day_0~5      0.887529\n",
       "Day_6        0.879371\n",
       "Day_7        0.847902\n",
       "Day_8        0.793706\n",
       "Day_9        0.800699"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_filename = save_SCADANN + '/predictions_' + algo_name + \".npy\"\n",
    "results = np.load(results_filename, allow_pickle=True)\n",
    "SCADANN_acc = results[0]\n",
    "SCADANN_acc_overall = np.mean(SCADANN_acc)\n",
    "SCADANN_df = pd.DataFrame(SCADANN_acc.transpose(), \n",
    "                       index = [f'Day_{i}' for i in index_participant_list],\n",
    "                        columns = ['Participant_5'])\n",
    "SCADANN_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "SCADANN_df.transpose().plot.bar(rot=0, figsize=(10,5))\n",
    "plt.title(\"SCADANN Accuracies\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ground_truths  =  (1, 5)\n",
      "predictions =  (1, 5)\n",
      "index_participant_list  ['0~5', 6, 7, 8, 9]\n",
      "accuracies_gestures =  (22, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Loc1_Sub5_Day0~5-&gt;0~5</th>\n",
       "      <th>Loc1_Sub5_Day0~5-&gt;6</th>\n",
       "      <th>Loc1_Sub5_Day0~5-&gt;7</th>\n",
       "      <th>Loc1_Sub5_Day0~5-&gt;8</th>\n",
       "      <th>Loc1_Sub5_Day0~5-&gt;9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M1</td>\n",
       "      <td>0.955128</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.576923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M2</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M3</td>\n",
       "      <td>0.942308</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.384615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M4</td>\n",
       "      <td>0.852564</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>M5</td>\n",
       "      <td>0.993590</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.961538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>M6</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>M7</td>\n",
       "      <td>0.980769</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>M8</td>\n",
       "      <td>0.935897</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>M9</td>\n",
       "      <td>0.935897</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.692308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>M10</td>\n",
       "      <td>0.852564</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>M11</td>\n",
       "      <td>0.756410</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>M12</td>\n",
       "      <td>0.724359</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.230769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>M13</td>\n",
       "      <td>0.737179</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.961538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>M14</td>\n",
       "      <td>0.673077</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.269231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>M15</td>\n",
       "      <td>0.782051</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.807692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>M16</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>M17</td>\n",
       "      <td>0.993590</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>M18</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>M19</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>M20</td>\n",
       "      <td>0.852564</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.884615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>M21</td>\n",
       "      <td>0.814103</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Mean</td>\n",
       "      <td>0.887529</td>\n",
       "      <td>0.879371</td>\n",
       "      <td>0.847902</td>\n",
       "      <td>0.793706</td>\n",
       "      <td>0.800699</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Loc1_Sub5_Day0~5->0~5  Loc1_Sub5_Day0~5->6  \\\n",
       "0          M0               1.000000             1.000000   \n",
       "1          M1               0.955128             0.846154   \n",
       "2          M2               0.846154             0.730769   \n",
       "3          M3               0.942308             0.923077   \n",
       "4          M4               0.852564             0.307692   \n",
       "5          M5               0.993590             1.000000   \n",
       "6          M6               0.961538             0.692308   \n",
       "7          M7               0.980769             1.000000   \n",
       "8          M8               0.935897             0.923077   \n",
       "9          M9               0.935897             0.923077   \n",
       "10        M10               0.852564             1.000000   \n",
       "11        M11               0.756410             0.961538   \n",
       "12        M12               0.724359             0.576923   \n",
       "13        M13               0.737179             0.961538   \n",
       "14        M14               0.673077             0.846154   \n",
       "15        M15               0.782051             0.769231   \n",
       "16        M16               0.961538             1.000000   \n",
       "17        M17               0.993590             1.000000   \n",
       "18        M18               0.974359             1.000000   \n",
       "19        M19               1.000000             1.000000   \n",
       "20        M20               0.852564             0.884615   \n",
       "21        M21               0.814103             1.000000   \n",
       "22       Mean               0.887529             0.879371   \n",
       "\n",
       "    Loc1_Sub5_Day0~5->7  Loc1_Sub5_Day0~5->8  Loc1_Sub5_Day0~5->9  \n",
       "0              1.000000             1.000000             1.000000  \n",
       "1              0.615385             0.423077             0.576923  \n",
       "2              0.846154             1.000000             0.923077  \n",
       "3              1.000000             1.000000             0.384615  \n",
       "4              0.115385             0.230769             0.000000  \n",
       "5              0.884615             1.000000             0.961538  \n",
       "6              1.000000             0.653846             1.000000  \n",
       "7              1.000000             1.000000             1.000000  \n",
       "8              0.461538             0.923077             0.923077  \n",
       "9              0.807692             0.923077             0.692308  \n",
       "10             0.846154             1.000000             1.000000  \n",
       "11             1.000000             1.000000             1.000000  \n",
       "12             0.769231             0.769231             0.230769  \n",
       "13             1.000000             1.000000             0.961538  \n",
       "14             0.846154             0.000000             0.269231  \n",
       "15             0.576923             0.538462             0.807692  \n",
       "16             1.000000             1.000000             1.000000  \n",
       "17             1.000000             1.000000             1.000000  \n",
       "18             1.000000             0.269231             1.000000  \n",
       "19             1.000000             1.000000             1.000000  \n",
       "20             0.884615             0.730769             0.884615  \n",
       "21             1.000000             1.000000             1.000000  \n",
       "22             0.847902             0.793706             0.800699  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truths = results[1]\n",
    "predictions = results[2]\n",
    "print(\"ground_truths  = \", np.shape(ground_truths))\n",
    "print(\"predictions = \", np.shape(predictions))\n",
    "df = get_gesture_accuracies(ground_truths, predictions, number_of_classes=number_of_classes, \n",
    "                            m_name=m_name, n_name=n_name, path=save_TSD, algo_name=algo_name,\n",
    "                           index_participant_list_customized=index_participant_list,\n",
    "                           lump_day_at_participant=5)\n",
    "df = pd.read_csv(save_SCADANN+'/'+algo_name+'.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "Suppose there is a ndarray of NxM dataloaders, then N group of models will be trained, and each group will have M model. Each group is independent of the other, and each model within a group is dependent on its previous training weights.\n",
    "\n",
    "In general, overall accuracies of SCADANN are better than DANN, and DANN is better than TSD.\n",
    "Occasionally accuracies of SCADANN end up a little smaller than DANN, reasons may be lack of datasets put into training model (fixed) and non-optimal percentage_same_gesture_sable (fixed). Code should be reproducible if processed dataset sticks to the shape defined above.  \n",
    "\n",
    "The amount of increase in accuracies from DANN to SCADANN looks random. But if the base model is better at classifying one session, then its corresponding SCADANN is also better at classifying the same session. Given such result, to obtain the best performance from SCADANN, a good model trained with good data should be the starting point.\n",
    "\n",
    "* What to check if sth goes wrong:\n",
    "    * percentage_same_gesture_sable\n",
    "    * number of cycles or sessions\n",
    "    * shape of dataloaders (combination of train, test, valid should include all dataset)\n",
    "    * shape of procssed datasets\n",
    "    * directory paths of weights and results\n",
    "    * if weights are stored or loaded correcltyTSD_acc_overall_one = np.mean(TSD_acc, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TSD\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Day_0~5</th>\n",
       "      <td>0.887529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_6</th>\n",
       "      <td>0.798951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_7</th>\n",
       "      <td>0.734266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_8</th>\n",
       "      <td>0.725524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_9</th>\n",
       "      <td>0.713287</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Participant_5\n",
       "Day_0~5      0.887529\n",
       "Day_6        0.798951\n",
       "Day_7        0.734266\n",
       "Day_8        0.725524\n",
       "Day_9        0.713287"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DANN\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Day_0~5</th>\n",
       "      <td>0.887529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_6</th>\n",
       "      <td>0.86014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_7</th>\n",
       "      <td>0.828671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_8</th>\n",
       "      <td>0.763986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_9</th>\n",
       "      <td>0.734266</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Participant_5\n",
       "Day_0~5      0.887529\n",
       "Day_6         0.86014\n",
       "Day_7        0.828671\n",
       "Day_8        0.763986\n",
       "Day_9        0.734266"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCADANN\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Day_0~5</th>\n",
       "      <td>0.887529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_6</th>\n",
       "      <td>0.879371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_7</th>\n",
       "      <td>0.847902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_8</th>\n",
       "      <td>0.793706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_9</th>\n",
       "      <td>0.800699</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Participant_5\n",
       "Day_0~5      0.887529\n",
       "Day_6        0.879371\n",
       "Day_7        0.847902\n",
       "Day_8        0.793706\n",
       "Day_9        0.800699"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"TSD\")\n",
    "display(TSD_df)\n",
    "print(\"DANN\")\n",
    "display(DANN_df)\n",
    "print(\"SCADANN\")\n",
    "display(SCADANN_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Day_6</th>\n",
       "      <td>0.08042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_7</th>\n",
       "      <td>0.113636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_8</th>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_9</th>\n",
       "      <td>0.087413</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Participant_5\n",
       "Day_6       0.08042\n",
       "Day_7      0.113636\n",
       "Day_8      0.068182\n",
       "Day_9      0.087413"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "diff_df = SCADANN_df-TSD_df\n",
    "diff_df = diff_df.drop('Day_'+index_participant_list[0])\n",
    "display(diff_df)\n",
    "diff_df.to_csv(save_TSD+'/diff_results/across_day_loc1_lump6_diff.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Overall_Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TSD</th>\n",
       "      <td>0.771911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DANN</th>\n",
       "      <td>0.814918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SCADANN</th>\n",
       "      <td>0.841841</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Overall_Acc\n",
       "TSD         0.771911\n",
       "DANN        0.814918\n",
       "SCADANN     0.841841"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_acc_df = pd.DataFrame([TSD_acc_overall, DANN_acc_overall, SCADANN_acc_overall],\n",
    "                             index = [\"TSD\", \"DANN\", \"SCADANN\"],\n",
    "                             columns = [\"Overall_Acc\"])\n",
    "overall_acc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1800x1800 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(25,25))\n",
    "acc_list = [TSD_df, DANN_df, SCADANN_df, overall_acc_df]\n",
    "title_list = [\"TSD\", \"DANN\", \"SCADANN\", \"Overall\"]\n",
    "for idx, ax in enumerate(axes.reshape(-1)): \n",
    "    acc_list[idx].transpose().plot.bar(ax = ax, rot=0)\n",
    "    ax.set_title(title_list[idx])\n",
    "    ax.set_ylim([0, 1.0])\n",
    "    for p in ax.patches:\n",
    "        ax.annotate(str(np.round(p.get_height(),2)), (p.get_x()+p.get_width()/2., p.get_height()),\n",
    "                    ha='center', va='center', xytext=(0, 8),textcoords='offset points')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
