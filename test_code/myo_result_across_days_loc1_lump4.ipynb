{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results of TSD, DANN, SCADANN models across 10 days of inward rotation starting at Day_0~3 for Subject_4\n",
    "\n",
    "Library used can be downloaded from https://github.com/aonai/long_term_EMG_myo   \n",
    "&emsp; Original by UlysseCoteAllard https://github.com/UlysseCoteAllard/LongTermEMG   \n",
    "Dataset recorded by https://github.com/Suguru55/Wearable_Sensor_Long-term_sEMG_Dataset   \n",
    "Extended robot project can be found in https://github.com/aonai/myo_robot_arm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "* weights for TSD are total of 50 training models, 10 for each day\n",
    "* weights for DANN and SCADANN are total of 45 trianing models, 9 for each day\n",
    "\n",
    "\n",
    "* training examples should have shape (1, 7,)\n",
    "* first session has shape (16, 572, 252)\n",
    "* the following sessions have shape (4, 572, 252)\n",
    "* training labels should have shape (1, 7,)\n",
    "\n",
    "\n",
    "* location 0, 1, and 2 corresponds to neutral position, inward rotation, and outward rotation respectively\n",
    "* session mentioned below are days, so number of sessions is 10\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "import torch\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_dir = \"/home/laiy/gitrepos/msr_final/LongTermEMG_myo\"\n",
    "os.chdir(code_dir)\n",
    "from PrepareAndLoadData.process_data import read_data_training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Prepare Data\n",
    "use `switch=2` to train across days and individually on wearing location 0 (`session_in_include=[0]`)\n",
    "\n",
    "### specify the directories used for running the code:\n",
    "* `code_diar`: path to long_term_EMG_myo library\n",
    "* `data_dir`: where raw dataset is loaded; raw data is in csv format\n",
    "* `processed_data_dir`: where processed dataset is loaded; processed data is in npy pickle format\n",
    "    * processed data should be a ndarray of shape   \n",
    "    (controlling_factor_1 x controlling_factor_2 x num_sessions_per_gesture x #examples_window*#mov(26*22=572) x processed_channel_shape(252 for TSD, (4,8,10) for ConvNet)\n",
    "* `path_<model_name>`: where model weights are saved\n",
    "    * weights should be saved in folder `/Weights/<model_name>`. Each folder has subfolders containing weights for the first controlling factor.\n",
    "    * weights for base model (TSD or ConvNet) contain m set of training model\n",
    "    * weights for DANN and SCADANN contain m-1 set of trianing model (these models are trianed based on TSD, so they do not have a best_state_0.pt model). \n",
    "* `save_<model_name>`: where model results are saved\n",
    "    * each result for testing a model on a group of dataset is saved in folder `results`. Each result has corresponding \n",
    "        * `<model_name>.txt` includes predictions, ground truths, array of accuracies for each participant and each session, and overall accuracy\n",
    "        * `predictions_<model_name>.npy` includes array of accuracies, ground truths, predictions, and model outputs (probability array for each prediction)\n",
    "        * remember to make blank files in these names before saving\n",
    "\n",
    "\n",
    "\n",
    "* use `read_data_training` to process raw dataset\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/home/laiy/gitrepos/msr_final/Wearable_Sensor_Long-term_sEMG_Dataset/data\"\n",
    "processed_data_dir = \"/home/laiy/gitrepos/msr_final/LongTermEMG_myo/Processed_datasets_all_across_day_loc_1_lump4\"\n",
    "code_dir = \"/home/laiy/gitrepos/msr_final/LongTermEMG_myo\"\n",
    "save_dir = \"/home/laiy/gitrepos/msr_final/LongTermEMG_myo/Results\"\n",
    "\n",
    "path_TSD =\"/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump4/TSD\"\n",
    "save_TSD = \"/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/results\"\n",
    "\n",
    "path_DANN =\"/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump4/DANN\"\n",
    "save_DANN = \"/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/results\"\n",
    "\n",
    "path_SCADANN =\"/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump4/SCADANN\"\n",
    "save_SCADANN = \"/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing Training datasets...\n",
      "session  1  --- process data in days  [2, 5, 6, 16, 17, 18, 22, 24, 25, 28]\n",
      "index_participant_list  [5]\n",
      "READ  Sub 5 _Loc 1 _Day 2\n",
      "examples_per_session =  (1, 4, 572, 252)\n",
      "READ  Sub 5 _Loc 1 _Day 5\n",
      "Include day  5  in first dataset  (4, 572, 252)\n",
      "examples of first session =  (8, 572, 252)\n",
      "examples_per_session =  (1, 8, 572, 252)\n",
      "READ  Sub 5 _Loc 1 _Day 6\n",
      "Include day  6  in first dataset  (8, 572, 252)\n",
      "examples of first session =  (12, 572, 252)\n",
      "examples_per_session =  (1, 12, 572, 252)\n",
      "READ  Sub 5 _Loc 1 _Day 16\n",
      "Include day  16  in first dataset  (12, 572, 252)\n",
      "examples of first session =  (16, 572, 252)\n",
      "examples_per_session =  (1, 16, 572, 252)\n",
      "READ  Sub 5 _Loc 1 _Day 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laiy/.local/lib/python3.8/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "examples_per_session =  (2,)\n",
      "READ  Sub 5 _Loc 1 _Day 18\n",
      "examples_per_session =  (3,)\n",
      "READ  Sub 5 _Loc 1 _Day 22\n",
      "examples_per_session =  (4,)\n",
      "READ  Sub 5 _Loc 1 _Day 24\n",
      "examples_per_session =  (5,)\n",
      "READ  Sub 5 _Loc 1 _Day 25\n",
      "examples_per_session =  (6,)\n",
      "READ  Sub 5 _Loc 1 _Day 28\n",
      "examples_per_session =  (7,)\n",
      "@ traning sessions =  (1, 7)\n",
      "traning examples  (1, 7)\n",
      "traning labels  (1, 7)\n",
      "all traning examples  (1, 7)\n",
      "all traning labels  (1, 7)\n"
     ]
    }
   ],
   "source": [
    "# read_data_training(path=data_dir, store_path = processed_data_dir,  \n",
    "#                    sessions_to_include =[1], switch=2, include_in_first=4,\n",
    "#                    start_at_participant=5, num_participant=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traning examples  (1, 7)\n",
      "traning labels  (1, 7)\n"
     ]
    }
   ],
   "source": [
    "# check stored pickle \n",
    "with open(processed_data_dir + \"/training_session.pickle\", 'rb') as f:\n",
    "    dataset_training = pickle.load(file=f)\n",
    "\n",
    "examples_datasets_train = dataset_training['examples_training']\n",
    "print('traning examples ', np.shape(examples_datasets_train))\n",
    "labels_datasets_train = dataset_training['labels_training']\n",
    "print('traning labels ', np.shape(labels_datasets_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  examples_per_session =  (16, 572, 252)\n",
      "0  labels_per_session =  (16, 572)\n",
      "1  examples_per_session =  (4, 572, 252)\n",
      "1  labels_per_session =  (4, 572)\n",
      "2  examples_per_session =  (4, 572, 252)\n",
      "2  labels_per_session =  (4, 572)\n",
      "3  examples_per_session =  (4, 572, 252)\n",
      "3  labels_per_session =  (4, 572)\n",
      "4  examples_per_session =  (4, 572, 252)\n",
      "4  labels_per_session =  (4, 572)\n",
      "5  examples_per_session =  (4, 572, 252)\n",
      "5  labels_per_session =  (4, 572)\n",
      "6  examples_per_session =  (4, 572, 252)\n",
      "6  labels_per_session =  (4, 572)\n"
     ]
    }
   ],
   "source": [
    "for idx, examples_per_session in enumerate (examples_datasets_train[0]):\n",
    "    print(idx, \" examples_per_session = \", np.shape(examples_per_session))\n",
    "    print(idx, \" labels_per_session = \", np.shape(labels_datasets_train[0][idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify params used for training and testing\n",
    "\n",
    "During training and testing, processed datasets are first put into pytorch dataloders, then feed to the model trainer; following are params for TSD model and dataloaders\n",
    "\n",
    "* `num_kernels`: list of integers defining number of neurons used in each linear layer (linear block has `dropout`=0.5)\n",
    "* `number_of_cycles_total`: number of trails performed for each session (assuming that all session have the same trail size)\n",
    "    * 4 for myo across day training\n",
    "* `number_of_classes`: total number of gestures performed in dataset\n",
    "    * 22 for myo\n",
    "* `batch_size`: number of examples stored in each batch\n",
    "* `feature_vector_input_length`: length of input array or each processed signal; i.e. size of one training example \n",
    "    * 252 for TSD\n",
    "* `learning_rate`= 0.002515\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number_of_cycle_for_first_training  16\n",
      "number_of_cycles_total  4\n"
     ]
    }
   ],
   "source": [
    "num_kernels=[200, 200, 200]                                \n",
    "number_of_cycle_for_first_training = np.shape(examples_datasets_train[0][0])[0]               \n",
    "number_of_cycles_total=np.shape(examples_datasets_train[-1][-1])[0]               \n",
    "print(\"number_of_cycle_for_first_training \", number_of_cycle_for_first_training)\n",
    "print(\"number_of_cycles_total \", number_of_cycles_total)\n",
    "number_of_classes=22\n",
    "batch_size=128          \n",
    "feature_vector_input_length=252                     \n",
    "learning_rate=0.002515"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. TSD_DNN\n",
    "* `train_fine_tuning`: used to train data using a base model (TSD or ConvNet)\n",
    "    * running this function will save num_sessions sets of TSD model weights (each is fine tuned based on the previous training)  \n",
    "    \n",
    "* `test_standard_model_on_training_sessions`: test model result\n",
    "\n",
    "\n",
    "### check if dataloaders are loaded correctly:\n",
    "* each participant has shape (num_session x 40 x 572 x 252)\n",
    "* each session has shape (40 x 572 x 252)\n",
    "* put these data into on group ends up with shape (40*572=22880, 252)\n",
    "    * shuffle on group of data and put into dataloaders\n",
    "    * each participant should have num_sessions sets of dataloaders, each correspond to one session\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TrainingsAndEvaluations.ForTrainingSessions.train_tsd_dnn_standard import \\\n",
    "            test_standard_model_on_training_sessions, train_fine_tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET one participant_examples  (7,)\n",
      "   GET one training_index_examples  (16, 572, 252)  at  0\n",
      "   GOT one group XY  (9152, 252)    (9152,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (8236, 252)    (8236,)\n",
      "       one group XY valid (916, 252)    (916, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  1\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  2\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  3\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  4\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  5\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  6\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "dataloaders: \n",
      "   train  (1, 7)\n",
      "   valid  (1, 7)\n",
      "   test  (1, 0)\n",
      "START TRAINING\n",
      "Participant:  0\n",
      "Session:  0\n",
      "<generator object Module.parameters at 0x7fdcf8eb3c80>\n",
      "Epoch 0/499\n",
      "----------\n",
      "train Loss: 0.01035736 Acc: 0.61108398\n",
      "val Loss: 0.00062743 Acc: 0.83078603\n",
      "New best validation loss: 0.0006274320673213776\n",
      "Epoch 1 of 500 took 0.424s\n",
      "Epoch 1/499\n",
      "----------\n",
      "train Loss: 0.00503649 Acc: 0.78393555\n",
      "val Loss: 0.00046467 Acc: 0.85371179\n",
      "New best validation loss: 0.00046466502430137066\n",
      "Epoch 2 of 500 took 0.425s\n",
      "Epoch 2/499\n",
      "----------\n",
      "train Loss: 0.00416981 Acc: 0.8182373\n",
      "val Loss: 0.00044866 Acc: 0.86790393\n",
      "Epoch 3 of 500 took 0.421s\n",
      "Epoch 3/499\n",
      "----------\n",
      "train Loss: 0.00369887 Acc: 0.83984375\n",
      "val Loss: 0.00038913 Acc: 0.87991266\n",
      "Epoch 4 of 500 took 0.425s\n",
      "Epoch 4/499\n",
      "----------\n",
      "train Loss: 0.00351466 Acc: 0.84069824\n",
      "val Loss: 0.00029005 Acc: 0.90829694\n",
      "New best validation loss: 0.0002900468079804333\n",
      "Epoch 5 of 500 took 0.423s\n",
      "Epoch 5/499\n",
      "----------\n",
      "train Loss: 0.00325992 Acc: 0.86083984\n",
      "val Loss: 0.00032804 Acc: 0.89956332\n",
      "Epoch 6 of 500 took 0.423s\n",
      "Epoch 6/499\n",
      "----------\n",
      "train Loss: 0.00303583 Acc: 0.86291504\n",
      "val Loss: 0.00037831 Acc: 0.86244541\n",
      "Epoch 7 of 500 took 0.421s\n",
      "Epoch 7/499\n",
      "----------\n",
      "train Loss: 0.00290505 Acc: 0.87023926\n",
      "val Loss: 0.00028234 Acc: 0.91484716\n",
      "Epoch 8 of 500 took 0.418s\n",
      "Epoch 8/499\n",
      "----------\n",
      "train Loss: 0.00283144 Acc: 0.87341309\n",
      "val Loss: 0.00030775 Acc: 0.88755459\n",
      "Epoch 9 of 500 took 0.422s\n",
      "Epoch 9/499\n",
      "----------\n",
      "train Loss: 0.00277709 Acc: 0.87658691\n",
      "val Loss: 0.00024809 Acc: 0.92467249\n",
      "Epoch 10 of 500 took 0.419s\n",
      "Epoch 10/499\n",
      "----------\n",
      "train Loss: 0.00269412 Acc: 0.87756348\n",
      "val Loss: 0.00027675 Acc: 0.91048035\n",
      "Epoch 11 of 500 took 0.421s\n",
      "Epoch 11/499\n",
      "----------\n",
      "train Loss: 0.00250887 Acc: 0.88391113\n",
      "val Loss: 0.00026999 Acc: 0.91266376\n",
      "Epoch 12 of 500 took 0.475s\n",
      "Epoch 12/499\n",
      "----------\n",
      "train Loss: 0.00243216 Acc: 0.890625\n",
      "val Loss: 0.00026718 Acc: 0.91375546\n",
      "Epoch 13 of 500 took 0.483s\n",
      "Epoch 13/499\n",
      "----------\n",
      "train Loss: 0.00236812 Acc: 0.89367676\n",
      "val Loss: 0.00025722 Acc: 0.91593886\n",
      "Epoch 14 of 500 took 0.421s\n",
      "Epoch 14/499\n",
      "----------\n",
      "train Loss: 0.00230724 Acc: 0.89697266\n",
      "val Loss: 0.00021220 Acc: 0.9268559\n",
      "Epoch 15 of 500 took 0.425s\n",
      "Epoch 15/499\n",
      "----------\n",
      "train Loss: 0.00225866 Acc: 0.89733887\n",
      "val Loss: 0.00030822 Acc: 0.8930131\n",
      "Epoch 16 of 500 took 0.424s\n",
      "\n",
      "Training complete in 0m 7s\n",
      "Best val loss: 0.000290\n",
      "Session:  1\n",
      "<generator object Module.parameters at 0x7fdcf8eb3c80>\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump4/TSD/participant_0/best_state_0.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump4/TSD/participant_0/best_state_0.pt' (epoch 5)\n",
      "Epoch 0/499\n",
      "----------\n",
      "train Loss: 0.00340313 Acc: 0.85693359\n",
      "val Loss: 0.00079608 Acc: 0.930131\n",
      "New best validation loss: 0.000796077813644076\n",
      "Epoch 1 of 500 took 0.107s\n",
      "Epoch 1/499\n",
      "----------\n",
      "train Loss: 0.00269437 Acc: 0.88232422\n",
      "val Loss: 0.00089085 Acc: 0.930131\n",
      "Epoch 2 of 500 took 0.108s\n",
      "Epoch 2/499\n",
      "----------\n",
      "train Loss: 0.00236538 Acc: 0.89648438\n",
      "val Loss: 0.00061800 Acc: 0.95196507\n",
      "New best validation loss: 0.0006180027556731712\n",
      "Epoch 3 of 500 took 0.109s\n",
      "Epoch 3/499\n",
      "----------\n",
      "train Loss: 0.00222721 Acc: 0.90771484\n",
      "val Loss: 0.00055952 Acc: 0.95196507\n",
      "Epoch 4 of 500 took 0.139s\n",
      "Epoch 4/499\n",
      "----------\n",
      "train Loss: 0.00205965 Acc: 0.90820312\n",
      "val Loss: 0.00060874 Acc: 0.95196507\n",
      "Epoch 5 of 500 took 0.140s\n",
      "Epoch 5/499\n",
      "----------\n",
      "train Loss: 0.00195051 Acc: 0.91894531\n",
      "val Loss: 0.00063803 Acc: 0.94323144\n",
      "Epoch 6 of 500 took 0.140s\n",
      "Epoch 6/499\n",
      "----------\n",
      "train Loss: 0.00166889 Acc: 0.92480469\n",
      "val Loss: 0.00055738 Acc: 0.95196507\n",
      "Epoch 7 of 500 took 0.139s\n",
      "Epoch 7/499\n",
      "----------\n",
      "train Loss: 0.00181495 Acc: 0.92333984\n",
      "val Loss: 0.00050275 Acc: 0.95633188\n",
      "New best validation loss: 0.0005027534009067252\n",
      "Epoch 8 of 500 took 0.143s\n",
      "Epoch 8/499\n",
      "----------\n",
      "train Loss: 0.00176603 Acc: 0.92480469\n",
      "val Loss: 0.00080640 Acc: 0.93449782\n",
      "Epoch 9 of 500 took 0.141s\n",
      "Epoch 9/499\n",
      "----------\n",
      "train Loss: 0.00160859 Acc: 0.92822266\n",
      "val Loss: 0.00059677 Acc: 0.95633188\n",
      "Epoch 10 of 500 took 0.143s\n",
      "Epoch 10/499\n",
      "----------\n",
      "train Loss: 0.00137717 Acc: 0.94189453\n",
      "val Loss: 0.00035652 Acc: 0.9650655\n",
      "New best validation loss: 0.0003565181876374124\n",
      "Epoch 11 of 500 took 0.140s\n",
      "Epoch 11/499\n",
      "----------\n",
      "train Loss: 0.00142115 Acc: 0.93896484\n",
      "val Loss: 0.00054875 Acc: 0.95196507\n",
      "Epoch 12 of 500 took 0.142s\n",
      "Epoch 12/499\n",
      "----------\n",
      "train Loss: 0.00147866 Acc: 0.93261719\n",
      "val Loss: 0.00055698 Acc: 0.96069869\n",
      "Epoch 13 of 500 took 0.140s\n",
      "Epoch 13/499\n",
      "----------\n",
      "train Loss: 0.00155202 Acc: 0.93505859\n",
      "val Loss: 0.00146913 Acc: 0.89956332\n",
      "Epoch 14 of 500 took 0.141s\n",
      "Epoch 14/499\n",
      "----------\n",
      "train Loss: 0.00146553 Acc: 0.93505859\n",
      "val Loss: 0.00041852 Acc: 0.97379913\n",
      "Epoch 15 of 500 took 0.141s\n",
      "Epoch 15/499\n",
      "----------\n",
      "train Loss: 0.00131768 Acc: 0.93798828\n",
      "val Loss: 0.00055774 Acc: 0.95633188\n",
      "Epoch 16 of 500 took 0.141s\n",
      "Epoch 16/499\n",
      "----------\n",
      "train Loss: 0.00113915 Acc: 0.95263672\n",
      "val Loss: 0.00085293 Acc: 0.930131\n",
      "Epoch    17: reducing learning rate of group 0 to 5.0300e-04.\n",
      "Epoch 17 of 500 took 0.140s\n",
      "Epoch 17/499\n",
      "----------\n",
      "train Loss: 0.00109251 Acc: 0.94921875\n",
      "val Loss: 0.00030970 Acc: 0.97379913\n",
      "Epoch 18 of 500 took 0.142s\n",
      "Epoch 18/499\n",
      "----------\n",
      "train Loss: 0.00094041 Acc: 0.95703125\n",
      "val Loss: 0.00026461 Acc: 0.97379913\n",
      "Epoch 19 of 500 took 0.112s\n",
      "Epoch 19/499\n",
      "----------\n",
      "train Loss: 0.00083923 Acc: 0.96484375\n",
      "val Loss: 0.00029506 Acc: 0.97379913\n",
      "Epoch 20 of 500 took 0.109s\n",
      "Epoch 20/499\n",
      "----------\n",
      "train Loss: 0.00079767 Acc: 0.97021484\n",
      "val Loss: 0.00027801 Acc: 0.97816594\n",
      "Epoch 21 of 500 took 0.106s\n",
      "Epoch 21/499\n",
      "----------\n",
      "train Loss: 0.00089073 Acc: 0.96240234\n",
      "val Loss: 0.00036195 Acc: 0.96069869\n",
      "Epoch 22 of 500 took 0.109s\n",
      "\n",
      "Training complete in 0m 3s\n",
      "Best val loss: 0.000357\n",
      "Session:  2\n",
      "<generator object Module.parameters at 0x7fdcf89150b0>\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump4/TSD/participant_0/best_state_1.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump4/TSD/participant_0/best_state_1.pt' (epoch 11)\n",
      "Epoch 0/499\n",
      "----------\n",
      "train Loss: 0.00323214 Acc: 0.86328125\n",
      "val Loss: 0.00095934 Acc: 0.93886463\n",
      "New best validation loss: 0.000959342614011473\n",
      "Epoch 1 of 500 took 0.112s\n",
      "Epoch 1/499\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.00274436 Acc: 0.87695312\n",
      "val Loss: 0.00130381 Acc: 0.88209607\n",
      "Epoch 2 of 500 took 0.110s\n",
      "Epoch 2/499\n",
      "----------\n",
      "train Loss: 0.00228437 Acc: 0.8984375\n",
      "val Loss: 0.00074150 Acc: 0.94323144\n",
      "New best validation loss: 0.0007415017576717394\n",
      "Epoch 3 of 500 took 0.107s\n",
      "Epoch 3/499\n",
      "----------\n",
      "train Loss: 0.00226591 Acc: 0.90283203\n",
      "val Loss: 0.00100911 Acc: 0.91703057\n",
      "Epoch 4 of 500 took 0.108s\n",
      "Epoch 4/499\n",
      "----------\n",
      "train Loss: 0.00188319 Acc: 0.91552734\n",
      "val Loss: 0.00060188 Acc: 0.95196507\n",
      "New best validation loss: 0.0006018760553093456\n",
      "Epoch 5 of 500 took 0.107s\n",
      "Epoch 5/499\n",
      "----------\n",
      "train Loss: 0.00184244 Acc: 0.91992188\n",
      "val Loss: 0.00076001 Acc: 0.94323144\n",
      "Epoch 6 of 500 took 0.108s\n",
      "Epoch 6/499\n",
      "----------\n",
      "train Loss: 0.00170199 Acc: 0.92822266\n",
      "val Loss: 0.00079851 Acc: 0.930131\n",
      "Epoch 7 of 500 took 0.106s\n",
      "Epoch 7/499\n",
      "----------\n",
      "train Loss: 0.00159528 Acc: 0.93066406\n",
      "val Loss: 0.00087858 Acc: 0.92576419\n",
      "Epoch 8 of 500 took 0.118s\n",
      "Epoch 8/499\n",
      "----------\n",
      "train Loss: 0.00150945 Acc: 0.93066406\n",
      "val Loss: 0.00066831 Acc: 0.93886463\n",
      "Epoch 9 of 500 took 0.116s\n",
      "Epoch 9/499\n",
      "----------\n",
      "train Loss: 0.00172895 Acc: 0.91650391\n",
      "val Loss: 0.00080231 Acc: 0.91266376\n",
      "Epoch 10 of 500 took 0.114s\n",
      "Epoch 10/499\n",
      "----------\n",
      "train Loss: 0.00137202 Acc: 0.93896484\n",
      "val Loss: 0.00055079 Acc: 0.96069869\n",
      "Epoch 11 of 500 took 0.105s\n",
      "Epoch 11/499\n",
      "----------\n",
      "train Loss: 0.00147051 Acc: 0.93408203\n",
      "val Loss: 0.00078171 Acc: 0.930131\n",
      "Epoch 12 of 500 took 0.108s\n",
      "Epoch 12/499\n",
      "----------\n",
      "train Loss: 0.00146991 Acc: 0.93457031\n",
      "val Loss: 0.00051865 Acc: 0.95196507\n",
      "Epoch 13 of 500 took 0.107s\n",
      "Epoch 13/499\n",
      "----------\n",
      "train Loss: 0.00137201 Acc: 0.9453125\n",
      "val Loss: 0.00067251 Acc: 0.930131\n",
      "Epoch 14 of 500 took 0.109s\n",
      "Epoch 14/499\n",
      "----------\n",
      "train Loss: 0.00137673 Acc: 0.94140625\n",
      "val Loss: 0.00043159 Acc: 0.95196507\n",
      "New best validation loss: 0.0004315865612446481\n",
      "Epoch 15 of 500 took 0.107s\n",
      "Epoch 15/499\n",
      "----------\n",
      "train Loss: 0.00124166 Acc: 0.94580078\n",
      "val Loss: 0.00058404 Acc: 0.95196507\n",
      "Epoch 16 of 500 took 0.108s\n",
      "Epoch 16/499\n",
      "----------\n",
      "train Loss: 0.00130277 Acc: 0.94042969\n",
      "val Loss: 0.00049884 Acc: 0.96943231\n",
      "Epoch 17 of 500 took 0.106s\n",
      "Epoch 17/499\n",
      "----------\n",
      "train Loss: 0.00138943 Acc: 0.93408203\n",
      "val Loss: 0.00065370 Acc: 0.94323144\n",
      "Epoch 18 of 500 took 0.118s\n",
      "Epoch 18/499\n",
      "----------\n",
      "train Loss: 0.00120705 Acc: 0.94287109\n",
      "val Loss: 0.00051174 Acc: 0.94759825\n",
      "Epoch 19 of 500 took 0.111s\n",
      "Epoch 19/499\n",
      "----------\n",
      "train Loss: 0.00125764 Acc: 0.94189453\n",
      "val Loss: 0.00072266 Acc: 0.93886463\n",
      "Epoch 20 of 500 took 0.108s\n",
      "Epoch 20/499\n",
      "----------\n",
      "train Loss: 0.00102236 Acc: 0.95703125\n",
      "val Loss: 0.00051933 Acc: 0.95633188\n",
      "Epoch    21: reducing learning rate of group 0 to 5.0300e-04.\n",
      "Epoch 21 of 500 took 0.106s\n",
      "Epoch 21/499\n",
      "----------\n",
      "train Loss: 0.00101132 Acc: 0.94873047\n",
      "val Loss: 0.00038545 Acc: 0.94759825\n",
      "Epoch 22 of 500 took 0.108s\n",
      "Epoch 22/499\n",
      "----------\n",
      "train Loss: 0.00093173 Acc: 0.95605469\n",
      "val Loss: 0.00035461 Acc: 0.9650655\n",
      "Epoch 23 of 500 took 0.107s\n",
      "Epoch 23/499\n",
      "----------\n",
      "train Loss: 0.00084150 Acc: 0.95996094\n",
      "val Loss: 0.00047670 Acc: 0.95633188\n",
      "Epoch 24 of 500 took 0.108s\n",
      "Epoch 24/499\n",
      "----------\n",
      "train Loss: 0.00079873 Acc: 0.96435547\n",
      "val Loss: 0.00037091 Acc: 0.96943231\n",
      "Epoch 25 of 500 took 0.111s\n",
      "Epoch 25/499\n",
      "----------\n",
      "train Loss: 0.00086620 Acc: 0.96630859\n",
      "val Loss: 0.00043929 Acc: 0.95196507\n",
      "Epoch 26 of 500 took 0.108s\n",
      "\n",
      "Training complete in 0m 3s\n",
      "Best val loss: 0.000432\n",
      "Session:  3\n",
      "<generator object Module.parameters at 0x7fdcf8915660>\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump4/TSD/participant_0/best_state_2.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump4/TSD/participant_0/best_state_2.pt' (epoch 15)\n",
      "Epoch 0/499\n",
      "----------\n",
      "train Loss: 0.00376544 Acc: 0.84375\n",
      "val Loss: 0.00107326 Acc: 0.89956332\n",
      "New best validation loss: 0.001073259713868387\n",
      "Epoch 1 of 500 took 0.107s\n",
      "Epoch 1/499\n",
      "----------\n",
      "train Loss: 0.00281861 Acc: 0.87597656\n",
      "val Loss: 0.00116785 Acc: 0.89082969\n",
      "Epoch 2 of 500 took 0.113s\n",
      "Epoch 2/499\n",
      "----------\n",
      "train Loss: 0.00260630 Acc: 0.88623047\n",
      "val Loss: 0.00119982 Acc: 0.87336245\n",
      "Epoch 3 of 500 took 0.106s\n",
      "Epoch 3/499\n",
      "----------\n",
      "train Loss: 0.00230714 Acc: 0.89892578\n",
      "val Loss: 0.00173524 Acc: 0.84279476\n",
      "Epoch 4 of 500 took 0.108s\n",
      "Epoch 4/499\n",
      "----------\n",
      "train Loss: 0.00220654 Acc: 0.89990234\n",
      "val Loss: 0.00076171 Acc: 0.930131\n",
      "New best validation loss: 0.0007617074068977322\n",
      "Epoch 5 of 500 took 0.108s\n",
      "Epoch 5/499\n",
      "----------\n",
      "train Loss: 0.00212774 Acc: 0.90332031\n",
      "val Loss: 0.00083019 Acc: 0.930131\n",
      "Epoch 6 of 500 took 0.108s\n",
      "Epoch 6/499\n",
      "----------\n",
      "train Loss: 0.00219740 Acc: 0.90283203\n",
      "val Loss: 0.00085271 Acc: 0.930131\n",
      "Epoch 7 of 500 took 0.106s\n",
      "Epoch 7/499\n",
      "----------\n",
      "train Loss: 0.00194495 Acc: 0.90673828\n",
      "val Loss: 0.00072823 Acc: 0.93886463\n",
      "Epoch 8 of 500 took 0.110s\n",
      "Epoch 8/499\n",
      "----------\n",
      "train Loss: 0.00186665 Acc: 0.91992188\n",
      "val Loss: 0.00089713 Acc: 0.930131\n",
      "Epoch 9 of 500 took 0.106s\n",
      "Epoch 9/499\n",
      "----------\n",
      "train Loss: 0.00192618 Acc: 0.91601562\n",
      "val Loss: 0.00096389 Acc: 0.92139738\n",
      "Epoch 10 of 500 took 0.110s\n",
      "Epoch 10/499\n",
      "----------\n",
      "train Loss: 0.00173865 Acc: 0.91943359\n",
      "val Loss: 0.00067580 Acc: 0.930131\n",
      "Epoch 11 of 500 took 0.105s\n",
      "Epoch 11/499\n",
      "----------\n",
      "train Loss: 0.00166836 Acc: 0.92382812\n",
      "val Loss: 0.00082199 Acc: 0.94323144\n",
      "Epoch 12 of 500 took 0.111s\n",
      "Epoch 12/499\n",
      "----------\n",
      "train Loss: 0.00146173 Acc: 0.93212891\n",
      "val Loss: 0.00066601 Acc: 0.95196507\n",
      "Epoch 13 of 500 took 0.106s\n",
      "Epoch 13/499\n",
      "----------\n",
      "train Loss: 0.00162160 Acc: 0.93212891\n",
      "val Loss: 0.00078587 Acc: 0.93886463\n",
      "Epoch 14 of 500 took 0.108s\n",
      "Epoch 14/499\n",
      "----------\n",
      "train Loss: 0.00141617 Acc: 0.9375\n",
      "val Loss: 0.00080463 Acc: 0.93449782\n",
      "Epoch 15 of 500 took 0.106s\n",
      "Epoch 15/499\n",
      "----------\n",
      "train Loss: 0.00145093 Acc: 0.93310547\n",
      "val Loss: 0.00074439 Acc: 0.94323144\n",
      "Epoch 16 of 500 took 0.108s\n",
      "\n",
      "Training complete in 0m 2s\n",
      "Best val loss: 0.000762\n",
      "Session:  4\n",
      "<generator object Module.parameters at 0x7fdcf8eb3c80>\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump4/TSD/participant_0/best_state_3.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump4/TSD/participant_0/best_state_3.pt' (epoch 5)\n",
      "Epoch 0/499\n",
      "----------\n",
      "train Loss: 0.00373168 Acc: 0.84033203\n",
      "val Loss: 0.00092504 Acc: 0.930131\n",
      "New best validation loss: 0.0009250446958833387\n",
      "Epoch 1 of 500 took 0.107s\n",
      "Epoch 1/499\n",
      "----------\n",
      "train Loss: 0.00303886 Acc: 0.85742188\n",
      "val Loss: 0.00095930 Acc: 0.91266376\n",
      "Epoch 2 of 500 took 0.111s\n",
      "Epoch 2/499\n",
      "----------\n",
      "train Loss: 0.00275366 Acc: 0.87109375\n",
      "val Loss: 0.00091653 Acc: 0.92139738\n",
      "Epoch 3 of 500 took 0.111s\n",
      "Epoch 3/499\n",
      "----------\n",
      "train Loss: 0.00264719 Acc: 0.87939453\n",
      "val Loss: 0.00114183 Acc: 0.89082969\n",
      "Epoch 4 of 500 took 0.110s\n",
      "Epoch 4/499\n",
      "----------\n",
      "train Loss: 0.00254785 Acc: 0.87939453\n",
      "val Loss: 0.00084524 Acc: 0.92576419\n",
      "Epoch 5 of 500 took 0.121s\n",
      "Epoch 5/499\n",
      "----------\n",
      "train Loss: 0.00225197 Acc: 0.90087891\n",
      "val Loss: 0.00068884 Acc: 0.95196507\n",
      "New best validation loss: 0.0006888368894960162\n",
      "Epoch 6 of 500 took 0.111s\n",
      "Epoch 6/499\n",
      "----------\n",
      "train Loss: 0.00206510 Acc: 0.90917969\n",
      "val Loss: 0.00072754 Acc: 0.93449782\n",
      "Epoch 7 of 500 took 0.105s\n",
      "Epoch 7/499\n",
      "----------\n",
      "train Loss: 0.00206608 Acc: 0.89941406\n",
      "val Loss: 0.00084569 Acc: 0.93449782\n",
      "Epoch 8 of 500 took 0.109s\n",
      "Epoch 8/499\n",
      "----------\n",
      "train Loss: 0.00205339 Acc: 0.90087891\n",
      "val Loss: 0.00068462 Acc: 0.930131\n",
      "Epoch 9 of 500 took 0.106s\n",
      "Epoch 9/499\n",
      "----------\n",
      "train Loss: 0.00181419 Acc: 0.91943359\n",
      "val Loss: 0.00067353 Acc: 0.94323144\n",
      "Epoch 10 of 500 took 0.109s\n",
      "Epoch 10/499\n",
      "----------\n",
      "train Loss: 0.00174176 Acc: 0.92285156\n",
      "val Loss: 0.00130211 Acc: 0.89519651\n",
      "Epoch 11 of 500 took 0.106s\n",
      "Epoch 11/499\n",
      "----------\n",
      "train Loss: 0.00183023 Acc: 0.91845703\n",
      "val Loss: 0.00086834 Acc: 0.92139738\n",
      "Epoch 12 of 500 took 0.108s\n",
      "Epoch 12/499\n",
      "----------\n",
      "train Loss: 0.00181644 Acc: 0.91992188\n",
      "val Loss: 0.00075739 Acc: 0.93886463\n",
      "Epoch 13 of 500 took 0.107s\n",
      "Epoch 13/499\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.00167655 Acc: 0.91992188\n",
      "val Loss: 0.00049886 Acc: 0.96069869\n",
      "New best validation loss: 0.0004988553807725031\n",
      "Epoch 14 of 500 took 0.117s\n",
      "Epoch 14/499\n",
      "----------\n",
      "train Loss: 0.00169952 Acc: 0.92333984\n",
      "val Loss: 0.00051025 Acc: 0.9650655\n",
      "Epoch 15 of 500 took 0.105s\n",
      "Epoch 15/499\n",
      "----------\n",
      "train Loss: 0.00174926 Acc: 0.91601562\n",
      "val Loss: 0.00043434 Acc: 0.96069869\n",
      "Epoch 16 of 500 took 0.109s\n",
      "Epoch 16/499\n",
      "----------\n",
      "train Loss: 0.00165600 Acc: 0.91796875\n",
      "val Loss: 0.00057041 Acc: 0.95196507\n",
      "Epoch 17 of 500 took 0.105s\n",
      "Epoch 17/499\n",
      "----------\n",
      "train Loss: 0.00149264 Acc: 0.92675781\n",
      "val Loss: 0.00050314 Acc: 0.94759825\n",
      "Epoch 18 of 500 took 0.109s\n",
      "Epoch 18/499\n",
      "----------\n",
      "train Loss: 0.00142636 Acc: 0.92919922\n",
      "val Loss: 0.00075427 Acc: 0.94323144\n",
      "Epoch 19 of 500 took 0.106s\n",
      "Epoch 19/499\n",
      "----------\n",
      "train Loss: 0.00148290 Acc: 0.93115234\n",
      "val Loss: 0.00045387 Acc: 0.95196507\n",
      "Epoch 20 of 500 took 0.109s\n",
      "Epoch 20/499\n",
      "----------\n",
      "train Loss: 0.00157963 Acc: 0.92675781\n",
      "val Loss: 0.00061892 Acc: 0.94759825\n",
      "Epoch 21 of 500 took 0.109s\n",
      "Epoch 21/499\n",
      "----------\n",
      "train Loss: 0.00139059 Acc: 0.93261719\n",
      "val Loss: 0.00060582 Acc: 0.93886463\n",
      "Epoch    22: reducing learning rate of group 0 to 5.0300e-04.\n",
      "Epoch 22 of 500 took 0.115s\n",
      "Epoch 22/499\n",
      "----------\n",
      "train Loss: 0.00142749 Acc: 0.9375\n",
      "val Loss: 0.00036164 Acc: 0.97379913\n",
      "New best validation loss: 0.0003616429507472109\n",
      "Epoch 23 of 500 took 0.111s\n",
      "Epoch 23/499\n",
      "----------\n",
      "train Loss: 0.00124437 Acc: 0.94140625\n",
      "val Loss: 0.00044608 Acc: 0.96069869\n",
      "Epoch 24 of 500 took 0.111s\n",
      "Epoch 24/499\n",
      "----------\n",
      "train Loss: 0.00125729 Acc: 0.93896484\n",
      "val Loss: 0.00039976 Acc: 0.9650655\n",
      "Epoch 25 of 500 took 0.105s\n",
      "Epoch 25/499\n",
      "----------\n",
      "train Loss: 0.00113947 Acc: 0.95166016\n",
      "val Loss: 0.00037063 Acc: 0.96069869\n",
      "Epoch 26 of 500 took 0.109s\n",
      "Epoch 26/499\n",
      "----------\n",
      "train Loss: 0.00101244 Acc: 0.95166016\n",
      "val Loss: 0.00048296 Acc: 0.95196507\n",
      "Epoch 27 of 500 took 0.106s\n",
      "Epoch 27/499\n",
      "----------\n",
      "train Loss: 0.00106681 Acc: 0.95800781\n",
      "val Loss: 0.00032322 Acc: 0.9650655\n",
      "Epoch 28 of 500 took 0.108s\n",
      "Epoch 28/499\n",
      "----------\n",
      "train Loss: 0.00103321 Acc: 0.95898438\n",
      "val Loss: 0.00032814 Acc: 0.97379913\n",
      "Epoch 29 of 500 took 0.106s\n",
      "Epoch 29/499\n",
      "----------\n",
      "train Loss: 0.00103944 Acc: 0.94824219\n",
      "val Loss: 0.00042612 Acc: 0.9650655\n",
      "Epoch 30 of 500 took 0.109s\n",
      "Epoch 30/499\n",
      "----------\n",
      "train Loss: 0.00100686 Acc: 0.95410156\n",
      "val Loss: 0.00030662 Acc: 0.96069869\n",
      "Epoch 31 of 500 took 0.106s\n",
      "Epoch 31/499\n",
      "----------\n",
      "train Loss: 0.00096075 Acc: 0.96044922\n",
      "val Loss: 0.00034321 Acc: 0.9650655\n",
      "Epoch 32 of 500 took 0.109s\n",
      "Epoch 32/499\n",
      "----------\n",
      "train Loss: 0.00083466 Acc: 0.96435547\n",
      "val Loss: 0.00033254 Acc: 0.96943231\n",
      "Epoch 33 of 500 took 0.110s\n",
      "Epoch 33/499\n",
      "----------\n",
      "train Loss: 0.00091962 Acc: 0.95458984\n",
      "val Loss: 0.00037080 Acc: 0.9650655\n",
      "Epoch 34 of 500 took 0.108s\n",
      "\n",
      "Training complete in 0m 4s\n",
      "Best val loss: 0.000362\n",
      "Session:  5\n",
      "<generator object Module.parameters at 0x7fdcf8eb3c80>\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump4/TSD/participant_0/best_state_4.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump4/TSD/participant_0/best_state_4.pt' (epoch 23)\n",
      "Epoch 0/499\n",
      "----------\n",
      "train Loss: 0.00356179 Acc: 0.85302734\n",
      "val Loss: 0.00088480 Acc: 0.92576419\n",
      "New best validation loss: 0.0008848027110620357\n",
      "Epoch 1 of 500 took 0.127s\n",
      "Epoch 1/499\n",
      "----------\n",
      "train Loss: 0.00304773 Acc: 0.8671875\n",
      "val Loss: 0.00110209 Acc: 0.90829694\n",
      "Epoch 2 of 500 took 0.110s\n",
      "Epoch 2/499\n",
      "----------\n",
      "train Loss: 0.00235289 Acc: 0.88964844\n",
      "val Loss: 0.00084557 Acc: 0.93449782\n",
      "Epoch 3 of 500 took 0.106s\n",
      "Epoch 3/499\n",
      "----------\n",
      "train Loss: 0.00230984 Acc: 0.89697266\n",
      "val Loss: 0.00061523 Acc: 0.93886463\n",
      "New best validation loss: 0.0006152267799627312\n",
      "Epoch 4 of 500 took 0.111s\n",
      "Epoch 4/499\n",
      "----------\n",
      "train Loss: 0.00212957 Acc: 0.89599609\n",
      "val Loss: 0.00065278 Acc: 0.93886463\n",
      "Epoch 5 of 500 took 0.107s\n",
      "Epoch 5/499\n",
      "----------\n",
      "train Loss: 0.00227246 Acc: 0.90332031\n",
      "val Loss: 0.00061649 Acc: 0.95633188\n",
      "Epoch 6 of 500 took 0.109s\n",
      "Epoch 6/499\n",
      "----------\n",
      "train Loss: 0.00207173 Acc: 0.90234375\n",
      "val Loss: 0.00070044 Acc: 0.94323144\n",
      "Epoch 7 of 500 took 0.108s\n",
      "Epoch 7/499\n",
      "----------\n",
      "train Loss: 0.00201067 Acc: 0.90527344\n",
      "val Loss: 0.00069146 Acc: 0.93886463\n",
      "Epoch 8 of 500 took 0.117s\n",
      "Epoch 8/499\n",
      "----------\n",
      "train Loss: 0.00171259 Acc: 0.91845703\n",
      "val Loss: 0.00043280 Acc: 0.96069869\n",
      "New best validation loss: 0.0004328013312348112\n",
      "Epoch 9 of 500 took 0.121s\n",
      "Epoch 9/499\n",
      "----------\n",
      "train Loss: 0.00181521 Acc: 0.91748047\n",
      "val Loss: 0.00054272 Acc: 0.95633188\n",
      "Epoch 10 of 500 took 0.109s\n",
      "Epoch 10/499\n",
      "----------\n",
      "train Loss: 0.00177210 Acc: 0.921875\n",
      "val Loss: 0.00038341 Acc: 0.9650655\n",
      "Epoch 11 of 500 took 0.109s\n",
      "Epoch 11/499\n",
      "----------\n",
      "train Loss: 0.00164400 Acc: 0.92822266\n",
      "val Loss: 0.00058360 Acc: 0.95196507\n",
      "Epoch 12 of 500 took 0.114s\n",
      "Epoch 12/499\n",
      "----------\n",
      "train Loss: 0.00145542 Acc: 0.93359375\n",
      "val Loss: 0.00052532 Acc: 0.95196507\n",
      "Epoch 13 of 500 took 0.105s\n",
      "Epoch 13/499\n",
      "----------\n",
      "train Loss: 0.00150291 Acc: 0.93505859\n",
      "val Loss: 0.00050045 Acc: 0.95196507\n",
      "Epoch 14 of 500 took 0.110s\n",
      "Epoch 14/499\n",
      "----------\n",
      "train Loss: 0.00154164 Acc: 0.92919922\n",
      "val Loss: 0.00038538 Acc: 0.96069869\n",
      "Epoch 15 of 500 took 0.107s\n",
      "Epoch 15/499\n",
      "----------\n",
      "train Loss: 0.00150513 Acc: 0.93798828\n",
      "val Loss: 0.00049632 Acc: 0.95633188\n",
      "Epoch 16 of 500 took 0.110s\n",
      "Epoch 16/499\n",
      "----------\n",
      "train Loss: 0.00138375 Acc: 0.93164062\n",
      "val Loss: 0.00044047 Acc: 0.9650655\n",
      "Epoch    17: reducing learning rate of group 0 to 5.0300e-04.\n",
      "Epoch 17 of 500 took 0.110s\n",
      "Epoch 17/499\n",
      "----------\n",
      "train Loss: 0.00137994 Acc: 0.94042969\n",
      "val Loss: 0.00034199 Acc: 0.9650655\n",
      "Epoch 18 of 500 took 0.110s\n",
      "Epoch 18/499\n",
      "----------\n",
      "train Loss: 0.00112969 Acc: 0.94873047\n",
      "val Loss: 0.00023810 Acc: 0.97379913\n",
      "New best validation loss: 0.00023809890364455343\n",
      "Epoch 19 of 500 took 0.107s\n",
      "Epoch 19/499\n",
      "----------\n",
      "train Loss: 0.00112902 Acc: 0.94921875\n",
      "val Loss: 0.00028057 Acc: 0.96943231\n",
      "Epoch 20 of 500 took 0.111s\n",
      "Epoch 20/499\n",
      "----------\n",
      "train Loss: 0.00101051 Acc: 0.95849609\n",
      "val Loss: 0.00030374 Acc: 0.96943231\n",
      "Epoch 21 of 500 took 0.106s\n",
      "Epoch 21/499\n",
      "----------\n",
      "train Loss: 0.00104041 Acc: 0.95263672\n",
      "val Loss: 0.00027573 Acc: 0.97816594\n",
      "Epoch 22 of 500 took 0.108s\n",
      "Epoch 22/499\n",
      "----------\n",
      "train Loss: 0.00117877 Acc: 0.94335938\n",
      "val Loss: 0.00023031 Acc: 0.97816594\n",
      "Epoch 23 of 500 took 0.107s\n",
      "Epoch 23/499\n",
      "----------\n",
      "train Loss: 0.00108055 Acc: 0.95410156\n",
      "val Loss: 0.00028322 Acc: 0.97379913\n",
      "Epoch 24 of 500 took 0.110s\n",
      "Epoch 24/499\n",
      "----------\n",
      "train Loss: 0.00103907 Acc: 0.953125\n",
      "val Loss: 0.00027002 Acc: 0.97379913\n",
      "Epoch 25 of 500 took 0.106s\n",
      "Epoch 25/499\n",
      "----------\n",
      "train Loss: 0.00085839 Acc: 0.96044922\n",
      "val Loss: 0.00027566 Acc: 0.9650655\n",
      "Epoch 26 of 500 took 0.113s\n",
      "Epoch 26/499\n",
      "----------\n",
      "train Loss: 0.00094516 Acc: 0.95458984\n",
      "val Loss: 0.00026979 Acc: 0.98253275\n",
      "Epoch 27 of 500 took 0.107s\n",
      "Epoch 27/499\n",
      "----------\n",
      "train Loss: 0.00102364 Acc: 0.95654297\n",
      "val Loss: 0.00024011 Acc: 0.97379913\n",
      "Epoch 28 of 500 took 0.110s\n",
      "Epoch 28/499\n",
      "----------\n",
      "train Loss: 0.00096340 Acc: 0.953125\n",
      "val Loss: 0.00047206 Acc: 0.95633188\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0060e-04.\n",
      "Epoch 29 of 500 took 0.107s\n",
      "Epoch 29/499\n",
      "----------\n",
      "train Loss: 0.00087856 Acc: 0.95507812\n",
      "val Loss: 0.00024108 Acc: 0.97379913\n",
      "Epoch 30 of 500 took 0.111s\n",
      "\n",
      "Training complete in 0m 3s\n",
      "Best val loss: 0.000238\n",
      "Session:  6\n",
      "<generator object Module.parameters at 0x7fdcf8eb3c80>\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump4/TSD/participant_0/best_state_5.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump4/TSD/participant_0/best_state_5.pt' (epoch 19)\n",
      "Epoch 0/499\n",
      "----------\n",
      "train Loss: 0.00360463 Acc: 0.84667969\n",
      "val Loss: 0.00086862 Acc: 0.92576419\n",
      "New best validation loss: 0.0008686187485940592\n",
      "Epoch 1 of 500 took 0.114s\n",
      "Epoch 1/499\n",
      "----------\n",
      "train Loss: 0.00244108 Acc: 0.88769531\n",
      "val Loss: 0.00075437 Acc: 0.92139738\n",
      "New best validation loss: 0.0007543666003572889\n",
      "Epoch 2 of 500 took 0.111s\n",
      "Epoch 2/499\n",
      "----------\n",
      "train Loss: 0.00232113 Acc: 0.89355469\n",
      "val Loss: 0.00071620 Acc: 0.930131\n",
      "Epoch 3 of 500 took 0.112s\n",
      "Epoch 3/499\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.00235713 Acc: 0.89111328\n",
      "val Loss: 0.00058866 Acc: 0.95196507\n",
      "New best validation loss: 0.0005886613932238916\n",
      "Epoch 4 of 500 took 0.110s\n",
      "Epoch 4/499\n",
      "----------\n",
      "train Loss: 0.00198372 Acc: 0.90283203\n",
      "val Loss: 0.00052304 Acc: 0.96069869\n",
      "Epoch 5 of 500 took 0.110s\n",
      "Epoch 5/499\n",
      "----------\n",
      "train Loss: 0.00192419 Acc: 0.91357422\n",
      "val Loss: 0.00048069 Acc: 0.95633188\n",
      "New best validation loss: 0.00048068631033709996\n",
      "Epoch 6 of 500 took 0.111s\n",
      "Epoch 6/499\n",
      "----------\n",
      "train Loss: 0.00200203 Acc: 0.90576172\n",
      "val Loss: 0.00067977 Acc: 0.93449782\n",
      "Epoch 7 of 500 took 0.106s\n",
      "Epoch 7/499\n",
      "----------\n",
      "train Loss: 0.00179858 Acc: 0.91845703\n",
      "val Loss: 0.00081746 Acc: 0.90829694\n",
      "Epoch 8 of 500 took 0.111s\n",
      "Epoch 8/499\n",
      "----------\n",
      "train Loss: 0.00178627 Acc: 0.92236328\n",
      "val Loss: 0.00051438 Acc: 0.95196507\n",
      "Epoch 9 of 500 took 0.107s\n",
      "Epoch 9/499\n",
      "----------\n",
      "train Loss: 0.00170822 Acc: 0.92138672\n",
      "val Loss: 0.00055183 Acc: 0.94759825\n",
      "Epoch 10 of 500 took 0.109s\n",
      "Epoch 10/499\n",
      "----------\n",
      "train Loss: 0.00158544 Acc: 0.92041016\n",
      "val Loss: 0.00036030 Acc: 0.9650655\n",
      "New best validation loss: 0.00036029875538755193\n",
      "Epoch 11 of 500 took 0.107s\n",
      "Epoch 11/499\n",
      "----------\n",
      "train Loss: 0.00159534 Acc: 0.92333984\n",
      "val Loss: 0.00034930 Acc: 0.95633188\n",
      "Epoch 12 of 500 took 0.109s\n",
      "Epoch 12/499\n",
      "----------\n",
      "train Loss: 0.00165378 Acc: 0.91748047\n",
      "val Loss: 0.00040355 Acc: 0.9650655\n",
      "Epoch 13 of 500 took 0.106s\n",
      "Epoch 13/499\n",
      "----------\n",
      "train Loss: 0.00151348 Acc: 0.93212891\n",
      "val Loss: 0.00029305 Acc: 0.97816594\n",
      "Epoch 14 of 500 took 0.110s\n",
      "Epoch 14/499\n",
      "----------\n",
      "train Loss: 0.00135607 Acc: 0.93505859\n",
      "val Loss: 0.00033184 Acc: 0.97379913\n",
      "Epoch 15 of 500 took 0.106s\n",
      "Epoch 15/499\n",
      "----------\n",
      "train Loss: 0.00141321 Acc: 0.92871094\n",
      "val Loss: 0.00046729 Acc: 0.95196507\n",
      "Epoch 16 of 500 took 0.109s\n",
      "Epoch 16/499\n",
      "----------\n",
      "train Loss: 0.00132183 Acc: 0.93945312\n",
      "val Loss: 0.00034212 Acc: 0.96943231\n",
      "Epoch 17 of 500 took 0.106s\n",
      "Epoch 17/499\n",
      "----------\n",
      "train Loss: 0.00135087 Acc: 0.93945312\n",
      "val Loss: 0.00042709 Acc: 0.96069869\n",
      "Epoch 18 of 500 took 0.110s\n",
      "Epoch 18/499\n",
      "----------\n",
      "train Loss: 0.00119840 Acc: 0.94189453\n",
      "val Loss: 0.00036783 Acc: 0.9650655\n",
      "Epoch 19 of 500 took 0.107s\n",
      "Epoch 19/499\n",
      "----------\n",
      "train Loss: 0.00136478 Acc: 0.93310547\n",
      "val Loss: 0.00031030 Acc: 0.96943231\n",
      "Epoch    20: reducing learning rate of group 0 to 5.0300e-04.\n",
      "Epoch 20 of 500 took 0.110s\n",
      "Epoch 20/499\n",
      "----------\n",
      "train Loss: 0.00119302 Acc: 0.94238281\n",
      "val Loss: 0.00031664 Acc: 0.97379913\n",
      "Epoch 21 of 500 took 0.107s\n",
      "Epoch 21/499\n",
      "----------\n",
      "train Loss: 0.00104032 Acc: 0.95507812\n",
      "val Loss: 0.00028350 Acc: 0.96943231\n",
      "Epoch 22 of 500 took 0.109s\n",
      "\n",
      "Training complete in 0m 2s\n",
      "Best val loss: 0.000360\n"
     ]
    }
   ],
   "source": [
    "# train_fine_tuning(examples_datasets_train, labels_datasets_train,\n",
    "#                   num_kernels=num_kernels, path_weight_to_save_to=path_TSD,\n",
    "#                   number_of_classes=number_of_classes, \n",
    "#                   number_of_cycles_total=number_of_cycles_total,\n",
    "#                   number_of_cycle_for_first_training = number_of_cycle_for_first_training,\n",
    "#                   batch_size=batch_size,\n",
    "#                   feature_vector_input_length=feature_vector_input_length,\n",
    "#                   learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET one participant_examples  (7,)\n",
      "   GET one training_index_examples  (16, 572, 252)  at  0\n",
      "   GOT one group XY  (9152, 252)    (9152,)\n",
      "       one group XY test  (2288, 252)    (2288, 252)\n",
      "       one group XY train (8236, 252)    (8236,)\n",
      "       one group XY valid (916, 252)    (916, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  1\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  2\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  3\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  4\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  5\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  6\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "dataloaders: \n",
      "   train  (1, 7)\n",
      "   valid  (1, 7)\n",
      "   test  (1, 7)\n",
      "0  SESSION   data =  2288\n",
      "Participant:  0  Accuracy:  0.9020979020979021\n",
      "1  SESSION   data =  572\n",
      "Participant:  0  Accuracy:  0.8548951048951049\n",
      "2  SESSION   data =  572\n",
      "Participant:  0  Accuracy:  0.777972027972028\n",
      "3  SESSION   data =  572\n",
      "Participant:  0  Accuracy:  0.7552447552447552\n",
      "4  SESSION   data =  572\n",
      "Participant:  0  Accuracy:  0.5821678321678322\n",
      "5  SESSION   data =  572\n",
      "Participant:  0  Accuracy:  0.5681818181818182\n",
      "6  SESSION   data =  572\n",
      "Participant:  0  Accuracy:  0.5576923076923077\n",
      "ACCURACY PARTICIPANT  0 :  [0.9020979020979021, 0.8548951048951049, 0.777972027972028, 0.7552447552447552, 0.5821678321678322, 0.5681818181818182, 0.5576923076923077]\n",
      "[array([0.9020979 , 0.8548951 , 0.77797203, 0.75524476, 0.58216783,\n",
      "       0.56818182, 0.55769231])]\n",
      "OVERALL ACCURACY: 0.7140359640359639\n"
     ]
    }
   ],
   "source": [
    "algo_name = \"standard_TSD\"\n",
    "test_standard_model_on_training_sessions(examples_datasets_train, labels_datasets_train,\n",
    "                                  num_neurons=num_kernels, use_only_first_training=True,\n",
    "                                  path_weights=path_TSD,\n",
    "                                  feature_vector_input_length=feature_vector_input_length,\n",
    "                                  save_path = save_TSD, algo_name=algo_name,\n",
    "                                  number_of_cycles_total=number_of_cycles_total,\n",
    "                                  number_of_cycle_for_first_training = number_of_cycle_for_first_training,\n",
    "                                  number_of_classes=number_of_classes, cycle_for_test=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Day_0~3</th>\n",
       "      <td>0.902098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_4</th>\n",
       "      <td>0.854895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_5</th>\n",
       "      <td>0.777972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_6</th>\n",
       "      <td>0.755245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_7</th>\n",
       "      <td>0.582168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_8</th>\n",
       "      <td>0.568182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_9</th>\n",
       "      <td>0.557692</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Participant_5\n",
       "Day_0~3      0.902098\n",
       "Day_4        0.854895\n",
       "Day_5        0.777972\n",
       "Day_6        0.755245\n",
       "Day_7        0.582168\n",
       "Day_8        0.568182\n",
       "Day_9        0.557692"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_filename = save_TSD + '/predictions_' + algo_name + \"_no_retraining.npy\"\n",
    "results = np.load(results_filename, allow_pickle=True)\n",
    "TSD_acc = results[0]\n",
    "TSD_acc_overall = np.mean(TSD_acc)\n",
    "index_participant_list = ['0~3', 4, 5, 6, 7, 8, 9]\n",
    "TSD_df = pd.DataFrame(TSD_acc.transpose(), \n",
    "                       index = [f'Day_{i}' for i in index_participant_list],\n",
    "                        columns = ['Participant_5'])\n",
    "TSD_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "TSD_df.transpose().plot.bar(rot=0, figsize=(10,5))\n",
    "plt.title(\"TSD Accuracies\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TrainingsAndEvaluations.ForTrainingSessions.utils import get_gesture_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ground_truths  =  (1, 7)\n",
      "predictions =  (1, 7)\n",
      "index_participant_list  ['0~3', 4, 5, 6, 7, 8, 9]\n",
      "accuracies_gestures =  (22, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Loc1_Sub5_Day0~3-&gt;0~3</th>\n",
       "      <th>Loc1_Sub5_Day0~3-&gt;4</th>\n",
       "      <th>Loc1_Sub5_Day0~3-&gt;5</th>\n",
       "      <th>Loc1_Sub5_Day0~3-&gt;6</th>\n",
       "      <th>Loc1_Sub5_Day0~3-&gt;7</th>\n",
       "      <th>Loc1_Sub5_Day0~3-&gt;8</th>\n",
       "      <th>Loc1_Sub5_Day0~3-&gt;9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M1</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.769231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M2</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>0.615385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M3</td>\n",
       "      <td>0.942308</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M4</td>\n",
       "      <td>0.788462</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>M5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.346154</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.576923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>M6</td>\n",
       "      <td>0.980769</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.192308</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.615385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>M7</td>\n",
       "      <td>0.971154</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.846154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>M8</td>\n",
       "      <td>0.942308</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.423077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>M9</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.538462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>M10</td>\n",
       "      <td>0.855769</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>0.538462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>M11</td>\n",
       "      <td>0.894231</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.423077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>M12</td>\n",
       "      <td>0.759615</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.307692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>M13</td>\n",
       "      <td>0.788462</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.807692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>M14</td>\n",
       "      <td>0.817308</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>M15</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.269231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>M16</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>M17</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>M18</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.576923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>M19</td>\n",
       "      <td>0.990385</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.423077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>M20</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>0.538462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>M21</td>\n",
       "      <td>0.894231</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Mean</td>\n",
       "      <td>0.902098</td>\n",
       "      <td>0.854895</td>\n",
       "      <td>0.777972</td>\n",
       "      <td>0.755245</td>\n",
       "      <td>0.582168</td>\n",
       "      <td>0.568182</td>\n",
       "      <td>0.557692</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Loc1_Sub5_Day0~3->0~3  Loc1_Sub5_Day0~3->4  \\\n",
       "0          M0               1.000000             1.000000   \n",
       "1          M1               0.961538             0.923077   \n",
       "2          M2               0.923077             1.000000   \n",
       "3          M3               0.942308             0.884615   \n",
       "4          M4               0.788462             1.000000   \n",
       "5          M5               1.000000             0.653846   \n",
       "6          M6               0.980769             0.846154   \n",
       "7          M7               0.971154             1.000000   \n",
       "8          M8               0.942308             1.000000   \n",
       "9          M9               0.875000             0.807692   \n",
       "10        M10               0.855769             0.615385   \n",
       "11        M11               0.894231             0.269231   \n",
       "12        M12               0.759615             0.846154   \n",
       "13        M13               0.788462             1.000000   \n",
       "14        M14               0.817308             0.884615   \n",
       "15        M15               0.653846             0.423077   \n",
       "16        M16               0.961538             1.000000   \n",
       "17        M17               1.000000             1.000000   \n",
       "18        M18               1.000000             0.961538   \n",
       "19        M19               0.990385             1.000000   \n",
       "20        M20               0.846154             0.961538   \n",
       "21        M21               0.894231             0.730769   \n",
       "22       Mean               0.902098             0.854895   \n",
       "\n",
       "    Loc1_Sub5_Day0~3->5  Loc1_Sub5_Day0~3->6  Loc1_Sub5_Day0~3->7  \\\n",
       "0              1.000000             1.000000             1.000000   \n",
       "1              1.000000             0.807692             0.576923   \n",
       "2              0.538462             0.692308             0.576923   \n",
       "3              0.884615             0.653846             0.000000   \n",
       "4              0.500000             0.000000             0.000000   \n",
       "5              0.730769             1.000000             0.346154   \n",
       "6              0.384615             0.461538             0.192308   \n",
       "7              0.846154             1.000000             0.961538   \n",
       "8              1.000000             1.000000             0.730769   \n",
       "9              0.615385             0.923077             0.692308   \n",
       "10             0.884615             0.884615             0.653846   \n",
       "11             0.500000             0.884615             0.230769   \n",
       "12             0.730769             0.538462             0.538462   \n",
       "13             1.000000             1.000000             1.000000   \n",
       "14             0.500000             0.230769             0.269231   \n",
       "15             0.423077             0.038462             0.000000   \n",
       "16             1.000000             1.000000             1.000000   \n",
       "17             1.000000             1.000000             1.000000   \n",
       "18             0.807692             0.576923             0.769231   \n",
       "19             0.846154             1.000000             0.269231   \n",
       "20             0.923077             0.923077             1.000000   \n",
       "21             1.000000             1.000000             1.000000   \n",
       "22             0.777972             0.755245             0.582168   \n",
       "\n",
       "    Loc1_Sub5_Day0~3->8  Loc1_Sub5_Day0~3->9  \n",
       "0              1.000000             1.000000  \n",
       "1              0.615385             0.769231  \n",
       "2              0.653846             0.615385  \n",
       "3              0.115385             0.000000  \n",
       "4              0.000000             0.000000  \n",
       "5              0.923077             0.576923  \n",
       "6              0.153846             0.615385  \n",
       "7              0.730769             0.846154  \n",
       "8              0.730769             0.423077  \n",
       "9              0.807692             0.538462  \n",
       "10             0.653846             0.538462  \n",
       "11             0.461538             0.423077  \n",
       "12             0.576923             0.307692  \n",
       "13             1.000000             0.807692  \n",
       "14             0.000000             0.000000  \n",
       "15             0.000000             0.269231  \n",
       "16             1.000000             1.000000  \n",
       "17             0.961538             1.000000  \n",
       "18             0.076923             0.576923  \n",
       "19             0.384615             0.423077  \n",
       "20             0.653846             0.538462  \n",
       "21             1.000000             1.000000  \n",
       "22             0.568182             0.557692  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truths = results[1]\n",
    "predictions = results[2]\n",
    "print(\"ground_truths  = \", np.shape(ground_truths))\n",
    "print(\"predictions = \", np.shape(predictions))\n",
    "m_name = \"Loc1_Sub\"\n",
    "n_name = \"Day0~3->\"\n",
    "df = get_gesture_accuracies(ground_truths, predictions, number_of_classes=number_of_classes, \n",
    "                            m_name=m_name, n_name=n_name, path=save_TSD, algo_name=algo_name,\n",
    "                           index_participant_list_customized=index_participant_list,\n",
    "                           lump_day_at_participant=5)\n",
    "df = pd.read_csv(save_TSD+'/'+algo_name+'.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. DANN\n",
    "* `train_DANN`: train DANN model using the first set of training weights from base model\n",
    "    * num_sessions-1 sets of training weights will be saved\n",
    "* `test_DANN_on_training_sessions`: test DANN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TrainingsAndEvaluations.ForTrainingSessions.train_tsd_dnn_DA import train_DANN, test_DANN_on_training_sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET one participant_examples  (7,)\n",
      "   GET one training_index_examples  (16, 572, 252)  at  0\n",
      "   GOT one group XY  (9152, 252)    (9152,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (8236, 252)    (8236,)\n",
      "       one group XY valid (916, 252)    (916, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  1\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  2\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  3\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  4\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  5\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  6\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "dataloaders: \n",
      "   train  (1, 7)\n",
      "   valid  (1, 7)\n",
      "   test  (1, 0)\n",
      "SHAPE SESSIONS:  (7,)\n",
      "()\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump4/TSD/participant_0/best_state_0.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump4/TSD/participant_0/best_state_0.pt' (epoch 5)\n",
      "STARTING TRAINING\n",
      "Accuracy source 0.855957, main loss classifier 0.297460, source classification loss 0.438298, loss domain distinction 0.245978, accuracy domain distinction 0.491943\n",
      "VALIDATION Loss: 0.29273424 Acc: 0.88864629\n",
      "New best validation loss:  0.2927342355251312\n",
      "Epoch 1 of 500 took 0.248s\n",
      "Accuracy source 0.836426, main loss classifier 0.302843, source classification loss 0.458569, loss domain distinction 0.192159, accuracy domain distinction 0.499512\n",
      "VALIDATION Loss: 0.29157096 Acc: 0.89082969\n",
      "New best validation loss:  0.2915709614753723\n",
      "Epoch 2 of 500 took 0.286s\n",
      "Accuracy source 0.833008, main loss classifier 0.302518, source classification loss 0.459709, loss domain distinction 0.189364, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.44941619 Acc: 0.83406114\n",
      "Epoch 3 of 500 took 0.240s\n",
      "Accuracy source 0.847168, main loss classifier 0.293710, source classification loss 0.443569, loss domain distinction 0.188494, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.41148657 Acc: 0.84825328\n",
      "Epoch 4 of 500 took 0.239s\n",
      "Accuracy source 0.848145, main loss classifier 0.302077, source classification loss 0.461462, loss domain distinction 0.187221, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.42202592 Acc: 0.83733624\n",
      "Epoch 5 of 500 took 0.237s\n",
      "Accuracy source 0.850098, main loss classifier 0.283261, source classification loss 0.424290, loss domain distinction 0.187024, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.29210022 Acc: 0.89847162\n",
      "Epoch    11: reducing learning rate of group 0 to 5.0300e-04.\n",
      "Epoch 6 of 500 took 0.239s\n",
      "Accuracy source 0.874023, main loss classifier 0.248912, source classification loss 0.361009, loss domain distinction 0.182087, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.26104119 Acc: 0.91048035\n",
      "New best validation loss:  0.26104119420051575\n",
      "Epoch 7 of 500 took 0.241s\n",
      "Accuracy source 0.871094, main loss classifier 0.247774, source classification loss 0.358534, loss domain distinction 0.183419, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.26676103 Acc: 0.90829694\n",
      "Epoch 8 of 500 took 0.235s\n",
      "Accuracy source 0.873047, main loss classifier 0.246985, source classification loss 0.357419, loss domain distinction 0.182353, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.25667027 Acc: 0.90829694\n",
      "New best validation loss:  0.2566702663898468\n",
      "Epoch 9 of 500 took 0.237s\n",
      "Accuracy source 0.879395, main loss classifier 0.239723, source classification loss 0.342625, loss domain distinction 0.182756, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.26519400 Acc: 0.90611354\n",
      "Epoch 10 of 500 took 0.241s\n",
      "Accuracy source 0.886230, main loss classifier 0.239239, source classification loss 0.341345, loss domain distinction 0.184083, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.26946968 Acc: 0.90393013\n",
      "Epoch 11 of 500 took 0.240s\n",
      "Accuracy source 0.876465, main loss classifier 0.234484, source classification loss 0.332237, loss domain distinction 0.181753, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.24002041 Acc: 0.92139738\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0060e-04.\n",
      "New best validation loss:  0.24002040922641754\n",
      "Epoch 12 of 500 took 0.238s\n",
      "Accuracy source 0.879883, main loss classifier 0.235779, source classification loss 0.335962, loss domain distinction 0.183165, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.32584581 Acc: 0.87991266\n",
      "Epoch 13 of 500 took 0.237s\n",
      "Accuracy source 0.890137, main loss classifier 0.225164, source classification loss 0.314813, loss domain distinction 0.183403, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.25882575 Acc: 0.90611354\n",
      "Epoch 14 of 500 took 0.239s\n",
      "Accuracy source 0.895996, main loss classifier 0.220239, source classification loss 0.304653, loss domain distinction 0.183064, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.26410630 Acc: 0.90720524\n",
      "Epoch 15 of 500 took 0.234s\n",
      "Accuracy source 0.906738, main loss classifier 0.221289, source classification loss 0.307199, loss domain distinction 0.181788, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.26301315 Acc: 0.90502183\n",
      "Epoch 16 of 500 took 0.234s\n",
      "Accuracy source 0.892578, main loss classifier 0.221867, source classification loss 0.308968, loss domain distinction 0.180411, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.25639594 Acc: 0.90502183\n",
      "Epoch 17 of 500 took 0.233s\n",
      "Accuracy source 0.884277, main loss classifier 0.232849, source classification loss 0.330069, loss domain distinction 0.182223, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.25072441 Acc: 0.90283843\n",
      "Epoch 18 of 500 took 0.232s\n",
      "Accuracy source 0.890137, main loss classifier 0.227736, source classification loss 0.320373, loss domain distinction 0.181538, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.22563444 Acc: 0.91921397\n",
      "New best validation loss:  0.22563444077968597\n",
      "Epoch 19 of 500 took 0.237s\n",
      "Accuracy source 0.899414, main loss classifier 0.218882, source classification loss 0.302026, loss domain distinction 0.183065, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.22494420 Acc: 0.91812227\n",
      "New best validation loss:  0.22494420409202576\n",
      "Epoch 20 of 500 took 0.256s\n",
      "Accuracy source 0.894531, main loss classifier 0.226323, source classification loss 0.318013, loss domain distinction 0.179990, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.22263086 Acc: 0.91375546\n",
      "New best validation loss:  0.22263085842132568\n",
      "Epoch 21 of 500 took 0.238s\n",
      "Accuracy source 0.895020, main loss classifier 0.220944, source classification loss 0.307029, loss domain distinction 0.181071, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.23303227 Acc: 0.91484716\n",
      "Epoch 22 of 500 took 0.260s\n",
      "Accuracy source 0.880371, main loss classifier 0.225522, source classification loss 0.315669, loss domain distinction 0.181321, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.21383238 Acc: 0.92358079\n",
      "New best validation loss:  0.21383237838745117\n",
      "Epoch 23 of 500 took 0.235s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy source 0.902832, main loss classifier 0.216831, source classification loss 0.298738, loss domain distinction 0.180297, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.25356966 Acc: 0.91157205\n",
      "Epoch 24 of 500 took 0.239s\n",
      "Accuracy source 0.900391, main loss classifier 0.216901, source classification loss 0.299097, loss domain distinction 0.180249, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.20968983 Acc: 0.9268559\n",
      "New best validation loss:  0.20968982577323914\n",
      "Epoch 25 of 500 took 0.236s\n",
      "Accuracy source 0.877930, main loss classifier 0.239814, source classification loss 0.344222, loss domain distinction 0.182970, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.24298152 Acc: 0.91484716\n",
      "Epoch 26 of 500 took 0.234s\n",
      "Accuracy source 0.882812, main loss classifier 0.228190, source classification loss 0.320998, loss domain distinction 0.181584, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.21509655 Acc: 0.91921397\n",
      "Epoch 27 of 500 took 0.233s\n",
      "Accuracy source 0.879883, main loss classifier 0.236196, source classification loss 0.337075, loss domain distinction 0.182124, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.23351972 Acc: 0.91484716\n",
      "Epoch 28 of 500 took 0.238s\n",
      "Accuracy source 0.892090, main loss classifier 0.227527, source classification loss 0.319850, loss domain distinction 0.181723, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.23972756 Acc: 0.91812227\n",
      "Epoch 29 of 500 took 0.233s\n",
      "Accuracy source 0.899414, main loss classifier 0.218927, source classification loss 0.302304, loss domain distinction 0.181571, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.21026836 Acc: 0.91812227\n",
      "Epoch 30 of 500 took 0.233s\n",
      "Accuracy source 0.895996, main loss classifier 0.212634, source classification loss 0.289977, loss domain distinction 0.182711, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.24704775 Acc: 0.91593886\n",
      "Epoch 31 of 500 took 0.240s\n",
      "Accuracy source 0.890137, main loss classifier 0.222571, source classification loss 0.309758, loss domain distinction 0.182145, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.23100702 Acc: 0.91812227\n",
      "Epoch 32 of 500 took 0.236s\n",
      "Accuracy source 0.893066, main loss classifier 0.222238, source classification loss 0.308699, loss domain distinction 0.183941, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.20976314 Acc: 0.93122271\n",
      "Epoch 33 of 500 took 0.234s\n",
      "Accuracy source 0.894043, main loss classifier 0.211530, source classification loss 0.287088, loss domain distinction 0.182622, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.25296277 Acc: 0.90502183\n",
      "Epoch 34 of 500 took 0.234s\n",
      "Accuracy source 0.895996, main loss classifier 0.224478, source classification loss 0.313023, loss domain distinction 0.184256, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.22031595 Acc: 0.9268559\n",
      "Epoch 35 of 500 took 0.233s\n",
      "Accuracy source 0.907227, main loss classifier 0.210716, source classification loss 0.286330, loss domain distinction 0.181530, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.23693280 Acc: 0.91157205\n",
      "Training complete in 0m 9s\n",
      "()\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump4/TSD/participant_0/best_state_0.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump4/TSD/participant_0/best_state_0.pt' (epoch 5)\n",
      "STARTING TRAINING\n",
      "Accuracy source 0.842285, main loss classifier 0.300847, source classification loss 0.445618, loss domain distinction 0.243086, accuracy domain distinction 0.487305\n",
      "VALIDATION Loss: 0.27408612 Acc: 0.90720524\n",
      "New best validation loss:  0.2740861177444458\n",
      "Epoch 1 of 500 took 0.238s\n",
      "Accuracy source 0.843262, main loss classifier 0.300270, source classification loss 0.454506, loss domain distinction 0.191040, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.29923931 Acc: 0.88646288\n",
      "Epoch 2 of 500 took 0.263s\n",
      "Accuracy source 0.849121, main loss classifier 0.298858, source classification loss 0.452585, loss domain distinction 0.188854, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.43917477 Acc: 0.84497817\n",
      "Epoch 3 of 500 took 0.276s\n",
      "Accuracy source 0.851562, main loss classifier 0.282168, source classification loss 0.421020, loss domain distinction 0.186940, accuracy domain distinction 0.499756\n",
      "VALIDATION Loss: 0.27738661 Acc: 0.90065502\n",
      "Epoch 4 of 500 took 0.270s\n",
      "Accuracy source 0.873535, main loss classifier 0.266981, source classification loss 0.389975, loss domain distinction 0.190172, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.25990063 Acc: 0.91375546\n",
      "New best validation loss:  0.25990062952041626\n",
      "Epoch 5 of 500 took 0.278s\n",
      "Accuracy source 0.855469, main loss classifier 0.277364, source classification loss 0.412647, loss domain distinction 0.187515, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.22930473 Acc: 0.91812227\n",
      "Epoch    11: reducing learning rate of group 0 to 5.0300e-04.\n",
      "New best validation loss:  0.2293047308921814\n",
      "Epoch 6 of 500 took 0.276s\n",
      "Accuracy source 0.865723, main loss classifier 0.261281, source classification loss 0.385629, loss domain distinction 0.185396, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.23067908 Acc: 0.91484716\n",
      "Epoch 7 of 500 took 0.277s\n",
      "Accuracy source 0.873047, main loss classifier 0.255181, source classification loss 0.372969, loss domain distinction 0.185020, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.23143460 Acc: 0.91484716\n",
      "Epoch 8 of 500 took 0.259s\n",
      "Accuracy source 0.893066, main loss classifier 0.232005, source classification loss 0.326683, loss domain distinction 0.184290, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.24557501 Acc: 0.91593886\n",
      "Epoch 9 of 500 took 0.233s\n",
      "Accuracy source 0.873535, main loss classifier 0.246705, source classification loss 0.356882, loss domain distinction 0.182590, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.26241526 Acc: 0.89956332\n",
      "Epoch 10 of 500 took 0.233s\n",
      "Accuracy source 0.876465, main loss classifier 0.245113, source classification loss 0.352898, loss domain distinction 0.185397, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.26216137 Acc: 0.89628821\n",
      "Epoch 11 of 500 took 0.283s\n",
      "Accuracy source 0.880859, main loss classifier 0.243151, source classification loss 0.349426, loss domain distinction 0.184643, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.24051581 Acc: 0.91375546\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0060e-04.\n",
      "Epoch 12 of 500 took 0.237s\n",
      "Accuracy source 0.892578, main loss classifier 0.219141, source classification loss 0.302530, loss domain distinction 0.181831, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.21909003 Acc: 0.91266376\n",
      "New best validation loss:  0.2190900295972824\n",
      "Epoch 13 of 500 took 0.235s\n",
      "Accuracy source 0.886230, main loss classifier 0.222518, source classification loss 0.309226, loss domain distinction 0.183417, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.24239208 Acc: 0.91266376\n",
      "Epoch 14 of 500 took 0.234s\n",
      "Accuracy source 0.880371, main loss classifier 0.226771, source classification loss 0.317800, loss domain distinction 0.182279, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.25844097 Acc: 0.90283843\n",
      "Epoch 15 of 500 took 0.240s\n",
      "Accuracy source 0.882812, main loss classifier 0.231104, source classification loss 0.327027, loss domain distinction 0.181257, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.26322532 Acc: 0.90283843\n",
      "Epoch 16 of 500 took 0.233s\n",
      "Accuracy source 0.893555, main loss classifier 0.224461, source classification loss 0.313417, loss domain distinction 0.182802, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.26441005 Acc: 0.90611354\n",
      "Epoch 17 of 500 took 0.236s\n",
      "Accuracy source 0.886230, main loss classifier 0.225778, source classification loss 0.315513, loss domain distinction 0.182523, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.24202237 Acc: 0.90938865\n",
      "Epoch 18 of 500 took 0.234s\n",
      "Accuracy source 0.890137, main loss classifier 0.224416, source classification loss 0.313211, loss domain distinction 0.182878, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.24427295 Acc: 0.91266376\n",
      "Epoch 19 of 500 took 0.235s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy source 0.895020, main loss classifier 0.224241, source classification loss 0.313114, loss domain distinction 0.182254, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.21621184 Acc: 0.91812227\n",
      "New best validation loss:  0.21621184051036835\n",
      "Epoch 20 of 500 took 0.235s\n",
      "Accuracy source 0.883301, main loss classifier 0.232545, source classification loss 0.328937, loss domain distinction 0.183650, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.21823618 Acc: 0.91921397\n",
      "Epoch 21 of 500 took 0.235s\n",
      "Accuracy source 0.896484, main loss classifier 0.223294, source classification loss 0.310689, loss domain distinction 0.182511, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.23975188 Acc: 0.91703057\n",
      "Epoch 22 of 500 took 0.232s\n",
      "Accuracy source 0.896973, main loss classifier 0.214810, source classification loss 0.294075, loss domain distinction 0.182109, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.24167511 Acc: 0.90502183\n",
      "Epoch 23 of 500 took 0.234s\n",
      "Accuracy source 0.889160, main loss classifier 0.228262, source classification loss 0.320984, loss domain distinction 0.182922, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.24510589 Acc: 0.92139738\n",
      "Epoch 24 of 500 took 0.234s\n",
      "Accuracy source 0.888184, main loss classifier 0.227357, source classification loss 0.318539, loss domain distinction 0.184276, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.23850821 Acc: 0.90720524\n",
      "Epoch 25 of 500 took 0.235s\n",
      "Accuracy source 0.883789, main loss classifier 0.226853, source classification loss 0.318216, loss domain distinction 0.182613, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.21338621 Acc: 0.9268559\n",
      "New best validation loss:  0.21338620781898499\n",
      "Epoch 26 of 500 took 0.235s\n",
      "Accuracy source 0.897949, main loss classifier 0.211422, source classification loss 0.287205, loss domain distinction 0.182356, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.23038106 Acc: 0.91703057\n",
      "Epoch 27 of 500 took 0.233s\n",
      "Accuracy source 0.887207, main loss classifier 0.224394, source classification loss 0.313257, loss domain distinction 0.182033, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.24102797 Acc: 0.90393013\n",
      "Epoch 28 of 500 took 0.234s\n",
      "Accuracy source 0.882812, main loss classifier 0.228351, source classification loss 0.321362, loss domain distinction 0.182613, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.22785507 Acc: 0.91157205\n",
      "Epoch 29 of 500 took 0.234s\n",
      "Accuracy source 0.890625, main loss classifier 0.223334, source classification loss 0.311097, loss domain distinction 0.182568, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.25519645 Acc: 0.90938865\n",
      "Epoch 30 of 500 took 0.237s\n",
      "Accuracy source 0.881348, main loss classifier 0.234062, source classification loss 0.332758, loss domain distinction 0.182173, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.22017075 Acc: 0.91812227\n",
      "Epoch 31 of 500 took 0.233s\n",
      "Accuracy source 0.888184, main loss classifier 0.222610, source classification loss 0.310035, loss domain distinction 0.182482, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.21729787 Acc: 0.91484716\n",
      "Epoch 32 of 500 took 0.236s\n",
      "Accuracy source 0.889648, main loss classifier 0.233084, source classification loss 0.330401, loss domain distinction 0.182763, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.23200828 Acc: 0.91048035\n",
      "Epoch 33 of 500 took 0.262s\n",
      "Accuracy source 0.899902, main loss classifier 0.214836, source classification loss 0.294576, loss domain distinction 0.182349, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.22642627 Acc: 0.91484716\n",
      "Epoch 34 of 500 took 0.236s\n",
      "Accuracy source 0.890625, main loss classifier 0.221871, source classification loss 0.308266, loss domain distinction 0.182145, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.21743773 Acc: 0.91921397\n",
      "Epoch 35 of 500 took 0.232s\n",
      "Accuracy source 0.899414, main loss classifier 0.215381, source classification loss 0.295228, loss domain distinction 0.182099, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.21756965 Acc: 0.91593886\n",
      "Epoch 36 of 500 took 0.240s\n",
      "Accuracy source 0.900391, main loss classifier 0.215226, source classification loss 0.294883, loss domain distinction 0.182384, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.22079761 Acc: 0.91921397\n",
      "Training complete in 0m 9s\n",
      "()\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump4/TSD/participant_0/best_state_0.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump4/TSD/participant_0/best_state_0.pt' (epoch 5)\n",
      "STARTING TRAINING\n",
      "Accuracy source 0.848145, main loss classifier 0.299508, source classification loss 0.444153, loss domain distinction 0.236880, accuracy domain distinction 0.493652\n",
      "VALIDATION Loss: 0.36306491 Acc: 0.87008734\n",
      "New best validation loss:  0.3630649149417877\n",
      "Epoch 1 of 500 took 0.237s\n",
      "Accuracy source 0.847168, main loss classifier 0.287125, source classification loss 0.429165, loss domain distinction 0.187633, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.51418471 Acc: 0.8209607\n",
      "Epoch 2 of 500 took 0.234s\n",
      "Accuracy source 0.856934, main loss classifier 0.284251, source classification loss 0.425143, loss domain distinction 0.187072, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.58268219 Acc: 0.80021834\n",
      "Epoch 3 of 500 took 0.233s\n",
      "Accuracy source 0.835938, main loss classifier 0.308353, source classification loss 0.472956, loss domain distinction 0.187720, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.39287207 Acc: 0.86572052\n",
      "Epoch 4 of 500 took 0.235s\n",
      "Accuracy source 0.846680, main loss classifier 0.287829, source classification loss 0.433284, loss domain distinction 0.187028, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.35173979 Acc: 0.87336245\n",
      "New best validation loss:  0.3517397940158844\n",
      "Epoch 5 of 500 took 0.236s\n",
      "Accuracy source 0.846680, main loss classifier 0.286232, source classification loss 0.430401, loss domain distinction 0.187511, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.42494392 Acc: 0.84388646\n",
      "Epoch    11: reducing learning rate of group 0 to 5.0300e-04.\n",
      "Epoch 6 of 500 took 0.232s\n",
      "Accuracy source 0.858398, main loss classifier 0.267951, source classification loss 0.399355, loss domain distinction 0.182399, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.33029538 Acc: 0.87663755\n",
      "New best validation loss:  0.3302953839302063\n",
      "Epoch 7 of 500 took 0.235s\n",
      "Accuracy source 0.875488, main loss classifier 0.248182, source classification loss 0.360910, loss domain distinction 0.181520, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.34070224 Acc: 0.87336245\n",
      "Epoch 8 of 500 took 0.232s\n",
      "Accuracy source 0.881348, main loss classifier 0.235123, source classification loss 0.333705, loss domain distinction 0.181710, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.30865517 Acc: 0.88100437\n",
      "New best validation loss:  0.30865517258644104\n",
      "Epoch 9 of 500 took 0.243s\n",
      "Accuracy source 0.879395, main loss classifier 0.246286, source classification loss 0.356332, loss domain distinction 0.181965, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.29969004 Acc: 0.8919214\n",
      "New best validation loss:  0.29969003796577454\n",
      "Epoch 10 of 500 took 0.273s\n",
      "Accuracy source 0.880371, main loss classifier 0.242924, source classification loss 0.349310, loss domain distinction 0.182858, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.33483660 Acc: 0.87772926\n",
      "Epoch 11 of 500 took 0.234s\n",
      "Accuracy source 0.883789, main loss classifier 0.233792, source classification loss 0.330273, loss domain distinction 0.184341, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.33569437 Acc: 0.86790393\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0060e-04.\n",
      "Epoch 12 of 500 took 0.235s\n",
      "Accuracy source 0.899414, main loss classifier 0.221080, source classification loss 0.307519, loss domain distinction 0.181271, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.28846112 Acc: 0.8919214\n",
      "New best validation loss:  0.2884611189365387\n",
      "Epoch 13 of 500 took 0.234s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy source 0.879883, main loss classifier 0.236503, source classification loss 0.337756, loss domain distinction 0.181456, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.34048954 Acc: 0.87554585\n",
      "Epoch 14 of 500 took 0.233s\n",
      "Accuracy source 0.885742, main loss classifier 0.225571, source classification loss 0.315392, loss domain distinction 0.182924, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.30424255 Acc: 0.87772926\n",
      "Epoch 15 of 500 took 0.234s\n",
      "Accuracy source 0.891602, main loss classifier 0.218309, source classification loss 0.301323, loss domain distinction 0.180670, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.27768800 Acc: 0.8919214\n",
      "New best validation loss:  0.27768799662590027\n",
      "Epoch 16 of 500 took 0.235s\n",
      "Accuracy source 0.896973, main loss classifier 0.216492, source classification loss 0.297376, loss domain distinction 0.180431, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.25806120 Acc: 0.90393013\n",
      "New best validation loss:  0.2580612003803253\n",
      "Epoch 17 of 500 took 0.234s\n",
      "Accuracy source 0.900391, main loss classifier 0.217049, source classification loss 0.299871, loss domain distinction 0.179815, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.26627049 Acc: 0.90283843\n",
      "Epoch 18 of 500 took 0.241s\n",
      "Accuracy source 0.881836, main loss classifier 0.233658, source classification loss 0.332623, loss domain distinction 0.180747, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.23114724 Acc: 0.91157205\n",
      "New best validation loss:  0.23114724457263947\n",
      "Epoch 19 of 500 took 0.234s\n",
      "Accuracy source 0.895020, main loss classifier 0.225206, source classification loss 0.315137, loss domain distinction 0.180776, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.28501430 Acc: 0.8919214\n",
      "Epoch 20 of 500 took 0.240s\n",
      "Accuracy source 0.884766, main loss classifier 0.227212, source classification loss 0.319708, loss domain distinction 0.180101, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.27660593 Acc: 0.8919214\n",
      "Epoch 21 of 500 took 0.289s\n",
      "Accuracy source 0.900391, main loss classifier 0.216803, source classification loss 0.298459, loss domain distinction 0.182160, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.35179204 Acc: 0.86462882\n",
      "Epoch 22 of 500 took 0.252s\n",
      "Accuracy source 0.900391, main loss classifier 0.216592, source classification loss 0.297804, loss domain distinction 0.182449, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.33140868 Acc: 0.87336245\n",
      "Epoch 23 of 500 took 0.247s\n",
      "Accuracy source 0.904297, main loss classifier 0.211758, source classification loss 0.288446, loss domain distinction 0.180884, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.35247341 Acc: 0.87008734\n",
      "Epoch 24 of 500 took 0.235s\n",
      "Accuracy source 0.901367, main loss classifier 0.217183, source classification loss 0.299381, loss domain distinction 0.182216, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.32126668 Acc: 0.87663755\n",
      "Epoch 25 of 500 took 0.238s\n",
      "Accuracy source 0.886719, main loss classifier 0.228271, source classification loss 0.321419, loss domain distinction 0.182904, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.30043730 Acc: 0.8919214\n",
      "Epoch 26 of 500 took 0.244s\n",
      "Accuracy source 0.895508, main loss classifier 0.222878, source classification loss 0.310438, loss domain distinction 0.182033, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.38790858 Acc: 0.8569869\n",
      "Epoch 27 of 500 took 0.234s\n",
      "Accuracy source 0.890625, main loss classifier 0.215628, source classification loss 0.296662, loss domain distinction 0.180506, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.33625633 Acc: 0.87008734\n",
      "Epoch 28 of 500 took 0.243s\n",
      "Accuracy source 0.894531, main loss classifier 0.223993, source classification loss 0.313034, loss domain distinction 0.179811, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.30205199 Acc: 0.88100437\n",
      "Epoch 29 of 500 took 0.240s\n",
      "Accuracy source 0.896973, main loss classifier 0.215713, source classification loss 0.296880, loss domain distinction 0.181551, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.27573204 Acc: 0.8930131\n",
      "Training complete in 0m 7s\n",
      "()\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump4/TSD/participant_0/best_state_0.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump4/TSD/participant_0/best_state_0.pt' (epoch 5)\n",
      "STARTING TRAINING\n",
      "Accuracy source 0.841797, main loss classifier 0.298772, source classification loss 0.441842, loss domain distinction 0.241095, accuracy domain distinction 0.492188\n",
      "VALIDATION Loss: 0.47688597 Acc: 0.84279476\n",
      "New best validation loss:  0.47688597440719604\n",
      "Epoch 1 of 500 took 0.282s\n",
      "Accuracy source 0.852539, main loss classifier 0.282383, source classification loss 0.417954, loss domain distinction 0.189100, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.60653275 Acc: 0.80240175\n",
      "Epoch 2 of 500 took 0.297s\n",
      "Accuracy source 0.827637, main loss classifier 0.312071, source classification loss 0.479921, loss domain distinction 0.187766, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.40157479 Acc: 0.8558952\n",
      "New best validation loss:  0.4015747904777527\n",
      "Epoch 3 of 500 took 0.280s\n",
      "Accuracy source 0.849609, main loss classifier 0.297206, source classification loss 0.452379, loss domain distinction 0.186433, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.36813852 Acc: 0.86462882\n",
      "New best validation loss:  0.36813852190971375\n",
      "Epoch 4 of 500 took 0.301s\n",
      "Accuracy source 0.855469, main loss classifier 0.268513, source classification loss 0.396235, loss domain distinction 0.184459, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.46768913 Acc: 0.84170306\n",
      "Epoch 5 of 500 took 0.254s\n",
      "Accuracy source 0.853516, main loss classifier 0.277155, source classification loss 0.413367, loss domain distinction 0.186003, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.44839665 Acc: 0.83733624\n",
      "Epoch    11: reducing learning rate of group 0 to 5.0300e-04.\n",
      "Epoch 6 of 500 took 0.273s\n",
      "Accuracy source 0.863281, main loss classifier 0.269861, source classification loss 0.403538, loss domain distinction 0.184578, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.42930526 Acc: 0.84934498\n",
      "Epoch 7 of 500 took 0.291s\n",
      "Accuracy source 0.874512, main loss classifier 0.253396, source classification loss 0.370974, loss domain distinction 0.181959, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.54040170 Acc: 0.82532751\n",
      "Epoch 8 of 500 took 0.275s\n",
      "Accuracy source 0.879395, main loss classifier 0.240933, source classification loss 0.345546, loss domain distinction 0.181579, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.41351923 Acc: 0.8558952\n",
      "Epoch 9 of 500 took 0.277s\n",
      "Accuracy source 0.878418, main loss classifier 0.236795, source classification loss 0.337353, loss domain distinction 0.181340, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.54072678 Acc: 0.82860262\n",
      "Epoch 10 of 500 took 0.235s\n",
      "Accuracy source 0.878418, main loss classifier 0.235621, source classification loss 0.335286, loss domain distinction 0.182247, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.48437753 Acc: 0.84061135\n",
      "Epoch 11 of 500 took 0.245s\n",
      "Accuracy source 0.887207, main loss classifier 0.233039, source classification loss 0.330114, loss domain distinction 0.181166, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.51483166 Acc: 0.82969432\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0060e-04.\n",
      "Epoch 12 of 500 took 0.283s\n",
      "Accuracy source 0.885742, main loss classifier 0.226584, source classification loss 0.317840, loss domain distinction 0.182027, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.29935485 Acc: 0.88973799\n",
      "New best validation loss:  0.2993548512458801\n",
      "Epoch 13 of 500 took 0.236s\n",
      "Accuracy source 0.882324, main loss classifier 0.239531, source classification loss 0.344262, loss domain distinction 0.180774, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.42030719 Acc: 0.8558952\n",
      "Epoch 14 of 500 took 0.237s\n",
      "Accuracy source 0.891602, main loss classifier 0.232603, source classification loss 0.330143, loss domain distinction 0.180838, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.34622222 Acc: 0.86681223\n",
      "Epoch 15 of 500 took 0.236s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy source 0.887207, main loss classifier 0.228274, source classification loss 0.321077, loss domain distinction 0.179640, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.34012339 Acc: 0.87445415\n",
      "Epoch 16 of 500 took 0.238s\n",
      "Accuracy source 0.877441, main loss classifier 0.241265, source classification loss 0.348016, loss domain distinction 0.180014, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.36722666 Acc: 0.87117904\n",
      "Epoch 17 of 500 took 0.235s\n",
      "Accuracy source 0.884277, main loss classifier 0.237195, source classification loss 0.339157, loss domain distinction 0.182213, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.39937311 Acc: 0.8558952\n",
      "Epoch 18 of 500 took 0.234s\n",
      "Accuracy source 0.892090, main loss classifier 0.228607, source classification loss 0.321994, loss domain distinction 0.181315, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.39240536 Acc: 0.8580786\n",
      "Epoch 19 of 500 took 0.235s\n",
      "Accuracy source 0.889160, main loss classifier 0.228188, source classification loss 0.321334, loss domain distinction 0.179037, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.34708649 Acc: 0.86681223\n",
      "Epoch 20 of 500 took 0.266s\n",
      "Accuracy source 0.892578, main loss classifier 0.230132, source classification loss 0.325879, loss domain distinction 0.179083, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.36817265 Acc: 0.86462882\n",
      "Epoch 21 of 500 took 0.266s\n",
      "Accuracy source 0.882812, main loss classifier 0.225893, source classification loss 0.316488, loss domain distinction 0.181312, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.52160996 Acc: 0.83842795\n",
      "Epoch 22 of 500 took 0.264s\n",
      "Accuracy source 0.897949, main loss classifier 0.220132, source classification loss 0.305073, loss domain distinction 0.181580, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.36654872 Acc: 0.86353712\n",
      "Epoch 23 of 500 took 0.275s\n",
      "Accuracy source 0.886719, main loss classifier 0.225457, source classification loss 0.316127, loss domain distinction 0.180698, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.42832035 Acc: 0.85480349\n",
      "Training complete in 0m 6s\n",
      "()\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump4/TSD/participant_0/best_state_0.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump4/TSD/participant_0/best_state_0.pt' (epoch 5)\n",
      "STARTING TRAINING\n",
      "Accuracy source 0.841309, main loss classifier 0.305508, source classification loss 0.455931, loss domain distinction 0.240815, accuracy domain distinction 0.493896\n",
      "VALIDATION Loss: 0.43931732 Acc: 0.83187773\n",
      "New best validation loss:  0.4393173158168793\n",
      "Epoch 1 of 500 took 0.264s\n",
      "Accuracy source 0.845215, main loss classifier 0.296488, source classification loss 0.446805, loss domain distinction 0.189226, accuracy domain distinction 0.499756\n",
      "VALIDATION Loss: 0.35692734 Acc: 0.87772926\n",
      "New best validation loss:  0.3569273352622986\n",
      "Epoch 2 of 500 took 0.239s\n",
      "Accuracy source 0.842285, main loss classifier 0.301723, source classification loss 0.459808, loss domain distinction 0.187156, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.44686499 Acc: 0.82969432\n",
      "Epoch 3 of 500 took 0.239s\n",
      "Accuracy source 0.842773, main loss classifier 0.292806, source classification loss 0.444590, loss domain distinction 0.184000, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.34970024 Acc: 0.87554585\n",
      "New best validation loss:  0.3497002422809601\n",
      "Epoch 4 of 500 took 0.242s\n",
      "Accuracy source 0.846191, main loss classifier 0.286057, source classification loss 0.429686, loss domain distinction 0.185707, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.33375409 Acc: 0.87772926\n",
      "New best validation loss:  0.3337540924549103\n",
      "Epoch 5 of 500 took 0.248s\n",
      "Accuracy source 0.858887, main loss classifier 0.280549, source classification loss 0.419903, loss domain distinction 0.185424, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.51193023 Acc: 0.819869\n",
      "Epoch    11: reducing learning rate of group 0 to 5.0300e-04.\n",
      "Epoch 6 of 500 took 0.258s\n",
      "Accuracy source 0.868164, main loss classifier 0.258696, source classification loss 0.380469, loss domain distinction 0.182264, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.30246949 Acc: 0.88100437\n",
      "New best validation loss:  0.30246949195861816\n",
      "Epoch 7 of 500 took 0.294s\n",
      "Accuracy source 0.869141, main loss classifier 0.258975, source classification loss 0.383073, loss domain distinction 0.180590, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.33642778 Acc: 0.86572052\n",
      "Epoch 8 of 500 took 0.286s\n",
      "Accuracy source 0.867676, main loss classifier 0.258943, source classification loss 0.382116, loss domain distinction 0.180876, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.35020879 Acc: 0.86135371\n",
      "Epoch 9 of 500 took 0.252s\n",
      "Accuracy source 0.873047, main loss classifier 0.245772, source classification loss 0.355577, loss domain distinction 0.180147, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.31616765 Acc: 0.87336245\n",
      "Epoch 10 of 500 took 0.261s\n",
      "Accuracy source 0.892578, main loss classifier 0.222065, source classification loss 0.308070, loss domain distinction 0.181609, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.28715637 Acc: 0.89519651\n",
      "New best validation loss:  0.2871563732624054\n",
      "Epoch 11 of 500 took 0.273s\n",
      "Accuracy source 0.881836, main loss classifier 0.242976, source classification loss 0.349531, loss domain distinction 0.182144, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.34525758 Acc: 0.87554585\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0060e-04.\n",
      "Epoch 12 of 500 took 0.332s\n",
      "Accuracy source 0.890137, main loss classifier 0.232417, source classification loss 0.331052, loss domain distinction 0.179774, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.32183418 Acc: 0.88100437\n",
      "Epoch 13 of 500 took 0.277s\n",
      "Accuracy source 0.883301, main loss classifier 0.231877, source classification loss 0.329103, loss domain distinction 0.179594, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.28238246 Acc: 0.88318777\n",
      "New best validation loss:  0.28238245844841003\n",
      "Epoch 14 of 500 took 0.260s\n",
      "Accuracy source 0.886230, main loss classifier 0.228666, source classification loss 0.322644, loss domain distinction 0.180517, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.28536552 Acc: 0.88427948\n",
      "Epoch 15 of 500 took 0.238s\n",
      "Accuracy source 0.892090, main loss classifier 0.222733, source classification loss 0.310732, loss domain distinction 0.179830, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.26349890 Acc: 0.89082969\n",
      "New best validation loss:  0.2634989023208618\n",
      "Epoch 16 of 500 took 0.237s\n",
      "Accuracy source 0.888672, main loss classifier 0.233222, source classification loss 0.331670, loss domain distinction 0.181501, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.33458880 Acc: 0.87663755\n",
      "Epoch 17 of 500 took 0.294s\n",
      "Accuracy source 0.888672, main loss classifier 0.224822, source classification loss 0.315099, loss domain distinction 0.179987, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.26202041 Acc: 0.8930131\n",
      "New best validation loss:  0.26202040910720825\n",
      "Epoch 18 of 500 took 0.289s\n",
      "Accuracy source 0.879395, main loss classifier 0.234441, source classification loss 0.334291, loss domain distinction 0.181021, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.32728568 Acc: 0.87336245\n",
      "Epoch 19 of 500 took 0.254s\n",
      "Accuracy source 0.893555, main loss classifier 0.232318, source classification loss 0.329498, loss domain distinction 0.181075, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.46862224 Acc: 0.84388646\n",
      "Epoch 20 of 500 took 0.248s\n",
      "Accuracy source 0.885742, main loss classifier 0.222865, source classification loss 0.310840, loss domain distinction 0.179720, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.31211045 Acc: 0.87227074\n",
      "Epoch 21 of 500 took 0.234s\n",
      "Accuracy source 0.895020, main loss classifier 0.219893, source classification loss 0.305608, loss domain distinction 0.178814, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.35147598 Acc: 0.86899563\n",
      "Epoch 22 of 500 took 0.236s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy source 0.883301, main loss classifier 0.234514, source classification loss 0.334721, loss domain distinction 0.179193, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.35044509 Acc: 0.86462882\n",
      "Epoch 23 of 500 took 0.275s\n",
      "Accuracy source 0.878418, main loss classifier 0.228943, source classification loss 0.323485, loss domain distinction 0.180072, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.25439516 Acc: 0.8930131\n",
      "New best validation loss:  0.25439515709877014\n",
      "Epoch 24 of 500 took 0.247s\n",
      "Accuracy source 0.890137, main loss classifier 0.225407, source classification loss 0.316364, loss domain distinction 0.180770, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.26193723 Acc: 0.89519651\n",
      "Epoch 25 of 500 took 0.235s\n",
      "Accuracy source 0.887695, main loss classifier 0.229828, source classification loss 0.324701, loss domain distinction 0.181072, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.28688437 Acc: 0.88755459\n",
      "Epoch 26 of 500 took 0.234s\n",
      "Accuracy source 0.896973, main loss classifier 0.220189, source classification loss 0.306443, loss domain distinction 0.179928, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.34317335 Acc: 0.86572052\n",
      "Epoch 27 of 500 took 0.281s\n",
      "Accuracy source 0.891113, main loss classifier 0.224440, source classification loss 0.314444, loss domain distinction 0.180698, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.35004640 Acc: 0.86572052\n",
      "Epoch 28 of 500 took 0.238s\n",
      "Accuracy source 0.888184, main loss classifier 0.229649, source classification loss 0.325080, loss domain distinction 0.178800, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.31443101 Acc: 0.87336245\n",
      "Epoch 29 of 500 took 0.278s\n",
      "Accuracy source 0.895020, main loss classifier 0.218990, source classification loss 0.303020, loss domain distinction 0.180928, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.26588970 Acc: 0.89737991\n",
      "Epoch 30 of 500 took 0.292s\n",
      "Accuracy source 0.900391, main loss classifier 0.221230, source classification loss 0.307667, loss domain distinction 0.180412, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.38047528 Acc: 0.8569869\n",
      "Epoch 31 of 500 took 0.285s\n",
      "Accuracy source 0.886719, main loss classifier 0.224363, source classification loss 0.314745, loss domain distinction 0.180117, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.27827403 Acc: 0.88973799\n",
      "Epoch 32 of 500 took 0.324s\n",
      "Accuracy source 0.890137, main loss classifier 0.228573, source classification loss 0.322659, loss domain distinction 0.181078, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.35156026 Acc: 0.87117904\n",
      "Epoch 33 of 500 took 0.266s\n",
      "Accuracy source 0.895020, main loss classifier 0.222962, source classification loss 0.310721, loss domain distinction 0.181761, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.27727583 Acc: 0.88755459\n",
      "Epoch 34 of 500 took 0.285s\n",
      "Accuracy source 0.884277, main loss classifier 0.228452, source classification loss 0.322078, loss domain distinction 0.180157, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.28980798 Acc: 0.88537118\n",
      "Training complete in 0m 9s\n",
      "()\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump4/TSD/participant_0/best_state_0.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump4/TSD/participant_0/best_state_0.pt' (epoch 5)\n",
      "STARTING TRAINING\n",
      "Accuracy source 0.850098, main loss classifier 0.300850, source classification loss 0.444602, loss domain distinction 0.242561, accuracy domain distinction 0.493164\n",
      "VALIDATION Loss: 0.40350479 Acc: 0.86572052\n",
      "New best validation loss:  0.40350478887557983\n",
      "Epoch 1 of 500 took 0.245s\n",
      "Accuracy source 0.855469, main loss classifier 0.282732, source classification loss 0.418562, loss domain distinction 0.192398, accuracy domain distinction 0.500244\n",
      "VALIDATION Loss: 0.56244856 Acc: 0.80567686\n",
      "Epoch 2 of 500 took 0.234s\n",
      "Accuracy source 0.853516, main loss classifier 0.281055, source classification loss 0.418030, loss domain distinction 0.187482, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.64391112 Acc: 0.79039301\n",
      "Epoch 3 of 500 took 0.283s\n",
      "Accuracy source 0.841309, main loss classifier 0.294805, source classification loss 0.446794, loss domain distinction 0.188107, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.30930957 Acc: 0.88864629\n",
      "New best validation loss:  0.30930957198143005\n",
      "Epoch 4 of 500 took 0.236s\n",
      "Accuracy source 0.853027, main loss classifier 0.286949, source classification loss 0.431691, loss domain distinction 0.186537, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.42324859 Acc: 0.84279476\n",
      "Epoch 5 of 500 took 0.240s\n",
      "Accuracy source 0.854004, main loss classifier 0.283652, source classification loss 0.425767, loss domain distinction 0.185545, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.25809234 Acc: 0.89956332\n",
      "Epoch    11: reducing learning rate of group 0 to 5.0300e-04.\n",
      "New best validation loss:  0.25809234380722046\n",
      "Epoch 6 of 500 took 0.235s\n",
      "Accuracy source 0.865234, main loss classifier 0.257916, source classification loss 0.379423, loss domain distinction 0.183643, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.31471032 Acc: 0.87663755\n",
      "Epoch 7 of 500 took 0.245s\n",
      "Accuracy source 0.867188, main loss classifier 0.256239, source classification loss 0.376545, loss domain distinction 0.182255, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.26897413 Acc: 0.89519651\n",
      "Epoch 8 of 500 took 0.234s\n",
      "Accuracy source 0.877441, main loss classifier 0.240101, source classification loss 0.345042, loss domain distinction 0.179969, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.33789462 Acc: 0.86572052\n",
      "Epoch 9 of 500 took 0.234s\n",
      "Accuracy source 0.890625, main loss classifier 0.231132, source classification loss 0.325992, loss domain distinction 0.182411, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.28369114 Acc: 0.88646288\n",
      "Epoch 10 of 500 took 0.234s\n",
      "Accuracy source 0.878418, main loss classifier 0.243129, source classification loss 0.350039, loss domain distinction 0.181576, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.30123588 Acc: 0.88864629\n",
      "Epoch 11 of 500 took 0.238s\n",
      "Accuracy source 0.886719, main loss classifier 0.231134, source classification loss 0.326011, loss domain distinction 0.181738, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.25028723 Acc: 0.90502183\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0060e-04.\n",
      "New best validation loss:  0.2502872347831726\n",
      "Epoch 12 of 500 took 0.236s\n",
      "Accuracy source 0.897461, main loss classifier 0.209905, source classification loss 0.284590, loss domain distinction 0.182443, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.24342966 Acc: 0.90502183\n",
      "New best validation loss:  0.24342966079711914\n",
      "Epoch 13 of 500 took 0.235s\n",
      "Accuracy source 0.886719, main loss classifier 0.238546, source classification loss 0.341421, loss domain distinction 0.181515, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.38418853 Acc: 0.86244541\n",
      "Epoch 14 of 500 took 0.233s\n",
      "Accuracy source 0.891113, main loss classifier 0.219946, source classification loss 0.305139, loss domain distinction 0.180948, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.29019243 Acc: 0.8930131\n",
      "Epoch 15 of 500 took 0.236s\n",
      "Accuracy source 0.888184, main loss classifier 0.228001, source classification loss 0.320313, loss domain distinction 0.183507, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.29345223 Acc: 0.88100437\n",
      "Epoch 16 of 500 took 0.234s\n",
      "Accuracy source 0.908203, main loss classifier 0.208180, source classification loss 0.281599, loss domain distinction 0.180807, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.24262396 Acc: 0.90938865\n",
      "New best validation loss:  0.2426239550113678\n",
      "Epoch 17 of 500 took 0.236s\n",
      "Accuracy source 0.876465, main loss classifier 0.247122, source classification loss 0.358891, loss domain distinction 0.181108, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.30208436 Acc: 0.88427948\n",
      "Epoch 18 of 500 took 0.232s\n",
      "Accuracy source 0.885742, main loss classifier 0.228720, source classification loss 0.323088, loss domain distinction 0.179377, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.29837060 Acc: 0.88537118\n",
      "Epoch 19 of 500 took 0.238s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy source 0.885254, main loss classifier 0.235682, source classification loss 0.335806, loss domain distinction 0.182261, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.26423252 Acc: 0.89956332\n",
      "Epoch 20 of 500 took 0.240s\n",
      "Accuracy source 0.886230, main loss classifier 0.229533, source classification loss 0.324389, loss domain distinction 0.182012, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.26661032 Acc: 0.8941048\n",
      "Epoch 21 of 500 took 0.233s\n",
      "Accuracy source 0.893555, main loss classifier 0.222476, source classification loss 0.309850, loss domain distinction 0.181176, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.30449620 Acc: 0.88318777\n",
      "Epoch 22 of 500 took 0.234s\n",
      "Accuracy source 0.888184, main loss classifier 0.224504, source classification loss 0.313847, loss domain distinction 0.180455, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.30120391 Acc: 0.88100437\n",
      "Epoch 23 of 500 took 0.233s\n",
      "Accuracy source 0.900879, main loss classifier 0.216424, source classification loss 0.298046, loss domain distinction 0.181471, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.30837417 Acc: 0.88318777\n",
      "Epoch 24 of 500 took 0.288s\n",
      "Accuracy source 0.889648, main loss classifier 0.224234, source classification loss 0.313637, loss domain distinction 0.180159, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.27496323 Acc: 0.89519651\n",
      "Epoch 25 of 500 took 0.235s\n",
      "Accuracy source 0.888184, main loss classifier 0.226670, source classification loss 0.319780, loss domain distinction 0.179910, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.29329050 Acc: 0.88973799\n",
      "Epoch 26 of 500 took 0.233s\n",
      "Accuracy source 0.876465, main loss classifier 0.234353, source classification loss 0.333873, loss domain distinction 0.181424, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.28852534 Acc: 0.88864629\n",
      "Epoch 27 of 500 took 0.233s\n",
      "Accuracy source 0.887695, main loss classifier 0.223867, source classification loss 0.312992, loss domain distinction 0.179961, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.33389819 Acc: 0.87227074\n",
      "Training complete in 0m 7s\n"
     ]
    }
   ],
   "source": [
    "# train_DANN(examples_datasets_train, labels_datasets_train, \n",
    "#           num_kernels=num_kernels,\n",
    "#           path_weights_fine_tuning=path_TSD,\n",
    "#           number_of_classes=number_of_classes,\n",
    "#           number_of_cycles_total = number_of_cycles_total,\n",
    "#           number_of_cycle_for_first_training = number_of_cycle_for_first_training,\n",
    "#           batch_size=batch_size,\n",
    "#           feature_vector_input_length=feature_vector_input_length,\n",
    "#           path_weights_to_save_to=path_DANN, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET one participant_examples  (7,)\n",
      "   GET one training_index_examples  (16, 572, 252)  at  0\n",
      "   GOT one group XY  (9152, 252)    (9152,)\n",
      "       one group XY test  (2288, 252)    (2288, 252)\n",
      "       one group XY train (8236, 252)    (8236,)\n",
      "       one group XY valid (916, 252)    (916, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  1\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  2\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  3\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  4\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  5\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  6\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "dataloaders: \n",
      "   train  (1, 7)\n",
      "   valid  (1, 7)\n",
      "   test  (1, 7)\n",
      "(7,)\n",
      "Participant ID:  0  Session ID:  0  Accuracy:  0.9020979020979021\n",
      "Participant ID:  0  Session ID:  1  Accuracy:  0.8758741258741258\n",
      "Participant ID:  0  Session ID:  2  Accuracy:  0.8461538461538461\n",
      "Participant ID:  0  Session ID:  3  Accuracy:  0.833916083916084\n",
      "Participant ID:  0  Session ID:  4  Accuracy:  0.7167832167832168\n",
      "Participant ID:  0  Session ID:  5  Accuracy:  0.7115384615384616\n",
      "Participant ID:  0  Session ID:  6  Accuracy:  0.6573426573426573\n",
      "ACCURACY PARTICIPANT:  [0.9020979020979021, 0.8758741258741258, 0.8461538461538461, 0.833916083916084, 0.7167832167832168, 0.7115384615384616, 0.6573426573426573]\n",
      "[[0.9020979  0.87587413 0.84615385 0.83391608 0.71678322 0.71153846\n",
      "  0.65734266]]\n",
      "[array([0.9020979 , 0.87587413, 0.84615385, 0.83391608, 0.71678322,\n",
      "       0.71153846, 0.65734266])]\n",
      "OVERALL ACCURACY: 0.791958041958042\n"
     ]
    }
   ],
   "source": [
    "algo_name = \"DANN\"\n",
    "test_DANN_on_training_sessions(examples_datasets_train, labels_datasets_train,\n",
    "                              feature_vector_input_length=feature_vector_input_length,\n",
    "                              num_neurons=num_kernels, path_weights_DA=path_DANN,\n",
    "                              algo_name=algo_name, save_path = save_DANN, \n",
    "                              number_of_cycles_total=number_of_cycles_total,\n",
    "                              number_of_cycle_for_first_training = number_of_cycle_for_first_training,\n",
    "                              path_weights_normal=path_TSD, number_of_classes=number_of_classes,\n",
    "                              cycle_for_test=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Day_0~3</th>\n",
       "      <td>0.902098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_4</th>\n",
       "      <td>0.875874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_5</th>\n",
       "      <td>0.846154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_6</th>\n",
       "      <td>0.833916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_7</th>\n",
       "      <td>0.716783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_8</th>\n",
       "      <td>0.711538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_9</th>\n",
       "      <td>0.657343</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Participant_5\n",
       "Day_0~3      0.902098\n",
       "Day_4        0.875874\n",
       "Day_5        0.846154\n",
       "Day_6        0.833916\n",
       "Day_7        0.716783\n",
       "Day_8        0.711538\n",
       "Day_9        0.657343"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_filename = save_DANN + '/predictions_' + algo_name + \".npy\"\n",
    "results = np.load(results_filename, allow_pickle=True)\n",
    "DANN_acc = results[0]\n",
    "DANN_acc_overall = np.mean(DANN_acc)\n",
    "DANN_df = pd.DataFrame(DANN_acc.transpose(), \n",
    "                       index = [f'Day_{i}' for i in index_participant_list],\n",
    "                        columns = ['Participant_5'])\n",
    "DANN_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "DANN_df.transpose().plot.bar(rot=0, figsize=(10,5))\n",
    "plt.title(\"DANN Accuracies\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ground_truths  =  (1, 7)\n",
      "predictions =  (1, 7)\n",
      "index_participant_list  ['0~3', 4, 5, 6, 7, 8, 9]\n",
      "accuracies_gestures =  (22, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Loc1_Sub5_Day0~3-&gt;0~3</th>\n",
       "      <th>Loc1_Sub5_Day0~3-&gt;4</th>\n",
       "      <th>Loc1_Sub5_Day0~3-&gt;5</th>\n",
       "      <th>Loc1_Sub5_Day0~3-&gt;6</th>\n",
       "      <th>Loc1_Sub5_Day0~3-&gt;7</th>\n",
       "      <th>Loc1_Sub5_Day0~3-&gt;8</th>\n",
       "      <th>Loc1_Sub5_Day0~3-&gt;9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M1</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.884615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M2</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.615385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M3</td>\n",
       "      <td>0.942308</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.346154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M4</td>\n",
       "      <td>0.788462</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>M5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.730769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>M6</td>\n",
       "      <td>0.980769</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>M7</td>\n",
       "      <td>0.971154</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.769231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>M8</td>\n",
       "      <td>0.942308</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.692308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>M9</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>0.269231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>M10</td>\n",
       "      <td>0.855769</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.692308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>M11</td>\n",
       "      <td>0.894231</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>M12</td>\n",
       "      <td>0.759615</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.346154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>M13</td>\n",
       "      <td>0.788462</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.807692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>M14</td>\n",
       "      <td>0.817308</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>M15</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.615385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>M16</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>M17</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.961538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>M18</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.730769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>M19</td>\n",
       "      <td>0.990385</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.846154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>M20</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.346154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>M21</td>\n",
       "      <td>0.894231</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.807692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Mean</td>\n",
       "      <td>0.902098</td>\n",
       "      <td>0.875874</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.833916</td>\n",
       "      <td>0.716783</td>\n",
       "      <td>0.711538</td>\n",
       "      <td>0.657343</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Loc1_Sub5_Day0~3->0~3  Loc1_Sub5_Day0~3->4  \\\n",
       "0          M0               1.000000             1.000000   \n",
       "1          M1               0.961538             0.961538   \n",
       "2          M2               0.923077             1.000000   \n",
       "3          M3               0.942308             0.884615   \n",
       "4          M4               0.788462             1.000000   \n",
       "5          M5               1.000000             1.000000   \n",
       "6          M6               0.980769             1.000000   \n",
       "7          M7               0.971154             1.000000   \n",
       "8          M8               0.942308             0.961538   \n",
       "9          M9               0.875000             0.730769   \n",
       "10        M10               0.855769             0.692308   \n",
       "11        M11               0.894231             0.500000   \n",
       "12        M12               0.759615             0.692308   \n",
       "13        M13               0.788462             0.961538   \n",
       "14        M14               0.817308             0.653846   \n",
       "15        M15               0.653846             0.576923   \n",
       "16        M16               0.961538             1.000000   \n",
       "17        M17               1.000000             1.000000   \n",
       "18        M18               1.000000             1.000000   \n",
       "19        M19               0.990385             1.000000   \n",
       "20        M20               0.846154             0.923077   \n",
       "21        M21               0.894231             0.730769   \n",
       "22       Mean               0.902098             0.875874   \n",
       "\n",
       "    Loc1_Sub5_Day0~3->5  Loc1_Sub5_Day0~3->6  Loc1_Sub5_Day0~3->7  \\\n",
       "0              1.000000             1.000000             1.000000   \n",
       "1              1.000000             0.846154             0.653846   \n",
       "2              0.500000             0.730769             0.615385   \n",
       "3              1.000000             0.923077             0.538462   \n",
       "4              0.576923             0.230769             0.038462   \n",
       "5              1.000000             1.000000             0.538462   \n",
       "6              0.730769             0.576923             0.692308   \n",
       "7              0.961538             1.000000             0.884615   \n",
       "8              1.000000             0.923077             0.807692   \n",
       "9              0.653846             0.923077             0.653846   \n",
       "10             0.884615             1.000000             0.884615   \n",
       "11             0.615385             0.884615             0.692308   \n",
       "12             0.730769             0.576923             0.730769   \n",
       "13             1.000000             0.923077             1.000000   \n",
       "14             0.576923             0.500000             0.500000   \n",
       "15             0.846154             0.730769             0.230769   \n",
       "16             0.923077             1.000000             1.000000   \n",
       "17             1.000000             1.000000             1.000000   \n",
       "18             0.807692             0.923077             0.961538   \n",
       "19             0.961538             1.000000             0.653846   \n",
       "20             0.884615             0.653846             0.884615   \n",
       "21             0.961538             1.000000             0.807692   \n",
       "22             0.846154             0.833916             0.716783   \n",
       "\n",
       "    Loc1_Sub5_Day0~3->8  Loc1_Sub5_Day0~3->9  \n",
       "0              1.000000             1.000000  \n",
       "1              0.615385             0.884615  \n",
       "2              0.576923             0.615385  \n",
       "3              0.923077             0.346154  \n",
       "4              0.307692             0.000000  \n",
       "5              0.961538             0.730769  \n",
       "6              0.538462             1.000000  \n",
       "7              0.692308             0.769231  \n",
       "8              0.923077             0.692308  \n",
       "9              0.653846             0.269231  \n",
       "10             0.884615             0.692308  \n",
       "11             0.884615             1.000000  \n",
       "12             0.807692             0.346154  \n",
       "13             1.000000             0.807692  \n",
       "14             0.000000             0.000000  \n",
       "15             0.038462             0.615385  \n",
       "16             1.000000             1.000000  \n",
       "17             1.000000             0.961538  \n",
       "18             0.576923             0.730769  \n",
       "19             0.692308             0.846154  \n",
       "20             0.576923             0.346154  \n",
       "21             1.000000             0.807692  \n",
       "22             0.711538             0.657343  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truths = results[1]\n",
    "predictions = results[2]\n",
    "print(\"ground_truths  = \", np.shape(ground_truths))\n",
    "print(\"predictions = \", np.shape(predictions))\n",
    "df = get_gesture_accuracies(ground_truths, predictions, number_of_classes=number_of_classes, \n",
    "                            m_name=m_name, n_name=n_name, path=save_TSD, algo_name=algo_name,\n",
    "                           index_participant_list_customized=index_participant_list,\n",
    "                           lump_day_at_participant=5)\n",
    "df = pd.read_csv(save_DANN+'/'+algo_name+'.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. SCADANN\n",
    "\n",
    "* `run_SCADANN_training_sessions`: train SCADANN model. The first session uses TSD model_0 wegits; others use DANN weights\n",
    "    * specify `percentage_same_gesture_stable` based on the performance of most pseudo labels: \n",
    "        * print accuracies out and check what percentage will optimize `ACCURACY MODEL` and `ACCURACY PSEUDO` without cutting out too much data \n",
    "    * num_sessions-1 sets of training weights will be saved\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TrainingsAndEvaluations.ForTrainingSessions.train_tsd_dnn_SCADANN import \\\n",
    "    run_SCADANN_training_sessions, test_network_SCADANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET one participant_examples  (7,)\n",
      "   GET one training_index_examples  (16, 572, 252)  at  0\n",
      "   GOT one group XY  (9152, 252)    (9152,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (9152, 252)    (9152,)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  1\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2288, 252)    (2288,)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  2\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2288, 252)    (2288,)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  3\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2288, 252)    (2288,)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  4\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2288, 252)    (2288,)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  5\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2288, 252)    (2288,)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  6\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2288, 252)    (2288,)\n",
      "dataloaders: \n",
      "   train  (1, 7)\n",
      "   valid  (0,)\n",
      "   test  (1, 0)\n",
      "participants_train =  1\n",
      "Optimizer =  <generator object Module.parameters at 0x7fdcf8e19eb0>\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump4/DANN/participant_0/best_state_1.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump4/DANN/participant_0/best_state_1.pt' (epoch 25)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump4/TSD/participant_0/best_state_0.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump4/TSD/participant_0/best_state_0.pt' (epoch 5)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump4/DANN/participant_0/best_state_1.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump4/DANN/participant_0/best_state_1.pt' (epoch 25)\n",
      "==== models_array =  (2,)  @ session  1\n",
      "HANDLING NEW SESSION  0\n",
      "HANDLING NEW SESSION  1\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  10\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  12\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.6538461538461539  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  9\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.0  len before:  26   len after:  4\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  13\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.8461538461538461  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.0  len before:  26   len after:  12\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  14\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.8754370629370629   Accuracy pseudo: 0.9600950118764846  len pseudo:  2105    len predictions 2288\n",
      "STARTING TRAINING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laiy/.local/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/laiy/.local/lib/python3.8/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy total 0.877103, main loss classifier 0.473170, source accuracy 0.878606 source classification loss 0.331184, target accuracy 0.875601 target loss 0.389597 accuracy domain distinction 0.500000 loss domain distinction 1.127796,\n",
      "VALIDATION Loss: 0.16102915 Acc: 0.95486936\n",
      "New best validation loss:  0.16102915044341767\n",
      "Epoch 2 of 500 took 0.375s\n",
      "Accuracy total 0.885216, main loss classifier 0.451355, source accuracy 0.894231 source classification loss 0.306788, target accuracy 0.876202 target loss 0.384776 accuracy domain distinction 0.500000 loss domain distinction 1.055736,\n",
      "VALIDATION Loss: 0.16418226 Acc: 0.94061758\n",
      "Epoch 3 of 500 took 0.342s\n",
      "Accuracy total 0.882812, main loss classifier 0.452513, source accuracy 0.878606 source classification loss 0.349953, target accuracy 0.887019 target loss 0.346757 accuracy domain distinction 0.500000 loss domain distinction 1.041577,\n",
      "VALIDATION Loss: 0.13997434 Acc: 0.95486936\n",
      "New best validation loss:  0.13997434292520797\n",
      "Epoch 4 of 500 took 0.346s\n",
      "Accuracy total 0.887921, main loss classifier 0.444131, source accuracy 0.885817 source classification loss 0.348146, target accuracy 0.890024 target loss 0.331707 accuracy domain distinction 0.500000 loss domain distinction 1.042049,\n",
      "VALIDATION Loss: 0.14238671 Acc: 0.95011876\n",
      "Epoch 5 of 500 took 0.344s\n",
      "Accuracy total 0.885517, main loss classifier 0.441408, source accuracy 0.880409 source classification loss 0.348925, target accuracy 0.890625 target loss 0.326549 accuracy domain distinction 0.500000 loss domain distinction 1.036708,\n",
      "VALIDATION Loss: 0.17115344 Acc: 0.94774347\n",
      "Epoch 6 of 500 took 0.343s\n",
      "Accuracy total 0.884916, main loss classifier 0.445439, source accuracy 0.876803 source classification loss 0.344731, target accuracy 0.893029 target loss 0.338068 accuracy domain distinction 0.500000 loss domain distinction 1.040393,\n",
      "VALIDATION Loss: 0.12169962 Acc: 0.96674584\n",
      "New best validation loss:  0.12169961897390229\n",
      "Epoch 7 of 500 took 0.346s\n",
      "Accuracy total 0.884615, main loss classifier 0.434179, source accuracy 0.881010 source classification loss 0.335254, target accuracy 0.888221 target loss 0.326168 accuracy domain distinction 0.500000 loss domain distinction 1.034677,\n",
      "VALIDATION Loss: 0.15122194 Acc: 0.95011876\n",
      "Epoch 8 of 500 took 0.342s\n",
      "Accuracy total 0.885216, main loss classifier 0.435617, source accuracy 0.875601 source classification loss 0.342053, target accuracy 0.894832 target loss 0.322291 accuracy domain distinction 0.500000 loss domain distinction 1.034449,\n",
      "VALIDATION Loss: 0.13725308 Acc: 0.95724466\n",
      "Epoch 9 of 500 took 0.346s\n",
      "Accuracy total 0.895733, main loss classifier 0.421119, source accuracy 0.885216 source classification loss 0.334512, target accuracy 0.906250 target loss 0.300442 accuracy domain distinction 0.500000 loss domain distinction 1.036419,\n",
      "VALIDATION Loss: 0.12100350 Acc: 0.96199525\n",
      "New best validation loss:  0.12100350404424327\n",
      "Epoch 10 of 500 took 0.344s\n",
      "Accuracy total 0.891827, main loss classifier 0.431319, source accuracy 0.878005 source classification loss 0.334432, target accuracy 0.905649 target loss 0.321697 accuracy domain distinction 0.500000 loss domain distinction 1.032542,\n",
      "VALIDATION Loss: 0.11786804 Acc: 0.96674584\n",
      "New best validation loss:  0.11786803709609169\n",
      "Epoch 11 of 500 took 0.344s\n",
      "Accuracy total 0.895433, main loss classifier 0.423895, source accuracy 0.878005 source classification loss 0.353084, target accuracy 0.912861 target loss 0.287621 accuracy domain distinction 0.500000 loss domain distinction 1.035430,\n",
      "VALIDATION Loss: 0.13584655 Acc: 0.95486936\n",
      "Epoch 12 of 500 took 0.344s\n",
      "Accuracy total 0.894231, main loss classifier 0.416125, source accuracy 0.887019 source classification loss 0.314984, target accuracy 0.901442 target loss 0.310356 accuracy domain distinction 0.500000 loss domain distinction 1.034550,\n",
      "VALIDATION Loss: 0.12665232 Acc: 0.95724466\n",
      "Epoch 13 of 500 took 0.343s\n",
      "Accuracy total 0.888522, main loss classifier 0.428743, source accuracy 0.876803 source classification loss 0.354672, target accuracy 0.900240 target loss 0.295157 accuracy domain distinction 0.500000 loss domain distinction 1.038283,\n",
      "VALIDATION Loss: 0.15250508 Acc: 0.95724466\n",
      "Epoch 14 of 500 took 0.349s\n",
      "Accuracy total 0.901442, main loss classifier 0.398855, source accuracy 0.888822 source classification loss 0.315922, target accuracy 0.914062 target loss 0.274929 accuracy domain distinction 0.500000 loss domain distinction 1.034296,\n",
      "VALIDATION Loss: 0.15982736 Acc: 0.94774347\n",
      "Epoch 15 of 500 took 0.347s\n",
      "Accuracy total 0.904748, main loss classifier 0.390211, source accuracy 0.896034 source classification loss 0.303107, target accuracy 0.913462 target loss 0.270623 accuracy domain distinction 0.500000 loss domain distinction 1.033459,\n",
      "VALIDATION Loss: 0.11538953 Acc: 0.96199525\n",
      "New best validation loss:  0.1153895344052996\n",
      "Epoch 16 of 500 took 0.405s\n",
      "Accuracy total 0.896935, main loss classifier 0.407252, source accuracy 0.890024 source classification loss 0.323452, target accuracy 0.903846 target loss 0.284173 accuracy domain distinction 0.500000 loss domain distinction 1.034393,\n",
      "VALIDATION Loss: 0.14519974 Acc: 0.94774347\n",
      "Epoch 17 of 500 took 0.369s\n",
      "Accuracy total 0.893329, main loss classifier 0.409179, source accuracy 0.879808 source classification loss 0.335512, target accuracy 0.906851 target loss 0.276042 accuracy domain distinction 0.500000 loss domain distinction 1.034022,\n",
      "VALIDATION Loss: 0.14954775 Acc: 0.95961995\n",
      "Epoch 18 of 500 took 0.344s\n",
      "Accuracy total 0.902945, main loss classifier 0.392694, source accuracy 0.895433 source classification loss 0.302727, target accuracy 0.910457 target loss 0.276299 accuracy domain distinction 0.500000 loss domain distinction 1.031814,\n",
      "VALIDATION Loss: 0.12483593 Acc: 0.96199525\n",
      "Epoch 19 of 500 took 0.345s\n",
      "Accuracy total 0.896935, main loss classifier 0.412198, source accuracy 0.883413 source classification loss 0.335211, target accuracy 0.910457 target loss 0.282866 accuracy domain distinction 0.500000 loss domain distinction 1.031597,\n",
      "VALIDATION Loss: 0.11604740 Acc: 0.96437055\n",
      "Epoch 20 of 500 took 0.355s\n",
      "Accuracy total 0.889123, main loss classifier 0.425809, source accuracy 0.871995 source classification loss 0.361766, target accuracy 0.906250 target loss 0.282192 accuracy domain distinction 0.500000 loss domain distinction 1.038306,\n",
      "VALIDATION Loss: 0.13123394 Acc: 0.96199525\n",
      "Epoch 21 of 500 took 0.344s\n",
      "Accuracy total 0.900541, main loss classifier 0.397070, source accuracy 0.889423 source classification loss 0.323911, target accuracy 0.911659 target loss 0.263638 accuracy domain distinction 0.500000 loss domain distinction 1.032952,\n",
      "VALIDATION Loss: 0.11515574 Acc: 0.96674584\n",
      "New best validation loss:  0.11515573784708977\n",
      "Epoch 22 of 500 took 0.366s\n",
      "Accuracy total 0.896034, main loss classifier 0.404861, source accuracy 0.878606 source classification loss 0.334055, target accuracy 0.913462 target loss 0.269540 accuracy domain distinction 0.500000 loss domain distinction 1.030638,\n",
      "VALIDATION Loss: 0.12077918 Acc: 0.97149644\n",
      "Epoch 23 of 500 took 0.343s\n",
      "Accuracy total 0.904447, main loss classifier 0.386130, source accuracy 0.891226 source classification loss 0.301263, target accuracy 0.917668 target loss 0.264736 accuracy domain distinction 0.500000 loss domain distinction 1.031310,\n",
      "VALIDATION Loss: 0.11915812 Acc: 0.96199525\n",
      "Epoch 24 of 500 took 0.343s\n",
      "Accuracy total 0.901442, main loss classifier 0.392045, source accuracy 0.888822 source classification loss 0.321793, target accuracy 0.914062 target loss 0.255124 accuracy domain distinction 0.500000 loss domain distinction 1.035865,\n",
      "VALIDATION Loss: 0.11313689 Acc: 0.96437055\n",
      "New best validation loss:  0.11313688967909132\n",
      "Epoch 25 of 500 took 0.346s\n",
      "Accuracy total 0.907151, main loss classifier 0.383077, source accuracy 0.905649 source classification loss 0.294124, target accuracy 0.908654 target loss 0.264981 accuracy domain distinction 0.500000 loss domain distinction 1.035250,\n",
      "VALIDATION Loss: 0.10814759 Acc: 0.96674584\n",
      "New best validation loss:  0.10814758762717247\n",
      "Epoch 26 of 500 took 0.344s\n",
      "Accuracy total 0.901142, main loss classifier 0.402182, source accuracy 0.890625 source classification loss 0.315614, target accuracy 0.911659 target loss 0.281297 accuracy domain distinction 0.500000 loss domain distinction 1.037265,\n",
      "VALIDATION Loss: 0.10386112 Acc: 0.96912114\n",
      "New best validation loss:  0.10386111746941294\n",
      "Epoch 27 of 500 took 0.346s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy total 0.899940, main loss classifier 0.399270, source accuracy 0.891226 source classification loss 0.324758, target accuracy 0.908654 target loss 0.266975 accuracy domain distinction 0.500000 loss domain distinction 1.034036,\n",
      "VALIDATION Loss: 0.12280629 Acc: 0.95724466\n",
      "Epoch 28 of 500 took 0.350s\n",
      "Accuracy total 0.896935, main loss classifier 0.396549, source accuracy 0.887620 source classification loss 0.320513, target accuracy 0.906250 target loss 0.265082 accuracy domain distinction 0.500000 loss domain distinction 1.037512,\n",
      "VALIDATION Loss: 0.10235185 Acc: 0.96437055\n",
      "New best validation loss:  0.10235185229352542\n",
      "Epoch 29 of 500 took 0.345s\n",
      "Accuracy total 0.920373, main loss classifier 0.354993, source accuracy 0.914062 source classification loss 0.261589, target accuracy 0.926683 target loss 0.242596 accuracy domain distinction 0.500000 loss domain distinction 1.029009,\n",
      "VALIDATION Loss: 0.12369751 Acc: 0.95961995\n",
      "Epoch 30 of 500 took 0.343s\n",
      "Accuracy total 0.907452, main loss classifier 0.379546, source accuracy 0.891827 source classification loss 0.309747, target accuracy 0.923077 target loss 0.242294 accuracy domain distinction 0.500000 loss domain distinction 1.035258,\n",
      "VALIDATION Loss: 0.12053740 Acc: 0.95961995\n",
      "Epoch 31 of 500 took 0.362s\n",
      "Accuracy total 0.905649, main loss classifier 0.386118, source accuracy 0.888822 source classification loss 0.326854, target accuracy 0.922476 target loss 0.238186 accuracy domain distinction 0.500000 loss domain distinction 1.035980,\n",
      "VALIDATION Loss: 0.10741599 Acc: 0.96674584\n",
      "Epoch 32 of 500 took 0.341s\n",
      "Accuracy total 0.908654, main loss classifier 0.393092, source accuracy 0.891827 source classification loss 0.324102, target accuracy 0.925481 target loss 0.255012 accuracy domain distinction 0.500000 loss domain distinction 1.035347,\n",
      "VALIDATION Loss: 0.10830438 Acc: 0.97149644\n",
      "Epoch 33 of 500 took 0.343s\n",
      "Accuracy total 0.899940, main loss classifier 0.390325, source accuracy 0.896034 source classification loss 0.293502, target accuracy 0.903846 target loss 0.281105 accuracy domain distinction 0.500000 loss domain distinction 1.030208,\n",
      "VALIDATION Loss: 0.12548788 Acc: 0.96437055\n",
      "Epoch 34 of 500 took 0.346s\n",
      "Accuracy total 0.903846, main loss classifier 0.385696, source accuracy 0.902043 source classification loss 0.306874, target accuracy 0.905649 target loss 0.257966 accuracy domain distinction 0.500000 loss domain distinction 1.032765,\n",
      "VALIDATION Loss: 0.13071863 Acc: 0.95011876\n",
      "Epoch    34: reducing learning rate of group 0 to 2.0120e-05.\n",
      "Epoch 35 of 500 took 0.343s\n",
      "Accuracy total 0.906550, main loss classifier 0.368413, source accuracy 0.893630 source classification loss 0.292129, target accuracy 0.919471 target loss 0.238631 accuracy domain distinction 0.500000 loss domain distinction 1.030329,\n",
      "VALIDATION Loss: 0.12660729 Acc: 0.95724466\n",
      "Epoch 36 of 500 took 0.344s\n",
      "Accuracy total 0.909555, main loss classifier 0.370733, source accuracy 0.900841 source classification loss 0.293030, target accuracy 0.918269 target loss 0.242139 accuracy domain distinction 0.500000 loss domain distinction 1.031485,\n",
      "VALIDATION Loss: 0.12402769 Acc: 0.95961995\n",
      "Epoch 37 of 500 took 0.345s\n",
      "Accuracy total 0.913762, main loss classifier 0.363634, source accuracy 0.900841 source classification loss 0.288765, target accuracy 0.926683 target loss 0.231586 accuracy domain distinction 0.500000 loss domain distinction 1.034583,\n",
      "VALIDATION Loss: 0.10106186 Acc: 0.96912114\n",
      "New best validation loss:  0.10106186302644866\n",
      "Epoch 38 of 500 took 0.346s\n",
      "Accuracy total 0.905349, main loss classifier 0.385646, source accuracy 0.889423 source classification loss 0.299924, target accuracy 0.921274 target loss 0.266113 accuracy domain distinction 0.500000 loss domain distinction 1.026268,\n",
      "VALIDATION Loss: 0.11371450 Acc: 0.95961995\n",
      "Epoch 39 of 500 took 0.343s\n",
      "Accuracy total 0.905950, main loss classifier 0.372552, source accuracy 0.881010 source classification loss 0.318149, target accuracy 0.930889 target loss 0.221310 accuracy domain distinction 0.500000 loss domain distinction 1.028227,\n",
      "VALIDATION Loss: 0.10498776 Acc: 0.96912114\n",
      "Epoch 40 of 500 took 0.346s\n",
      "Accuracy total 0.905349, main loss classifier 0.373628, source accuracy 0.887620 source classification loss 0.308858, target accuracy 0.923077 target loss 0.232498 accuracy domain distinction 0.500000 loss domain distinction 1.029492,\n",
      "VALIDATION Loss: 0.10865616 Acc: 0.97149644\n",
      "Epoch 41 of 500 took 0.345s\n",
      "Accuracy total 0.909255, main loss classifier 0.369476, source accuracy 0.891226 source classification loss 0.300396, target accuracy 0.927284 target loss 0.233346 accuracy domain distinction 0.500000 loss domain distinction 1.026056,\n",
      "VALIDATION Loss: 0.11424713 Acc: 0.96912114\n",
      "Epoch 42 of 500 took 0.347s\n",
      "Accuracy total 0.904147, main loss classifier 0.384795, source accuracy 0.890625 source classification loss 0.306468, target accuracy 0.917668 target loss 0.255888 accuracy domain distinction 0.500000 loss domain distinction 1.036168,\n",
      "VALIDATION Loss: 0.14733360 Acc: 0.94536817\n",
      "Epoch 43 of 500 took 0.346s\n",
      "Accuracy total 0.906550, main loss classifier 0.380744, source accuracy 0.897837 source classification loss 0.301568, target accuracy 0.915264 target loss 0.253684 accuracy domain distinction 0.500000 loss domain distinction 1.031180,\n",
      "VALIDATION Loss: 0.10226560 Acc: 0.96674584\n",
      "Epoch    43: reducing learning rate of group 0 to 4.0240e-06.\n",
      "Epoch 44 of 500 took 0.343s\n",
      "Accuracy total 0.910457, main loss classifier 0.367657, source accuracy 0.902043 source classification loss 0.287969, target accuracy 0.918870 target loss 0.241549 accuracy domain distinction 0.500000 loss domain distinction 1.028982,\n",
      "VALIDATION Loss: 0.10897548 Acc: 0.96437055\n",
      "Epoch 45 of 500 took 0.362s\n",
      "Accuracy total 0.901442, main loss classifier 0.385193, source accuracy 0.882212 source classification loss 0.331268, target accuracy 0.920673 target loss 0.232988 accuracy domain distinction 0.500000 loss domain distinction 1.030654,\n",
      "VALIDATION Loss: 0.10696969 Acc: 0.96437055\n",
      "Epoch 46 of 500 took 0.357s\n",
      "Accuracy total 0.911358, main loss classifier 0.380927, source accuracy 0.903245 source classification loss 0.296942, target accuracy 0.919471 target loss 0.258254 accuracy domain distinction 0.500000 loss domain distinction 1.033294,\n",
      "VALIDATION Loss: 0.10600571 Acc: 0.95961995\n",
      "Epoch 47 of 500 took 0.375s\n",
      "Accuracy total 0.908053, main loss classifier 0.370822, source accuracy 0.893029 source classification loss 0.290593, target accuracy 0.923077 target loss 0.245528 accuracy domain distinction 0.500000 loss domain distinction 1.027614,\n",
      "VALIDATION Loss: 0.10475789 Acc: 0.96912114\n",
      "Epoch 48 of 500 took 0.350s\n",
      "Accuracy total 0.907151, main loss classifier 0.376603, source accuracy 0.888822 source classification loss 0.305554, target accuracy 0.925481 target loss 0.242470 accuracy domain distinction 0.500000 loss domain distinction 1.025907,\n",
      "VALIDATION Loss: 0.11034032 Acc: 0.96674584\n",
      "Epoch 49 of 500 took 0.350s\n",
      "Training complete in 0m 17s\n",
      "['participant_0']\n",
      "Optimizer =  <generator object Module.parameters at 0x7fdced2fdf90>\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump4/DANN/participant_0/best_state_2.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump4/DANN/participant_0/best_state_2.pt' (epoch 26)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump4/TSD/participant_0/best_state_0.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump4/TSD/participant_0/best_state_0.pt' (epoch 5)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump4/DANN/participant_0/best_state_1.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump4/DANN/participant_0/best_state_1.pt' (epoch 25)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump4/DANN/participant_0/best_state_2.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump4/DANN/participant_0/best_state_2.pt' (epoch 26)\n",
      "==== models_array =  (3,)  @ session  2\n",
      "HANDLING NEW SESSION  0\n",
      "HANDLING NEW SESSION  1\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  10\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  12\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.6538461538461539  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  9\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.0  len before:  26   len after:  4\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  13\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.8461538461538461  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.0  len before:  26   len after:  12\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  14\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.8754370629370629   Accuracy pseudo: 0.9600950118764846  len pseudo:  2105    len predictions 2288\n",
      "HANDLING NEW SESSION  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.0  len before:  26   len after:  17\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  8\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.9615384615384616  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  9\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.0  len before:  26   len after:  10\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.6923076923076923  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  13\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.3076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  14\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  1.0  len before:  26   len after:  16\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.8461538461538461  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  9\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  15\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  1\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.5384615384615384  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  1\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  15\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.6538461538461539  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.4117647058823529  len before:  26   len after:  17\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.8229895104895105   Accuracy pseudo: 0.9351351351351351  len pseudo:  2035    len predictions 2288\n",
      "STARTING TRAINING\n",
      "Accuracy total 0.856250, main loss classifier 0.525360, source accuracy 0.865000 source classification loss 0.352845, target accuracy 0.847500 target loss 0.470343 accuracy domain distinction 0.500000 loss domain distinction 1.137665,\n",
      "VALIDATION Loss: 0.29842716 Acc: 0.88943489\n",
      "New best validation loss:  0.29842716455459595\n",
      "Epoch 2 of 500 took 0.416s\n",
      "Accuracy total 0.875000, main loss classifier 0.477197, source accuracy 0.876875 source classification loss 0.345072, target accuracy 0.873125 target loss 0.398595 accuracy domain distinction 0.500000 loss domain distinction 1.053630,\n",
      "VALIDATION Loss: 0.21162104 Acc: 0.92628993\n",
      "New best validation loss:  0.2116210354225976\n",
      "Epoch 3 of 500 took 0.496s\n",
      "Accuracy total 0.875000, main loss classifier 0.464306, source accuracy 0.882500 source classification loss 0.326780, target accuracy 0.867500 target loss 0.393781 accuracy domain distinction 0.500000 loss domain distinction 1.040254,\n",
      "VALIDATION Loss: 0.19598320 Acc: 0.92137592\n",
      "New best validation loss:  0.1959832033940724\n",
      "Epoch 4 of 500 took 0.429s\n",
      "Accuracy total 0.872812, main loss classifier 0.461363, source accuracy 0.875000 source classification loss 0.345767, target accuracy 0.870625 target loss 0.368946 accuracy domain distinction 0.500000 loss domain distinction 1.040063,\n",
      "VALIDATION Loss: 0.15982617 Acc: 0.93857494\n",
      "New best validation loss:  0.15982617331402643\n",
      "Epoch 5 of 500 took 0.434s\n",
      "Accuracy total 0.885625, main loss classifier 0.462378, source accuracy 0.881250 source classification loss 0.369907, target accuracy 0.890000 target loss 0.347540 accuracy domain distinction 0.500000 loss domain distinction 1.036541,\n",
      "VALIDATION Loss: 0.17313170 Acc: 0.94594595\n",
      "Epoch 6 of 500 took 0.474s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy total 0.884375, main loss classifier 0.459715, source accuracy 0.883750 source classification loss 0.356240, target accuracy 0.885000 target loss 0.355106 accuracy domain distinction 0.500000 loss domain distinction 1.040424,\n",
      "VALIDATION Loss: 0.15754411 Acc: 0.94348894\n",
      "New best validation loss:  0.15754411263125284\n",
      "Epoch 7 of 500 took 0.361s\n",
      "Accuracy total 0.876563, main loss classifier 0.461708, source accuracy 0.871250 source classification loss 0.353589, target accuracy 0.881875 target loss 0.362455 accuracy domain distinction 0.500000 loss domain distinction 1.036862,\n",
      "VALIDATION Loss: 0.17302247 Acc: 0.93366093\n",
      "Epoch 8 of 500 took 0.331s\n",
      "Accuracy total 0.885938, main loss classifier 0.437840, source accuracy 0.883125 source classification loss 0.345201, target accuracy 0.888750 target loss 0.322560 accuracy domain distinction 0.500000 loss domain distinction 1.039595,\n",
      "VALIDATION Loss: 0.18418456 Acc: 0.92628993\n",
      "Epoch 9 of 500 took 0.382s\n",
      "Accuracy total 0.883125, main loss classifier 0.438856, source accuracy 0.880625 source classification loss 0.354137, target accuracy 0.885625 target loss 0.316959 accuracy domain distinction 0.500000 loss domain distinction 1.033077,\n",
      "VALIDATION Loss: 0.16782335 Acc: 0.92874693\n",
      "Epoch 10 of 500 took 0.372s\n",
      "Accuracy total 0.889687, main loss classifier 0.439106, source accuracy 0.876875 source classification loss 0.349780, target accuracy 0.902500 target loss 0.321432 accuracy domain distinction 0.500000 loss domain distinction 1.034997,\n",
      "VALIDATION Loss: 0.13128241 Acc: 0.95085995\n",
      "New best validation loss:  0.1312824147088187\n",
      "Epoch 11 of 500 took 0.330s\n",
      "Accuracy total 0.888125, main loss classifier 0.434077, source accuracy 0.880625 source classification loss 0.341886, target accuracy 0.895625 target loss 0.319021 accuracy domain distinction 0.500000 loss domain distinction 1.036232,\n",
      "VALIDATION Loss: 0.15140106 Acc: 0.95085995\n",
      "Epoch 12 of 500 took 0.331s\n",
      "Accuracy total 0.887813, main loss classifier 0.431836, source accuracy 0.880625 source classification loss 0.346788, target accuracy 0.895000 target loss 0.309896 accuracy domain distinction 0.500000 loss domain distinction 1.034937,\n",
      "VALIDATION Loss: 0.20550281 Acc: 0.93120393\n",
      "Epoch 13 of 500 took 0.327s\n",
      "Accuracy total 0.891563, main loss classifier 0.410758, source accuracy 0.881250 source classification loss 0.328017, target accuracy 0.901875 target loss 0.287327 accuracy domain distinction 0.500000 loss domain distinction 1.030859,\n",
      "VALIDATION Loss: 0.18738102 Acc: 0.94103194\n",
      "Epoch 14 of 500 took 0.342s\n",
      "Accuracy total 0.894375, main loss classifier 0.413671, source accuracy 0.879375 source classification loss 0.342010, target accuracy 0.909375 target loss 0.278150 accuracy domain distinction 0.500000 loss domain distinction 1.035912,\n",
      "VALIDATION Loss: 0.16506452 Acc: 0.93366093\n",
      "Epoch 15 of 500 took 0.431s\n",
      "Accuracy total 0.895938, main loss classifier 0.410895, source accuracy 0.883750 source classification loss 0.330328, target accuracy 0.908125 target loss 0.284304 accuracy domain distinction 0.500000 loss domain distinction 1.035787,\n",
      "VALIDATION Loss: 0.15323431 Acc: 0.95331695\n",
      "Epoch 16 of 500 took 0.414s\n",
      "Accuracy total 0.879375, main loss classifier 0.441813, source accuracy 0.870000 source classification loss 0.358067, target accuracy 0.888750 target loss 0.318194 accuracy domain distinction 0.500000 loss domain distinction 1.036821,\n",
      "VALIDATION Loss: 0.15229838 Acc: 0.95331695\n",
      "Epoch    16: reducing learning rate of group 0 to 2.0120e-05.\n",
      "Epoch 17 of 500 took 0.338s\n",
      "Accuracy total 0.901875, main loss classifier 0.401090, source accuracy 0.888125 source classification loss 0.332246, target accuracy 0.915625 target loss 0.263903 accuracy domain distinction 0.500000 loss domain distinction 1.030152,\n",
      "VALIDATION Loss: 0.19205863 Acc: 0.92383292\n",
      "Epoch 18 of 500 took 0.336s\n",
      "Accuracy total 0.900625, main loss classifier 0.398638, source accuracy 0.890625 source classification loss 0.327449, target accuracy 0.910625 target loss 0.263438 accuracy domain distinction 0.500000 loss domain distinction 1.031945,\n",
      "VALIDATION Loss: 0.13068152 Acc: 0.95085995\n",
      "New best validation loss:  0.1306815200618335\n",
      "Epoch 19 of 500 took 0.336s\n",
      "Accuracy total 0.899062, main loss classifier 0.405292, source accuracy 0.892500 source classification loss 0.313883, target accuracy 0.905625 target loss 0.290272 accuracy domain distinction 0.500000 loss domain distinction 1.032141,\n",
      "VALIDATION Loss: 0.17274379 Acc: 0.93120393\n",
      "Epoch 20 of 500 took 0.335s\n",
      "Accuracy total 0.897500, main loss classifier 0.420581, source accuracy 0.889375 source classification loss 0.321723, target accuracy 0.905625 target loss 0.313116 accuracy domain distinction 0.500000 loss domain distinction 1.031612,\n",
      "VALIDATION Loss: 0.17463360 Acc: 0.94103194\n",
      "Epoch 21 of 500 took 0.330s\n",
      "Accuracy total 0.895312, main loss classifier 0.417608, source accuracy 0.881875 source classification loss 0.345741, target accuracy 0.908750 target loss 0.283297 accuracy domain distinction 0.500000 loss domain distinction 1.030896,\n",
      "VALIDATION Loss: 0.15768477 Acc: 0.93857494\n",
      "Epoch 22 of 500 took 0.329s\n",
      "Accuracy total 0.902500, main loss classifier 0.388764, source accuracy 0.889375 source classification loss 0.305026, target accuracy 0.915625 target loss 0.265389 accuracy domain distinction 0.500000 loss domain distinction 1.035563,\n",
      "VALIDATION Loss: 0.16966083 Acc: 0.93857494\n",
      "Epoch 23 of 500 took 0.330s\n",
      "Accuracy total 0.895625, main loss classifier 0.412768, source accuracy 0.886875 source classification loss 0.331857, target accuracy 0.904375 target loss 0.288248 accuracy domain distinction 0.500000 loss domain distinction 1.027152,\n",
      "VALIDATION Loss: 0.16686097 Acc: 0.93120393\n",
      "Epoch 24 of 500 took 0.333s\n",
      "Accuracy total 0.895938, main loss classifier 0.392396, source accuracy 0.878750 source classification loss 0.318064, target accuracy 0.913125 target loss 0.260666 accuracy domain distinction 0.500000 loss domain distinction 1.030308,\n",
      "VALIDATION Loss: 0.15061146 Acc: 0.95331695\n",
      "Epoch    24: reducing learning rate of group 0 to 4.0240e-06.\n",
      "Epoch 25 of 500 took 0.369s\n",
      "Accuracy total 0.891563, main loss classifier 0.414488, source accuracy 0.881250 source classification loss 0.331715, target accuracy 0.901875 target loss 0.290812 accuracy domain distinction 0.500000 loss domain distinction 1.032244,\n",
      "VALIDATION Loss: 0.15442987 Acc: 0.95085995\n",
      "Epoch 26 of 500 took 0.327s\n",
      "Accuracy total 0.898750, main loss classifier 0.417789, source accuracy 0.883750 source classification loss 0.358351, target accuracy 0.913750 target loss 0.271165 accuracy domain distinction 0.500000 loss domain distinction 1.030312,\n",
      "VALIDATION Loss: 0.20445042 Acc: 0.91891892\n",
      "Epoch 27 of 500 took 0.329s\n",
      "Accuracy total 0.902813, main loss classifier 0.393216, source accuracy 0.898125 source classification loss 0.289434, target accuracy 0.907500 target loss 0.290625 accuracy domain distinction 0.500000 loss domain distinction 1.031866,\n",
      "VALIDATION Loss: 0.13910236 Acc: 0.95085995\n",
      "Epoch 28 of 500 took 0.325s\n",
      "Accuracy total 0.896875, main loss classifier 0.419049, source accuracy 0.881250 source classification loss 0.347625, target accuracy 0.912500 target loss 0.284228 accuracy domain distinction 0.500000 loss domain distinction 1.031226,\n",
      "VALIDATION Loss: 0.14496756 Acc: 0.95085995\n",
      "Epoch 29 of 500 took 0.326s\n",
      "Accuracy total 0.905625, main loss classifier 0.386135, source accuracy 0.895000 source classification loss 0.310447, target accuracy 0.916250 target loss 0.255036 accuracy domain distinction 0.500000 loss domain distinction 1.033929,\n",
      "VALIDATION Loss: 0.15023373 Acc: 0.94594595\n",
      "Epoch 30 of 500 took 0.329s\n",
      "Training complete in 0m 11s\n",
      "['participant_0']\n",
      "Optimizer =  <generator object Module.parameters at 0x7fdced2fdf20>\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump4/DANN/participant_0/best_state_3.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump4/DANN/participant_0/best_state_3.pt' (epoch 19)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump4/TSD/participant_0/best_state_0.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump4/TSD/participant_0/best_state_0.pt' (epoch 5)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump4/DANN/participant_0/best_state_1.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump4/DANN/participant_0/best_state_1.pt' (epoch 25)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump4/DANN/participant_0/best_state_2.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump4/DANN/participant_0/best_state_2.pt' (epoch 26)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump4/DANN/participant_0/best_state_3.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump4/DANN/participant_0/best_state_3.pt' (epoch 19)\n",
      "==== models_array =  (4,)  @ session  3\n",
      "HANDLING NEW SESSION  0\n",
      "HANDLING NEW SESSION  1\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  10\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  12\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.6538461538461539  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  9\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.0  len before:  26   len after:  4\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  13\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.8461538461538461  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.0  len before:  26   len after:  12\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  14\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.8754370629370629   Accuracy pseudo: 0.9600950118764846  len pseudo:  2105    len predictions 2288\n",
      "HANDLING NEW SESSION  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.0  len before:  26   len after:  17\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  8\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.9615384615384616  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  9\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.0  len before:  26   len after:  10\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.6923076923076923  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  13\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.3076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  14\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  1.0  len before:  26   len after:  16\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.8461538461538461  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  9\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  15\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  1\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.5384615384615384  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  1\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  15\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.6538461538461539  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.4117647058823529  len before:  26   len after:  17\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.8229895104895105   Accuracy pseudo: 0.9351351351351351  len pseudo:  2035    len predictions 2288\n",
      "HANDLING NEW SESSION  3\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.4230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.6538461538461539  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.6923076923076923  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.6923076923076923  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  14\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.0  len before:  26   len after:  6\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.0  len before:  26   len after:  5\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  12\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.0  len before:  26   len after:  6\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  3\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.0  len before:  26   len after:  17\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  13\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  7\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.0  len before:  26   len after:  4\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.8190559440559441   Accuracy pseudo: 0.9246305418719212  len pseudo:  2030    len predictions 2288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING TRAINING\n",
      "Accuracy total 0.855313, main loss classifier 0.541084, source accuracy 0.855000 source classification loss 0.397094, target accuracy 0.855625 target loss 0.457818 accuracy domain distinction 0.500000 loss domain distinction 1.136281,\n",
      "VALIDATION Loss: 0.30122627 Acc: 0.90147783\n",
      "New best validation loss:  0.30122627317905426\n",
      "Epoch 2 of 500 took 0.335s\n",
      "Accuracy total 0.869062, main loss classifier 0.486682, source accuracy 0.878125 source classification loss 0.359899, target accuracy 0.860000 target loss 0.402001 accuracy domain distinction 0.500000 loss domain distinction 1.057323,\n",
      "VALIDATION Loss: 0.24777134 Acc: 0.93103448\n",
      "New best validation loss:  0.2477713397571019\n",
      "Epoch 3 of 500 took 0.331s\n",
      "Accuracy total 0.880000, main loss classifier 0.474173, source accuracy 0.876875 source classification loss 0.362599, target accuracy 0.883125 target loss 0.377699 accuracy domain distinction 0.500000 loss domain distinction 1.040240,\n",
      "VALIDATION Loss: 0.22991141 Acc: 0.92610837\n",
      "New best validation loss:  0.2299114050609725\n",
      "Epoch 4 of 500 took 0.335s\n",
      "Accuracy total 0.874687, main loss classifier 0.480283, source accuracy 0.866875 source classification loss 0.390507, target accuracy 0.882500 target loss 0.362459 accuracy domain distinction 0.500000 loss domain distinction 1.037995,\n",
      "VALIDATION Loss: 0.22724805 Acc: 0.93103448\n",
      "New best validation loss:  0.2272480513368334\n",
      "Epoch 5 of 500 took 0.335s\n",
      "Accuracy total 0.885312, main loss classifier 0.460046, source accuracy 0.881875 source classification loss 0.363321, target accuracy 0.888750 target loss 0.348372 accuracy domain distinction 0.500000 loss domain distinction 1.041996,\n",
      "VALIDATION Loss: 0.25627626 Acc: 0.92610837\n",
      "Epoch 6 of 500 took 0.350s\n",
      "Accuracy total 0.885000, main loss classifier 0.450793, source accuracy 0.880000 source classification loss 0.356639, target accuracy 0.890000 target loss 0.338234 accuracy domain distinction 0.500000 loss domain distinction 1.033567,\n",
      "VALIDATION Loss: 0.22462635 Acc: 0.9408867\n",
      "New best validation loss:  0.22462634955133712\n",
      "Epoch 7 of 500 took 0.331s\n",
      "Accuracy total 0.894375, main loss classifier 0.426820, source accuracy 0.886250 source classification loss 0.336317, target accuracy 0.902500 target loss 0.310346 accuracy domain distinction 0.500000 loss domain distinction 1.034883,\n",
      "VALIDATION Loss: 0.21067911 Acc: 0.93349754\n",
      "New best validation loss:  0.21067911386489868\n",
      "Epoch 8 of 500 took 0.334s\n",
      "Accuracy total 0.883750, main loss classifier 0.448899, source accuracy 0.875625 source classification loss 0.359013, target accuracy 0.891875 target loss 0.331533 accuracy domain distinction 0.500000 loss domain distinction 1.036253,\n",
      "VALIDATION Loss: 0.25311111 Acc: 0.9137931\n",
      "Epoch 9 of 500 took 0.328s\n",
      "Accuracy total 0.888750, main loss classifier 0.443286, source accuracy 0.876250 source classification loss 0.361333, target accuracy 0.901250 target loss 0.318290 accuracy domain distinction 0.500000 loss domain distinction 1.034746,\n",
      "VALIDATION Loss: 0.22061469 Acc: 0.93596059\n",
      "Epoch 10 of 500 took 0.329s\n",
      "Accuracy total 0.891563, main loss classifier 0.427858, source accuracy 0.885625 source classification loss 0.336767, target accuracy 0.897500 target loss 0.312117 accuracy domain distinction 0.500000 loss domain distinction 1.034161,\n",
      "VALIDATION Loss: 0.22641246 Acc: 0.93103448\n",
      "Epoch 11 of 500 took 0.332s\n",
      "Accuracy total 0.896250, main loss classifier 0.434105, source accuracy 0.890625 source classification loss 0.351186, target accuracy 0.901875 target loss 0.310357 accuracy domain distinction 0.500000 loss domain distinction 1.033333,\n",
      "VALIDATION Loss: 0.23806766 Acc: 0.92610837\n",
      "Epoch 12 of 500 took 0.328s\n",
      "Accuracy total 0.881250, main loss classifier 0.453047, source accuracy 0.876250 source classification loss 0.348483, target accuracy 0.886250 target loss 0.350502 accuracy domain distinction 0.500000 loss domain distinction 1.035541,\n",
      "VALIDATION Loss: 0.19295218 Acc: 0.93103448\n",
      "New best validation loss:  0.1929521837404796\n",
      "Epoch 13 of 500 took 0.334s\n",
      "Accuracy total 0.887813, main loss classifier 0.428886, source accuracy 0.870000 source classification loss 0.358872, target accuracy 0.905625 target loss 0.291815 accuracy domain distinction 0.500000 loss domain distinction 1.035426,\n",
      "VALIDATION Loss: 0.19096213 Acc: 0.94827586\n",
      "New best validation loss:  0.19096212834119797\n",
      "Epoch 14 of 500 took 0.335s\n",
      "Accuracy total 0.892813, main loss classifier 0.436138, source accuracy 0.894375 source classification loss 0.317902, target accuracy 0.891250 target loss 0.348017 accuracy domain distinction 0.500000 loss domain distinction 1.031786,\n",
      "VALIDATION Loss: 0.17185214 Acc: 0.94334975\n",
      "New best validation loss:  0.1718521352325167\n",
      "Epoch 15 of 500 took 0.331s\n",
      "Accuracy total 0.888750, main loss classifier 0.431298, source accuracy 0.871250 source classification loss 0.358022, target accuracy 0.906250 target loss 0.298223 accuracy domain distinction 0.500000 loss domain distinction 1.031755,\n",
      "VALIDATION Loss: 0.18669681 Acc: 0.94827586\n",
      "Epoch 16 of 500 took 0.331s\n",
      "Accuracy total 0.894687, main loss classifier 0.419623, source accuracy 0.881250 source classification loss 0.340433, target accuracy 0.908125 target loss 0.292272 accuracy domain distinction 0.500000 loss domain distinction 1.032714,\n",
      "VALIDATION Loss: 0.15887290 Acc: 0.95320197\n",
      "New best validation loss:  0.15887290239334106\n",
      "Epoch 17 of 500 took 0.335s\n",
      "Accuracy total 0.896250, main loss classifier 0.417957, source accuracy 0.895625 source classification loss 0.324275, target accuracy 0.896875 target loss 0.304668 accuracy domain distinction 0.500000 loss domain distinction 1.034853,\n",
      "VALIDATION Loss: 0.19823490 Acc: 0.93349754\n",
      "Epoch 18 of 500 took 0.330s\n",
      "Accuracy total 0.900000, main loss classifier 0.421007, source accuracy 0.891250 source classification loss 0.340978, target accuracy 0.908750 target loss 0.294314 accuracy domain distinction 0.500000 loss domain distinction 1.033613,\n",
      "VALIDATION Loss: 0.23421386 Acc: 0.95073892\n",
      "Epoch 19 of 500 took 0.335s\n",
      "Accuracy total 0.897188, main loss classifier 0.404875, source accuracy 0.885000 source classification loss 0.320543, target accuracy 0.909375 target loss 0.282061 accuracy domain distinction 0.500000 loss domain distinction 1.035732,\n",
      "VALIDATION Loss: 0.17436331 Acc: 0.95320197\n",
      "Epoch 20 of 500 took 0.334s\n",
      "Accuracy total 0.892188, main loss classifier 0.415499, source accuracy 0.886250 source classification loss 0.341521, target accuracy 0.898125 target loss 0.282250 accuracy domain distinction 0.500000 loss domain distinction 1.036133,\n",
      "VALIDATION Loss: 0.18176027 Acc: 0.94334975\n",
      "Epoch 21 of 500 took 0.333s\n",
      "Accuracy total 0.904375, main loss classifier 0.401341, source accuracy 0.891250 source classification loss 0.330165, target accuracy 0.917500 target loss 0.265729 accuracy domain distinction 0.500000 loss domain distinction 1.033936,\n",
      "VALIDATION Loss: 0.15651078 Acc: 0.95566502\n",
      "New best validation loss:  0.15651077750538075\n",
      "Epoch 22 of 500 took 0.329s\n",
      "Accuracy total 0.892813, main loss classifier 0.425294, source accuracy 0.879375 source classification loss 0.358210, target accuracy 0.906250 target loss 0.285733 accuracy domain distinction 0.500000 loss domain distinction 1.033226,\n",
      "VALIDATION Loss: 0.19620981 Acc: 0.93842365\n",
      "Epoch 23 of 500 took 0.328s\n",
      "Accuracy total 0.905625, main loss classifier 0.404761, source accuracy 0.890000 source classification loss 0.347834, target accuracy 0.921250 target loss 0.254407 accuracy domain distinction 0.500000 loss domain distinction 1.036413,\n",
      "VALIDATION Loss: 0.17234956 Acc: 0.95320197\n",
      "Epoch 24 of 500 took 0.336s\n",
      "Accuracy total 0.900937, main loss classifier 0.406553, source accuracy 0.888125 source classification loss 0.331442, target accuracy 0.913750 target loss 0.274673 accuracy domain distinction 0.500000 loss domain distinction 1.034959,\n",
      "VALIDATION Loss: 0.16300704 Acc: 0.95320197\n",
      "Epoch 25 of 500 took 0.331s\n",
      "Accuracy total 0.893750, main loss classifier 0.420597, source accuracy 0.873750 source classification loss 0.354879, target accuracy 0.913750 target loss 0.279471 accuracy domain distinction 0.500000 loss domain distinction 1.034223,\n",
      "VALIDATION Loss: 0.20655698 Acc: 0.93103448\n",
      "Epoch 26 of 500 took 0.329s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy total 0.906563, main loss classifier 0.395951, source accuracy 0.887500 source classification loss 0.330109, target accuracy 0.925625 target loss 0.256319 accuracy domain distinction 0.500000 loss domain distinction 1.027373,\n",
      "VALIDATION Loss: 0.16230953 Acc: 0.94581281\n",
      "Epoch 27 of 500 took 0.332s\n",
      "Accuracy total 0.908125, main loss classifier 0.388030, source accuracy 0.891250 source classification loss 0.324947, target accuracy 0.925000 target loss 0.244313 accuracy domain distinction 0.500000 loss domain distinction 1.033991,\n",
      "VALIDATION Loss: 0.21504525 Acc: 0.93842365\n",
      "Epoch    27: reducing learning rate of group 0 to 2.0120e-05.\n",
      "Epoch 28 of 500 took 0.366s\n",
      "Accuracy total 0.903438, main loss classifier 0.400154, source accuracy 0.897500 source classification loss 0.331517, target accuracy 0.909375 target loss 0.263092 accuracy domain distinction 0.500000 loss domain distinction 1.028502,\n",
      "VALIDATION Loss: 0.20608390 Acc: 0.93596059\n",
      "Epoch 29 of 500 took 0.330s\n",
      "Accuracy total 0.906250, main loss classifier 0.406636, source accuracy 0.889375 source classification loss 0.342160, target accuracy 0.923125 target loss 0.265446 accuracy domain distinction 0.500000 loss domain distinction 1.028332,\n",
      "VALIDATION Loss: 0.16606906 Acc: 0.95320197\n",
      "Epoch 30 of 500 took 0.334s\n",
      "Accuracy total 0.902500, main loss classifier 0.408244, source accuracy 0.888750 source classification loss 0.335729, target accuracy 0.916250 target loss 0.274948 accuracy domain distinction 0.500000 loss domain distinction 1.029054,\n",
      "VALIDATION Loss: 0.19815243 Acc: 0.93103448\n",
      "Epoch 31 of 500 took 0.328s\n",
      "Accuracy total 0.904375, main loss classifier 0.407216, source accuracy 0.890625 source classification loss 0.334980, target accuracy 0.918125 target loss 0.274501 accuracy domain distinction 0.500000 loss domain distinction 1.024757,\n",
      "VALIDATION Loss: 0.15890710 Acc: 0.95812808\n",
      "Epoch 32 of 500 took 0.330s\n",
      "Accuracy total 0.906875, main loss classifier 0.394068, source accuracy 0.895625 source classification loss 0.315518, target accuracy 0.918125 target loss 0.266748 accuracy domain distinction 0.500000 loss domain distinction 1.029351,\n",
      "VALIDATION Loss: 0.17736791 Acc: 0.94581281\n",
      "Epoch 33 of 500 took 0.332s\n",
      "Training complete in 0m 11s\n",
      "['participant_0']\n",
      "Optimizer =  <generator object Module.parameters at 0x7fdcf8e19900>\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump4/DANN/participant_0/best_state_4.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump4/DANN/participant_0/best_state_4.pt' (epoch 13)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump4/TSD/participant_0/best_state_0.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump4/TSD/participant_0/best_state_0.pt' (epoch 5)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump4/DANN/participant_0/best_state_1.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump4/DANN/participant_0/best_state_1.pt' (epoch 25)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump4/DANN/participant_0/best_state_2.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump4/DANN/participant_0/best_state_2.pt' (epoch 26)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump4/DANN/participant_0/best_state_3.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump4/DANN/participant_0/best_state_3.pt' (epoch 19)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump4/DANN/participant_0/best_state_4.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump4/DANN/participant_0/best_state_4.pt' (epoch 13)\n",
      "==== models_array =  (5,)  @ session  4\n",
      "HANDLING NEW SESSION  0\n",
      "HANDLING NEW SESSION  1\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  10\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  12\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.6538461538461539  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  9\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.0  len before:  26   len after:  4\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  13\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.8461538461538461  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.0  len before:  26   len after:  12\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  14\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.8754370629370629   Accuracy pseudo: 0.9600950118764846  len pseudo:  2105    len predictions 2288\n",
      "HANDLING NEW SESSION  2\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.0  len before:  26   len after:  17\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  8\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.9615384615384616  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  9\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.0  len before:  26   len after:  10\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.6923076923076923  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  13\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.3076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  14\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  1.0  len before:  26   len after:  16\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.8461538461538461  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  9\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  15\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  1\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.5384615384615384  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  1\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  15\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.6538461538461539  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.4117647058823529  len before:  26   len after:  17\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.8229895104895105   Accuracy pseudo: 0.9351351351351351  len pseudo:  2035    len predictions 2288\n",
      "HANDLING NEW SESSION  3\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.4230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.6538461538461539  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.6923076923076923  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.6923076923076923  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  14\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.0  len before:  26   len after:  6\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.0  len before:  26   len after:  5\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  12\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.0  len before:  26   len after:  6\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  3\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.0  len before:  26   len after:  17\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  13\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  7\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.0  len before:  26   len after:  4\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.8190559440559441   Accuracy pseudo: 0.9246305418719212  len pseudo:  2030    len predictions 2288\n",
      "HANDLING NEW SESSION  4\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.5769230769230769  len before:  26   len after:  26\n",
      "BEFORE:  0.11538461538461539   AFTER:  0.0  len before:  26   len after:  8\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  11\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  8\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.5384615384615384  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.6923076923076923  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  7\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  12\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.0  len before:  26   len after:  4\n",
      "BEFORE:  0.4230769230769231   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  14\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.0  len before:  26   len after:  8\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.7058823529411765  len before:  26   len after:  17\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.0  len before:  26   len after:  1\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.46153846153846156  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  7\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.6538461538461539  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.7333333333333333  len before:  26   len after:  15\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.0  len before:  26   len after:  7\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  1\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.5384615384615384  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.6  len before:  26   len after:  10\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  15\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.23076923076923078   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.7027972027972028   Accuracy pseudo: 0.8534858387799564  len pseudo:  1836    len predictions 2288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING TRAINING\n",
      "Accuracy total 0.824929, main loss classifier 0.672508, source accuracy 0.862216 source classification loss 0.401190, target accuracy 0.787642 target loss 0.713835 accuracy domain distinction 0.500000 loss domain distinction 1.149958,\n",
      "VALIDATION Loss: 0.34840690 Acc: 0.9048913\n",
      "New best validation loss:  0.34840689847866696\n",
      "Epoch 2 of 500 took 0.295s\n",
      "Accuracy total 0.841619, main loss classifier 0.625489, source accuracy 0.860795 source classification loss 0.420337, target accuracy 0.822443 target loss 0.617457 accuracy domain distinction 0.500000 loss domain distinction 1.065920,\n",
      "VALIDATION Loss: 0.28564647 Acc: 0.92663043\n",
      "New best validation loss:  0.2856464708844821\n",
      "Epoch 3 of 500 took 0.293s\n",
      "Accuracy total 0.856889, main loss classifier 0.555869, source accuracy 0.884943 source classification loss 0.365599, target accuracy 0.828835 target loss 0.536745 accuracy domain distinction 0.500000 loss domain distinction 1.046972,\n",
      "VALIDATION Loss: 0.28151293 Acc: 0.9076087\n",
      "New best validation loss:  0.28151293098926544\n",
      "Epoch 4 of 500 took 0.293s\n",
      "Accuracy total 0.855824, main loss classifier 0.535890, source accuracy 0.862216 source classification loss 0.372533, target accuracy 0.849432 target loss 0.492072 accuracy domain distinction 0.500000 loss domain distinction 1.035876,\n",
      "VALIDATION Loss: 0.30773732 Acc: 0.91304348\n",
      "Epoch 5 of 500 took 0.296s\n",
      "Accuracy total 0.854759, main loss classifier 0.565786, source accuracy 0.868608 source classification loss 0.383923, target accuracy 0.840909 target loss 0.538598 accuracy domain distinction 0.500000 loss domain distinction 1.045260,\n",
      "VALIDATION Loss: 0.27228273 Acc: 0.93206522\n",
      "New best validation loss:  0.27228272954622906\n",
      "Epoch 6 of 500 took 0.294s\n",
      "Accuracy total 0.871804, main loss classifier 0.498779, source accuracy 0.880682 source classification loss 0.355234, target accuracy 0.862926 target loss 0.435214 accuracy domain distinction 0.500000 loss domain distinction 1.035542,\n",
      "VALIDATION Loss: 0.24600527 Acc: 0.9375\n",
      "New best validation loss:  0.24600527063012123\n",
      "Epoch 7 of 500 took 0.294s\n",
      "Accuracy total 0.862571, main loss classifier 0.517162, source accuracy 0.881392 source classification loss 0.344077, target accuracy 0.843750 target loss 0.482925 accuracy domain distinction 0.500000 loss domain distinction 1.036607,\n",
      "VALIDATION Loss: 0.23611183 Acc: 0.93478261\n",
      "New best validation loss:  0.23611183216174444\n",
      "Epoch 8 of 500 took 0.293s\n",
      "Accuracy total 0.861861, main loss classifier 0.534042, source accuracy 0.867188 source classification loss 0.404044, target accuracy 0.856534 target loss 0.456643 accuracy domain distinction 0.500000 loss domain distinction 1.036985,\n",
      "VALIDATION Loss: 0.26118149 Acc: 0.92119565\n",
      "Epoch 9 of 500 took 0.297s\n",
      "Accuracy total 0.861506, main loss classifier 0.530327, source accuracy 0.870739 source classification loss 0.404070, target accuracy 0.852273 target loss 0.450553 accuracy domain distinction 0.500000 loss domain distinction 1.030157,\n",
      "VALIDATION Loss: 0.25496383 Acc: 0.91032609\n",
      "Epoch 10 of 500 took 0.293s\n",
      "Accuracy total 0.870028, main loss classifier 0.509888, source accuracy 0.880682 source classification loss 0.389697, target accuracy 0.859375 target loss 0.423300 accuracy domain distinction 0.500000 loss domain distinction 1.033888,\n",
      "VALIDATION Loss: 0.23771655 Acc: 0.9375\n",
      "Epoch 11 of 500 took 0.289s\n",
      "Accuracy total 0.872159, main loss classifier 0.498685, source accuracy 0.875710 source classification loss 0.363743, target accuracy 0.868608 target loss 0.426034 accuracy domain distinction 0.500000 loss domain distinction 1.037967,\n",
      "VALIDATION Loss: 0.24381087 Acc: 0.92663043\n",
      "Epoch 12 of 500 took 0.295s\n",
      "Accuracy total 0.861506, main loss classifier 0.505414, source accuracy 0.867188 source classification loss 0.367733, target accuracy 0.855824 target loss 0.436017 accuracy domain distinction 0.500000 loss domain distinction 1.035394,\n",
      "VALIDATION Loss: 0.23615123 Acc: 0.94565217\n",
      "Epoch 13 of 500 took 0.290s\n",
      "Accuracy total 0.876776, main loss classifier 0.482589, source accuracy 0.875710 source classification loss 0.354457, target accuracy 0.877841 target loss 0.403956 accuracy domain distinction 0.500000 loss domain distinction 1.033827,\n",
      "VALIDATION Loss: 0.22862910 Acc: 0.93478261\n",
      "New best validation loss:  0.22862910479307175\n",
      "Epoch 14 of 500 took 0.295s\n",
      "Accuracy total 0.869673, main loss classifier 0.496763, source accuracy 0.869318 source classification loss 0.389840, target accuracy 0.870028 target loss 0.397105 accuracy domain distinction 0.500000 loss domain distinction 1.032910,\n",
      "VALIDATION Loss: 0.25272135 Acc: 0.91847826\n",
      "Epoch 15 of 500 took 0.294s\n",
      "Accuracy total 0.876420, main loss classifier 0.478086, source accuracy 0.876420 source classification loss 0.358866, target accuracy 0.876420 target loss 0.390377 accuracy domain distinction 0.500000 loss domain distinction 1.034639,\n",
      "VALIDATION Loss: 0.28666482 Acc: 0.91847826\n",
      "Epoch 16 of 500 took 0.332s\n",
      "Accuracy total 0.872514, main loss classifier 0.501268, source accuracy 0.871449 source classification loss 0.401052, target accuracy 0.873580 target loss 0.395558 accuracy domain distinction 0.500000 loss domain distinction 1.029624,\n",
      "VALIDATION Loss: 0.22457869 Acc: 0.93478261\n",
      "New best validation loss:  0.22457868605852127\n",
      "Epoch 17 of 500 took 0.298s\n",
      "Accuracy total 0.877841, main loss classifier 0.481536, source accuracy 0.882812 source classification loss 0.377539, target accuracy 0.872869 target loss 0.379651 accuracy domain distinction 0.500000 loss domain distinction 1.029409,\n",
      "VALIDATION Loss: 0.19983262 Acc: 0.95108696\n",
      "New best validation loss:  0.1998326194783052\n",
      "Epoch 18 of 500 took 0.306s\n",
      "Accuracy total 0.871094, main loss classifier 0.473426, source accuracy 0.873580 source classification loss 0.351216, target accuracy 0.868608 target loss 0.389348 accuracy domain distinction 0.500000 loss domain distinction 1.031439,\n",
      "VALIDATION Loss: 0.21498300 Acc: 0.94021739\n",
      "Epoch 19 of 500 took 0.293s\n",
      "Accuracy total 0.868963, main loss classifier 0.491190, source accuracy 0.872159 source classification loss 0.378764, target accuracy 0.865767 target loss 0.396905 accuracy domain distinction 0.500000 loss domain distinction 1.033558,\n",
      "VALIDATION Loss: 0.23857520 Acc: 0.94021739\n",
      "Epoch 20 of 500 took 0.292s\n",
      "Accuracy total 0.878196, main loss classifier 0.470566, source accuracy 0.887784 source classification loss 0.337856, target accuracy 0.868608 target loss 0.395636 accuracy domain distinction 0.500000 loss domain distinction 1.038203,\n",
      "VALIDATION Loss: 0.20537411 Acc: 0.95108696\n",
      "Epoch 21 of 500 took 0.301s\n",
      "Accuracy total 0.876420, main loss classifier 0.493931, source accuracy 0.876420 source classification loss 0.367187, target accuracy 0.876420 target loss 0.413927 accuracy domain distinction 0.500000 loss domain distinction 1.033739,\n",
      "VALIDATION Loss: 0.17201106 Acc: 0.95652174\n",
      "New best validation loss:  0.17201105567316213\n",
      "Epoch 22 of 500 took 0.294s\n",
      "Accuracy total 0.873580, main loss classifier 0.491273, source accuracy 0.867898 source classification loss 0.415006, target accuracy 0.879261 target loss 0.361692 accuracy domain distinction 0.500000 loss domain distinction 1.029239,\n",
      "VALIDATION Loss: 0.16638935 Acc: 0.95380435\n",
      "New best validation loss:  0.1663893535733223\n",
      "Epoch 23 of 500 took 0.295s\n",
      "Accuracy total 0.882102, main loss classifier 0.446747, source accuracy 0.889915 source classification loss 0.326379, target accuracy 0.874290 target loss 0.360559 accuracy domain distinction 0.500000 loss domain distinction 1.032781,\n",
      "VALIDATION Loss: 0.20205906 Acc: 0.95380435\n",
      "Epoch 24 of 500 took 0.293s\n",
      "Accuracy total 0.883168, main loss classifier 0.466794, source accuracy 0.882812 source classification loss 0.363080, target accuracy 0.883523 target loss 0.363425 accuracy domain distinction 0.500000 loss domain distinction 1.035413,\n",
      "VALIDATION Loss: 0.26417711 Acc: 0.92119565\n",
      "Epoch 25 of 500 took 0.291s\n",
      "Accuracy total 0.879261, main loss classifier 0.471975, source accuracy 0.879972 source classification loss 0.360008, target accuracy 0.878551 target loss 0.378463 accuracy domain distinction 0.500000 loss domain distinction 1.027393,\n",
      "VALIDATION Loss: 0.18638313 Acc: 0.94021739\n",
      "Epoch 26 of 500 took 0.298s\n",
      "Accuracy total 0.882812, main loss classifier 0.453963, source accuracy 0.895597 source classification loss 0.336011, target accuracy 0.870028 target loss 0.365861 accuracy domain distinction 0.500000 loss domain distinction 1.030271,\n",
      "VALIDATION Loss: 0.20001343 Acc: 0.9375\n",
      "Epoch 27 of 500 took 0.291s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy total 0.882102, main loss classifier 0.462093, source accuracy 0.887074 source classification loss 0.362556, target accuracy 0.877131 target loss 0.355845 accuracy domain distinction 0.500000 loss domain distinction 1.028929,\n",
      "VALIDATION Loss: 0.19896138 Acc: 0.94293478\n",
      "Epoch 28 of 500 took 0.293s\n",
      "Accuracy total 0.882457, main loss classifier 0.463802, source accuracy 0.872159 source classification loss 0.374930, target accuracy 0.892756 target loss 0.345792 accuracy domain distinction 0.500000 loss domain distinction 1.034406,\n",
      "VALIDATION Loss: 0.18434013 Acc: 0.94836957\n",
      "Epoch    28: reducing learning rate of group 0 to 2.0120e-05.\n",
      "Epoch 29 of 500 took 0.295s\n",
      "Accuracy total 0.885298, main loss classifier 0.441679, source accuracy 0.877131 source classification loss 0.364139, target accuracy 0.893466 target loss 0.312882 accuracy domain distinction 0.500000 loss domain distinction 1.031685,\n",
      "VALIDATION Loss: 0.19114443 Acc: 0.94565217\n",
      "Epoch 30 of 500 took 0.292s\n",
      "Accuracy total 0.871804, main loss classifier 0.474179, source accuracy 0.867188 source classification loss 0.380173, target accuracy 0.876420 target loss 0.363008 accuracy domain distinction 0.500000 loss domain distinction 1.025885,\n",
      "VALIDATION Loss: 0.20710894 Acc: 0.94293478\n",
      "Epoch 31 of 500 took 0.293s\n",
      "Accuracy total 0.883878, main loss classifier 0.462470, source accuracy 0.881392 source classification loss 0.361843, target accuracy 0.886364 target loss 0.356503 accuracy domain distinction 0.500000 loss domain distinction 1.032972,\n",
      "VALIDATION Loss: 0.23344425 Acc: 0.92119565\n",
      "Epoch 32 of 500 took 0.293s\n",
      "Accuracy total 0.883168, main loss classifier 0.460974, source accuracy 0.887784 source classification loss 0.351649, target accuracy 0.878551 target loss 0.365552 accuracy domain distinction 0.500000 loss domain distinction 1.023740,\n",
      "VALIDATION Loss: 0.17649797 Acc: 0.94836957\n",
      "Epoch 33 of 500 took 0.296s\n",
      "Accuracy total 0.888849, main loss classifier 0.445183, source accuracy 0.889205 source classification loss 0.335605, target accuracy 0.888494 target loss 0.348082 accuracy domain distinction 0.500000 loss domain distinction 1.033389,\n",
      "VALIDATION Loss: 0.19998692 Acc: 0.95380435\n",
      "Epoch 34 of 500 took 0.297s\n",
      "Training complete in 0m 10s\n",
      "['participant_0']\n",
      "Optimizer =  <generator object Module.parameters at 0x7fdcf8e19dd0>\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump4/DANN/participant_0/best_state_5.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump4/DANN/participant_0/best_state_5.pt' (epoch 24)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump4/TSD/participant_0/best_state_0.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump4/TSD/participant_0/best_state_0.pt' (epoch 5)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump4/DANN/participant_0/best_state_1.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump4/DANN/participant_0/best_state_1.pt' (epoch 25)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump4/DANN/participant_0/best_state_2.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump4/DANN/participant_0/best_state_2.pt' (epoch 26)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump4/DANN/participant_0/best_state_3.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump4/DANN/participant_0/best_state_3.pt' (epoch 19)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump4/DANN/participant_0/best_state_4.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump4/DANN/participant_0/best_state_4.pt' (epoch 13)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump4/DANN/participant_0/best_state_5.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump4/DANN/participant_0/best_state_5.pt' (epoch 24)\n",
      "==== models_array =  (6,)  @ session  5\n",
      "HANDLING NEW SESSION  0\n",
      "HANDLING NEW SESSION  1\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  10\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  12\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.6538461538461539  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  9\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.0  len before:  26   len after:  4\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  13\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.8461538461538461  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.0  len before:  26   len after:  12\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  14\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.8754370629370629   Accuracy pseudo: 0.9600950118764846  len pseudo:  2105    len predictions 2288\n",
      "HANDLING NEW SESSION  2\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.0  len before:  26   len after:  17\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  8\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.9615384615384616  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  9\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.0  len before:  26   len after:  10\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.6923076923076923  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  13\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.3076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  14\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  1.0  len before:  26   len after:  16\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.8461538461538461  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  9\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  15\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  1\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.5384615384615384  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  1\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  15\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.6538461538461539  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.4117647058823529  len before:  26   len after:  17\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.8229895104895105   Accuracy pseudo: 0.9351351351351351  len pseudo:  2035    len predictions 2288\n",
      "HANDLING NEW SESSION  3\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.4230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.6538461538461539  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.6923076923076923  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.6923076923076923  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  14\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.0  len before:  26   len after:  6\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.0  len before:  26   len after:  5\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  12\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.0  len before:  26   len after:  6\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  3\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.0  len before:  26   len after:  17\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  13\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  7\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.0  len before:  26   len after:  4\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.8190559440559441   Accuracy pseudo: 0.9246305418719212  len pseudo:  2030    len predictions 2288\n",
      "HANDLING NEW SESSION  4\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.5769230769230769  len before:  26   len after:  26\n",
      "BEFORE:  0.11538461538461539   AFTER:  0.0  len before:  26   len after:  8\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  11\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  8\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.5384615384615384  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.6923076923076923  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  7\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  12\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.0  len before:  26   len after:  4\n",
      "BEFORE:  0.4230769230769231   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  14\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.0  len before:  26   len after:  8\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.7058823529411765  len before:  26   len after:  17\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.0  len before:  26   len after:  1\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.46153846153846156  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  7\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.6538461538461539  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.7333333333333333  len before:  26   len after:  15\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.0  len before:  26   len after:  7\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  1\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.5384615384615384  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.6  len before:  26   len after:  10\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  15\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.23076923076923078   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE:  0.6538461538461539   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.7027972027972028   Accuracy pseudo: 0.8534858387799564  len pseudo:  1836    len predictions 2288\n",
      "HANDLING NEW SESSION  5\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.6538461538461539  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.4230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  1\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.46153846153846156  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  13\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  15\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.6875  len before:  26   len after:  16\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  9\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  16\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  8\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.0  len before:  26   len after:  6\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  16\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.0  len before:  26   len after:  11\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  3\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.0  len before:  26   len after:  13\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.3076923076923077   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.0  len before:  26   len after:  1\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.0  len before:  26   len after:  13\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.875  len before:  26   len after:  8\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.5769230769230769  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.0  len before:  26   len after:  18\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  9\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.2222222222222222  len before:  26   len after:  9\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.5384615384615384  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.7084790209790209   Accuracy pseudo: 0.8112513144058885  len pseudo:  1902    len predictions 2288\n",
      "STARTING TRAINING\n",
      "Accuracy total 0.830842, main loss classifier 0.647985, source accuracy 0.863451 source classification loss 0.405638, target accuracy 0.798234 target loss 0.660667 accuracy domain distinction 0.500000 loss domain distinction 1.148323,\n",
      "VALIDATION Loss: 0.33258926 Acc: 0.90551181\n",
      "New best validation loss:  0.3325892587502797\n",
      "Epoch 2 of 500 took 0.313s\n",
      "Accuracy total 0.845788, main loss classifier 0.609416, source accuracy 0.855299 source classification loss 0.455520, target accuracy 0.836277 target loss 0.551032 accuracy domain distinction 0.500000 loss domain distinction 1.061402,\n",
      "VALIDATION Loss: 0.25256933 Acc: 0.92125984\n",
      "New best validation loss:  0.2525693252682686\n",
      "Epoch 3 of 500 took 0.308s\n",
      "Accuracy total 0.851223, main loss classifier 0.563145, source accuracy 0.870924 source classification loss 0.403414, target accuracy 0.831522 target loss 0.514692 accuracy domain distinction 0.500000 loss domain distinction 1.040927,\n",
      "VALIDATION Loss: 0.27839636 Acc: 0.92125984\n",
      "Epoch 4 of 500 took 0.305s\n",
      "Accuracy total 0.852921, main loss classifier 0.544748, source accuracy 0.864810 source classification loss 0.402017, target accuracy 0.841033 target loss 0.479871 accuracy domain distinction 0.500000 loss domain distinction 1.038037,\n",
      "VALIDATION Loss: 0.24158613 Acc: 0.92125984\n",
      "New best validation loss:  0.2415861338376999\n",
      "Epoch 5 of 500 took 0.305s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy total 0.865489, main loss classifier 0.520510, source accuracy 0.860734 source classification loss 0.408716, target accuracy 0.870245 target loss 0.425278 accuracy domain distinction 0.500000 loss domain distinction 1.035124,\n",
      "VALIDATION Loss: 0.23224241 Acc: 0.93700787\n",
      "New best validation loss:  0.2322424128651619\n",
      "Epoch 6 of 500 took 0.305s\n",
      "Accuracy total 0.864470, main loss classifier 0.522003, source accuracy 0.848505 source classification loss 0.449114, target accuracy 0.880435 target loss 0.387739 accuracy domain distinction 0.500000 loss domain distinction 1.035762,\n",
      "VALIDATION Loss: 0.21225858 Acc: 0.92650919\n",
      "New best validation loss:  0.2122585823138555\n",
      "Epoch 7 of 500 took 0.306s\n",
      "Accuracy total 0.864470, main loss classifier 0.530743, source accuracy 0.858696 source classification loss 0.435206, target accuracy 0.870245 target loss 0.417976 accuracy domain distinction 0.500000 loss domain distinction 1.041519,\n",
      "VALIDATION Loss: 0.24779855 Acc: 0.9160105\n",
      "Epoch 8 of 500 took 0.305s\n",
      "Accuracy total 0.864470, main loss classifier 0.501569, source accuracy 0.862772 source classification loss 0.389339, target accuracy 0.866168 target loss 0.405764 accuracy domain distinction 0.500000 loss domain distinction 1.040172,\n",
      "VALIDATION Loss: 0.22757355 Acc: 0.92125984\n",
      "Epoch 9 of 500 took 0.301s\n",
      "Accuracy total 0.870584, main loss classifier 0.493874, source accuracy 0.860054 source classification loss 0.411568, target accuracy 0.881114 target loss 0.369416 accuracy domain distinction 0.500000 loss domain distinction 1.033819,\n",
      "VALIDATION Loss: 0.21537868 Acc: 0.93175853\n",
      "Epoch 10 of 500 took 0.306s\n",
      "Accuracy total 0.875000, main loss classifier 0.482670, source accuracy 0.867527 source classification loss 0.390579, target accuracy 0.882473 target loss 0.369752 accuracy domain distinction 0.500000 loss domain distinction 1.025039,\n",
      "VALIDATION Loss: 0.21160622 Acc: 0.93963255\n",
      "New best validation loss:  0.21160622437795004\n",
      "Epoch 11 of 500 took 0.306s\n",
      "Accuracy total 0.879416, main loss classifier 0.471758, source accuracy 0.873641 source classification loss 0.386645, target accuracy 0.885190 target loss 0.349553 accuracy domain distinction 0.500000 loss domain distinction 1.036586,\n",
      "VALIDATION Loss: 0.23405217 Acc: 0.92913386\n",
      "Epoch 12 of 500 took 0.302s\n",
      "Accuracy total 0.863791, main loss classifier 0.530722, source accuracy 0.853261 source classification loss 0.466502, target accuracy 0.874321 target loss 0.388187 accuracy domain distinction 0.500000 loss domain distinction 1.033780,\n",
      "VALIDATION Loss: 0.20964925 Acc: 0.9343832\n",
      "New best validation loss:  0.2096492499113083\n",
      "Epoch 13 of 500 took 0.312s\n",
      "Accuracy total 0.868546, main loss classifier 0.499412, source accuracy 0.855978 source classification loss 0.414081, target accuracy 0.881114 target loss 0.378946 accuracy domain distinction 0.500000 loss domain distinction 1.028987,\n",
      "VALIDATION Loss: 0.22517439 Acc: 0.92650919\n",
      "Epoch 14 of 500 took 0.303s\n",
      "Accuracy total 0.871603, main loss classifier 0.483994, source accuracy 0.862772 source classification loss 0.402131, target accuracy 0.880435 target loss 0.358750 accuracy domain distinction 0.500000 loss domain distinction 1.035536,\n",
      "VALIDATION Loss: 0.19967114 Acc: 0.94225722\n",
      "New best validation loss:  0.19967113683621088\n",
      "Epoch 15 of 500 took 0.313s\n",
      "Accuracy total 0.869226, main loss classifier 0.488505, source accuracy 0.852582 source classification loss 0.419061, target accuracy 0.885870 target loss 0.352120 accuracy domain distinction 0.500000 loss domain distinction 1.029145,\n",
      "VALIDATION Loss: 0.23158951 Acc: 0.92650919\n",
      "Epoch 16 of 500 took 0.303s\n",
      "Accuracy total 0.888587, main loss classifier 0.451159, source accuracy 0.877038 source classification loss 0.350324, target accuracy 0.900136 target loss 0.345323 accuracy domain distinction 0.500000 loss domain distinction 1.033350,\n",
      "VALIDATION Loss: 0.20884735 Acc: 0.93963255\n",
      "Epoch 17 of 500 took 0.306s\n",
      "Accuracy total 0.885190, main loss classifier 0.462912, source accuracy 0.873641 source classification loss 0.383745, target accuracy 0.896739 target loss 0.336297 accuracy domain distinction 0.500000 loss domain distinction 1.028908,\n",
      "VALIDATION Loss: 0.23416157 Acc: 0.91338583\n",
      "Epoch 18 of 500 took 0.303s\n",
      "Accuracy total 0.866508, main loss classifier 0.483207, source accuracy 0.854620 source classification loss 0.404563, target accuracy 0.878397 target loss 0.356281 accuracy domain distinction 0.500000 loss domain distinction 1.027854,\n",
      "VALIDATION Loss: 0.21587865 Acc: 0.92650919\n",
      "Epoch 19 of 500 took 0.302s\n",
      "Accuracy total 0.876698, main loss classifier 0.467038, source accuracy 0.868886 source classification loss 0.389172, target accuracy 0.884511 target loss 0.338841 accuracy domain distinction 0.500000 loss domain distinction 1.030312,\n",
      "VALIDATION Loss: 0.19933461 Acc: 0.94225722\n",
      "New best validation loss:  0.19933460528651872\n",
      "Epoch 20 of 500 took 0.324s\n",
      "Accuracy total 0.878397, main loss classifier 0.468469, source accuracy 0.862092 source classification loss 0.398254, target accuracy 0.894701 target loss 0.332026 accuracy domain distinction 0.500000 loss domain distinction 1.033286,\n",
      "VALIDATION Loss: 0.19507727 Acc: 0.94488189\n",
      "New best validation loss:  0.19507726530234018\n",
      "Epoch 21 of 500 took 0.349s\n",
      "Accuracy total 0.885870, main loss classifier 0.461168, source accuracy 0.868886 source classification loss 0.389597, target accuracy 0.902853 target loss 0.325942 accuracy domain distinction 0.500000 loss domain distinction 1.033986,\n",
      "VALIDATION Loss: 0.19148297 Acc: 0.95013123\n",
      "New best validation loss:  0.19148297111193338\n",
      "Epoch 22 of 500 took 0.305s\n",
      "Accuracy total 0.880435, main loss classifier 0.457480, source accuracy 0.879755 source classification loss 0.361586, target accuracy 0.881114 target loss 0.346592 accuracy domain distinction 0.500000 loss domain distinction 1.033907,\n",
      "VALIDATION Loss: 0.23173632 Acc: 0.91863517\n",
      "Epoch 23 of 500 took 0.308s\n",
      "Accuracy total 0.875000, main loss classifier 0.456445, source accuracy 0.876359 source classification loss 0.348327, target accuracy 0.873641 target loss 0.358085 accuracy domain distinction 0.500000 loss domain distinction 1.032392,\n",
      "VALIDATION Loss: 0.18230296 Acc: 0.95013123\n",
      "New best validation loss:  0.18230295553803444\n",
      "Epoch 24 of 500 took 0.305s\n",
      "Accuracy total 0.885190, main loss classifier 0.444696, source accuracy 0.877717 source classification loss 0.350597, target accuracy 0.892663 target loss 0.331697 accuracy domain distinction 0.500000 loss domain distinction 1.035487,\n",
      "VALIDATION Loss: 0.28601010 Acc: 0.90026247\n",
      "Epoch 25 of 500 took 0.303s\n",
      "Accuracy total 0.873302, main loss classifier 0.486922, source accuracy 0.850543 source classification loss 0.425335, target accuracy 0.896060 target loss 0.342053 accuracy domain distinction 0.500000 loss domain distinction 1.032285,\n",
      "VALIDATION Loss: 0.20425979 Acc: 0.94225722\n",
      "Epoch 26 of 500 took 0.308s\n",
      "Accuracy total 0.886889, main loss classifier 0.456554, source accuracy 0.875679 source classification loss 0.388197, target accuracy 0.898098 target loss 0.319359 accuracy domain distinction 0.500000 loss domain distinction 1.027767,\n",
      "VALIDATION Loss: 0.28840576 Acc: 0.87664042\n",
      "Epoch 27 of 500 took 0.305s\n",
      "Accuracy total 0.879416, main loss classifier 0.451377, source accuracy 0.875679 source classification loss 0.364428, target accuracy 0.883152 target loss 0.331064 accuracy domain distinction 0.500000 loss domain distinction 1.036307,\n",
      "VALIDATION Loss: 0.23422175 Acc: 0.92388451\n",
      "Epoch 28 of 500 took 0.302s\n",
      "Accuracy total 0.887568, main loss classifier 0.436240, source accuracy 0.873641 source classification loss 0.364582, target accuracy 0.901495 target loss 0.300797 accuracy domain distinction 0.500000 loss domain distinction 1.035501,\n",
      "VALIDATION Loss: 0.22986885 Acc: 0.92650919\n",
      "Epoch 29 of 500 took 0.305s\n",
      "Accuracy total 0.889606, main loss classifier 0.436911, source accuracy 0.877038 source classification loss 0.366300, target accuracy 0.902174 target loss 0.301172 accuracy domain distinction 0.500000 loss domain distinction 1.031747,\n",
      "VALIDATION Loss: 0.20253330 Acc: 0.9343832\n",
      "Epoch    29: reducing learning rate of group 0 to 2.0120e-05.\n",
      "Epoch 30 of 500 took 0.308s\n",
      "Accuracy total 0.884851, main loss classifier 0.449260, source accuracy 0.869565 source classification loss 0.378972, target accuracy 0.900136 target loss 0.313040 accuracy domain distinction 0.500000 loss domain distinction 1.032539,\n",
      "VALIDATION Loss: 0.25375854 Acc: 0.90551181\n",
      "Epoch 31 of 500 took 0.311s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy total 0.882133, main loss classifier 0.451599, source accuracy 0.867527 source classification loss 0.373661, target accuracy 0.896739 target loss 0.322247 accuracy domain distinction 0.500000 loss domain distinction 1.036452,\n",
      "VALIDATION Loss: 0.20623416 Acc: 0.92913386\n",
      "Epoch 32 of 500 took 0.305s\n",
      "Accuracy total 0.891984, main loss classifier 0.421133, source accuracy 0.871603 source classification loss 0.356743, target accuracy 0.912364 target loss 0.279013 accuracy domain distinction 0.500000 loss domain distinction 1.032544,\n",
      "VALIDATION Loss: 0.18127891 Acc: 0.94225722\n",
      "New best validation loss:  0.1812789055208365\n",
      "Epoch 33 of 500 took 0.313s\n",
      "Accuracy total 0.893003, main loss classifier 0.431680, source accuracy 0.873641 source classification loss 0.380359, target accuracy 0.912364 target loss 0.276332 accuracy domain distinction 0.500000 loss domain distinction 1.033343,\n",
      "VALIDATION Loss: 0.17887544 Acc: 0.93700787\n",
      "New best validation loss:  0.17887543514370918\n",
      "Epoch 34 of 500 took 0.308s\n",
      "Accuracy total 0.898438, main loss classifier 0.424668, source accuracy 0.887228 source classification loss 0.347562, target accuracy 0.909647 target loss 0.296417 accuracy domain distinction 0.500000 loss domain distinction 1.026783,\n",
      "VALIDATION Loss: 0.18535871 Acc: 0.93175853\n",
      "Epoch 35 of 500 took 0.302s\n",
      "Accuracy total 0.890625, main loss classifier 0.439557, source accuracy 0.880435 source classification loss 0.356164, target accuracy 0.900815 target loss 0.316576 accuracy domain distinction 0.500000 loss domain distinction 1.031878,\n",
      "VALIDATION Loss: 0.19937245 Acc: 0.93963255\n",
      "Epoch 36 of 500 took 0.304s\n",
      "Accuracy total 0.891984, main loss classifier 0.439859, source accuracy 0.873641 source classification loss 0.391336, target accuracy 0.910326 target loss 0.283402 accuracy domain distinction 0.500000 loss domain distinction 1.024902,\n",
      "VALIDATION Loss: 0.22920852 Acc: 0.92388451\n",
      "Epoch 37 of 500 took 0.305s\n",
      "Accuracy total 0.886549, main loss classifier 0.449078, source accuracy 0.866848 source classification loss 0.399876, target accuracy 0.906250 target loss 0.292392 accuracy domain distinction 0.500000 loss domain distinction 1.029438,\n",
      "VALIDATION Loss: 0.19597917 Acc: 0.93175853\n",
      "Epoch 38 of 500 took 0.301s\n",
      "Accuracy total 0.882812, main loss classifier 0.434788, source accuracy 0.854620 source classification loss 0.371829, target accuracy 0.911005 target loss 0.291188 accuracy domain distinction 0.500000 loss domain distinction 1.032800,\n",
      "VALIDATION Loss: 0.19256413 Acc: 0.9343832\n",
      "Epoch 39 of 500 took 0.304s\n",
      "Accuracy total 0.890285, main loss classifier 0.433824, source accuracy 0.872962 source classification loss 0.367820, target accuracy 0.907609 target loss 0.293749 accuracy domain distinction 0.500000 loss domain distinction 1.030398,\n",
      "VALIDATION Loss: 0.20650496 Acc: 0.93175853\n",
      "Epoch    39: reducing learning rate of group 0 to 4.0240e-06.\n",
      "Epoch 40 of 500 took 0.305s\n",
      "Accuracy total 0.893003, main loss classifier 0.429704, source accuracy 0.887908 source classification loss 0.352522, target accuracy 0.898098 target loss 0.301805 accuracy domain distinction 0.500000 loss domain distinction 1.025405,\n",
      "VALIDATION Loss: 0.20573632 Acc: 0.92388451\n",
      "Epoch 41 of 500 took 0.308s\n",
      "Accuracy total 0.893682, main loss classifier 0.424862, source accuracy 0.874321 source classification loss 0.383256, target accuracy 0.913043 target loss 0.261353 accuracy domain distinction 0.500000 loss domain distinction 1.025576,\n",
      "VALIDATION Loss: 0.16960749 Acc: 0.93175853\n",
      "New best validation loss:  0.1696074903011322\n",
      "Epoch 42 of 500 took 0.396s\n",
      "Accuracy total 0.887568, main loss classifier 0.428071, source accuracy 0.872962 source classification loss 0.361438, target accuracy 0.902174 target loss 0.288824 accuracy domain distinction 0.500000 loss domain distinction 1.029401,\n",
      "VALIDATION Loss: 0.22008608 Acc: 0.91338583\n",
      "Epoch 43 of 500 took 0.327s\n",
      "Accuracy total 0.889946, main loss classifier 0.442157, source accuracy 0.870245 source classification loss 0.377043, target accuracy 0.909647 target loss 0.301278 accuracy domain distinction 0.500000 loss domain distinction 1.029961,\n",
      "VALIDATION Loss: 0.20758062 Acc: 0.9343832\n",
      "Epoch 44 of 500 took 0.384s\n",
      "Accuracy total 0.894022, main loss classifier 0.420941, source accuracy 0.881114 source classification loss 0.352918, target accuracy 0.906929 target loss 0.283576 accuracy domain distinction 0.500000 loss domain distinction 1.026940,\n",
      "VALIDATION Loss: 0.18065468 Acc: 0.93963255\n",
      "Epoch 45 of 500 took 0.320s\n",
      "Accuracy total 0.882133, main loss classifier 0.457850, source accuracy 0.858016 source classification loss 0.402900, target accuracy 0.906250 target loss 0.307026 accuracy domain distinction 0.500000 loss domain distinction 1.028869,\n",
      "VALIDATION Loss: 0.21096352 Acc: 0.93700787\n",
      "Epoch 46 of 500 took 0.313s\n",
      "Accuracy total 0.899117, main loss classifier 0.421882, source accuracy 0.887908 source classification loss 0.358566, target accuracy 0.910326 target loss 0.279479 accuracy domain distinction 0.500000 loss domain distinction 1.028599,\n",
      "VALIDATION Loss: 0.26983380 Acc: 0.8976378\n",
      "Epoch 47 of 500 took 0.311s\n",
      "Accuracy total 0.886209, main loss classifier 0.452269, source accuracy 0.881793 source classification loss 0.360218, target accuracy 0.890625 target loss 0.339055 accuracy domain distinction 0.500000 loss domain distinction 1.026323,\n",
      "VALIDATION Loss: 0.22223677 Acc: 0.91076115\n",
      "Epoch    47: reducing learning rate of group 0 to 8.0480e-07.\n",
      "Epoch 48 of 500 took 0.305s\n",
      "Accuracy total 0.895380, main loss classifier 0.429500, source accuracy 0.888587 source classification loss 0.344313, target accuracy 0.902174 target loss 0.308207 accuracy domain distinction 0.500000 loss domain distinction 1.032402,\n",
      "VALIDATION Loss: 0.20648299 Acc: 0.92650919\n",
      "Epoch 49 of 500 took 0.317s\n",
      "Accuracy total 0.884511, main loss classifier 0.440793, source accuracy 0.859375 source classification loss 0.383175, target accuracy 0.909647 target loss 0.292672 accuracy domain distinction 0.500000 loss domain distinction 1.028698,\n",
      "VALIDATION Loss: 0.18981160 Acc: 0.93700787\n",
      "Epoch 50 of 500 took 0.307s\n",
      "Accuracy total 0.894361, main loss classifier 0.424564, source accuracy 0.883832 source classification loss 0.348545, target accuracy 0.904891 target loss 0.294500 accuracy domain distinction 0.500000 loss domain distinction 1.030411,\n",
      "VALIDATION Loss: 0.21249912 Acc: 0.92388451\n",
      "Epoch 51 of 500 took 0.305s\n",
      "Accuracy total 0.892323, main loss classifier 0.436056, source accuracy 0.878397 source classification loss 0.358398, target accuracy 0.906250 target loss 0.308105 accuracy domain distinction 0.500000 loss domain distinction 1.028042,\n",
      "VALIDATION Loss: 0.20015687 Acc: 0.92388451\n",
      "Epoch 52 of 500 took 0.312s\n",
      "Accuracy total 0.884851, main loss classifier 0.447157, source accuracy 0.872962 source classification loss 0.385453, target accuracy 0.896739 target loss 0.303202 accuracy domain distinction 0.500000 loss domain distinction 1.028289,\n",
      "VALIDATION Loss: 0.22148037 Acc: 0.93963255\n",
      "Epoch 53 of 500 took 0.305s\n",
      "Training complete in 0m 16s\n",
      "['participant_0']\n",
      "Optimizer =  <generator object Module.parameters at 0x7fdcf8e19900>\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump4/DANN/participant_0/best_state_6.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump4/DANN/participant_0/best_state_6.pt' (epoch 17)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump4/TSD/participant_0/best_state_0.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump4/TSD/participant_0/best_state_0.pt' (epoch 5)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump4/DANN/participant_0/best_state_1.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump4/DANN/participant_0/best_state_1.pt' (epoch 25)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump4/DANN/participant_0/best_state_2.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump4/DANN/participant_0/best_state_2.pt' (epoch 26)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump4/DANN/participant_0/best_state_3.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump4/DANN/participant_0/best_state_3.pt' (epoch 19)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump4/DANN/participant_0/best_state_4.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump4/DANN/participant_0/best_state_4.pt' (epoch 13)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump4/DANN/participant_0/best_state_5.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump4/DANN/participant_0/best_state_5.pt' (epoch 24)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump4/DANN/participant_0/best_state_6.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump4/DANN/participant_0/best_state_6.pt' (epoch 17)\n",
      "==== models_array =  (7,)  @ session  6\n",
      "HANDLING NEW SESSION  0\n",
      "HANDLING NEW SESSION  1\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  10\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  12\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.6538461538461539  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  9\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.0  len before:  26   len after:  4\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  13\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.8461538461538461  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.0  len before:  26   len after:  12\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  14\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY MODEL:  0.8754370629370629   Accuracy pseudo: 0.9600950118764846  len pseudo:  2105    len predictions 2288\n",
      "HANDLING NEW SESSION  2\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.0  len before:  26   len after:  17\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  8\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.9615384615384616  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  9\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.0  len before:  26   len after:  10\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.6923076923076923  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  13\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.3076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  14\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  1.0  len before:  26   len after:  16\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.8461538461538461  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  9\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  15\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  1\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.5384615384615384  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  1\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  15\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.6538461538461539  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.4117647058823529  len before:  26   len after:  17\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.8229895104895105   Accuracy pseudo: 0.9351351351351351  len pseudo:  2035    len predictions 2288\n",
      "HANDLING NEW SESSION  3\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.4230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.6538461538461539  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.6923076923076923  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.6923076923076923  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  14\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.0  len before:  26   len after:  6\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.0  len before:  26   len after:  5\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  12\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.0  len before:  26   len after:  6\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  3\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.0  len before:  26   len after:  17\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  13\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  7\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.0  len before:  26   len after:  4\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.8190559440559441   Accuracy pseudo: 0.9246305418719212  len pseudo:  2030    len predictions 2288\n",
      "HANDLING NEW SESSION  4\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.5769230769230769  len before:  26   len after:  26\n",
      "BEFORE:  0.11538461538461539   AFTER:  0.0  len before:  26   len after:  8\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  11\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  8\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.5384615384615384  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.6923076923076923  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  7\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  12\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.0  len before:  26   len after:  4\n",
      "BEFORE:  0.4230769230769231   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  14\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.0  len before:  26   len after:  8\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.7058823529411765  len before:  26   len after:  17\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.0  len before:  26   len after:  1\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.46153846153846156  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  7\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.6538461538461539  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.7333333333333333  len before:  26   len after:  15\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.0  len before:  26   len after:  7\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  1\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.5384615384615384  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.6  len before:  26   len after:  10\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  15\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.23076923076923078   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.7027972027972028   Accuracy pseudo: 0.8534858387799564  len pseudo:  1836    len predictions 2288\n",
      "HANDLING NEW SESSION  5\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.6538461538461539  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.4230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  1\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.46153846153846156  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  13\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  15\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.6875  len before:  26   len after:  16\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  9\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  16\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  8\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.0  len before:  26   len after:  6\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  16\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.0  len before:  26   len after:  11\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  3\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.0  len before:  26   len after:  13\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.3076923076923077   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.0  len before:  26   len after:  1\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.0  len before:  26   len after:  13\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.875  len before:  26   len after:  8\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.5769230769230769  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.0  len before:  26   len after:  18\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  9\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.2222222222222222  len before:  26   len after:  9\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.5384615384615384  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.7084790209790209   Accuracy pseudo: 0.8112513144058885  len pseudo:  1902    len predictions 2288\n",
      "HANDLING NEW SESSION  6\n",
      "Finish segment dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.6666666666666666  len before:  26   len after:  18\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  5\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.625  len before:  26   len after:  16\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.0  len before:  26   len after:  6\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.5769230769230769  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  5\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  16\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.46153846153846156  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  1.0  len before:  26   len after:  1\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  2\n",
      "BEFORE:  0.5   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.6470588235294118  len before:  26   len after:  17\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  16\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.9230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  1.0  len before:  26   len after:  12\n",
      "BEFORE:  0.6538461538461539   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  13\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.0  len before:  26   len after:  9\n",
      "BEFORE:  0.15384615384615385   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  18\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.6923076923076923  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.5882352941176471  len before:  26   len after:  17\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.2222222222222222  len before:  26   len after:  9\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  18\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.3333333333333333  len before:  26   len after:  9\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.645541958041958   Accuracy pseudo: 0.796277145811789  len pseudo:  1934    len predictions 2288\n",
      "STARTING TRAINING\n",
      "Accuracy total 0.785807, main loss classifier 0.800821, source accuracy 0.840495 source classification loss 0.471229, target accuracy 0.731120 target loss 0.903704 accuracy domain distinction 0.500000 loss domain distinction 1.133546,\n",
      "VALIDATION Loss: 0.40256533 Acc: 0.8630491\n",
      "New best validation loss:  0.40256533133132116\n",
      "Epoch 2 of 500 took 0.328s\n",
      "Accuracy total 0.819010, main loss classifier 0.674601, source accuracy 0.856771 source classification loss 0.417258, target accuracy 0.781250 target loss 0.721152 accuracy domain distinction 0.500000 loss domain distinction 1.053963,\n",
      "VALIDATION Loss: 0.56646462 Acc: 0.88372093\n",
      "Epoch 3 of 500 took 0.321s\n",
      "Accuracy total 0.823893, main loss classifier 0.633418, source accuracy 0.839193 source classification loss 0.443449, target accuracy 0.808594 target loss 0.615654 accuracy domain distinction 0.500000 loss domain distinction 1.038663,\n",
      "VALIDATION Loss: 0.46562223 Acc: 0.86046512\n",
      "Epoch 4 of 500 took 0.321s\n",
      "Accuracy total 0.830078, main loss classifier 0.637525, source accuracy 0.860026 source classification loss 0.408906, target accuracy 0.800130 target loss 0.658250 accuracy domain distinction 0.500000 loss domain distinction 1.039474,\n",
      "VALIDATION Loss: 0.54001572 Acc: 0.86821705\n",
      "Epoch 5 of 500 took 0.320s\n",
      "Accuracy total 0.829427, main loss classifier 0.632204, source accuracy 0.854818 source classification loss 0.432930, target accuracy 0.804036 target loss 0.623210 accuracy domain distinction 0.500000 loss domain distinction 1.041337,\n",
      "VALIDATION Loss: 0.33789487 Acc: 0.88113695\n",
      "New best validation loss:  0.33789486650909695\n",
      "Epoch 6 of 500 took 0.321s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy total 0.840169, main loss classifier 0.606744, source accuracy 0.868490 source classification loss 0.414648, target accuracy 0.811849 target loss 0.591628 accuracy domain distinction 0.500000 loss domain distinction 1.036059,\n",
      "VALIDATION Loss: 0.47746197 Acc: 0.87080103\n",
      "Epoch 7 of 500 took 0.323s\n",
      "Accuracy total 0.827799, main loss classifier 0.634405, source accuracy 0.846354 source classification loss 0.458374, target accuracy 0.809245 target loss 0.603529 accuracy domain distinction 0.500000 loss domain distinction 1.034540,\n",
      "VALIDATION Loss: 0.37363949 Acc: 0.87338501\n",
      "Epoch 8 of 500 took 0.320s\n",
      "Accuracy total 0.839844, main loss classifier 0.594726, source accuracy 0.863281 source classification loss 0.395104, target accuracy 0.816406 target loss 0.587223 accuracy domain distinction 0.500000 loss domain distinction 1.035621,\n",
      "VALIDATION Loss: 0.43068958 Acc: 0.88372093\n",
      "Epoch 9 of 500 took 0.317s\n",
      "Accuracy total 0.847656, main loss classifier 0.572658, source accuracy 0.870443 source classification loss 0.414225, target accuracy 0.824870 target loss 0.524468 accuracy domain distinction 0.500000 loss domain distinction 1.033111,\n",
      "VALIDATION Loss: 0.33178684 Acc: 0.88630491\n",
      "New best validation loss:  0.3317868390253612\n",
      "Epoch 10 of 500 took 0.321s\n",
      "Accuracy total 0.847331, main loss classifier 0.572479, source accuracy 0.858724 source classification loss 0.400390, target accuracy 0.835938 target loss 0.538159 accuracy domain distinction 0.500000 loss domain distinction 1.032045,\n",
      "VALIDATION Loss: 0.48453012 Acc: 0.86046512\n",
      "Epoch 11 of 500 took 0.319s\n",
      "Accuracy total 0.844727, main loss classifier 0.561065, source accuracy 0.860677 source classification loss 0.386458, target accuracy 0.828776 target loss 0.529362 accuracy domain distinction 0.500000 loss domain distinction 1.031553,\n",
      "VALIDATION Loss: 0.38718924 Acc: 0.87596899\n",
      "Epoch 12 of 500 took 0.319s\n",
      "Accuracy total 0.853190, main loss classifier 0.548598, source accuracy 0.861979 source classification loss 0.393603, target accuracy 0.844401 target loss 0.497577 accuracy domain distinction 0.500000 loss domain distinction 1.030085,\n",
      "VALIDATION Loss: 0.36485917 Acc: 0.88888889\n",
      "Epoch 13 of 500 took 0.321s\n",
      "Accuracy total 0.849935, main loss classifier 0.575795, source accuracy 0.849609 source classification loss 0.443002, target accuracy 0.850260 target loss 0.501400 accuracy domain distinction 0.500000 loss domain distinction 1.035933,\n",
      "VALIDATION Loss: 0.30800382 Acc: 0.90439276\n",
      "New best validation loss:  0.3080038194145475\n",
      "Epoch 14 of 500 took 0.322s\n",
      "Accuracy total 0.857422, main loss classifier 0.538481, source accuracy 0.873047 source classification loss 0.370590, target accuracy 0.841797 target loss 0.499016 accuracy domain distinction 0.500000 loss domain distinction 1.036781,\n",
      "VALIDATION Loss: 0.33521601 Acc: 0.87855297\n",
      "Epoch 15 of 500 took 0.318s\n",
      "Accuracy total 0.856445, main loss classifier 0.530566, source accuracy 0.863932 source classification loss 0.380769, target accuracy 0.848958 target loss 0.473508 accuracy domain distinction 0.500000 loss domain distinction 1.034272,\n",
      "VALIDATION Loss: 0.30807273 Acc: 0.88372093\n",
      "Epoch 16 of 500 took 0.321s\n",
      "Accuracy total 0.848633, main loss classifier 0.572438, source accuracy 0.863932 source classification loss 0.430214, target accuracy 0.833333 target loss 0.508372 accuracy domain distinction 0.500000 loss domain distinction 1.031450,\n",
      "VALIDATION Loss: 0.32337500 Acc: 0.87338501\n",
      "Epoch 17 of 500 took 0.319s\n",
      "Accuracy total 0.858073, main loss classifier 0.533724, source accuracy 0.866536 source classification loss 0.374935, target accuracy 0.849609 target loss 0.485296 accuracy domain distinction 0.500000 loss domain distinction 1.036085,\n",
      "VALIDATION Loss: 0.32586969 Acc: 0.89922481\n",
      "Epoch 18 of 500 took 0.321s\n",
      "Accuracy total 0.858073, main loss classifier 0.531859, source accuracy 0.867839 source classification loss 0.379494, target accuracy 0.848307 target loss 0.478519 accuracy domain distinction 0.500000 loss domain distinction 1.028526,\n",
      "VALIDATION Loss: 0.34164606 Acc: 0.88113695\n",
      "Epoch 19 of 500 took 0.322s\n",
      "Accuracy total 0.854818, main loss classifier 0.543763, source accuracy 0.870443 source classification loss 0.391899, target accuracy 0.839193 target loss 0.488766 accuracy domain distinction 0.500000 loss domain distinction 1.034307,\n",
      "VALIDATION Loss: 0.30797688 Acc: 0.88888889\n",
      "Epoch    19: reducing learning rate of group 0 to 2.0120e-05.\n",
      "New best validation loss:  0.3079768770507404\n",
      "Epoch 20 of 500 took 0.320s\n",
      "Accuracy total 0.857747, main loss classifier 0.521484, source accuracy 0.869792 source classification loss 0.374479, target accuracy 0.845703 target loss 0.462755 accuracy domain distinction 0.500000 loss domain distinction 1.028670,\n",
      "VALIDATION Loss: 0.30384809 Acc: 0.88888889\n",
      "New best validation loss:  0.3038480856588909\n",
      "Epoch 21 of 500 took 0.319s\n",
      "Accuracy total 0.868164, main loss classifier 0.507665, source accuracy 0.875000 source classification loss 0.381914, target accuracy 0.861328 target loss 0.427149 accuracy domain distinction 0.500000 loss domain distinction 1.031336,\n",
      "VALIDATION Loss: 0.30856313 Acc: 0.89405685\n",
      "Epoch 22 of 500 took 0.325s\n",
      "Accuracy total 0.861979, main loss classifier 0.532792, source accuracy 0.865234 source classification loss 0.395125, target accuracy 0.858724 target loss 0.464865 accuracy domain distinction 0.500000 loss domain distinction 1.027970,\n",
      "VALIDATION Loss: 0.28415631 Acc: 0.89664083\n",
      "New best validation loss:  0.28415631289993015\n",
      "Epoch 23 of 500 took 0.324s\n",
      "Accuracy total 0.866862, main loss classifier 0.517365, source accuracy 0.882161 source classification loss 0.371259, target accuracy 0.851562 target loss 0.457448 accuracy domain distinction 0.500000 loss domain distinction 1.030115,\n",
      "VALIDATION Loss: 0.36670373 Acc: 0.89147287\n",
      "Epoch 24 of 500 took 0.321s\n",
      "Accuracy total 0.856771, main loss classifier 0.537285, source accuracy 0.860026 source classification loss 0.395598, target accuracy 0.853516 target loss 0.472320 accuracy domain distinction 0.500000 loss domain distinction 1.033264,\n",
      "VALIDATION Loss: 0.31887320 Acc: 0.89922481\n",
      "Epoch 25 of 500 took 0.322s\n",
      "Accuracy total 0.858398, main loss classifier 0.551315, source accuracy 0.865234 source classification loss 0.427921, target accuracy 0.851562 target loss 0.469100 accuracy domain distinction 0.500000 loss domain distinction 1.028046,\n",
      "VALIDATION Loss: 0.29735061 Acc: 0.88888889\n",
      "Epoch 26 of 500 took 0.318s\n",
      "Accuracy total 0.862630, main loss classifier 0.525050, source accuracy 0.879557 source classification loss 0.359406, target accuracy 0.845703 target loss 0.484849 accuracy domain distinction 0.500000 loss domain distinction 1.029222,\n",
      "VALIDATION Loss: 0.32982579 Acc: 0.87338501\n",
      "Epoch 27 of 500 took 0.320s\n",
      "Accuracy total 0.869466, main loss classifier 0.503581, source accuracy 0.879557 source classification loss 0.365508, target accuracy 0.859375 target loss 0.436287 accuracy domain distinction 0.500000 loss domain distinction 1.026833,\n",
      "VALIDATION Loss: 0.28054293 Acc: 0.87855297\n",
      "New best validation loss:  0.28054292500019073\n",
      "Epoch 28 of 500 took 0.337s\n",
      "Accuracy total 0.857422, main loss classifier 0.531546, source accuracy 0.858724 source classification loss 0.402409, target accuracy 0.856120 target loss 0.455544 accuracy domain distinction 0.500000 loss domain distinction 1.025696,\n",
      "VALIDATION Loss: 0.31863025 Acc: 0.89922481\n",
      "Epoch 29 of 500 took 0.319s\n",
      "Accuracy total 0.862956, main loss classifier 0.524173, source accuracy 0.868490 source classification loss 0.387923, target accuracy 0.857422 target loss 0.454535 accuracy domain distinction 0.500000 loss domain distinction 1.029436,\n",
      "VALIDATION Loss: 0.34022700 Acc: 0.89147287\n",
      "Epoch 30 of 500 took 0.320s\n",
      "Accuracy total 0.853190, main loss classifier 0.541791, source accuracy 0.860677 source classification loss 0.412461, target accuracy 0.845703 target loss 0.465434 accuracy domain distinction 0.500000 loss domain distinction 1.028439,\n",
      "VALIDATION Loss: 0.26451835 Acc: 0.90180879\n",
      "New best validation loss:  0.26451835089496206\n",
      "Epoch 31 of 500 took 0.321s\n",
      "Accuracy total 0.864909, main loss classifier 0.519362, source accuracy 0.876953 source classification loss 0.381205, target accuracy 0.852865 target loss 0.451336 accuracy domain distinction 0.500000 loss domain distinction 1.030913,\n",
      "VALIDATION Loss: 0.28299015 Acc: 0.89664083\n",
      "Epoch 32 of 500 took 0.323s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy total 0.865234, main loss classifier 0.517892, source accuracy 0.871094 source classification loss 0.383486, target accuracy 0.859375 target loss 0.445213 accuracy domain distinction 0.500000 loss domain distinction 1.035424,\n",
      "VALIDATION Loss: 0.27787806 Acc: 0.89664083\n",
      "Epoch 33 of 500 took 0.322s\n",
      "Accuracy total 0.862630, main loss classifier 0.533411, source accuracy 0.867839 source classification loss 0.397485, target accuracy 0.857422 target loss 0.463260 accuracy domain distinction 0.500000 loss domain distinction 1.030386,\n",
      "VALIDATION Loss: 0.34558134 Acc: 0.89405685\n",
      "Epoch 34 of 500 took 0.320s\n",
      "Accuracy total 0.861003, main loss classifier 0.518596, source accuracy 0.871094 source classification loss 0.380826, target accuracy 0.850911 target loss 0.450391 accuracy domain distinction 0.500000 loss domain distinction 1.029871,\n",
      "VALIDATION Loss: 0.39474562 Acc: 0.88113695\n",
      "Epoch 35 of 500 took 0.321s\n",
      "Accuracy total 0.863281, main loss classifier 0.523250, source accuracy 0.870443 source classification loss 0.381376, target accuracy 0.856120 target loss 0.458975 accuracy domain distinction 0.500000 loss domain distinction 1.030742,\n",
      "VALIDATION Loss: 0.30274546 Acc: 0.89664083\n",
      "Epoch 36 of 500 took 0.319s\n",
      "Accuracy total 0.875651, main loss classifier 0.495558, source accuracy 0.894531 source classification loss 0.342661, target accuracy 0.856771 target loss 0.443614 accuracy domain distinction 0.500000 loss domain distinction 1.024205,\n",
      "VALIDATION Loss: 0.32308792 Acc: 0.88113695\n",
      "Epoch    36: reducing learning rate of group 0 to 4.0240e-06.\n",
      "Epoch 37 of 500 took 0.320s\n",
      "Accuracy total 0.865560, main loss classifier 0.520315, source accuracy 0.861979 source classification loss 0.385189, target accuracy 0.869141 target loss 0.450152 accuracy domain distinction 0.500000 loss domain distinction 1.026448,\n",
      "VALIDATION Loss: 0.32532134 Acc: 0.90180879\n",
      "Epoch 38 of 500 took 0.326s\n",
      "Accuracy total 0.859701, main loss classifier 0.522598, source accuracy 0.873698 source classification loss 0.379261, target accuracy 0.845703 target loss 0.459472 accuracy domain distinction 0.500000 loss domain distinction 1.032320,\n",
      "VALIDATION Loss: 0.44618619 Acc: 0.90697674\n",
      "Epoch 39 of 500 took 0.320s\n",
      "Accuracy total 0.858724, main loss classifier 0.529865, source accuracy 0.869792 source classification loss 0.385750, target accuracy 0.847656 target loss 0.468758 accuracy domain distinction 0.500000 loss domain distinction 1.026109,\n",
      "VALIDATION Loss: 0.31680096 Acc: 0.86821705\n",
      "Epoch 40 of 500 took 0.321s\n",
      "Accuracy total 0.867839, main loss classifier 0.515541, source accuracy 0.866536 source classification loss 0.388687, target accuracy 0.869141 target loss 0.435329 accuracy domain distinction 0.500000 loss domain distinction 1.035329,\n",
      "VALIDATION Loss: 0.29471903 Acc: 0.88372093\n",
      "Epoch 41 of 500 took 0.321s\n",
      "Accuracy total 0.853516, main loss classifier 0.510450, source accuracy 0.860677 source classification loss 0.370972, target accuracy 0.846354 target loss 0.444811 accuracy domain distinction 0.500000 loss domain distinction 1.025581,\n",
      "VALIDATION Loss: 0.29737841 Acc: 0.88372093\n",
      "Epoch 42 of 500 took 0.320s\n",
      "Training complete in 0m 13s\n",
      "['participant_0']\n"
     ]
    }
   ],
   "source": [
    "# percentage_same_gesture_stable = 0.75 \n",
    "# run_SCADANN_training_sessions(examples_datasets=examples_datasets_train, labels_datasets=labels_datasets_train,\n",
    "#                               num_kernels=num_kernels, feature_vector_input_length=feature_vector_input_length,\n",
    "#                               path_weights_to_save_to=path_SCADANN,\n",
    "#                               path_weights_Adversarial_training=path_DANN,\n",
    "#                               path_weights_Normal_training=path_TSD,\n",
    "#                               number_of_cycles_total = number_of_cycles_total, \n",
    "#                               number_of_cycle_for_first_training = number_of_cycle_for_first_training,\n",
    "#                               number_of_classes=number_of_classes,\n",
    "#                               learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET one participant_examples  (7,)\n",
      "   GET one training_index_examples  (16, 572, 252)  at  0\n",
      "   GOT one group XY  (9152, 252)    (9152,)\n",
      "       one group XY test  (2288, 252)    (2288, 252)\n",
      "       one group XY train (8236, 252)    (8236,)\n",
      "       one group XY valid (916, 252)    (916, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  1\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  2\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  3\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  4\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  5\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  6\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "dataloaders: \n",
      "   train  (1, 7)\n",
      "   valid  (1, 7)\n",
      "   test  (1, 7)\n",
      "Participant:  0  Accuracy:  0.9020979020979021\n",
      "Participant:  0  Accuracy:  0.8881118881118881\n",
      "Participant:  0  Accuracy:  0.8916083916083916\n",
      "Participant:  0  Accuracy:  0.8653846153846154\n",
      "Participant:  0  Accuracy:  0.791958041958042\n",
      "Participant:  0  Accuracy:  0.7290209790209791\n",
      "Participant:  0  Accuracy:  0.7465034965034965\n",
      "ACCURACY PARTICIPANT:  [0.9020979020979021, 0.8881118881118881, 0.8916083916083916, 0.8653846153846154, 0.791958041958042, 0.7290209790209791, 0.7465034965034965]\n",
      "[[0.9020979  0.88811189 0.89160839 0.86538462 0.79195804 0.72902098\n",
      "  0.7465035 ]]\n",
      "[array([0.9020979 , 0.88811189, 0.89160839, 0.86538462, 0.79195804,\n",
      "       0.72902098, 0.7465035 ])]\n",
      "OVERALL ACCURACY: 0.8306693306693307\n"
     ]
    }
   ],
   "source": [
    "algo_name = \"SCADANN\"\n",
    "test_network_SCADANN(examples_datasets_train=examples_datasets_train, labels_datasets_train=labels_datasets_train,\n",
    "                     num_neurons=num_kernels, feature_vector_input_length=feature_vector_input_length,\n",
    "                     path_weights_SCADANN =path_SCADANN, path_weights_normal=path_TSD,\n",
    "                     algo_name=algo_name, cycle_test=3, number_of_cycles_total=number_of_cycles_total,\n",
    "                     number_of_cycle_for_first_training = number_of_cycle_for_first_training,\n",
    "                     number_of_classes=number_of_classes, save_path = save_SCADANN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Day_0~3</th>\n",
       "      <td>0.902098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_4</th>\n",
       "      <td>0.888112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_5</th>\n",
       "      <td>0.891608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_6</th>\n",
       "      <td>0.865385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_7</th>\n",
       "      <td>0.791958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_8</th>\n",
       "      <td>0.729021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_9</th>\n",
       "      <td>0.746503</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Participant_5\n",
       "Day_0~3      0.902098\n",
       "Day_4        0.888112\n",
       "Day_5        0.891608\n",
       "Day_6        0.865385\n",
       "Day_7        0.791958\n",
       "Day_8        0.729021\n",
       "Day_9        0.746503"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_filename = save_SCADANN + '/predictions_' + algo_name + \".npy\"\n",
    "results = np.load(results_filename, allow_pickle=True)\n",
    "SCADANN_acc = results[0]\n",
    "SCADANN_acc_overall = np.mean(SCADANN_acc)\n",
    "SCADANN_df = pd.DataFrame(SCADANN_acc.transpose(), \n",
    "                       index = [f'Day_{i}' for i in index_participant_list],\n",
    "                        columns = ['Participant_5'])\n",
    "SCADANN_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "SCADANN_df.transpose().plot.bar(rot=0, figsize=(10,5))\n",
    "plt.title(\"SCADANN Accuracies\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ground_truths  =  (1, 7)\n",
      "predictions =  (1, 7)\n",
      "index_participant_list  ['0~3', 4, 5, 6, 7, 8, 9]\n",
      "accuracies_gestures =  (22, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Loc1_Sub5_Day0~3-&gt;0~3</th>\n",
       "      <th>Loc1_Sub5_Day0~3-&gt;4</th>\n",
       "      <th>Loc1_Sub5_Day0~3-&gt;5</th>\n",
       "      <th>Loc1_Sub5_Day0~3-&gt;6</th>\n",
       "      <th>Loc1_Sub5_Day0~3-&gt;7</th>\n",
       "      <th>Loc1_Sub5_Day0~3-&gt;8</th>\n",
       "      <th>Loc1_Sub5_Day0~3-&gt;9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M1</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.884615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M2</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.692308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M3</td>\n",
       "      <td>0.942308</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.192308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M4</td>\n",
       "      <td>0.788462</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>0.346154</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>M5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>M6</td>\n",
       "      <td>0.980769</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>M7</td>\n",
       "      <td>0.971154</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>M8</td>\n",
       "      <td>0.942308</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>M9</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.461538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>M10</td>\n",
       "      <td>0.855769</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>M11</td>\n",
       "      <td>0.894231</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>M12</td>\n",
       "      <td>0.759615</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.423077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>M13</td>\n",
       "      <td>0.788462</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.961538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>M14</td>\n",
       "      <td>0.817308</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.346154</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>M15</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.769231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>M16</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>M17</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>M18</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>M19</td>\n",
       "      <td>0.990385</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.961538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>M20</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.384615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>M21</td>\n",
       "      <td>0.894231</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Mean</td>\n",
       "      <td>0.902098</td>\n",
       "      <td>0.888112</td>\n",
       "      <td>0.891608</td>\n",
       "      <td>0.865385</td>\n",
       "      <td>0.791958</td>\n",
       "      <td>0.729021</td>\n",
       "      <td>0.746503</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Loc1_Sub5_Day0~3->0~3  Loc1_Sub5_Day0~3->4  \\\n",
       "0          M0               1.000000             1.000000   \n",
       "1          M1               0.961538             1.000000   \n",
       "2          M2               0.923077             1.000000   \n",
       "3          M3               0.942308             0.923077   \n",
       "4          M4               0.788462             1.000000   \n",
       "5          M5               1.000000             1.000000   \n",
       "6          M6               0.980769             1.000000   \n",
       "7          M7               0.971154             1.000000   \n",
       "8          M8               0.942308             0.961538   \n",
       "9          M9               0.875000             0.769231   \n",
       "10        M10               0.855769             0.692308   \n",
       "11        M11               0.894231             0.307692   \n",
       "12        M12               0.759615             0.769231   \n",
       "13        M13               0.788462             1.000000   \n",
       "14        M14               0.817308             0.769231   \n",
       "15        M15               0.653846             0.576923   \n",
       "16        M16               0.961538             1.000000   \n",
       "17        M17               1.000000             1.000000   \n",
       "18        M18               1.000000             1.000000   \n",
       "19        M19               0.990385             1.000000   \n",
       "20        M20               0.846154             0.923077   \n",
       "21        M21               0.894231             0.846154   \n",
       "22       Mean               0.902098             0.888112   \n",
       "\n",
       "    Loc1_Sub5_Day0~3->5  Loc1_Sub5_Day0~3->6  Loc1_Sub5_Day0~3->7  \\\n",
       "0              1.000000             1.000000             1.000000   \n",
       "1              1.000000             1.000000             0.692308   \n",
       "2              0.538462             0.730769             0.692308   \n",
       "3              1.000000             1.000000             0.538462   \n",
       "4              0.269231             0.346154             0.038462   \n",
       "5              1.000000             1.000000             0.538462   \n",
       "6              1.000000             0.653846             1.000000   \n",
       "7              0.923077             1.000000             1.000000   \n",
       "8              1.000000             0.923077             1.000000   \n",
       "9              0.692308             0.730769             0.615385   \n",
       "10             0.961538             1.000000             1.000000   \n",
       "11             0.961538             0.923077             1.000000   \n",
       "12             0.807692             0.769231             0.961538   \n",
       "13             1.000000             1.000000             1.000000   \n",
       "14             0.769231             0.346154             0.576923   \n",
       "15             0.884615             0.769231             0.000000   \n",
       "16             1.000000             1.000000             1.000000   \n",
       "17             1.000000             1.000000             1.000000   \n",
       "18             0.884615             1.000000             1.000000   \n",
       "19             1.000000             1.000000             0.884615   \n",
       "20             0.923077             0.884615             0.923077   \n",
       "21             1.000000             0.961538             0.961538   \n",
       "22             0.891608             0.865385             0.791958   \n",
       "\n",
       "    Loc1_Sub5_Day0~3->8  Loc1_Sub5_Day0~3->9  \n",
       "0              1.000000             1.000000  \n",
       "1              0.692308             0.884615  \n",
       "2              0.576923             0.692308  \n",
       "3              1.000000             0.192308  \n",
       "4              0.000000             0.000000  \n",
       "5              0.884615             0.923077  \n",
       "6              0.538462             1.000000  \n",
       "7              1.000000             1.000000  \n",
       "8              1.000000             0.846154  \n",
       "9              0.730769             0.461538  \n",
       "10             0.923077             0.923077  \n",
       "11             1.000000             1.000000  \n",
       "12             0.807692             0.423077  \n",
       "13             1.000000             0.961538  \n",
       "14             0.000000             0.000000  \n",
       "15             0.000000             0.769231  \n",
       "16             1.000000             1.000000  \n",
       "17             1.000000             1.000000  \n",
       "18             0.307692             1.000000  \n",
       "19             1.000000             0.961538  \n",
       "20             0.576923             0.384615  \n",
       "21             1.000000             1.000000  \n",
       "22             0.729021             0.746503  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truths = results[1]\n",
    "predictions = results[2]\n",
    "print(\"ground_truths  = \", np.shape(ground_truths))\n",
    "print(\"predictions = \", np.shape(predictions))\n",
    "df = get_gesture_accuracies(ground_truths, predictions, number_of_classes=number_of_classes, \n",
    "                            m_name=m_name, n_name=n_name, path=save_TSD, algo_name=algo_name,\n",
    "                           index_participant_list_customized=index_participant_list,\n",
    "                           lump_day_at_participant=5)\n",
    "df = pd.read_csv(save_SCADANN+'/'+algo_name+'.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "Suppose there is a ndarray of NxM dataloaders, then N group of models will be trained, and each group will have M model. Each group is independent of the other, and each model within a group is dependent on its previous training weights.\n",
    "\n",
    "In general, overall accuracies of SCADANN are better than DANN, and DANN is better than TSD.\n",
    "Occasionally accuracies of SCADANN end up a little smaller than DANN, reasons may be lack of datasets put into training model (fixed) and non-optimal percentage_same_gesture_sable (fixed). Code should be reproducible if processed dataset sticks to the shape defined above.  \n",
    "\n",
    "The amount of increase in accuracies from DANN to SCADANN looks random. But if the base model is better at classifying one session, then its corresponding SCADANN is also better at classifying the same session. Given such result, to obtain the best performance from SCADANN, a good model trained with good data should be the starting point.\n",
    "\n",
    "* What to check if sth goes wrong:\n",
    "    * percentage_same_gesture_sable\n",
    "    * number of cycles or sessions\n",
    "    * shape of dataloaders (combination of train, test, valid should include all dataset)\n",
    "    * shape of procssed datasets\n",
    "    * directory paths of weights and results\n",
    "    * if weights are stored or loaded correcltyTSD_acc_overall_one = np.mean(TSD_acc, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TSD\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Day_0~3</th>\n",
       "      <td>0.902098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_4</th>\n",
       "      <td>0.854895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_5</th>\n",
       "      <td>0.777972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_6</th>\n",
       "      <td>0.755245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_7</th>\n",
       "      <td>0.582168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_8</th>\n",
       "      <td>0.568182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_9</th>\n",
       "      <td>0.557692</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Participant_5\n",
       "Day_0~3      0.902098\n",
       "Day_4        0.854895\n",
       "Day_5        0.777972\n",
       "Day_6        0.755245\n",
       "Day_7        0.582168\n",
       "Day_8        0.568182\n",
       "Day_9        0.557692"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DANN\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Day_0~3</th>\n",
       "      <td>0.902098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_4</th>\n",
       "      <td>0.875874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_5</th>\n",
       "      <td>0.846154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_6</th>\n",
       "      <td>0.833916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_7</th>\n",
       "      <td>0.716783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_8</th>\n",
       "      <td>0.711538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_9</th>\n",
       "      <td>0.657343</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Participant_5\n",
       "Day_0~3      0.902098\n",
       "Day_4        0.875874\n",
       "Day_5        0.846154\n",
       "Day_6        0.833916\n",
       "Day_7        0.716783\n",
       "Day_8        0.711538\n",
       "Day_9        0.657343"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCADANN\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Day_0~3</th>\n",
       "      <td>0.902098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_4</th>\n",
       "      <td>0.888112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_5</th>\n",
       "      <td>0.891608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_6</th>\n",
       "      <td>0.865385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_7</th>\n",
       "      <td>0.791958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_8</th>\n",
       "      <td>0.729021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_9</th>\n",
       "      <td>0.746503</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Participant_5\n",
       "Day_0~3      0.902098\n",
       "Day_4        0.888112\n",
       "Day_5        0.891608\n",
       "Day_6        0.865385\n",
       "Day_7        0.791958\n",
       "Day_8        0.729021\n",
       "Day_9        0.746503"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"TSD\")\n",
    "display(TSD_df)\n",
    "print(\"DANN\")\n",
    "display(DANN_df)\n",
    "print(\"SCADANN\")\n",
    "display(SCADANN_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Day_4</th>\n",
       "      <td>0.033217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_5</th>\n",
       "      <td>0.113636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_6</th>\n",
       "      <td>0.11014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_7</th>\n",
       "      <td>0.20979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_8</th>\n",
       "      <td>0.160839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_9</th>\n",
       "      <td>0.188811</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Participant_5\n",
       "Day_4      0.033217\n",
       "Day_5      0.113636\n",
       "Day_6       0.11014\n",
       "Day_7       0.20979\n",
       "Day_8      0.160839\n",
       "Day_9      0.188811"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "diff_df = SCADANN_df-TSD_df\n",
    "diff_df = diff_df.drop('Day_'+index_participant_list[0])\n",
    "display(diff_df)\n",
    "diff_df.to_csv(save_TSD+'/diff_results/across_day_loc1_lump4_diff.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Overall_Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TSD</th>\n",
       "      <td>0.714036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DANN</th>\n",
       "      <td>0.791958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SCADANN</th>\n",
       "      <td>0.830669</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Overall_Acc\n",
       "TSD         0.714036\n",
       "DANN        0.791958\n",
       "SCADANN     0.830669"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_acc_df = pd.DataFrame([TSD_acc_overall, DANN_acc_overall, SCADANN_acc_overall],\n",
    "                             index = [\"TSD\", \"DANN\", \"SCADANN\"],\n",
    "                             columns = [\"Overall_Acc\"])\n",
    "overall_acc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1800x1800 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(25,25))\n",
    "acc_list = [TSD_df, DANN_df, SCADANN_df, overall_acc_df]\n",
    "title_list = [\"TSD\", \"DANN\", \"SCADANN\", \"Overall\"]\n",
    "for idx, ax in enumerate(axes.reshape(-1)): \n",
    "    acc_list[idx].transpose().plot.bar(ax = ax, rot=0)\n",
    "    ax.set_title(title_list[idx])\n",
    "    ax.set_ylim([0, 1.0])\n",
    "    for p in ax.patches:\n",
    "        ax.annotate(str(np.round(p.get_height(),2)), (p.get_x()+p.get_width()/2., p.get_height()),\n",
    "                    ha='center', va='center', xytext=(0, 8),textcoords='offset points')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
