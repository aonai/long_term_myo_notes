{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results of TSD, DANN, SCADANN models across 10 days of inward rotation starting at Day_0~6 for Subject_4\n",
    "\n",
    "Library used can be downloaded from https://github.com/aonai/long_term_EMG_myo   \n",
    "&emsp; Original by UlysseCoteAllard https://github.com/UlysseCoteAllard/LongTermEMG   \n",
    "Dataset recorded by https://github.com/Suguru55/Wearable_Sensor_Long-term_sEMG_Dataset   \n",
    "Extended robot project can be found in https://github.com/aonai/myo_robot_arm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "* weights for TSD are total of 50 training models, 10 for each day\n",
    "* weights for DANN and SCADANN are total of 45 trianing models, 9 for each day\n",
    "\n",
    "\n",
    "* training examples should have shape (1, 4,)\n",
    "* first session has shape (28, 572, 252)\n",
    "* the following sessions have shape (4, 572, 252)\n",
    "* training labels should have shape (1, 4,)\n",
    "\n",
    "\n",
    "* location 0, 1, and 2 corresponds to neutral position, inward rotation, and outward rotation respectively\n",
    "* session mentioned below are days, so number of sessions is 10\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "import torch\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_dir = \"/home/laiy/gitrepos/msr_final/LongTermEMG_myo\"\n",
    "os.chdir(code_dir)\n",
    "from PrepareAndLoadData.process_data import read_data_training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Prepare Data\n",
    "use `switch=2` to train across days and individually on wearing location 0 (`session_in_include=[0]`)\n",
    "\n",
    "### specify the directories used for running the code:\n",
    "* `code_diar`: path to long_term_EMG_myo library\n",
    "* `data_dir`: where raw dataset is loaded; raw data is in csv format\n",
    "* `processed_data_dir`: where processed dataset is loaded; processed data is in npy pickle format\n",
    "    * processed data should be a ndarray of shape   \n",
    "    (controlling_factor_1 x controlling_factor_2 x num_sessions_per_gesture x #examples_window*#mov(26*22=572) x processed_channel_shape(252 for TSD, (4,8,10) for ConvNet)\n",
    "* `path_<model_name>`: where model weights are saved\n",
    "    * weights should be saved in folder `/Weights/<model_name>`. Each folder has subfolders containing weights for the first controlling factor.\n",
    "    * weights for base model (TSD or ConvNet) contain m set of training model\n",
    "    * weights for DANN and SCADANN contain m-1 set of trianing model (these models are trianed based on TSD, so they do not have a best_state_0.pt model). \n",
    "* `save_<model_name>`: where model results are saved\n",
    "    * each result for testing a model on a group of dataset is saved in folder `results`. Each result has corresponding \n",
    "        * `<model_name>.txt` includes predictions, ground truths, array of accuracies for each participant and each session, and overall accuracy\n",
    "        * `predictions_<model_name>.npy` includes array of accuracies, ground truths, predictions, and model outputs (probability array for each prediction)\n",
    "        * remember to make blank files in these names before saving\n",
    "\n",
    "\n",
    "\n",
    "* use `read_data_training` to process raw dataset\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/home/laiy/gitrepos/msr_final/Wearable_Sensor_Long-term_sEMG_Dataset/data\"\n",
    "processed_data_dir = \"/home/laiy/gitrepos/msr_final/LongTermEMG_myo/Processed_datasets_all_across_day_loc_1_lump7\"\n",
    "code_dir = \"/home/laiy/gitrepos/msr_final/LongTermEMG_myo\"\n",
    "save_dir = \"/home/laiy/gitrepos/msr_final/LongTermEMG_myo/Results\"\n",
    "\n",
    "path_TSD =\"/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump7/TSD\"\n",
    "save_TSD = \"/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/results\"\n",
    "\n",
    "path_DANN =\"/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump7/DANN\"\n",
    "save_DANN = \"/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/results\"\n",
    "\n",
    "path_SCADANN =\"/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump7/SCADANN\"\n",
    "save_SCADANN = \"/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing Training datasets...\n",
      "session  1  --- process data in days  [2, 5, 6, 16, 17, 18, 22, 24, 25, 28]\n",
      "index_participant_list  [5]\n",
      "READ  Sub 5 _Loc 1 _Day 2\n",
      "examples_per_session =  (1, 4, 572, 252)\n",
      "READ  Sub 5 _Loc 1 _Day 5\n",
      "Include day  5  in first dataset  (4, 572, 252)\n",
      "examples of first session =  (8, 572, 252)\n",
      "examples_per_session =  (1, 8, 572, 252)\n",
      "READ  Sub 5 _Loc 1 _Day 6\n",
      "Include day  6  in first dataset  (8, 572, 252)\n",
      "examples of first session =  (12, 572, 252)\n",
      "examples_per_session =  (1, 12, 572, 252)\n",
      "READ  Sub 5 _Loc 1 _Day 16\n",
      "Include day  16  in first dataset  (12, 572, 252)\n",
      "examples of first session =  (16, 572, 252)\n",
      "examples_per_session =  (1, 16, 572, 252)\n",
      "READ  Sub 5 _Loc 1 _Day 17\n",
      "Include day  17  in first dataset  (16, 572, 252)\n",
      "examples of first session =  (20, 572, 252)\n",
      "examples_per_session =  (1, 20, 572, 252)\n",
      "READ  Sub 5 _Loc 1 _Day 18\n",
      "Include day  18  in first dataset  (20, 572, 252)\n",
      "examples of first session =  (24, 572, 252)\n",
      "examples_per_session =  (1, 24, 572, 252)\n",
      "READ  Sub 5 _Loc 1 _Day 22\n",
      "Include day  22  in first dataset  (24, 572, 252)\n",
      "examples of first session =  (28, 572, 252)\n",
      "examples_per_session =  (1, 28, 572, 252)\n",
      "READ  Sub 5 _Loc 1 _Day 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laiy/.local/lib/python3.8/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "examples_per_session =  (2,)\n",
      "READ  Sub 5 _Loc 1 _Day 25\n",
      "examples_per_session =  (3,)\n",
      "READ  Sub 5 _Loc 1 _Day 28\n",
      "examples_per_session =  (4,)\n",
      "@ traning sessions =  (1, 4)\n",
      "traning examples  (1, 4)\n",
      "traning labels  (1, 4)\n",
      "all traning examples  (1, 4)\n",
      "all traning labels  (1, 4)\n"
     ]
    }
   ],
   "source": [
    "read_data_training(path=data_dir, store_path = processed_data_dir,  \n",
    "                   sessions_to_include =[1], switch=2, include_in_first=7,\n",
    "                   start_at_participant=5, num_participant=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traning examples  (1, 4)\n",
      "traning labels  (1, 4)\n"
     ]
    }
   ],
   "source": [
    "# check stored pickle \n",
    "with open(processed_data_dir + \"/training_session.pickle\", 'rb') as f:\n",
    "    dataset_training = pickle.load(file=f)\n",
    "\n",
    "examples_datasets_train = dataset_training['examples_training']\n",
    "print('traning examples ', np.shape(examples_datasets_train))\n",
    "labels_datasets_train = dataset_training['labels_training']\n",
    "print('traning labels ', np.shape(labels_datasets_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  examples_per_session =  (28, 572, 252)\n",
      "0  labels_per_session =  (28, 572)\n",
      "1  examples_per_session =  (4, 572, 252)\n",
      "1  labels_per_session =  (4, 572)\n",
      "2  examples_per_session =  (4, 572, 252)\n",
      "2  labels_per_session =  (4, 572)\n",
      "3  examples_per_session =  (4, 572, 252)\n",
      "3  labels_per_session =  (4, 572)\n"
     ]
    }
   ],
   "source": [
    "for idx, examples_per_session in enumerate (examples_datasets_train[0]):\n",
    "    print(idx, \" examples_per_session = \", np.shape(examples_per_session))\n",
    "    print(idx, \" labels_per_session = \", np.shape(labels_datasets_train[0][idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify params used for training and testing\n",
    "\n",
    "During training and testing, processed datasets are first put into pytorch dataloders, then feed to the model trainer; following are params for TSD model and dataloaders\n",
    "\n",
    "* `num_kernels`: list of integers defining number of neurons used in each linear layer (linear block has `dropout`=0.5)\n",
    "* `number_of_cycles_total`: number of trails performed for each session (assuming that all session have the same trail size)\n",
    "    * 4 for myo across day training\n",
    "* `number_of_classes`: total number of gestures performed in dataset\n",
    "    * 22 for myo\n",
    "* `batch_size`: number of examples stored in each batch\n",
    "* `feature_vector_input_length`: length of input array or each processed signal; i.e. size of one training example \n",
    "    * 252 for TSD\n",
    "* `learning_rate`= 0.002515\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number_of_cycle_for_first_training  28\n",
      "number_of_cycles_total  4\n"
     ]
    }
   ],
   "source": [
    "num_kernels=[200, 200, 200]                                \n",
    "number_of_cycle_for_first_training = np.shape(examples_datasets_train[0][0])[0]               \n",
    "number_of_cycles_total=np.shape(examples_datasets_train[-1][-1])[0]               \n",
    "print(\"number_of_cycle_for_first_training \", number_of_cycle_for_first_training)\n",
    "print(\"number_of_cycles_total \", number_of_cycles_total)\n",
    "number_of_classes=22\n",
    "batch_size=128          \n",
    "feature_vector_input_length=252                     \n",
    "learning_rate=0.002515"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. TSD_DNN\n",
    "* `train_fine_tuning`: used to train data using a base model (TSD or ConvNet)\n",
    "    * running this function will save num_sessions sets of TSD model weights (each is fine tuned based on the previous training)  \n",
    "    \n",
    "* `test_standard_model_on_training_sessions`: test model result\n",
    "\n",
    "\n",
    "### check if dataloaders are loaded correctly:\n",
    "* each participant has shape (num_session x 40 x 572 x 252)\n",
    "* each session has shape (40 x 572 x 252)\n",
    "* put these data into on group ends up with shape (40*572=22880, 252)\n",
    "    * shuffle on group of data and put into dataloaders\n",
    "    * each participant should have num_sessions sets of dataloaders, each correspond to one session\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TrainingsAndEvaluations.ForTrainingSessions.train_tsd_dnn_standard import \\\n",
    "            test_standard_model_on_training_sessions, train_fine_tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET one participant_examples  (4,)\n",
      "   GET one training_index_examples  (28, 572, 252)  at  0\n",
      "   GOT one group XY  (16016, 252)    (16016,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (14414, 252)    (14414,)\n",
      "       one group XY valid (1602, 252)    (1602, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  1\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  2\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  3\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "dataloaders: \n",
      "   train  (1, 4)\n",
      "   valid  (1, 4)\n",
      "   test  (1, 0)\n",
      "START TRAINING\n",
      "Participant:  0\n",
      "Session:  0\n",
      "<generator object Module.parameters at 0x7f5039e6dc10>\n",
      "Epoch 0/499\n",
      "----------\n",
      "train Loss: 0.00849866 Acc: 0.66768973\n",
      "val Loss: 0.00031357 Acc: 0.8258427\n",
      "New best validation loss: 0.0003135715903116671\n",
      "Epoch 1 of 500 took 0.730s\n",
      "Epoch 1/499\n",
      "----------\n",
      "train Loss: 0.00459996 Acc: 0.79638672\n",
      "val Loss: 0.00022405 Acc: 0.88139825\n",
      "Epoch 2 of 500 took 0.727s\n",
      "Epoch 2/499\n",
      "----------\n",
      "train Loss: 0.00399360 Acc: 0.82407924\n",
      "val Loss: 0.00021846 Acc: 0.87265918\n",
      "Epoch 3 of 500 took 0.726s\n",
      "Epoch 3/499\n",
      "----------\n",
      "train Loss: 0.00364483 Acc: 0.83642578\n",
      "val Loss: 0.00018391 Acc: 0.90137328\n",
      "New best validation loss: 0.00018391355444280695\n",
      "Epoch 4 of 500 took 0.725s\n",
      "Epoch 4/499\n",
      "----------\n",
      "train Loss: 0.00339973 Acc: 0.84612165\n",
      "val Loss: 0.00021301 Acc: 0.87578027\n",
      "Epoch 5 of 500 took 0.722s\n",
      "Epoch 5/499\n",
      "----------\n",
      "train Loss: 0.00326783 Acc: 0.85281808\n",
      "val Loss: 0.00019702 Acc: 0.88764045\n",
      "Epoch 6 of 500 took 0.724s\n",
      "Epoch 6/499\n",
      "----------\n",
      "train Loss: 0.00316794 Acc: 0.85721261\n",
      "val Loss: 0.00017308 Acc: 0.90324594\n",
      "Epoch 7 of 500 took 0.729s\n",
      "Epoch 7/499\n",
      "----------\n",
      "train Loss: 0.00300315 Acc: 0.86530413\n",
      "val Loss: 0.00015347 Acc: 0.91198502\n",
      "Epoch 8 of 500 took 0.722s\n",
      "Epoch 8/499\n",
      "----------\n",
      "train Loss: 0.00288901 Acc: 0.87109375\n",
      "val Loss: 0.00015564 Acc: 0.91011236\n",
      "Epoch 9 of 500 took 0.774s\n",
      "Epoch 9/499\n",
      "----------\n",
      "train Loss: 0.00273349 Acc: 0.87716239\n",
      "val Loss: 0.00019291 Acc: 0.88764045\n",
      "Epoch 10 of 500 took 0.725s\n",
      "Epoch 10/499\n",
      "----------\n",
      "train Loss: 0.00273175 Acc: 0.87716239\n",
      "val Loss: 0.00013514 Acc: 0.91635456\n",
      "Epoch 11 of 500 took 0.724s\n",
      "Epoch 11/499\n",
      "----------\n",
      "train Loss: 0.00261318 Acc: 0.87953404\n",
      "val Loss: 0.00015132 Acc: 0.91260924\n",
      "Epoch 12 of 500 took 0.725s\n",
      "Epoch 12/499\n",
      "----------\n",
      "train Loss: 0.00271882 Acc: 0.87702288\n",
      "val Loss: 0.00016424 Acc: 0.91011236\n",
      "Epoch 13 of 500 took 0.769s\n",
      "Epoch 13/499\n",
      "----------\n",
      "train Loss: 0.00249034 Acc: 0.8828125\n",
      "val Loss: 0.00015980 Acc: 0.89263421\n",
      "Epoch 14 of 500 took 0.733s\n",
      "Epoch 14/499\n",
      "----------\n",
      "train Loss: 0.00250284 Acc: 0.88692801\n",
      "val Loss: 0.00012981 Acc: 0.92322097\n",
      "Epoch 15 of 500 took 0.760s\n",
      "\n",
      "Training complete in 0m 11s\n",
      "Best val loss: 0.000184\n",
      "Session:  1\n",
      "<generator object Module.parameters at 0x7f5039e6dac0>\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump7/TSD/participant_0/best_state_0.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump7/TSD/participant_0/best_state_0.pt' (epoch 4)\n",
      "Epoch 0/499\n",
      "----------\n",
      "train Loss: 0.00417196 Acc: 0.81152344\n",
      "val Loss: 0.00100498 Acc: 0.93886463\n",
      "New best validation loss: 0.0010049809665138543\n",
      "Epoch 1 of 500 took 0.108s\n",
      "Epoch 1/499\n",
      "----------\n",
      "train Loss: 0.00330742 Acc: 0.85498047\n",
      "val Loss: 0.00098486 Acc: 0.91703057\n",
      "Epoch 2 of 500 took 0.116s\n",
      "Epoch 2/499\n",
      "----------\n",
      "train Loss: 0.00317324 Acc: 0.86474609\n",
      "val Loss: 0.00092848 Acc: 0.93449782\n",
      "Epoch 3 of 500 took 0.133s\n",
      "Epoch 3/499\n",
      "----------\n",
      "train Loss: 0.00274381 Acc: 0.87890625\n",
      "val Loss: 0.00079079 Acc: 0.93886463\n",
      "New best validation loss: 0.0007907870555028125\n",
      "Epoch 4 of 500 took 0.145s\n",
      "Epoch 4/499\n",
      "----------\n",
      "train Loss: 0.00258799 Acc: 0.88427734\n",
      "val Loss: 0.00073690 Acc: 0.93886463\n",
      "Epoch 5 of 500 took 0.138s\n",
      "Epoch 5/499\n",
      "----------\n",
      "train Loss: 0.00264263 Acc: 0.87792969\n",
      "val Loss: 0.00074773 Acc: 0.92576419\n",
      "Epoch 6 of 500 took 0.139s\n",
      "Epoch 6/499\n",
      "----------\n",
      "train Loss: 0.00248478 Acc: 0.88183594\n",
      "val Loss: 0.00085007 Acc: 0.92139738\n",
      "Epoch 7 of 500 took 0.138s\n",
      "Epoch 7/499\n",
      "----------\n",
      "train Loss: 0.00238930 Acc: 0.89160156\n",
      "val Loss: 0.00064620 Acc: 0.94759825\n",
      "New best validation loss: 0.0006462012185800544\n",
      "Epoch 8 of 500 took 0.143s\n",
      "Epoch 8/499\n",
      "----------\n",
      "train Loss: 0.00218283 Acc: 0.89941406\n",
      "val Loss: 0.00060935 Acc: 0.95196507\n",
      "Epoch 9 of 500 took 0.139s\n",
      "Epoch 9/499\n",
      "----------\n",
      "train Loss: 0.00217248 Acc: 0.89501953\n",
      "val Loss: 0.00066953 Acc: 0.95196507\n",
      "Epoch 10 of 500 took 0.141s\n",
      "Epoch 10/499\n",
      "----------\n",
      "train Loss: 0.00200728 Acc: 0.91210938\n",
      "val Loss: 0.00076752 Acc: 0.93449782\n",
      "Epoch 11 of 500 took 0.144s\n",
      "Epoch 11/499\n",
      "----------\n",
      "train Loss: 0.00193720 Acc: 0.91064453\n",
      "val Loss: 0.00057728 Acc: 0.94323144\n",
      "Epoch 12 of 500 took 0.144s\n",
      "Epoch 12/499\n",
      "----------\n",
      "train Loss: 0.00182700 Acc: 0.91601562\n",
      "val Loss: 0.00062887 Acc: 0.93449782\n",
      "Epoch 13 of 500 took 0.139s\n",
      "Epoch 13/499\n",
      "----------\n",
      "train Loss: 0.00190550 Acc: 0.90869141\n",
      "val Loss: 0.00049565 Acc: 0.96069869\n",
      "New best validation loss: 0.0004956529325272839\n",
      "Epoch 14 of 500 took 0.144s\n",
      "Epoch 14/499\n",
      "----------\n",
      "train Loss: 0.00173471 Acc: 0.92529297\n",
      "val Loss: 0.00065371 Acc: 0.94323144\n",
      "Epoch 15 of 500 took 0.140s\n",
      "Epoch 15/499\n",
      "----------\n",
      "train Loss: 0.00180371 Acc: 0.91992188\n",
      "val Loss: 0.00052522 Acc: 0.95196507\n",
      "Epoch 16 of 500 took 0.140s\n",
      "Epoch 16/499\n",
      "----------\n",
      "train Loss: 0.00164242 Acc: 0.93212891\n",
      "val Loss: 0.00062529 Acc: 0.94759825\n",
      "Epoch 17 of 500 took 0.144s\n",
      "Epoch 17/499\n",
      "----------\n",
      "train Loss: 0.00159025 Acc: 0.93066406\n",
      "val Loss: 0.00042550 Acc: 0.95633188\n",
      "Epoch 18 of 500 took 0.142s\n",
      "Epoch 18/499\n",
      "----------\n",
      "train Loss: 0.00161629 Acc: 0.92724609\n",
      "val Loss: 0.00101741 Acc: 0.90393013\n",
      "Epoch 19 of 500 took 0.129s\n",
      "Epoch 19/499\n",
      "----------\n",
      "train Loss: 0.00164522 Acc: 0.92822266\n",
      "val Loss: 0.00082251 Acc: 0.92576419\n",
      "Epoch 20 of 500 took 0.132s\n",
      "Epoch 20/499\n",
      "----------\n",
      "train Loss: 0.00159951 Acc: 0.92822266\n",
      "val Loss: 0.00059259 Acc: 0.94323144\n",
      "Epoch 21 of 500 took 0.120s\n",
      "Epoch 21/499\n",
      "----------\n",
      "train Loss: 0.00148698 Acc: 0.93310547\n",
      "val Loss: 0.00054977 Acc: 0.95196507\n",
      "Epoch 22 of 500 took 0.115s\n",
      "Epoch 22/499\n",
      "----------\n",
      "train Loss: 0.00150967 Acc: 0.93310547\n",
      "val Loss: 0.00092164 Acc: 0.91266376\n",
      "Epoch 23 of 500 took 0.105s\n",
      "Epoch 23/499\n",
      "----------\n",
      "train Loss: 0.00135296 Acc: 0.94140625\n",
      "val Loss: 0.00045490 Acc: 0.9650655\n",
      "Epoch    24: reducing learning rate of group 0 to 5.0300e-04.\n",
      "Epoch 24 of 500 took 0.109s\n",
      "Epoch 24/499\n",
      "----------\n",
      "train Loss: 0.00130333 Acc: 0.9453125\n",
      "val Loss: 0.00043060 Acc: 0.96943231\n",
      "Epoch 25 of 500 took 0.106s\n",
      "\n",
      "Training complete in 0m 3s\n",
      "Best val loss: 0.000496\n",
      "Session:  2\n",
      "<generator object Module.parameters at 0x7f5039e6dac0>\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump7/TSD/participant_0/best_state_1.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump7/TSD/participant_0/best_state_1.pt' (epoch 14)\n",
      "Epoch 0/499\n",
      "----------\n",
      "train Loss: 0.00358302 Acc: 0.84472656\n",
      "val Loss: 0.00162081 Acc: 0.86026201\n",
      "New best validation loss: 0.0016208117966048062\n",
      "Epoch 1 of 500 took 0.111s\n",
      "Epoch 1/499\n",
      "----------\n",
      "train Loss: 0.00293356 Acc: 0.86425781\n",
      "val Loss: 0.00117641 Acc: 0.88646288\n",
      "New best validation loss: 0.0011764089353219912\n",
      "Epoch 2 of 500 took 0.111s\n",
      "Epoch 2/499\n",
      "----------\n",
      "train Loss: 0.00251971 Acc: 0.89013672\n",
      "val Loss: 0.00122033 Acc: 0.89519651\n",
      "Epoch 3 of 500 took 0.115s\n",
      "Epoch 3/499\n",
      "----------\n",
      "train Loss: 0.00235651 Acc: 0.89111328\n",
      "val Loss: 0.00098758 Acc: 0.89082969\n",
      "New best validation loss: 0.0009875847392727714\n",
      "Epoch 4 of 500 took 0.107s\n",
      "Epoch 4/499\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.00216739 Acc: 0.89794922\n",
      "val Loss: 0.00108362 Acc: 0.89519651\n",
      "Epoch 5 of 500 took 0.109s\n",
      "Epoch 5/499\n",
      "----------\n",
      "train Loss: 0.00210767 Acc: 0.89746094\n",
      "val Loss: 0.00097440 Acc: 0.91266376\n",
      "Epoch 6 of 500 took 0.105s\n",
      "Epoch 6/499\n",
      "----------\n",
      "train Loss: 0.00207078 Acc: 0.90527344\n",
      "val Loss: 0.00091503 Acc: 0.93449782\n",
      "Epoch 7 of 500 took 0.107s\n",
      "Epoch 7/499\n",
      "----------\n",
      "train Loss: 0.00204688 Acc: 0.90869141\n",
      "val Loss: 0.00100958 Acc: 0.90393013\n",
      "Epoch 8 of 500 took 0.107s\n",
      "Epoch 8/499\n",
      "----------\n",
      "train Loss: 0.00185047 Acc: 0.91162109\n",
      "val Loss: 0.00101136 Acc: 0.93449782\n",
      "Epoch 9 of 500 took 0.109s\n",
      "Epoch 9/499\n",
      "----------\n",
      "train Loss: 0.00185405 Acc: 0.91992188\n",
      "val Loss: 0.00082231 Acc: 0.90829694\n",
      "New best validation loss: 0.0008223055491801433\n",
      "Epoch 10 of 500 took 0.108s\n",
      "Epoch 10/499\n",
      "----------\n",
      "train Loss: 0.00172687 Acc: 0.91845703\n",
      "val Loss: 0.00095957 Acc: 0.930131\n",
      "Epoch 11 of 500 took 0.114s\n",
      "Epoch 11/499\n",
      "----------\n",
      "train Loss: 0.00179355 Acc: 0.91796875\n",
      "val Loss: 0.00124312 Acc: 0.89519651\n",
      "Epoch 12 of 500 took 0.106s\n",
      "Epoch 12/499\n",
      "----------\n",
      "train Loss: 0.00177799 Acc: 0.92138672\n",
      "val Loss: 0.00061935 Acc: 0.95633188\n",
      "New best validation loss: 0.0006193482199090016\n",
      "Epoch 13 of 500 took 0.109s\n",
      "Epoch 13/499\n",
      "----------\n",
      "train Loss: 0.00149341 Acc: 0.93212891\n",
      "val Loss: 0.00047505 Acc: 0.94759825\n",
      "New best validation loss: 0.00047505321591181525\n",
      "Epoch 14 of 500 took 0.106s\n",
      "Epoch 14/499\n",
      "----------\n",
      "train Loss: 0.00162036 Acc: 0.92724609\n",
      "val Loss: 0.00079783 Acc: 0.92139738\n",
      "Epoch 15 of 500 took 0.109s\n",
      "Epoch 15/499\n",
      "----------\n",
      "train Loss: 0.00153158 Acc: 0.93701172\n",
      "val Loss: 0.00087151 Acc: 0.90393013\n",
      "Epoch 16 of 500 took 0.106s\n",
      "Epoch 16/499\n",
      "----------\n",
      "train Loss: 0.00155354 Acc: 0.9296875\n",
      "val Loss: 0.00051778 Acc: 0.96069869\n",
      "Epoch 17 of 500 took 0.108s\n",
      "Epoch 17/499\n",
      "----------\n",
      "train Loss: 0.00127879 Acc: 0.94482422\n",
      "val Loss: 0.00059055 Acc: 0.94323144\n",
      "Epoch 18 of 500 took 0.106s\n",
      "Epoch 18/499\n",
      "----------\n",
      "train Loss: 0.00130247 Acc: 0.94384766\n",
      "val Loss: 0.00043655 Acc: 0.97379913\n",
      "Epoch 19 of 500 took 0.109s\n",
      "Epoch 19/499\n",
      "----------\n",
      "train Loss: 0.00121830 Acc: 0.94970703\n",
      "val Loss: 0.00055701 Acc: 0.95633188\n",
      "Epoch 20 of 500 took 0.105s\n",
      "Epoch 20/499\n",
      "----------\n",
      "train Loss: 0.00130281 Acc: 0.93701172\n",
      "val Loss: 0.00043078 Acc: 0.96069869\n",
      "Epoch 21 of 500 took 0.112s\n",
      "Epoch 21/499\n",
      "----------\n",
      "train Loss: 0.00124830 Acc: 0.94384766\n",
      "val Loss: 0.00195929 Acc: 0.85152838\n",
      "Epoch 22 of 500 took 0.106s\n",
      "Epoch 22/499\n",
      "----------\n",
      "train Loss: 0.00105364 Acc: 0.95410156\n",
      "val Loss: 0.00040122 Acc: 0.9650655\n",
      "Epoch 23 of 500 took 0.108s\n",
      "Epoch 23/499\n",
      "----------\n",
      "train Loss: 0.00136765 Acc: 0.94091797\n",
      "val Loss: 0.00084448 Acc: 0.930131\n",
      "Epoch 24 of 500 took 0.106s\n",
      "Epoch 24/499\n",
      "----------\n",
      "train Loss: 0.00119936 Acc: 0.94628906\n",
      "val Loss: 0.00072134 Acc: 0.93449782\n",
      "Epoch 25 of 500 took 0.110s\n",
      "\n",
      "Training complete in 0m 3s\n",
      "Best val loss: 0.000475\n",
      "Session:  3\n",
      "<generator object Module.parameters at 0x7f5039e6dc10>\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump7/TSD/participant_0/best_state_2.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump7/TSD/participant_0/best_state_2.pt' (epoch 14)\n",
      "Epoch 0/499\n",
      "----------\n",
      "train Loss: 0.00338624 Acc: 0.85058594\n",
      "val Loss: 0.00094990 Acc: 0.91266376\n",
      "New best validation loss: 0.0009499045178359252\n",
      "Epoch 1 of 500 took 0.124s\n",
      "Epoch 1/499\n",
      "----------\n",
      "train Loss: 0.00280599 Acc: 0.87060547\n",
      "val Loss: 0.00080062 Acc: 0.94323144\n",
      "New best validation loss: 0.0008006183731503882\n",
      "Epoch 2 of 500 took 0.114s\n",
      "Epoch 2/499\n",
      "----------\n",
      "train Loss: 0.00224275 Acc: 0.89697266\n",
      "val Loss: 0.00112670 Acc: 0.91703057\n",
      "Epoch 3 of 500 took 0.106s\n",
      "Epoch 3/499\n",
      "----------\n",
      "train Loss: 0.00223867 Acc: 0.89355469\n",
      "val Loss: 0.00073228 Acc: 0.93886463\n",
      "Epoch 4 of 500 took 0.111s\n",
      "Epoch 4/499\n",
      "----------\n",
      "train Loss: 0.00217000 Acc: 0.89648438\n",
      "val Loss: 0.00082111 Acc: 0.90829694\n",
      "Epoch 5 of 500 took 0.107s\n",
      "Epoch 5/499\n",
      "----------\n",
      "train Loss: 0.00205540 Acc: 0.90087891\n",
      "val Loss: 0.00060292 Acc: 0.94323144\n",
      "New best validation loss: 0.0006029221298392683\n",
      "Epoch 6 of 500 took 0.111s\n",
      "Epoch 6/499\n",
      "----------\n",
      "train Loss: 0.00195478 Acc: 0.90527344\n",
      "val Loss: 0.00071879 Acc: 0.93449782\n",
      "Epoch 7 of 500 took 0.106s\n",
      "Epoch 7/499\n",
      "----------\n",
      "train Loss: 0.00194953 Acc: 0.91113281\n",
      "val Loss: 0.00068418 Acc: 0.94759825\n",
      "Epoch 8 of 500 took 0.109s\n",
      "Epoch 8/499\n",
      "----------\n",
      "train Loss: 0.00180484 Acc: 0.90917969\n",
      "val Loss: 0.00071071 Acc: 0.930131\n",
      "Epoch 9 of 500 took 0.106s\n",
      "Epoch 9/499\n",
      "----------\n",
      "train Loss: 0.00179589 Acc: 0.91699219\n",
      "val Loss: 0.00068184 Acc: 0.93449782\n",
      "Epoch 10 of 500 took 0.110s\n",
      "Epoch 10/499\n",
      "----------\n",
      "train Loss: 0.00176497 Acc: 0.91748047\n",
      "val Loss: 0.00062162 Acc: 0.94323144\n",
      "Epoch 11 of 500 took 0.106s\n",
      "Epoch 11/499\n",
      "----------\n",
      "train Loss: 0.00183169 Acc: 0.91650391\n",
      "val Loss: 0.00060783 Acc: 0.95633188\n",
      "Epoch    12: reducing learning rate of group 0 to 5.0300e-04.\n",
      "Epoch 12 of 500 took 0.110s\n",
      "Epoch 12/499\n",
      "----------\n",
      "train Loss: 0.00161253 Acc: 0.92675781\n",
      "val Loss: 0.00051953 Acc: 0.95196507\n",
      "Epoch 13 of 500 took 0.106s\n",
      "Epoch 13/499\n",
      "----------\n",
      "train Loss: 0.00156487 Acc: 0.93164062\n",
      "val Loss: 0.00046260 Acc: 0.96069869\n",
      "New best validation loss: 0.0004626044995399542\n",
      "Epoch 14 of 500 took 0.114s\n",
      "Epoch 14/499\n",
      "----------\n",
      "train Loss: 0.00126810 Acc: 0.93847656\n",
      "val Loss: 0.00044448 Acc: 0.95196507\n",
      "Epoch 15 of 500 took 0.105s\n",
      "Epoch 15/499\n",
      "----------\n",
      "train Loss: 0.00131759 Acc: 0.93505859\n",
      "val Loss: 0.00046847 Acc: 0.95633188\n",
      "Epoch 16 of 500 took 0.109s\n",
      "Epoch 16/499\n",
      "----------\n",
      "train Loss: 0.00123319 Acc: 0.94042969\n",
      "val Loss: 0.00046594 Acc: 0.96069869\n",
      "Epoch 17 of 500 took 0.106s\n",
      "Epoch 17/499\n",
      "----------\n",
      "train Loss: 0.00117123 Acc: 0.94726562\n",
      "val Loss: 0.00049740 Acc: 0.96069869\n",
      "Epoch 18 of 500 took 0.109s\n",
      "Epoch 18/499\n",
      "----------\n",
      "train Loss: 0.00121597 Acc: 0.94677734\n",
      "val Loss: 0.00040189 Acc: 0.9650655\n",
      "Epoch 19 of 500 took 0.116s\n",
      "Epoch 19/499\n",
      "----------\n",
      "train Loss: 0.00115275 Acc: 0.94628906\n",
      "val Loss: 0.00049777 Acc: 0.95196507\n",
      "Epoch 20 of 500 took 0.110s\n",
      "Epoch 20/499\n",
      "----------\n",
      "train Loss: 0.00114083 Acc: 0.94824219\n",
      "val Loss: 0.00038915 Acc: 0.9650655\n",
      "Epoch 21 of 500 took 0.115s\n",
      "Epoch 21/499\n",
      "----------\n",
      "train Loss: 0.00112938 Acc: 0.94824219\n",
      "val Loss: 0.00039951 Acc: 0.95196507\n",
      "Epoch 22 of 500 took 0.109s\n",
      "Epoch 22/499\n",
      "----------\n",
      "train Loss: 0.00116618 Acc: 0.94433594\n",
      "val Loss: 0.00037700 Acc: 0.96069869\n",
      "Epoch 23 of 500 took 0.110s\n",
      "Epoch 23/499\n",
      "----------\n",
      "train Loss: 0.00111377 Acc: 0.94921875\n",
      "val Loss: 0.00037963 Acc: 0.96943231\n",
      "Epoch 24 of 500 took 0.116s\n",
      "Epoch 24/499\n",
      "----------\n",
      "train Loss: 0.00101891 Acc: 0.953125\n",
      "val Loss: 0.00047958 Acc: 0.95633188\n",
      "Epoch 25 of 500 took 0.126s\n",
      "\n",
      "Training complete in 0m 3s\n",
      "Best val loss: 0.000463\n"
     ]
    }
   ],
   "source": [
    "train_fine_tuning(examples_datasets_train, labels_datasets_train,\n",
    "                  num_kernels=num_kernels, path_weight_to_save_to=path_TSD,\n",
    "                  number_of_classes=number_of_classes, \n",
    "                  number_of_cycles_total=number_of_cycles_total,\n",
    "                  number_of_cycle_for_first_training = number_of_cycle_for_first_training,\n",
    "                  batch_size=batch_size,\n",
    "                  feature_vector_input_length=feature_vector_input_length,\n",
    "                  learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET one participant_examples  (4,)\n",
      "   GET one training_index_examples  (28, 572, 252)  at  0\n",
      "   GOT one group XY  (16016, 252)    (16016,)\n",
      "       one group XY test  (4004, 252)    (4004, 252)\n",
      "       one group XY train (14414, 252)    (14414,)\n",
      "       one group XY valid (1602, 252)    (1602, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  1\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  2\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  3\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "dataloaders: \n",
      "   train  (1, 4)\n",
      "   valid  (1, 4)\n",
      "   test  (1, 4)\n",
      "0  SESSION   data =  4004\n",
      "Participant:  0  Accuracy:  0.8968531468531469\n",
      "1  SESSION   data =  572\n",
      "Participant:  0  Accuracy:  0.8006993006993007\n",
      "2  SESSION   data =  572\n",
      "Participant:  0  Accuracy:  0.7587412587412588\n",
      "3  SESSION   data =  572\n",
      "Participant:  0  Accuracy:  0.7692307692307693\n",
      "ACCURACY PARTICIPANT  0 :  [0.8968531468531469, 0.8006993006993007, 0.7587412587412588, 0.7692307692307693]\n",
      "[array([0.89685315, 0.8006993 , 0.75874126, 0.76923077])]\n",
      "OVERALL ACCURACY: 0.8063811188811189\n"
     ]
    }
   ],
   "source": [
    "algo_name = \"standard_TSD\"\n",
    "test_standard_model_on_training_sessions(examples_datasets_train, labels_datasets_train,\n",
    "                                  num_neurons=num_kernels, use_only_first_training=True,\n",
    "                                  path_weights=path_TSD,\n",
    "                                  feature_vector_input_length=feature_vector_input_length,\n",
    "                                  save_path = save_TSD, algo_name=algo_name,\n",
    "                                  number_of_cycles_total=number_of_cycles_total,\n",
    "                                  number_of_cycle_for_first_training = number_of_cycle_for_first_training,\n",
    "                                  number_of_classes=number_of_classes, cycle_for_test=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Day_0~6</th>\n",
       "      <td>0.896853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_7</th>\n",
       "      <td>0.800699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_8</th>\n",
       "      <td>0.758741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_9</th>\n",
       "      <td>0.769231</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Participant_5\n",
       "Day_0~6      0.896853\n",
       "Day_7        0.800699\n",
       "Day_8        0.758741\n",
       "Day_9        0.769231"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_filename = save_TSD + '/predictions_' + algo_name + \"_no_retraining.npy\"\n",
    "results = np.load(results_filename, allow_pickle=True)\n",
    "TSD_acc = results[0]\n",
    "TSD_acc_overall = np.mean(TSD_acc)\n",
    "index_participant_list = ['0~6', 7, 8, 9]\n",
    "TSD_df = pd.DataFrame(TSD_acc.transpose(), \n",
    "                       index = [f'Day_{i}' for i in index_participant_list],\n",
    "                        columns = ['Participant_5'])\n",
    "TSD_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "TSD_df.transpose().plot.bar(rot=0, figsize=(10,5))\n",
    "plt.title(\"TSD Accuracies\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TrainingsAndEvaluations.ForTrainingSessions.utils import get_gesture_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ground_truths  =  (1, 4)\n",
      "predictions =  (1, 4)\n",
      "index_participant_list  ['0~6', 7, 8, 9]\n",
      "accuracies_gestures =  (22, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Loc1_Sub5_Day0~6-&gt;0~6</th>\n",
       "      <th>Loc1_Sub5_Day0~6-&gt;7</th>\n",
       "      <th>Loc1_Sub5_Day0~6-&gt;8</th>\n",
       "      <th>Loc1_Sub5_Day0~6-&gt;9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M1</td>\n",
       "      <td>0.868132</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.346154</td>\n",
       "      <td>0.384615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M2</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.961538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M3</td>\n",
       "      <td>0.912088</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.076923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M4</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>M5</td>\n",
       "      <td>0.983516</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>M6</td>\n",
       "      <td>0.950549</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>M7</td>\n",
       "      <td>0.956044</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.961538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>M8</td>\n",
       "      <td>0.890110</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>M9</td>\n",
       "      <td>0.868132</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>M10</td>\n",
       "      <td>0.890110</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.846154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>M11</td>\n",
       "      <td>0.890110</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>M12</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.461538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>M13</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.961538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>M14</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.384615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>M15</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.730769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>M16</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>M17</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>M18</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>M19</td>\n",
       "      <td>0.994505</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.884615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>M20</td>\n",
       "      <td>0.835165</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>0.730769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>M21</td>\n",
       "      <td>0.906593</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Mean</td>\n",
       "      <td>0.896853</td>\n",
       "      <td>0.800699</td>\n",
       "      <td>0.758741</td>\n",
       "      <td>0.769231</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Loc1_Sub5_Day0~6->0~6  Loc1_Sub5_Day0~6->7  \\\n",
       "0          M0               1.000000             1.000000   \n",
       "1          M1               0.868132             0.423077   \n",
       "2          M2               0.884615             0.730769   \n",
       "3          M3               0.912088             0.115385   \n",
       "4          M4               0.730769             0.038462   \n",
       "5          M5               0.983516             1.000000   \n",
       "6          M6               0.950549             1.000000   \n",
       "7          M7               0.956044             1.000000   \n",
       "8          M8               0.890110             0.884615   \n",
       "9          M9               0.868132             0.923077   \n",
       "10        M10               0.890110             0.884615   \n",
       "11        M11               0.890110             0.961538   \n",
       "12        M12               0.769231             0.884615   \n",
       "13        M13               0.846154             1.000000   \n",
       "14        M14               0.857143             0.769231   \n",
       "15        M15               0.807692             0.423077   \n",
       "16        M16               0.967033             1.000000   \n",
       "17        M17               0.961538             1.000000   \n",
       "18        M18               0.961538             1.000000   \n",
       "19        M19               0.994505             0.769231   \n",
       "20        M20               0.835165             0.846154   \n",
       "21        M21               0.906593             0.961538   \n",
       "22       Mean               0.896853             0.800699   \n",
       "\n",
       "    Loc1_Sub5_Day0~6->8  Loc1_Sub5_Day0~6->9  \n",
       "0              1.000000             1.000000  \n",
       "1              0.346154             0.384615  \n",
       "2              0.807692             0.961538  \n",
       "3              0.730769             0.076923  \n",
       "4              0.153846             0.000000  \n",
       "5              0.884615             0.923077  \n",
       "6              0.615385             1.000000  \n",
       "7              1.000000             0.961538  \n",
       "8              1.000000             0.923077  \n",
       "9              0.884615             0.923077  \n",
       "10             0.961538             0.846154  \n",
       "11             1.000000             0.923077  \n",
       "12             0.807692             0.461538  \n",
       "13             1.000000             0.961538  \n",
       "14             0.000000             0.384615  \n",
       "15             0.500000             0.730769  \n",
       "16             1.000000             1.000000  \n",
       "17             0.961538             1.000000  \n",
       "18             0.692308             1.000000  \n",
       "19             0.692308             0.884615  \n",
       "20             0.653846             0.730769  \n",
       "21             1.000000             0.846154  \n",
       "22             0.758741             0.769231  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truths = results[1]\n",
    "predictions = results[2]\n",
    "print(\"ground_truths  = \", np.shape(ground_truths))\n",
    "print(\"predictions = \", np.shape(predictions))\n",
    "m_name = \"Loc1_Sub\"\n",
    "n_name = \"Day0~6->\"\n",
    "df = get_gesture_accuracies(ground_truths, predictions, number_of_classes=number_of_classes, \n",
    "                            m_name=m_name, n_name=n_name, path=save_TSD, algo_name=algo_name,\n",
    "                           index_participant_list_customized=index_participant_list,\n",
    "                           lump_day_at_participant=5)\n",
    "df = pd.read_csv(save_TSD+'/'+algo_name+'.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. DANN\n",
    "* `train_DANN`: train DANN model using the first set of training weights from base model\n",
    "    * num_sessions-1 sets of training weights will be saved\n",
    "* `test_DANN_on_training_sessions`: test DANN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TrainingsAndEvaluations.ForTrainingSessions.train_tsd_dnn_DA import train_DANN, test_DANN_on_training_sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET one participant_examples  (4,)\n",
      "   GET one training_index_examples  (28, 572, 252)  at  0\n",
      "   GOT one group XY  (16016, 252)    (16016,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (14414, 252)    (14414,)\n",
      "       one group XY valid (1602, 252)    (1602, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  1\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  2\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  3\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "dataloaders: \n",
      "   train  (1, 4)\n",
      "   valid  (1, 4)\n",
      "   test  (1, 0)\n",
      "SHAPE SESSIONS:  (4,)\n",
      "()\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump7/TSD/participant_0/best_state_0.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump7/TSD/participant_0/best_state_0.pt' (epoch 4)\n",
      "STARTING TRAINING\n",
      "Accuracy source 0.829102, main loss classifier 0.313697, source classification loss 0.471072, loss domain distinction 0.266549, accuracy domain distinction 0.488037\n",
      "VALIDATION Loss: 0.37975565 Acc: 0.85892634\n",
      "New best validation loss:  0.3797556459903717\n",
      "Epoch 1 of 500 took 0.244s\n",
      "Accuracy source 0.831543, main loss classifier 0.315968, source classification loss 0.485181, loss domain distinction 0.188905, accuracy domain distinction 0.500488\n",
      "VALIDATION Loss: 0.41268265 Acc: 0.85143571\n",
      "Epoch 2 of 500 took 0.238s\n",
      "Accuracy source 0.832520, main loss classifier 0.323817, source classification loss 0.503417, loss domain distinction 0.189021, accuracy domain distinction 0.499756\n",
      "VALIDATION Loss: 0.47933874 Acc: 0.83270911\n",
      "Epoch 3 of 500 took 0.239s\n",
      "Accuracy source 0.820801, main loss classifier 0.313765, source classification loss 0.484930, loss domain distinction 0.187234, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.55131471 Acc: 0.80524345\n",
      "Epoch 4 of 500 took 0.241s\n",
      "Accuracy source 0.837402, main loss classifier 0.307732, source classification loss 0.473933, loss domain distinction 0.185158, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.34765115 Acc: 0.87265918\n",
      "New best validation loss:  0.3476511538028717\n",
      "Epoch 5 of 500 took 0.245s\n",
      "Accuracy source 0.854004, main loss classifier 0.286823, source classification loss 0.432780, loss domain distinction 0.186362, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.46159661 Acc: 0.83707865\n",
      "Epoch    10: reducing learning rate of group 0 to 5.0300e-04.\n",
      "Epoch 6 of 500 took 0.239s\n",
      "Accuracy source 0.851074, main loss classifier 0.281926, source classification loss 0.428004, loss domain distinction 0.180637, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.39397535 Acc: 0.85081149\n",
      "Epoch 7 of 500 took 0.241s\n",
      "Accuracy source 0.855469, main loss classifier 0.267995, source classification loss 0.400476, loss domain distinction 0.181409, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.29837218 Acc: 0.88576779\n",
      "New best validation loss:  0.29837217926979065\n",
      "Epoch 8 of 500 took 0.289s\n",
      "Accuracy source 0.848633, main loss classifier 0.280613, source classification loss 0.425923, loss domain distinction 0.180364, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.31875181 Acc: 0.88202247\n",
      "Epoch 9 of 500 took 0.240s\n",
      "Accuracy source 0.874023, main loss classifier 0.245585, source classification loss 0.355206, loss domain distinction 0.180894, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.40560389 Acc: 0.85580524\n",
      "Epoch 10 of 500 took 0.239s\n",
      "Accuracy source 0.870117, main loss classifier 0.256514, source classification loss 0.377102, loss domain distinction 0.180951, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.33411011 Acc: 0.87515605\n",
      "Epoch 11 of 500 took 0.237s\n",
      "Accuracy source 0.869141, main loss classifier 0.251188, source classification loss 0.366432, loss domain distinction 0.181118, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.34755388 Acc: 0.87827715\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0060e-04.\n",
      "Epoch 12 of 500 took 0.241s\n",
      "Accuracy source 0.860840, main loss classifier 0.263734, source classification loss 0.393221, loss domain distinction 0.179588, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.32941809 Acc: 0.88014981\n",
      "Epoch 13 of 500 took 0.240s\n",
      "Accuracy source 0.877441, main loss classifier 0.241975, source classification loss 0.348746, loss domain distinction 0.180075, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.36791500 Acc: 0.8732834\n",
      "Epoch 14 of 500 took 0.245s\n",
      "Accuracy source 0.863770, main loss classifier 0.252430, source classification loss 0.370087, loss domain distinction 0.181712, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.41264254 Acc: 0.85143571\n",
      "Epoch 15 of 500 took 0.241s\n",
      "Accuracy source 0.878906, main loss classifier 0.231999, source classification loss 0.328940, loss domain distinction 0.179689, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.34170547 Acc: 0.88264669\n",
      "Epoch 16 of 500 took 0.238s\n",
      "Accuracy source 0.870117, main loss classifier 0.252932, source classification loss 0.369884, loss domain distinction 0.183011, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.42317587 Acc: 0.85268414\n",
      "Epoch 17 of 500 took 0.239s\n",
      "Accuracy source 0.872070, main loss classifier 0.245323, source classification loss 0.355791, loss domain distinction 0.179822, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.39498273 Acc: 0.85892634\n",
      "Epoch 18 of 500 took 0.252s\n",
      "Accuracy source 0.870605, main loss classifier 0.246954, source classification loss 0.358829, loss domain distinction 0.180919, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.35193512 Acc: 0.87203496\n",
      "Training complete in 0m 5s\n",
      "()\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump7/TSD/participant_0/best_state_0.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump7/TSD/participant_0/best_state_0.pt' (epoch 4)\n",
      "STARTING TRAINING\n",
      "Accuracy source 0.845703, main loss classifier 0.305073, source classification loss 0.453936, loss domain distinction 0.262361, accuracy domain distinction 0.489990\n",
      "VALIDATION Loss: 0.32818827 Acc: 0.87203496\n",
      "New best validation loss:  0.3281882703304291\n",
      "Epoch 1 of 500 took 0.239s\n",
      "Accuracy source 0.838379, main loss classifier 0.302779, source classification loss 0.460541, loss domain distinction 0.188650, accuracy domain distinction 0.499756\n",
      "VALIDATION Loss: 0.39695743 Acc: 0.85830212\n",
      "Epoch 2 of 500 took 0.266s\n",
      "Accuracy source 0.834961, main loss classifier 0.311300, source classification loss 0.479648, loss domain distinction 0.185596, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.37412900 Acc: 0.86454432\n",
      "Epoch 3 of 500 took 0.277s\n",
      "Accuracy source 0.849121, main loss classifier 0.290138, source classification loss 0.437439, loss domain distinction 0.186315, accuracy domain distinction 0.500244\n",
      "VALIDATION Loss: 0.33622178 Acc: 0.87952559\n",
      "Epoch 4 of 500 took 0.275s\n",
      "Accuracy source 0.832031, main loss classifier 0.309682, source classification loss 0.475982, loss domain distinction 0.187610, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.30889681 Acc: 0.88514357\n",
      "New best validation loss:  0.3088968098163605\n",
      "Epoch 5 of 500 took 0.279s\n",
      "Accuracy source 0.837402, main loss classifier 0.302980, source classification loss 0.464956, loss domain distinction 0.184474, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.28349900 Acc: 0.89825218\n",
      "Epoch    10: reducing learning rate of group 0 to 5.0300e-04.\n",
      "New best validation loss:  0.28349900245666504\n",
      "Epoch 6 of 500 took 0.352s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy source 0.851562, main loss classifier 0.287381, source classification loss 0.439080, loss domain distinction 0.180559, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.27691364 Acc: 0.90137328\n",
      "New best validation loss:  0.2769136428833008\n",
      "Epoch 7 of 500 took 0.306s\n",
      "Accuracy source 0.851074, main loss classifier 0.265568, source classification loss 0.395229, loss domain distinction 0.180536, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.28874215 Acc: 0.89388265\n",
      "Epoch 8 of 500 took 0.238s\n",
      "Accuracy source 0.859375, main loss classifier 0.262038, source classification loss 0.388294, loss domain distinction 0.181121, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.31248415 Acc: 0.87952559\n",
      "Epoch 9 of 500 took 0.238s\n",
      "Accuracy source 0.856445, main loss classifier 0.265920, source classification loss 0.396349, loss domain distinction 0.181417, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.28264993 Acc: 0.89138577\n",
      "Epoch 10 of 500 took 0.242s\n",
      "Accuracy source 0.863281, main loss classifier 0.264220, source classification loss 0.392538, loss domain distinction 0.181791, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.28139511 Acc: 0.89637953\n",
      "Epoch 11 of 500 took 0.284s\n",
      "Accuracy source 0.872070, main loss classifier 0.258557, source classification loss 0.381041, loss domain distinction 0.181370, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.26888207 Acc: 0.9019975\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0060e-04.\n",
      "New best validation loss:  0.26888206601142883\n",
      "Epoch 12 of 500 took 0.312s\n",
      "Accuracy source 0.862305, main loss classifier 0.263926, source classification loss 0.392106, loss domain distinction 0.182505, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.26106274 Acc: 0.90074906\n",
      "New best validation loss:  0.26106274127960205\n",
      "Epoch 13 of 500 took 0.243s\n",
      "Accuracy source 0.858398, main loss classifier 0.262622, source classification loss 0.390496, loss domain distinction 0.180256, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.29081947 Acc: 0.88888889\n",
      "Epoch 14 of 500 took 0.280s\n",
      "Accuracy source 0.865723, main loss classifier 0.257927, source classification loss 0.380838, loss domain distinction 0.182010, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.27258217 Acc: 0.9019975\n",
      "Epoch 15 of 500 took 0.275s\n",
      "Accuracy source 0.863281, main loss classifier 0.262558, source classification loss 0.390043, loss domain distinction 0.180148, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.24751763 Acc: 0.91011236\n",
      "New best validation loss:  0.2475176304578781\n",
      "Epoch 16 of 500 took 0.238s\n",
      "Accuracy source 0.878906, main loss classifier 0.239836, source classification loss 0.344481, loss domain distinction 0.180028, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.27305016 Acc: 0.89762797\n",
      "Epoch 17 of 500 took 0.237s\n",
      "Accuracy source 0.866211, main loss classifier 0.257772, source classification loss 0.380500, loss domain distinction 0.181372, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.29251230 Acc: 0.89325843\n",
      "Epoch 18 of 500 took 0.246s\n",
      "Accuracy source 0.864258, main loss classifier 0.252265, source classification loss 0.369774, loss domain distinction 0.179967, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.30673039 Acc: 0.89263421\n",
      "Epoch 19 of 500 took 0.238s\n",
      "Accuracy source 0.880371, main loss classifier 0.238334, source classification loss 0.341871, loss domain distinction 0.182022, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.29606611 Acc: 0.89637953\n",
      "Epoch 20 of 500 took 0.237s\n",
      "Accuracy source 0.868164, main loss classifier 0.246843, source classification loss 0.359171, loss domain distinction 0.180635, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.30694151 Acc: 0.88764045\n",
      "Epoch 21 of 500 took 0.244s\n",
      "Accuracy source 0.880859, main loss classifier 0.244219, source classification loss 0.353923, loss domain distinction 0.181145, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.32928243 Acc: 0.87952559\n",
      "Epoch 22 of 500 took 0.246s\n",
      "Accuracy source 0.871582, main loss classifier 0.236679, source classification loss 0.338965, loss domain distinction 0.180890, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.26984122 Acc: 0.89950062\n",
      "Epoch 23 of 500 took 0.284s\n",
      "Accuracy source 0.875977, main loss classifier 0.241025, source classification loss 0.347414, loss domain distinction 0.180487, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.24625377 Acc: 0.89950062\n",
      "New best validation loss:  0.24625377357006073\n",
      "Epoch 24 of 500 took 0.240s\n",
      "Accuracy source 0.875977, main loss classifier 0.244911, source classification loss 0.355022, loss domain distinction 0.180123, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.30191395 Acc: 0.89076155\n",
      "Epoch 25 of 500 took 0.237s\n",
      "Accuracy source 0.884766, main loss classifier 0.233048, source classification loss 0.331547, loss domain distinction 0.180671, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.28632990 Acc: 0.90012484\n",
      "Epoch 26 of 500 took 0.248s\n",
      "Accuracy source 0.854492, main loss classifier 0.259961, source classification loss 0.385906, loss domain distinction 0.179960, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.28535470 Acc: 0.89138577\n",
      "Epoch 27 of 500 took 0.243s\n",
      "Accuracy source 0.875000, main loss classifier 0.247163, source classification loss 0.359763, loss domain distinction 0.181604, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.28241080 Acc: 0.89825218\n",
      "Epoch 28 of 500 took 0.240s\n",
      "Accuracy source 0.871094, main loss classifier 0.241453, source classification loss 0.348606, loss domain distinction 0.179199, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.28995556 Acc: 0.89325843\n",
      "Epoch 29 of 500 took 0.287s\n",
      "Accuracy source 0.875488, main loss classifier 0.239444, source classification loss 0.344489, loss domain distinction 0.179383, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.27908882 Acc: 0.89950062\n",
      "Epoch 30 of 500 took 0.244s\n",
      "Accuracy source 0.879883, main loss classifier 0.239604, source classification loss 0.345236, loss domain distinction 0.179733, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.26568764 Acc: 0.9019975\n",
      "Epoch 31 of 500 took 0.239s\n",
      "Accuracy source 0.869141, main loss classifier 0.254188, source classification loss 0.373994, loss domain distinction 0.180206, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.26534605 Acc: 0.89513109\n",
      "Epoch 32 of 500 took 0.237s\n",
      "Accuracy source 0.882812, main loss classifier 0.235494, source classification loss 0.336643, loss domain distinction 0.179929, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.29946908 Acc: 0.89200999\n",
      "Epoch 33 of 500 took 0.238s\n",
      "Accuracy source 0.879883, main loss classifier 0.242570, source classification loss 0.350458, loss domain distinction 0.180260, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.25787497 Acc: 0.9019975\n",
      "Epoch 34 of 500 took 0.240s\n",
      "Accuracy source 0.884277, main loss classifier 0.236589, source classification loss 0.338251, loss domain distinction 0.180604, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.27058065 Acc: 0.90137328\n",
      "Training complete in 0m 9s\n",
      "()\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump7/TSD/participant_0/best_state_0.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump7/TSD/participant_0/best_state_0.pt' (epoch 4)\n",
      "STARTING TRAINING\n",
      "Accuracy source 0.821289, main loss classifier 0.327142, source classification loss 0.497278, loss domain distinction 0.265671, accuracy domain distinction 0.485840\n",
      "VALIDATION Loss: 0.30985063 Acc: 0.89825218\n",
      "New best validation loss:  0.30985063314437866\n",
      "Epoch 1 of 500 took 0.238s\n",
      "Accuracy source 0.842285, main loss classifier 0.311286, source classification loss 0.478165, loss domain distinction 0.188263, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.35201305 Acc: 0.87765293\n",
      "Epoch 2 of 500 took 0.237s\n",
      "Accuracy source 0.826172, main loss classifier 0.314515, source classification loss 0.485201, loss domain distinction 0.188451, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.34113386 Acc: 0.86516854\n",
      "Epoch 3 of 500 took 0.244s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy source 0.833008, main loss classifier 0.312188, source classification loss 0.480630, loss domain distinction 0.188128, accuracy domain distinction 0.499756\n",
      "VALIDATION Loss: 0.30941877 Acc: 0.88764045\n",
      "New best validation loss:  0.30941876769065857\n",
      "Epoch 4 of 500 took 0.238s\n",
      "Accuracy source 0.816406, main loss classifier 0.322469, source classification loss 0.503350, loss domain distinction 0.186453, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.45634153 Acc: 0.84144819\n",
      "Epoch 5 of 500 took 0.235s\n",
      "Accuracy source 0.842773, main loss classifier 0.297012, source classification loss 0.452592, loss domain distinction 0.186524, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.31176162 Acc: 0.88826467\n",
      "Epoch    10: reducing learning rate of group 0 to 5.0300e-04.\n",
      "Epoch 6 of 500 took 0.237s\n",
      "Accuracy source 0.856934, main loss classifier 0.281530, source classification loss 0.426414, loss domain distinction 0.183674, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.29335722 Acc: 0.88826467\n",
      "New best validation loss:  0.2933572232723236\n",
      "Epoch 7 of 500 took 0.237s\n",
      "Accuracy source 0.851562, main loss classifier 0.282755, source classification loss 0.427946, loss domain distinction 0.183083, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.28215238 Acc: 0.89700375\n",
      "New best validation loss:  0.282152384519577\n",
      "Epoch 8 of 500 took 0.292s\n",
      "Accuracy source 0.861816, main loss classifier 0.264878, source classification loss 0.393273, loss domain distinction 0.182722, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.29213914 Acc: 0.88701623\n",
      "Epoch 9 of 500 took 0.237s\n",
      "Accuracy source 0.866211, main loss classifier 0.266488, source classification loss 0.396893, loss domain distinction 0.181709, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.27540153 Acc: 0.90074906\n",
      "New best validation loss:  0.2754015326499939\n",
      "Epoch 10 of 500 took 0.237s\n",
      "Accuracy source 0.871094, main loss classifier 0.249659, source classification loss 0.363096, loss domain distinction 0.182109, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.28572765 Acc: 0.89138577\n",
      "Epoch 11 of 500 took 0.236s\n",
      "Accuracy source 0.851074, main loss classifier 0.273432, source classification loss 0.410643, loss domain distinction 0.182470, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.29696643 Acc: 0.89825218\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0060e-04.\n",
      "Epoch 12 of 500 took 0.242s\n",
      "Accuracy source 0.856445, main loss classifier 0.265467, source classification loss 0.396249, loss domain distinction 0.182644, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.29056454 Acc: 0.88888889\n",
      "Epoch 13 of 500 took 0.237s\n",
      "Accuracy source 0.880371, main loss classifier 0.249089, source classification loss 0.362988, loss domain distinction 0.181025, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.30887425 Acc: 0.88327091\n",
      "Epoch 14 of 500 took 0.236s\n",
      "Accuracy source 0.862305, main loss classifier 0.258179, source classification loss 0.381880, loss domain distinction 0.181304, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.27709714 Acc: 0.90137328\n",
      "Epoch 15 of 500 took 0.234s\n",
      "Accuracy source 0.871582, main loss classifier 0.255830, source classification loss 0.376912, loss domain distinction 0.181956, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.30047572 Acc: 0.88389513\n",
      "Epoch 16 of 500 took 0.238s\n",
      "Accuracy source 0.850098, main loss classifier 0.271715, source classification loss 0.408216, loss domain distinction 0.180700, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.29365146 Acc: 0.88701623\n",
      "Epoch 17 of 500 took 0.237s\n",
      "Accuracy source 0.875977, main loss classifier 0.243772, source classification loss 0.352527, loss domain distinction 0.181143, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.28955269 Acc: 0.89263421\n",
      "Epoch 18 of 500 took 0.236s\n",
      "Accuracy source 0.871094, main loss classifier 0.261440, source classification loss 0.387930, loss domain distinction 0.181133, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.28410670 Acc: 0.89575531\n",
      "Epoch 19 of 500 took 0.235s\n",
      "Accuracy source 0.862305, main loss classifier 0.257780, source classification loss 0.380300, loss domain distinction 0.180820, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.29691219 Acc: 0.89138577\n",
      "Epoch 20 of 500 took 0.242s\n",
      "Accuracy source 0.874023, main loss classifier 0.241466, source classification loss 0.347543, loss domain distinction 0.182841, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.23370777 Acc: 0.9113608\n",
      "New best validation loss:  0.23370777070522308\n",
      "Epoch 21 of 500 took 0.238s\n",
      "Accuracy source 0.881836, main loss classifier 0.228758, source classification loss 0.322659, loss domain distinction 0.180439, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.27158636 Acc: 0.89450687\n",
      "Epoch 22 of 500 took 0.235s\n",
      "Accuracy source 0.875977, main loss classifier 0.242681, source classification loss 0.349666, loss domain distinction 0.182545, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.26985043 Acc: 0.90137328\n",
      "Epoch 23 of 500 took 0.237s\n",
      "Accuracy source 0.874023, main loss classifier 0.250433, source classification loss 0.365318, loss domain distinction 0.182160, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.25985372 Acc: 0.90137328\n",
      "Epoch 24 of 500 took 0.248s\n",
      "Accuracy source 0.874023, main loss classifier 0.248142, source classification loss 0.361350, loss domain distinction 0.182192, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.30336854 Acc: 0.88764045\n",
      "Epoch 25 of 500 took 0.237s\n",
      "Accuracy source 0.859375, main loss classifier 0.244006, source classification loss 0.352590, loss domain distinction 0.181763, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.24952818 Acc: 0.90761548\n",
      "Epoch 26 of 500 took 0.286s\n",
      "Accuracy source 0.876465, main loss classifier 0.242949, source classification loss 0.351017, loss domain distinction 0.179993, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.24896877 Acc: 0.90636704\n",
      "Epoch 27 of 500 took 0.237s\n",
      "Accuracy source 0.872559, main loss classifier 0.247486, source classification loss 0.360022, loss domain distinction 0.180868, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.28968006 Acc: 0.89450687\n",
      "Epoch 28 of 500 took 0.241s\n",
      "Accuracy source 0.876465, main loss classifier 0.235958, source classification loss 0.337039, loss domain distinction 0.180389, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.26437289 Acc: 0.90324594\n",
      "Epoch 29 of 500 took 0.241s\n",
      "Accuracy source 0.875488, main loss classifier 0.242517, source classification loss 0.350006, loss domain distinction 0.180741, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.25940618 Acc: 0.90761548\n",
      "Epoch 30 of 500 took 0.238s\n",
      "Accuracy source 0.873047, main loss classifier 0.248362, source classification loss 0.362025, loss domain distinction 0.180554, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.26800540 Acc: 0.90262172\n",
      "Epoch 31 of 500 took 0.236s\n",
      "Accuracy source 0.866211, main loss classifier 0.253015, source classification loss 0.371298, loss domain distinction 0.182547, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.28108779 Acc: 0.89762797\n",
      "Training complete in 0m 8s\n"
     ]
    }
   ],
   "source": [
    "train_DANN(examples_datasets_train, labels_datasets_train, \n",
    "          num_kernels=num_kernels,\n",
    "          path_weights_fine_tuning=path_TSD,\n",
    "          number_of_classes=number_of_classes,\n",
    "          number_of_cycles_total = number_of_cycles_total,\n",
    "          number_of_cycle_for_first_training = number_of_cycle_for_first_training,\n",
    "          batch_size=batch_size,\n",
    "          feature_vector_input_length=feature_vector_input_length,\n",
    "          path_weights_to_save_to=path_DANN, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET one participant_examples  (4,)\n",
      "   GET one training_index_examples  (28, 572, 252)  at  0\n",
      "   GOT one group XY  (16016, 252)    (16016,)\n",
      "       one group XY test  (4004, 252)    (4004, 252)\n",
      "       one group XY train (14414, 252)    (14414,)\n",
      "       one group XY valid (1602, 252)    (1602, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  1\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  2\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  3\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "dataloaders: \n",
      "   train  (1, 4)\n",
      "   valid  (1, 4)\n",
      "   test  (1, 4)\n",
      "(4,)\n",
      "Participant ID:  0  Session ID:  0  Accuracy:  0.8968531468531469\n",
      "Participant ID:  0  Session ID:  1  Accuracy:  0.8513986013986014\n",
      "Participant ID:  0  Session ID:  2  Accuracy:  0.7727272727272727\n",
      "Participant ID:  0  Session ID:  3  Accuracy:  0.7954545454545454\n",
      "ACCURACY PARTICIPANT:  [0.8968531468531469, 0.8513986013986014, 0.7727272727272727, 0.7954545454545454]\n",
      "[[0.89685315 0.8513986  0.77272727 0.79545455]]\n",
      "[array([0.89685315, 0.8513986 , 0.77272727, 0.79545455])]\n",
      "OVERALL ACCURACY: 0.8291083916083916\n"
     ]
    }
   ],
   "source": [
    "algo_name = \"DANN\"\n",
    "test_DANN_on_training_sessions(examples_datasets_train, labels_datasets_train,\n",
    "                              feature_vector_input_length=feature_vector_input_length,\n",
    "                              num_neurons=num_kernels, path_weights_DA=path_DANN,\n",
    "                              algo_name=algo_name, save_path = save_DANN, \n",
    "                              number_of_cycles_total=number_of_cycles_total,\n",
    "                              number_of_cycle_for_first_training = number_of_cycle_for_first_training,\n",
    "                              path_weights_normal=path_TSD, number_of_classes=number_of_classes,\n",
    "                              cycle_for_test=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Day_0~6</th>\n",
       "      <td>0.896853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_7</th>\n",
       "      <td>0.851399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_8</th>\n",
       "      <td>0.772727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_9</th>\n",
       "      <td>0.795455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Participant_5\n",
       "Day_0~6      0.896853\n",
       "Day_7        0.851399\n",
       "Day_8        0.772727\n",
       "Day_9        0.795455"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_filename = save_DANN + '/predictions_' + algo_name + \".npy\"\n",
    "results = np.load(results_filename, allow_pickle=True)\n",
    "DANN_acc = results[0]\n",
    "DANN_acc_overall = np.mean(DANN_acc)\n",
    "DANN_df = pd.DataFrame(DANN_acc.transpose(), \n",
    "                       index = [f'Day_{i}' for i in index_participant_list],\n",
    "                        columns = ['Participant_5'])\n",
    "DANN_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "DANN_df.transpose().plot.bar(rot=0, figsize=(10,5))\n",
    "plt.title(\"DANN Accuracies\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ground_truths  =  (1, 4)\n",
      "predictions =  (1, 4)\n",
      "index_participant_list  ['0~6', 7, 8, 9]\n",
      "accuracies_gestures =  (22, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Loc1_Sub5_Day0~6-&gt;0~6</th>\n",
       "      <th>Loc1_Sub5_Day0~6-&gt;7</th>\n",
       "      <th>Loc1_Sub5_Day0~6-&gt;8</th>\n",
       "      <th>Loc1_Sub5_Day0~6-&gt;9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M1</td>\n",
       "      <td>0.868132</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.615385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M2</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.884615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M3</td>\n",
       "      <td>0.912088</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.307692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M4</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.038462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>M5</td>\n",
       "      <td>0.983516</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.884615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>M6</td>\n",
       "      <td>0.950549</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>M7</td>\n",
       "      <td>0.956044</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>M8</td>\n",
       "      <td>0.890110</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>M9</td>\n",
       "      <td>0.868132</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.807692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>M10</td>\n",
       "      <td>0.890110</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.961538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>M11</td>\n",
       "      <td>0.890110</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>M12</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.384615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>M13</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>M14</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.384615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>M15</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.807692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>M16</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>M17</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>M18</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>M19</td>\n",
       "      <td>0.994505</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.961538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>M20</td>\n",
       "      <td>0.835165</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.692308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>M21</td>\n",
       "      <td>0.906593</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Mean</td>\n",
       "      <td>0.896853</td>\n",
       "      <td>0.851399</td>\n",
       "      <td>0.772727</td>\n",
       "      <td>0.795455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Loc1_Sub5_Day0~6->0~6  Loc1_Sub5_Day0~6->7  \\\n",
       "0          M0               1.000000             1.000000   \n",
       "1          M1               0.868132             0.692308   \n",
       "2          M2               0.884615             0.923077   \n",
       "3          M3               0.912088             0.692308   \n",
       "4          M4               0.730769             0.461538   \n",
       "5          M5               0.983516             0.807692   \n",
       "6          M6               0.950549             1.000000   \n",
       "7          M7               0.956044             1.000000   \n",
       "8          M8               0.890110             0.653846   \n",
       "9          M9               0.868132             0.884615   \n",
       "10        M10               0.890110             0.884615   \n",
       "11        M11               0.890110             0.961538   \n",
       "12        M12               0.769231             0.807692   \n",
       "13        M13               0.846154             0.923077   \n",
       "14        M14               0.857143             0.807692   \n",
       "15        M15               0.807692             0.576923   \n",
       "16        M16               0.967033             1.000000   \n",
       "17        M17               0.961538             1.000000   \n",
       "18        M18               0.961538             1.000000   \n",
       "19        M19               0.994505             0.884615   \n",
       "20        M20               0.835165             1.000000   \n",
       "21        M21               0.906593             0.769231   \n",
       "22       Mean               0.896853             0.851399   \n",
       "\n",
       "    Loc1_Sub5_Day0~6->8  Loc1_Sub5_Day0~6->9  \n",
       "0              1.000000             1.000000  \n",
       "1              0.423077             0.615385  \n",
       "2              0.923077             0.884615  \n",
       "3              0.884615             0.307692  \n",
       "4              0.423077             0.038462  \n",
       "5              1.000000             0.884615  \n",
       "6              0.615385             1.000000  \n",
       "7              1.000000             1.000000  \n",
       "8              0.807692             0.923077  \n",
       "9              0.769231             0.807692  \n",
       "10             0.807692             0.961538  \n",
       "11             1.000000             1.000000  \n",
       "12             0.769231             0.384615  \n",
       "13             1.000000             0.923077  \n",
       "14             0.076923             0.384615  \n",
       "15             0.576923             0.807692  \n",
       "16             1.000000             1.000000  \n",
       "17             0.961538             1.000000  \n",
       "18             0.538462             1.000000  \n",
       "19             0.730769             0.961538  \n",
       "20             0.692308             0.692308  \n",
       "21             1.000000             0.923077  \n",
       "22             0.772727             0.795455  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truths = results[1]\n",
    "predictions = results[2]\n",
    "print(\"ground_truths  = \", np.shape(ground_truths))\n",
    "print(\"predictions = \", np.shape(predictions))\n",
    "df = get_gesture_accuracies(ground_truths, predictions, number_of_classes=number_of_classes, \n",
    "                            m_name=m_name, n_name=n_name, path=save_TSD, algo_name=algo_name,\n",
    "                           index_participant_list_customized=index_participant_list,\n",
    "                           lump_day_at_participant=5)\n",
    "df = pd.read_csv(save_DANN+'/'+algo_name+'.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. SCADANN\n",
    "\n",
    "* `run_SCADANN_training_sessions`: train SCADANN model. The first session uses TSD model_0 wegits; others use DANN weights\n",
    "    * specify `percentage_same_gesture_stable` based on the performance of most pseudo labels: \n",
    "        * print accuracies out and check what percentage will optimize `ACCURACY MODEL` and `ACCURACY PSEUDO` without cutting out too much data \n",
    "    * num_sessions-1 sets of training weights will be saved\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TrainingsAndEvaluations.ForTrainingSessions.train_tsd_dnn_SCADANN import \\\n",
    "    run_SCADANN_training_sessions, test_network_SCADANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET one participant_examples  (4,)\n",
      "   GET one training_index_examples  (28, 572, 252)  at  0\n",
      "   GOT one group XY  (16016, 252)    (16016,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (16016, 252)    (16016,)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  1\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2288, 252)    (2288,)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  2\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2288, 252)    (2288,)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  3\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2288, 252)    (2288,)\n",
      "dataloaders: \n",
      "   train  (1, 4)\n",
      "   valid  (0,)\n",
      "   test  (1, 0)\n",
      "participants_train =  1\n",
      "Optimizer =  <generator object Module.parameters at 0x7f5035ae6f90>\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump7/DANN/participant_0/best_state_1.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump7/DANN/participant_0/best_state_1.pt' (epoch 8)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump7/TSD/participant_0/best_state_0.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump7/TSD/participant_0/best_state_0.pt' (epoch 4)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump7/DANN/participant_0/best_state_1.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump7/DANN/participant_0/best_state_1.pt' (epoch 8)\n",
      "==== models_array =  (2,)  @ session  1\n",
      "HANDLING NEW SESSION  0\n",
      "HANDLING NEW SESSION  1\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.5769230769230769  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  2\n",
      "BEFORE:  0.15384615384615385   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  8\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.625  len before:  26   len after:  16\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.8461538461538461  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.0  len before:  26   len after:  12\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.8846153846153846  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  9\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.8846153846153846  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.5384615384615384  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.9  len before:  26   len after:  10\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.0  len before:  26   len after:  5\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.0  len before:  26   len after:  3\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.8823529411764706  len before:  26   len after:  17\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  18\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.46153846153846156  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.75  len before:  26   len after:  8\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.8059440559440559   Accuracy pseudo: 0.9127680311890838  len pseudo:  2052    len predictions 2288\n",
      "STARTING TRAINING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laiy/.local/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/laiy/.local/lib/python3.8/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy total 0.837187, main loss classifier 0.584376, source accuracy 0.842500 source classification loss 0.460137, target accuracy 0.831875 target loss 0.493223 accuracy domain distinction 0.500000 loss domain distinction 1.076955,\n",
      "VALIDATION Loss: 0.28151787 Acc: 0.89781022\n",
      "New best validation loss:  0.28151786965983255\n",
      "Epoch 2 of 500 took 0.376s\n",
      "Accuracy total 0.850625, main loss classifier 0.543257, source accuracy 0.849375 source classification loss 0.448937, target accuracy 0.851875 target loss 0.427172 accuracy domain distinction 0.500000 loss domain distinction 1.052021,\n",
      "VALIDATION Loss: 0.28477376 Acc: 0.90024331\n",
      "Epoch 3 of 500 took 0.332s\n",
      "Accuracy total 0.862500, main loss classifier 0.510580, source accuracy 0.862500 source classification loss 0.403111, target accuracy 0.862500 target loss 0.407934 accuracy domain distinction 0.500000 loss domain distinction 1.050573,\n",
      "VALIDATION Loss: 0.34498978 Acc: 0.86618005\n",
      "Epoch 4 of 500 took 0.331s\n",
      "Accuracy total 0.852187, main loss classifier 0.523116, source accuracy 0.850000 source classification loss 0.430508, target accuracy 0.854375 target loss 0.406918 accuracy domain distinction 0.500000 loss domain distinction 1.044027,\n",
      "VALIDATION Loss: 0.21235098 Acc: 0.9270073\n",
      "New best validation loss:  0.2123509794473648\n",
      "Epoch 5 of 500 took 0.334s\n",
      "Accuracy total 0.877812, main loss classifier 0.482161, source accuracy 0.876875 source classification loss 0.380243, target accuracy 0.878750 target loss 0.374681 accuracy domain distinction 0.500000 loss domain distinction 1.046985,\n",
      "VALIDATION Loss: 0.21271431 Acc: 0.93430657\n",
      "Epoch 6 of 500 took 0.327s\n",
      "Accuracy total 0.868437, main loss classifier 0.483720, source accuracy 0.860000 source classification loss 0.393547, target accuracy 0.876875 target loss 0.365463 accuracy domain distinction 0.500000 loss domain distinction 1.042151,\n",
      "VALIDATION Loss: 0.21720913 Acc: 0.9270073\n",
      "Epoch 7 of 500 took 0.327s\n",
      "Accuracy total 0.860625, main loss classifier 0.507230, source accuracy 0.836250 source classification loss 0.455534, target accuracy 0.885000 target loss 0.350425 accuracy domain distinction 0.500000 loss domain distinction 1.042504,\n",
      "VALIDATION Loss: 0.20533917 Acc: 0.93430657\n",
      "New best validation loss:  0.20533916779926845\n",
      "Epoch 8 of 500 took 0.337s\n",
      "Accuracy total 0.867812, main loss classifier 0.485955, source accuracy 0.843750 source classification loss 0.439811, target accuracy 0.891875 target loss 0.322391 accuracy domain distinction 0.500000 loss domain distinction 1.048545,\n",
      "VALIDATION Loss: 0.21905140 Acc: 0.91970803\n",
      "Epoch 9 of 500 took 0.328s\n",
      "Accuracy total 0.870625, main loss classifier 0.496636, source accuracy 0.860625 source classification loss 0.418408, target accuracy 0.880625 target loss 0.365445 accuracy domain distinction 0.500000 loss domain distinction 1.047095,\n",
      "VALIDATION Loss: 0.20744772 Acc: 0.93917275\n",
      "Epoch 10 of 500 took 0.327s\n",
      "Accuracy total 0.877188, main loss classifier 0.480389, source accuracy 0.871875 source classification loss 0.391854, target accuracy 0.882500 target loss 0.360442 accuracy domain distinction 0.500000 loss domain distinction 1.042406,\n",
      "VALIDATION Loss: 0.19767141 Acc: 0.9270073\n",
      "New best validation loss:  0.1976714123572622\n",
      "Epoch 11 of 500 took 0.358s\n",
      "Accuracy total 0.879687, main loss classifier 0.458946, source accuracy 0.866875 source classification loss 0.376532, target accuracy 0.892500 target loss 0.334205 accuracy domain distinction 0.500000 loss domain distinction 1.035775,\n",
      "VALIDATION Loss: 0.20683542 Acc: 0.92457421\n",
      "Epoch 12 of 500 took 0.331s\n",
      "Accuracy total 0.877500, main loss classifier 0.453768, source accuracy 0.862500 source classification loss 0.382644, target accuracy 0.892500 target loss 0.317051 accuracy domain distinction 0.500000 loss domain distinction 1.039209,\n",
      "VALIDATION Loss: 0.20838408 Acc: 0.92214112\n",
      "Epoch 13 of 500 took 0.358s\n",
      "Accuracy total 0.869375, main loss classifier 0.498794, source accuracy 0.853750 source classification loss 0.436402, target accuracy 0.885000 target loss 0.352022 accuracy domain distinction 0.500000 loss domain distinction 1.045826,\n",
      "VALIDATION Loss: 0.18658380 Acc: 0.93430657\n",
      "New best validation loss:  0.186583796782153\n",
      "Epoch 14 of 500 took 0.337s\n",
      "Accuracy total 0.882188, main loss classifier 0.450882, source accuracy 0.870000 source classification loss 0.381938, target accuracy 0.894375 target loss 0.311995 accuracy domain distinction 0.500000 loss domain distinction 1.039153,\n",
      "VALIDATION Loss: 0.16179318 Acc: 0.94647202\n",
      "New best validation loss:  0.16179317555257253\n",
      "Epoch 15 of 500 took 0.351s\n",
      "Accuracy total 0.880938, main loss classifier 0.455843, source accuracy 0.865625 source classification loss 0.391747, target accuracy 0.896250 target loss 0.311737 accuracy domain distinction 0.500000 loss domain distinction 1.041014,\n",
      "VALIDATION Loss: 0.19829333 Acc: 0.91970803\n",
      "Epoch 16 of 500 took 0.333s\n",
      "Accuracy total 0.881250, main loss classifier 0.439958, source accuracy 0.868125 source classification loss 0.356148, target accuracy 0.894375 target loss 0.316345 accuracy domain distinction 0.500000 loss domain distinction 1.037115,\n",
      "VALIDATION Loss: 0.16772744 Acc: 0.94160584\n",
      "Epoch 17 of 500 took 0.331s\n",
      "Accuracy total 0.885000, main loss classifier 0.444177, source accuracy 0.867500 source classification loss 0.371335, target accuracy 0.902500 target loss 0.308722 accuracy domain distinction 0.500000 loss domain distinction 1.041483,\n",
      "VALIDATION Loss: 0.17581213 Acc: 0.92944039\n",
      "Epoch 18 of 500 took 0.326s\n",
      "Accuracy total 0.890938, main loss classifier 0.429164, source accuracy 0.881250 source classification loss 0.348449, target accuracy 0.900625 target loss 0.301196 accuracy domain distinction 0.500000 loss domain distinction 1.043418,\n",
      "VALIDATION Loss: 0.18545405 Acc: 0.93187348\n",
      "Epoch 19 of 500 took 0.325s\n",
      "Accuracy total 0.887813, main loss classifier 0.450431, source accuracy 0.875000 source classification loss 0.389135, target accuracy 0.900625 target loss 0.303695 accuracy domain distinction 0.500000 loss domain distinction 1.040166,\n",
      "VALIDATION Loss: 0.19147471 Acc: 0.93187348\n",
      "Epoch 20 of 500 took 0.327s\n",
      "Accuracy total 0.880313, main loss classifier 0.449691, source accuracy 0.854375 source classification loss 0.404045, target accuracy 0.906250 target loss 0.286432 accuracy domain distinction 0.500000 loss domain distinction 1.044527,\n",
      "VALIDATION Loss: 0.18024834 Acc: 0.93430657\n",
      "Epoch    20: reducing learning rate of group 0 to 1.0060e-04.\n",
      "Epoch 21 of 500 took 0.332s\n",
      "Accuracy total 0.887500, main loss classifier 0.434249, source accuracy 0.868125 source classification loss 0.372411, target accuracy 0.906875 target loss 0.290645 accuracy domain distinction 0.500000 loss domain distinction 1.027213,\n",
      "VALIDATION Loss: 0.18677585 Acc: 0.92944039\n",
      "Epoch 22 of 500 took 0.334s\n",
      "Accuracy total 0.897500, main loss classifier 0.411767, source accuracy 0.881875 source classification loss 0.349781, target accuracy 0.913125 target loss 0.267924 accuracy domain distinction 0.500000 loss domain distinction 1.029143,\n",
      "VALIDATION Loss: 0.19756837 Acc: 0.93673966\n",
      "Epoch 23 of 500 took 0.331s\n",
      "Accuracy total 0.885000, main loss classifier 0.428486, source accuracy 0.870000 source classification loss 0.367163, target accuracy 0.900000 target loss 0.283457 accuracy domain distinction 0.500000 loss domain distinction 1.031758,\n",
      "VALIDATION Loss: 0.21977896 Acc: 0.92457421\n",
      "Epoch 24 of 500 took 0.332s\n",
      "Accuracy total 0.895312, main loss classifier 0.418685, source accuracy 0.879375 source classification loss 0.345288, target accuracy 0.911250 target loss 0.285579 accuracy domain distinction 0.500000 loss domain distinction 1.032516,\n",
      "VALIDATION Loss: 0.17297946 Acc: 0.93673966\n",
      "Epoch 25 of 500 took 0.326s\n",
      "Accuracy total 0.891250, main loss classifier 0.415687, source accuracy 0.875625 source classification loss 0.337826, target accuracy 0.906875 target loss 0.286900 accuracy domain distinction 0.500000 loss domain distinction 1.033234,\n",
      "VALIDATION Loss: 0.15967817 Acc: 0.94647202\n",
      "New best validation loss:  0.15967817498104914\n",
      "Epoch 26 of 500 took 0.327s\n",
      "Accuracy total 0.894687, main loss classifier 0.412390, source accuracy 0.873750 source classification loss 0.358080, target accuracy 0.915625 target loss 0.260199 accuracy domain distinction 0.500000 loss domain distinction 1.032510,\n",
      "VALIDATION Loss: 0.15810870 Acc: 0.95377129\n",
      "New best validation loss:  0.15810869527714594\n",
      "Epoch 27 of 500 took 0.331s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy total 0.889062, main loss classifier 0.424719, source accuracy 0.870000 source classification loss 0.378833, target accuracy 0.908125 target loss 0.264527 accuracy domain distinction 0.500000 loss domain distinction 1.030388,\n",
      "VALIDATION Loss: 0.14405385 Acc: 0.9513382\n",
      "New best validation loss:  0.14405384766204016\n",
      "Epoch 28 of 500 took 0.328s\n",
      "Accuracy total 0.900937, main loss classifier 0.403012, source accuracy 0.876875 source classification loss 0.359949, target accuracy 0.925000 target loss 0.240251 accuracy domain distinction 0.500000 loss domain distinction 1.029122,\n",
      "VALIDATION Loss: 0.21445950 Acc: 0.91970803\n",
      "Epoch 29 of 500 took 0.326s\n",
      "Accuracy total 0.892500, main loss classifier 0.410564, source accuracy 0.869375 source classification loss 0.360410, target accuracy 0.915625 target loss 0.255001 accuracy domain distinction 0.500000 loss domain distinction 1.028585,\n",
      "VALIDATION Loss: 0.14313738 Acc: 0.95620438\n",
      "New best validation loss:  0.14313738367387227\n",
      "Epoch 30 of 500 took 0.332s\n",
      "Accuracy total 0.899375, main loss classifier 0.405189, source accuracy 0.880000 source classification loss 0.349229, target accuracy 0.918750 target loss 0.255296 accuracy domain distinction 0.500000 loss domain distinction 1.029265,\n",
      "VALIDATION Loss: 0.13802300 Acc: 0.94647202\n",
      "New best validation loss:  0.13802300233926093\n",
      "Epoch 31 of 500 took 0.329s\n",
      "Accuracy total 0.892813, main loss classifier 0.413875, source accuracy 0.868125 source classification loss 0.369624, target accuracy 0.917500 target loss 0.251528 accuracy domain distinction 0.500000 loss domain distinction 1.032992,\n",
      "VALIDATION Loss: 0.15691716 Acc: 0.94890511\n",
      "Epoch 32 of 500 took 0.328s\n",
      "Accuracy total 0.894062, main loss classifier 0.415978, source accuracy 0.871250 source classification loss 0.360486, target accuracy 0.916875 target loss 0.266101 accuracy domain distinction 0.500000 loss domain distinction 1.026846,\n",
      "VALIDATION Loss: 0.15397161 Acc: 0.95377129\n",
      "Epoch 33 of 500 took 0.348s\n",
      "Accuracy total 0.903438, main loss classifier 0.389768, source accuracy 0.887500 source classification loss 0.322066, target accuracy 0.919375 target loss 0.252179 accuracy domain distinction 0.500000 loss domain distinction 1.026451,\n",
      "VALIDATION Loss: 0.15417325 Acc: 0.94890511\n",
      "Epoch 34 of 500 took 0.328s\n",
      "Accuracy total 0.900312, main loss classifier 0.395727, source accuracy 0.890625 source classification loss 0.330029, target accuracy 0.910000 target loss 0.255000 accuracy domain distinction 0.500000 loss domain distinction 1.032127,\n",
      "VALIDATION Loss: 0.14747016 Acc: 0.94160584\n",
      "Epoch 35 of 500 took 0.327s\n",
      "Accuracy total 0.898750, main loss classifier 0.397042, source accuracy 0.881875 source classification loss 0.346338, target accuracy 0.915625 target loss 0.241679 accuracy domain distinction 0.500000 loss domain distinction 1.030338,\n",
      "VALIDATION Loss: 0.14194381 Acc: 0.9513382\n",
      "Epoch 36 of 500 took 0.332s\n",
      "Accuracy total 0.895625, main loss classifier 0.402515, source accuracy 0.876250 source classification loss 0.334998, target accuracy 0.915000 target loss 0.265339 accuracy domain distinction 0.500000 loss domain distinction 1.023466,\n",
      "VALIDATION Loss: 0.13902296 Acc: 0.9513382\n",
      "Epoch    36: reducing learning rate of group 0 to 2.0120e-05.\n",
      "Epoch 37 of 500 took 0.328s\n",
      "Accuracy total 0.903125, main loss classifier 0.396759, source accuracy 0.880625 source classification loss 0.342347, target accuracy 0.925625 target loss 0.245013 accuracy domain distinction 0.500000 loss domain distinction 1.030790,\n",
      "VALIDATION Loss: 0.15848468 Acc: 0.94403893\n",
      "Epoch 38 of 500 took 0.334s\n",
      "Accuracy total 0.900937, main loss classifier 0.397157, source accuracy 0.879375 source classification loss 0.341956, target accuracy 0.922500 target loss 0.245578 accuracy domain distinction 0.500000 loss domain distinction 1.033896,\n",
      "VALIDATION Loss: 0.15245562 Acc: 0.94160584\n",
      "Epoch 39 of 500 took 0.331s\n",
      "Accuracy total 0.895938, main loss classifier 0.407450, source accuracy 0.872500 source classification loss 0.364103, target accuracy 0.919375 target loss 0.245770 accuracy domain distinction 0.500000 loss domain distinction 1.025131,\n",
      "VALIDATION Loss: 0.16474576 Acc: 0.94160584\n",
      "Epoch 40 of 500 took 0.328s\n",
      "Accuracy total 0.900937, main loss classifier 0.391569, source accuracy 0.876250 source classification loss 0.341082, target accuracy 0.925625 target loss 0.236380 accuracy domain distinction 0.500000 loss domain distinction 1.028376,\n",
      "VALIDATION Loss: 0.16089930 Acc: 0.94890511\n",
      "Epoch 41 of 500 took 0.327s\n",
      "Accuracy total 0.891875, main loss classifier 0.417610, source accuracy 0.872500 source classification loss 0.366727, target accuracy 0.911250 target loss 0.262618 accuracy domain distinction 0.500000 loss domain distinction 1.029377,\n",
      "VALIDATION Loss: 0.14337137 Acc: 0.9513382\n",
      "Epoch 42 of 500 took 0.330s\n",
      "Training complete in 0m 14s\n",
      "['participant_0']\n",
      "Optimizer =  <generator object Module.parameters at 0x7f5035ae6dd0>\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump7/DANN/participant_0/best_state_2.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump7/DANN/participant_0/best_state_2.pt' (epoch 24)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump7/TSD/participant_0/best_state_0.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump7/TSD/participant_0/best_state_0.pt' (epoch 4)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump7/DANN/participant_0/best_state_1.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump7/DANN/participant_0/best_state_1.pt' (epoch 8)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump7/DANN/participant_0/best_state_2.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump7/DANN/participant_0/best_state_2.pt' (epoch 24)\n",
      "==== models_array =  (3,)  @ session  2\n",
      "HANDLING NEW SESSION  0\n",
      "HANDLING NEW SESSION  1\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.5769230769230769  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  2\n",
      "BEFORE:  0.15384615384615385   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  8\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.625  len before:  26   len after:  16\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.8461538461538461  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.0  len before:  26   len after:  12\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.8846153846153846  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  9\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.8846153846153846  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.5384615384615384  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.9  len before:  26   len after:  10\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.0  len before:  26   len after:  5\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.0  len before:  26   len after:  3\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.8823529411764706  len before:  26   len after:  17\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  18\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.46153846153846156  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.75  len before:  26   len after:  8\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.8059440559440559   Accuracy pseudo: 0.9127680311890838  len pseudo:  2052    len predictions 2288\n",
      "HANDLING NEW SESSION  2\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.9615384615384616  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.6538461538461539  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.5769230769230769  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  9\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  16\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  8\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  4\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  8\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.4230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.4230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  8\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  7\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.7058823529411765  len before:  26   len after:  17\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  12\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  9\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.8461538461538461  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.8111888111888111   Accuracy pseudo: 0.8800383877159309  len pseudo:  2084    len predictions 2288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING TRAINING\n",
      "Accuracy total 0.853966, main loss classifier 0.542673, source accuracy 0.855168 source classification loss 0.427201, target accuracy 0.852764 target loss 0.431231 accuracy domain distinction 0.500000 loss domain distinction 1.134574,\n",
      "VALIDATION Loss: 0.27804608 Acc: 0.90647482\n",
      "New best validation loss:  0.2780460800443377\n",
      "Epoch 2 of 500 took 0.523s\n",
      "Accuracy total 0.851262, main loss classifier 0.535147, source accuracy 0.852764 source classification loss 0.412477, target accuracy 0.849760 target loss 0.446606 accuracy domain distinction 0.500000 loss domain distinction 1.056050,\n",
      "VALIDATION Loss: 0.21288599 Acc: 0.92565947\n",
      "New best validation loss:  0.21288599180323736\n",
      "Epoch 3 of 500 took 0.532s\n",
      "Accuracy total 0.858774, main loss classifier 0.515963, source accuracy 0.853966 source classification loss 0.412653, target accuracy 0.863582 target loss 0.411200 accuracy domain distinction 0.500000 loss domain distinction 1.040366,\n",
      "VALIDATION Loss: 0.19966124 Acc: 0.93045564\n",
      "New best validation loss:  0.1996612399816513\n",
      "Epoch 4 of 500 took 0.543s\n",
      "Accuracy total 0.872296, main loss classifier 0.471110, source accuracy 0.866587 source classification loss 0.366477, target accuracy 0.878005 target loss 0.368667 accuracy domain distinction 0.500000 loss domain distinction 1.035379,\n",
      "VALIDATION Loss: 0.20958790 Acc: 0.92565947\n",
      "Epoch 5 of 500 took 0.527s\n",
      "Accuracy total 0.856370, main loss classifier 0.519500, source accuracy 0.843750 source classification loss 0.434028, target accuracy 0.868990 target loss 0.398363 accuracy domain distinction 0.500000 loss domain distinction 1.033038,\n",
      "VALIDATION Loss: 0.21780702 Acc: 0.94244604\n",
      "Epoch 6 of 500 took 0.459s\n",
      "Accuracy total 0.866887, main loss classifier 0.502877, source accuracy 0.854567 source classification loss 0.425457, target accuracy 0.879207 target loss 0.373089 accuracy domain distinction 0.500000 loss domain distinction 1.036041,\n",
      "VALIDATION Loss: 0.22137303 Acc: 0.92805755\n",
      "Epoch 7 of 500 took 0.342s\n",
      "Accuracy total 0.863582, main loss classifier 0.496550, source accuracy 0.855168 source classification loss 0.414605, target accuracy 0.871995 target loss 0.371523 accuracy domain distinction 0.500000 loss domain distinction 1.034865,\n",
      "VALIDATION Loss: 0.23835973 Acc: 0.92086331\n",
      "Epoch 8 of 500 took 0.347s\n",
      "Accuracy total 0.869591, main loss classifier 0.485256, source accuracy 0.859976 source classification loss 0.404720, target accuracy 0.879207 target loss 0.358508 accuracy domain distinction 0.500000 loss domain distinction 1.036423,\n",
      "VALIDATION Loss: 0.20122792 Acc: 0.93285372\n",
      "Epoch 9 of 500 took 0.346s\n",
      "Accuracy total 0.868690, main loss classifier 0.484472, source accuracy 0.866587 source classification loss 0.387244, target accuracy 0.870793 target loss 0.375671 accuracy domain distinction 0.500000 loss domain distinction 1.030153,\n",
      "VALIDATION Loss: 0.20608036 Acc: 0.93045564\n",
      "Epoch     9: reducing learning rate of group 0 to 2.0120e-05.\n",
      "Epoch 10 of 500 took 0.341s\n",
      "Accuracy total 0.870192, main loss classifier 0.488224, source accuracy 0.864784 source classification loss 0.383397, target accuracy 0.875601 target loss 0.387729 accuracy domain distinction 0.500000 loss domain distinction 1.026613,\n",
      "VALIDATION Loss: 0.19151557 Acc: 0.9352518\n",
      "New best validation loss:  0.1915155691759927\n",
      "Epoch 11 of 500 took 0.346s\n",
      "Accuracy total 0.872596, main loss classifier 0.469010, source accuracy 0.864183 source classification loss 0.370341, target accuracy 0.881010 target loss 0.361914 accuracy domain distinction 0.500000 loss domain distinction 1.028832,\n",
      "VALIDATION Loss: 0.18366404 Acc: 0.94004796\n",
      "New best validation loss:  0.18366403877735138\n",
      "Epoch 12 of 500 took 0.348s\n",
      "Accuracy total 0.872897, main loss classifier 0.476543, source accuracy 0.875000 source classification loss 0.364076, target accuracy 0.870793 target loss 0.383737 accuracy domain distinction 0.500000 loss domain distinction 1.026372,\n",
      "VALIDATION Loss: 0.24419468 Acc: 0.92326139\n",
      "Epoch 13 of 500 took 0.343s\n",
      "Accuracy total 0.876502, main loss classifier 0.465073, source accuracy 0.866587 source classification loss 0.376855, target accuracy 0.886418 target loss 0.347997 accuracy domain distinction 0.500000 loss domain distinction 1.026467,\n",
      "VALIDATION Loss: 0.20675798 Acc: 0.94004796\n",
      "Epoch 14 of 500 took 0.341s\n",
      "Accuracy total 0.876803, main loss classifier 0.476527, source accuracy 0.869591 source classification loss 0.395801, target accuracy 0.884014 target loss 0.350244 accuracy domain distinction 0.500000 loss domain distinction 1.035045,\n",
      "VALIDATION Loss: 0.18064045 Acc: 0.93764988\n",
      "New best validation loss:  0.18064045480319432\n",
      "Epoch 15 of 500 took 0.351s\n",
      "Accuracy total 0.878305, main loss classifier 0.460759, source accuracy 0.875601 source classification loss 0.379111, target accuracy 0.881010 target loss 0.336131 accuracy domain distinction 0.500000 loss domain distinction 1.031377,\n",
      "VALIDATION Loss: 0.19359580 Acc: 0.93764988\n",
      "Epoch 16 of 500 took 0.342s\n",
      "Accuracy total 0.869892, main loss classifier 0.477777, source accuracy 0.856370 source classification loss 0.411293, target accuracy 0.883413 target loss 0.337242 accuracy domain distinction 0.500000 loss domain distinction 1.035089,\n",
      "VALIDATION Loss: 0.20703425 Acc: 0.94244604\n",
      "Epoch 17 of 500 took 0.344s\n",
      "Accuracy total 0.880108, main loss classifier 0.447919, source accuracy 0.868389 source classification loss 0.373526, target accuracy 0.891827 target loss 0.315693 accuracy domain distinction 0.500000 loss domain distinction 1.033096,\n",
      "VALIDATION Loss: 0.18088063 Acc: 0.94724221\n",
      "Epoch 18 of 500 took 0.347s\n",
      "Accuracy total 0.876202, main loss classifier 0.452761, source accuracy 0.865986 source classification loss 0.375204, target accuracy 0.886418 target loss 0.323070 accuracy domain distinction 0.500000 loss domain distinction 1.036239,\n",
      "VALIDATION Loss: 0.20938934 Acc: 0.9352518\n",
      "Epoch 19 of 500 took 0.342s\n",
      "Accuracy total 0.870192, main loss classifier 0.472064, source accuracy 0.853966 source classification loss 0.407669, target accuracy 0.886418 target loss 0.330649 accuracy domain distinction 0.500000 loss domain distinction 1.029044,\n",
      "VALIDATION Loss: 0.19467056 Acc: 0.93764988\n",
      "Epoch 20 of 500 took 0.340s\n",
      "Accuracy total 0.874399, main loss classifier 0.479621, source accuracy 0.865385 source classification loss 0.404046, target accuracy 0.883413 target loss 0.349919 accuracy domain distinction 0.500000 loss domain distinction 1.026388,\n",
      "VALIDATION Loss: 0.20299758 Acc: 0.92086331\n",
      "Epoch    20: reducing learning rate of group 0 to 4.0240e-06.\n",
      "Epoch 21 of 500 took 0.349s\n",
      "Accuracy total 0.874099, main loss classifier 0.485463, source accuracy 0.864784 source classification loss 0.411135, target accuracy 0.883413 target loss 0.353281 accuracy domain distinction 0.500000 loss domain distinction 1.032554,\n",
      "VALIDATION Loss: 0.21269102 Acc: 0.94724221\n",
      "Epoch 22 of 500 took 0.347s\n",
      "Accuracy total 0.877704, main loss classifier 0.461785, source accuracy 0.864183 source classification loss 0.380352, target accuracy 0.891226 target loss 0.336959 accuracy domain distinction 0.500000 loss domain distinction 1.031297,\n",
      "VALIDATION Loss: 0.21825789 Acc: 0.92805755\n",
      "Epoch 23 of 500 took 0.360s\n",
      "Accuracy total 0.873498, main loss classifier 0.456496, source accuracy 0.863582 source classification loss 0.372209, target accuracy 0.883413 target loss 0.334170 accuracy domain distinction 0.500000 loss domain distinction 1.033066,\n",
      "VALIDATION Loss: 0.21551122 Acc: 0.92326139\n",
      "Epoch 24 of 500 took 0.347s\n",
      "Accuracy total 0.879808, main loss classifier 0.459643, source accuracy 0.862981 source classification loss 0.374877, target accuracy 0.896635 target loss 0.337593 accuracy domain distinction 0.500000 loss domain distinction 1.034081,\n",
      "VALIDATION Loss: 0.19149422 Acc: 0.94484412\n",
      "Epoch 25 of 500 took 0.342s\n",
      "Accuracy total 0.870493, main loss classifier 0.476946, source accuracy 0.861178 source classification loss 0.404436, target accuracy 0.879808 target loss 0.343872 accuracy domain distinction 0.500000 loss domain distinction 1.027920,\n",
      "VALIDATION Loss: 0.18518863 Acc: 0.94484412\n",
      "Epoch 26 of 500 took 0.342s\n",
      "Training complete in 0m 9s\n",
      "['participant_0']\n",
      "Optimizer =  <generator object Module.parameters at 0x7f5035ae6dd0>\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump7/DANN/participant_0/best_state_3.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump7/DANN/participant_0/best_state_3.pt' (epoch 21)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump7/TSD/participant_0/best_state_0.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump7/TSD/participant_0/best_state_0.pt' (epoch 4)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump7/DANN/participant_0/best_state_1.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump7/DANN/participant_0/best_state_1.pt' (epoch 8)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump7/DANN/participant_0/best_state_2.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump7/DANN/participant_0/best_state_2.pt' (epoch 24)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump7/DANN/participant_0/best_state_3.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump7/DANN/participant_0/best_state_3.pt' (epoch 21)\n",
      "==== models_array =  (4,)  @ session  3\n",
      "HANDLING NEW SESSION  0\n",
      "HANDLING NEW SESSION  1\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.5769230769230769  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  2\n",
      "BEFORE:  0.15384615384615385   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  8\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.625  len before:  26   len after:  16\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.8461538461538461  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.0  len before:  26   len after:  12\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.8846153846153846  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  9\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.8846153846153846  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.5384615384615384  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.9  len before:  26   len after:  10\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.0  len before:  26   len after:  5\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.0  len before:  26   len after:  3\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.8823529411764706  len before:  26   len after:  17\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  18\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.46153846153846156  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.75  len before:  26   len after:  8\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.8059440559440559   Accuracy pseudo: 0.9127680311890838  len pseudo:  2052    len predictions 2288\n",
      "HANDLING NEW SESSION  2\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.9615384615384616  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.6538461538461539  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.5769230769230769  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  9\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  16\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  8\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  4\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  8\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.4230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.4230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  8\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  7\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.7058823529411765  len before:  26   len after:  17\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  12\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  9\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.8461538461538461  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.8111888111888111   Accuracy pseudo: 0.8800383877159309  len pseudo:  2084    len predictions 2288\n",
      "HANDLING NEW SESSION  3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.0  len before:  26   len after:  5\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.8846153846153846  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  9\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  1\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  1.0  len before:  26   len after:  15\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.0  len before:  26   len after:  4\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  2\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.0  len before:  26   len after:  17\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.4444444444444444  len before:  26   len after:  9\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.19230769230769232  len before:  26   len after:  26\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  18\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.6666666666666666  len before:  26   len after:  15\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.8125   Accuracy pseudo: 0.9498069498069498  len pseudo:  2072    len predictions 2288\n",
      "STARTING TRAINING\n",
      "Accuracy total 0.831250, main loss classifier 0.604456, source accuracy 0.853750 source classification loss 0.420907, target accuracy 0.808750 target loss 0.560538 accuracy domain distinction 0.500000 loss domain distinction 1.137338,\n",
      "VALIDATION Loss: 0.27369700 Acc: 0.90120482\n",
      "New best validation loss:  0.2736969973359789\n",
      "Epoch 2 of 500 took 0.333s\n",
      "Accuracy total 0.855625, main loss classifier 0.550931, source accuracy 0.868125 source classification loss 0.397476, target accuracy 0.843125 target loss 0.491601 accuracy domain distinction 0.500000 loss domain distinction 1.063922,\n",
      "VALIDATION Loss: 0.28733776 Acc: 0.90843373\n",
      "Epoch 3 of 500 took 0.336s\n",
      "Accuracy total 0.841250, main loss classifier 0.542925, source accuracy 0.860000 source classification loss 0.394904, target accuracy 0.822500 target loss 0.482496 accuracy domain distinction 0.500000 loss domain distinction 1.042252,\n",
      "VALIDATION Loss: 0.22478527 Acc: 0.92771084\n",
      "New best validation loss:  0.22478526617799485\n",
      "Epoch 4 of 500 took 0.331s\n",
      "Accuracy total 0.846562, main loss classifier 0.546874, source accuracy 0.853125 source classification loss 0.410720, target accuracy 0.840000 target loss 0.475257 accuracy domain distinction 0.500000 loss domain distinction 1.038859,\n",
      "VALIDATION Loss: 0.22742040 Acc: 0.92048193\n",
      "Epoch 5 of 500 took 0.331s\n",
      "Accuracy total 0.848437, main loss classifier 0.530982, source accuracy 0.866875 source classification loss 0.378371, target accuracy 0.830000 target loss 0.475141 accuracy domain distinction 0.500000 loss domain distinction 1.042259,\n",
      "VALIDATION Loss: 0.23867463 Acc: 0.91807229\n",
      "Epoch 6 of 500 took 0.335s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy total 0.860313, main loss classifier 0.492953, source accuracy 0.872500 source classification loss 0.362652, target accuracy 0.848125 target loss 0.416514 accuracy domain distinction 0.500000 loss domain distinction 1.033701,\n",
      "VALIDATION Loss: 0.25537248 Acc: 0.91807229\n",
      "Epoch 7 of 500 took 0.330s\n",
      "Accuracy total 0.857812, main loss classifier 0.531010, source accuracy 0.851250 source classification loss 0.462720, target accuracy 0.864375 target loss 0.392823 accuracy domain distinction 0.500000 loss domain distinction 1.032383,\n",
      "VALIDATION Loss: 0.20691175 Acc: 0.92048193\n",
      "New best validation loss:  0.20691174694469996\n",
      "Epoch 8 of 500 took 0.331s\n",
      "Accuracy total 0.864062, main loss classifier 0.491693, source accuracy 0.865000 source classification loss 0.389457, target accuracy 0.863125 target loss 0.386370 accuracy domain distinction 0.500000 loss domain distinction 1.037796,\n",
      "VALIDATION Loss: 0.22215134 Acc: 0.91807229\n",
      "Epoch 9 of 500 took 0.335s\n",
      "Accuracy total 0.875625, main loss classifier 0.474810, source accuracy 0.880625 source classification loss 0.351702, target accuracy 0.870625 target loss 0.390571 accuracy domain distinction 0.500000 loss domain distinction 1.036728,\n",
      "VALIDATION Loss: 0.19831147 Acc: 0.9253012\n",
      "New best validation loss:  0.19831146725586482\n",
      "Epoch 10 of 500 took 0.336s\n",
      "Accuracy total 0.868125, main loss classifier 0.485586, source accuracy 0.864375 source classification loss 0.397520, target accuracy 0.871875 target loss 0.366229 accuracy domain distinction 0.500000 loss domain distinction 1.037113,\n",
      "VALIDATION Loss: 0.21914286 Acc: 0.9253012\n",
      "Epoch 11 of 500 took 0.328s\n",
      "Accuracy total 0.868437, main loss classifier 0.495321, source accuracy 0.866250 source classification loss 0.397171, target accuracy 0.870625 target loss 0.386445 accuracy domain distinction 0.500000 loss domain distinction 1.035129,\n",
      "VALIDATION Loss: 0.20728131 Acc: 0.93012048\n",
      "Epoch 12 of 500 took 0.333s\n",
      "Accuracy total 0.866563, main loss classifier 0.490850, source accuracy 0.863125 source classification loss 0.394328, target accuracy 0.870000 target loss 0.380574 accuracy domain distinction 0.500000 loss domain distinction 1.033995,\n",
      "VALIDATION Loss: 0.22754571 Acc: 0.92048193\n",
      "Epoch 13 of 500 took 0.330s\n",
      "Accuracy total 0.864375, main loss classifier 0.489756, source accuracy 0.856250 source classification loss 0.408691, target accuracy 0.872500 target loss 0.364090 accuracy domain distinction 0.500000 loss domain distinction 1.033651,\n",
      "VALIDATION Loss: 0.20011630 Acc: 0.93253012\n",
      "Epoch 14 of 500 took 0.329s\n",
      "Accuracy total 0.860000, main loss classifier 0.505253, source accuracy 0.864375 source classification loss 0.391987, target accuracy 0.855625 target loss 0.410976 accuracy domain distinction 0.500000 loss domain distinction 1.037719,\n",
      "VALIDATION Loss: 0.19943670 Acc: 0.9373494\n",
      "Epoch 15 of 500 took 0.334s\n",
      "Accuracy total 0.870313, main loss classifier 0.476043, source accuracy 0.861875 source classification loss 0.390158, target accuracy 0.878750 target loss 0.354890 accuracy domain distinction 0.500000 loss domain distinction 1.035196,\n",
      "VALIDATION Loss: 0.19951218 Acc: 0.92771084\n",
      "Epoch    15: reducing learning rate of group 0 to 2.0120e-05.\n",
      "Epoch 16 of 500 took 0.334s\n",
      "Accuracy total 0.877812, main loss classifier 0.457136, source accuracy 0.882500 source classification loss 0.361615, target accuracy 0.873125 target loss 0.345742 accuracy domain distinction 0.500000 loss domain distinction 1.034576,\n",
      "VALIDATION Loss: 0.20984129 Acc: 0.92048193\n",
      "Epoch 17 of 500 took 0.331s\n",
      "Accuracy total 0.869375, main loss classifier 0.476054, source accuracy 0.868125 source classification loss 0.394212, target accuracy 0.870625 target loss 0.350864 accuracy domain distinction 0.500000 loss domain distinction 1.035163,\n",
      "VALIDATION Loss: 0.21263253 Acc: 0.91807229\n",
      "Epoch 18 of 500 took 0.337s\n",
      "Accuracy total 0.864062, main loss classifier 0.492173, source accuracy 0.850000 source classification loss 0.402435, target accuracy 0.878125 target loss 0.373929 accuracy domain distinction 0.500000 loss domain distinction 1.039911,\n",
      "VALIDATION Loss: 0.19943906 Acc: 0.92289157\n",
      "Epoch 19 of 500 took 0.329s\n",
      "Accuracy total 0.870000, main loss classifier 0.476280, source accuracy 0.861875 source classification loss 0.403862, target accuracy 0.878125 target loss 0.342961 accuracy domain distinction 0.500000 loss domain distinction 1.028694,\n",
      "VALIDATION Loss: 0.18048842 Acc: 0.93493976\n",
      "New best validation loss:  0.18048841719116485\n",
      "Epoch 20 of 500 took 0.333s\n",
      "Accuracy total 0.865000, main loss classifier 0.489751, source accuracy 0.854375 source classification loss 0.423686, target accuracy 0.875625 target loss 0.349172 accuracy domain distinction 0.500000 loss domain distinction 1.033220,\n",
      "VALIDATION Loss: 0.19946212 Acc: 0.92771084\n",
      "Epoch 21 of 500 took 0.338s\n",
      "Accuracy total 0.873750, main loss classifier 0.491079, source accuracy 0.875000 source classification loss 0.371212, target accuracy 0.872500 target loss 0.404184 accuracy domain distinction 0.500000 loss domain distinction 1.033809,\n",
      "VALIDATION Loss: 0.20428252 Acc: 0.93493976\n",
      "Epoch 22 of 500 took 0.330s\n",
      "Accuracy total 0.871250, main loss classifier 0.456834, source accuracy 0.865625 source classification loss 0.372125, target accuracy 0.876875 target loss 0.335607 accuracy domain distinction 0.500000 loss domain distinction 1.029681,\n",
      "VALIDATION Loss: 0.21544511 Acc: 0.92771084\n",
      "Epoch 23 of 500 took 0.330s\n",
      "Accuracy total 0.876875, main loss classifier 0.459981, source accuracy 0.881250 source classification loss 0.359067, target accuracy 0.872500 target loss 0.355297 accuracy domain distinction 0.500000 loss domain distinction 1.027994,\n",
      "VALIDATION Loss: 0.20531905 Acc: 0.9253012\n",
      "Epoch 24 of 500 took 0.336s\n",
      "Accuracy total 0.864375, main loss classifier 0.494421, source accuracy 0.858125 source classification loss 0.417060, target accuracy 0.870625 target loss 0.366091 accuracy domain distinction 0.500000 loss domain distinction 1.028453,\n",
      "VALIDATION Loss: 0.17602895 Acc: 0.9373494\n",
      "New best validation loss:  0.17602894561631338\n",
      "Epoch 25 of 500 took 0.339s\n",
      "Accuracy total 0.862500, main loss classifier 0.476835, source accuracy 0.857500 source classification loss 0.393929, target accuracy 0.867500 target loss 0.353650 accuracy domain distinction 0.500000 loss domain distinction 1.030452,\n",
      "VALIDATION Loss: 0.22520205 Acc: 0.91084337\n",
      "Epoch 26 of 500 took 0.331s\n",
      "Accuracy total 0.874375, main loss classifier 0.475297, source accuracy 0.868125 source classification loss 0.397240, target accuracy 0.880625 target loss 0.346717 accuracy domain distinction 0.500000 loss domain distinction 1.033183,\n",
      "VALIDATION Loss: 0.22760846 Acc: 0.92289157\n",
      "Epoch 27 of 500 took 0.335s\n",
      "Accuracy total 0.863437, main loss classifier 0.480222, source accuracy 0.854375 source classification loss 0.392187, target accuracy 0.872500 target loss 0.363021 accuracy domain distinction 0.500000 loss domain distinction 1.026188,\n",
      "VALIDATION Loss: 0.18886339 Acc: 0.94216867\n",
      "Epoch 28 of 500 took 0.332s\n",
      "Accuracy total 0.865938, main loss classifier 0.487176, source accuracy 0.848750 source classification loss 0.430056, target accuracy 0.883125 target loss 0.337595 accuracy domain distinction 0.500000 loss domain distinction 1.033507,\n",
      "VALIDATION Loss: 0.23599132 Acc: 0.90843373\n",
      "Epoch 29 of 500 took 0.331s\n",
      "Accuracy total 0.874062, main loss classifier 0.478366, source accuracy 0.866875 source classification loss 0.402698, target accuracy 0.881250 target loss 0.347974 accuracy domain distinction 0.500000 loss domain distinction 1.030301,\n",
      "VALIDATION Loss: 0.19794623 Acc: 0.93253012\n",
      "Epoch 30 of 500 took 0.336s\n",
      "Accuracy total 0.881250, main loss classifier 0.449837, source accuracy 0.881875 source classification loss 0.364024, target accuracy 0.880625 target loss 0.329518 accuracy domain distinction 0.500000 loss domain distinction 1.030658,\n",
      "VALIDATION Loss: 0.18484342 Acc: 0.92048193\n",
      "Epoch    30: reducing learning rate of group 0 to 4.0240e-06.\n",
      "Epoch 31 of 500 took 0.331s\n",
      "Accuracy total 0.877188, main loss classifier 0.464547, source accuracy 0.880000 source classification loss 0.359110, target accuracy 0.874375 target loss 0.363877 accuracy domain distinction 0.500000 loss domain distinction 1.030541,\n",
      "VALIDATION Loss: 0.24259876 Acc: 0.92289157\n",
      "Epoch 32 of 500 took 0.332s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy total 0.868125, main loss classifier 0.487098, source accuracy 0.861875 source classification loss 0.414562, target accuracy 0.874375 target loss 0.354083 accuracy domain distinction 0.500000 loss domain distinction 1.027763,\n",
      "VALIDATION Loss: 0.18444123 Acc: 0.93012048\n",
      "Epoch 33 of 500 took 0.337s\n",
      "Accuracy total 0.868437, main loss classifier 0.470649, source accuracy 0.867500 source classification loss 0.381358, target accuracy 0.869375 target loss 0.353089 accuracy domain distinction 0.500000 loss domain distinction 1.034259,\n",
      "VALIDATION Loss: 0.20797724 Acc: 0.92771084\n",
      "Epoch 34 of 500 took 0.332s\n",
      "Accuracy total 0.870313, main loss classifier 0.470933, source accuracy 0.868125 source classification loss 0.377555, target accuracy 0.872500 target loss 0.357942 accuracy domain distinction 0.500000 loss domain distinction 1.031842,\n",
      "VALIDATION Loss: 0.21481342 Acc: 0.9253012\n",
      "Epoch 35 of 500 took 0.333s\n",
      "Accuracy total 0.877812, main loss classifier 0.464905, source accuracy 0.868750 source classification loss 0.394572, target accuracy 0.886875 target loss 0.329144 accuracy domain distinction 0.500000 loss domain distinction 1.030464,\n",
      "VALIDATION Loss: 0.19020618 Acc: 0.93493976\n",
      "Epoch 36 of 500 took 0.338s\n",
      "Training complete in 0m 12s\n",
      "['participant_0']\n"
     ]
    }
   ],
   "source": [
    "percentage_same_gesture_stable = 0.75 \n",
    "run_SCADANN_training_sessions(examples_datasets=examples_datasets_train, labels_datasets=labels_datasets_train,\n",
    "                              num_kernels=num_kernels, feature_vector_input_length=feature_vector_input_length,\n",
    "                              path_weights_to_save_to=path_SCADANN,\n",
    "                              path_weights_Adversarial_training=path_DANN,\n",
    "                              path_weights_Normal_training=path_TSD,\n",
    "                              number_of_cycles_total = number_of_cycles_total, \n",
    "                              number_of_cycle_for_first_training = number_of_cycle_for_first_training,\n",
    "                              number_of_classes=number_of_classes,\n",
    "                              learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET one participant_examples  (4,)\n",
      "   GET one training_index_examples  (28, 572, 252)  at  0\n",
      "   GOT one group XY  (16016, 252)    (16016,)\n",
      "       one group XY test  (4004, 252)    (4004, 252)\n",
      "       one group XY train (14414, 252)    (14414,)\n",
      "       one group XY valid (1602, 252)    (1602, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  1\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  2\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  3\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "dataloaders: \n",
      "   train  (1, 4)\n",
      "   valid  (1, 4)\n",
      "   test  (1, 4)\n",
      "Participant:  0  Accuracy:  0.8968531468531469\n",
      "Participant:  0  Accuracy:  0.8951048951048951\n",
      "Participant:  0  Accuracy:  0.7797202797202797\n",
      "Participant:  0  Accuracy:  0.8286713286713286\n",
      "ACCURACY PARTICIPANT:  [0.8968531468531469, 0.8951048951048951, 0.7797202797202797, 0.8286713286713286]\n",
      "[[0.89685315 0.8951049  0.77972028 0.82867133]]\n",
      "[array([0.89685315, 0.8951049 , 0.77972028, 0.82867133])]\n",
      "OVERALL ACCURACY: 0.8500874125874126\n"
     ]
    }
   ],
   "source": [
    "algo_name = \"SCADANN\"\n",
    "test_network_SCADANN(examples_datasets_train=examples_datasets_train, labels_datasets_train=labels_datasets_train,\n",
    "                     num_neurons=num_kernels, feature_vector_input_length=feature_vector_input_length,\n",
    "                     path_weights_SCADANN =path_SCADANN, path_weights_normal=path_TSD,\n",
    "                     algo_name=algo_name, cycle_test=3, number_of_cycles_total=number_of_cycles_total,\n",
    "                     number_of_cycle_for_first_training = number_of_cycle_for_first_training,\n",
    "                     number_of_classes=number_of_classes, save_path = save_SCADANN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Day_0~6</th>\n",
       "      <td>0.896853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_7</th>\n",
       "      <td>0.895105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_8</th>\n",
       "      <td>0.77972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_9</th>\n",
       "      <td>0.828671</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Participant_5\n",
       "Day_0~6      0.896853\n",
       "Day_7        0.895105\n",
       "Day_8         0.77972\n",
       "Day_9        0.828671"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_filename = save_SCADANN + '/predictions_' + algo_name + \".npy\"\n",
    "results = np.load(results_filename, allow_pickle=True)\n",
    "SCADANN_acc = results[0]\n",
    "SCADANN_acc_overall = np.mean(SCADANN_acc)\n",
    "SCADANN_df = pd.DataFrame(SCADANN_acc.transpose(), \n",
    "                       index = [f'Day_{i}' for i in index_participant_list],\n",
    "                        columns = ['Participant_5'])\n",
    "SCADANN_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "SCADANN_df.transpose().plot.bar(rot=0, figsize=(10,5))\n",
    "plt.title(\"SCADANN Accuracies\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ground_truths  =  (1, 4)\n",
      "predictions =  (1, 4)\n",
      "index_participant_list  ['0~6', 7, 8, 9]\n",
      "accuracies_gestures =  (22, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Loc1_Sub5_Day0~6-&gt;0~6</th>\n",
       "      <th>Loc1_Sub5_Day0~6-&gt;7</th>\n",
       "      <th>Loc1_Sub5_Day0~6-&gt;8</th>\n",
       "      <th>Loc1_Sub5_Day0~6-&gt;9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M1</td>\n",
       "      <td>0.868132</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.692308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M2</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.961538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M3</td>\n",
       "      <td>0.912088</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.192308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M4</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>M5</td>\n",
       "      <td>0.983516</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.961538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>M6</td>\n",
       "      <td>0.950549</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>M7</td>\n",
       "      <td>0.956044</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>M8</td>\n",
       "      <td>0.890110</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>M9</td>\n",
       "      <td>0.868132</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.884615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>M10</td>\n",
       "      <td>0.890110</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>M11</td>\n",
       "      <td>0.890110</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>M12</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.461538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>M13</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>M14</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.461538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>M15</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.846154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>M16</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>M17</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>M18</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>M19</td>\n",
       "      <td>0.994505</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>M20</td>\n",
       "      <td>0.835165</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>M21</td>\n",
       "      <td>0.906593</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Mean</td>\n",
       "      <td>0.896853</td>\n",
       "      <td>0.895105</td>\n",
       "      <td>0.779720</td>\n",
       "      <td>0.828671</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Loc1_Sub5_Day0~6->0~6  Loc1_Sub5_Day0~6->7  \\\n",
       "0          M0               1.000000             1.000000   \n",
       "1          M1               0.868132             0.692308   \n",
       "2          M2               0.884615             1.000000   \n",
       "3          M3               0.912088             0.884615   \n",
       "4          M4               0.730769             0.461538   \n",
       "5          M5               0.983516             0.961538   \n",
       "6          M6               0.950549             1.000000   \n",
       "7          M7               0.956044             1.000000   \n",
       "8          M8               0.890110             0.615385   \n",
       "9          M9               0.868132             0.961538   \n",
       "10        M10               0.890110             0.923077   \n",
       "11        M11               0.890110             1.000000   \n",
       "12        M12               0.769231             0.846154   \n",
       "13        M13               0.846154             1.000000   \n",
       "14        M14               0.857143             0.846154   \n",
       "15        M15               0.807692             0.538462   \n",
       "16        M16               0.967033             1.000000   \n",
       "17        M17               0.961538             1.000000   \n",
       "18        M18               0.961538             1.000000   \n",
       "19        M19               0.994505             1.000000   \n",
       "20        M20               0.835165             0.961538   \n",
       "21        M21               0.906593             1.000000   \n",
       "22       Mean               0.896853             0.895105   \n",
       "\n",
       "    Loc1_Sub5_Day0~6->8  Loc1_Sub5_Day0~6->9  \n",
       "0              1.000000             1.000000  \n",
       "1              0.423077             0.692308  \n",
       "2              1.000000             0.961538  \n",
       "3              0.923077             0.192308  \n",
       "4              0.384615             0.000000  \n",
       "5              0.961538             0.961538  \n",
       "6              0.769231             1.000000  \n",
       "7              1.000000             1.000000  \n",
       "8              0.692308             0.923077  \n",
       "9              0.692308             0.884615  \n",
       "10             0.846154             1.000000  \n",
       "11             1.000000             1.000000  \n",
       "12             0.769231             0.461538  \n",
       "13             1.000000             0.923077  \n",
       "14             0.000000             0.461538  \n",
       "15             0.461538             0.846154  \n",
       "16             1.000000             1.000000  \n",
       "17             1.000000             1.000000  \n",
       "18             0.461538             1.000000  \n",
       "19             1.000000             1.000000  \n",
       "20             0.769231             0.923077  \n",
       "21             1.000000             1.000000  \n",
       "22             0.779720             0.828671  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truths = results[1]\n",
    "predictions = results[2]\n",
    "print(\"ground_truths  = \", np.shape(ground_truths))\n",
    "print(\"predictions = \", np.shape(predictions))\n",
    "df = get_gesture_accuracies(ground_truths, predictions, number_of_classes=number_of_classes, \n",
    "                            m_name=m_name, n_name=n_name, path=save_TSD, algo_name=algo_name,\n",
    "                           index_participant_list_customized=index_participant_list,\n",
    "                           lump_day_at_participant=5)\n",
    "df = pd.read_csv(save_SCADANN+'/'+algo_name+'.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "Suppose there is a ndarray of NxM dataloaders, then N group of models will be trained, and each group will have M model. Each group is independent of the other, and each model within a group is dependent on its previous training weights.\n",
    "\n",
    "In general, overall accuracies of SCADANN are better than DANN, and DANN is better than TSD.\n",
    "Occasionally accuracies of SCADANN end up a little smaller than DANN, reasons may be lack of datasets put into training model (fixed) and non-optimal percentage_same_gesture_sable (fixed). Code should be reproducible if processed dataset sticks to the shape defined above.  \n",
    "\n",
    "The amount of increase in accuracies from DANN to SCADANN looks random. But if the base model is better at classifying one session, then its corresponding SCADANN is also better at classifying the same session. Given such result, to obtain the best performance from SCADANN, a good model trained with good data should be the starting point.\n",
    "\n",
    "* What to check if sth goes wrong:\n",
    "    * percentage_same_gesture_sable\n",
    "    * number of cycles or sessions\n",
    "    * shape of dataloaders (combination of train, test, valid should include all dataset)\n",
    "    * shape of procssed datasets\n",
    "    * directory paths of weights and results\n",
    "    * if weights are stored or loaded correcltyTSD_acc_overall_one = np.mean(TSD_acc, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TSD\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Day_0~6</th>\n",
       "      <td>0.896853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_7</th>\n",
       "      <td>0.800699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_8</th>\n",
       "      <td>0.758741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_9</th>\n",
       "      <td>0.769231</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Participant_5\n",
       "Day_0~6      0.896853\n",
       "Day_7        0.800699\n",
       "Day_8        0.758741\n",
       "Day_9        0.769231"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DANN\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Day_0~6</th>\n",
       "      <td>0.896853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_7</th>\n",
       "      <td>0.851399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_8</th>\n",
       "      <td>0.772727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_9</th>\n",
       "      <td>0.795455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Participant_5\n",
       "Day_0~6      0.896853\n",
       "Day_7        0.851399\n",
       "Day_8        0.772727\n",
       "Day_9        0.795455"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCADANN\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Day_0~6</th>\n",
       "      <td>0.896853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_7</th>\n",
       "      <td>0.895105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_8</th>\n",
       "      <td>0.77972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_9</th>\n",
       "      <td>0.828671</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Participant_5\n",
       "Day_0~6      0.896853\n",
       "Day_7        0.895105\n",
       "Day_8         0.77972\n",
       "Day_9        0.828671"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"TSD\")\n",
    "display(TSD_df)\n",
    "print(\"DANN\")\n",
    "display(DANN_df)\n",
    "print(\"SCADANN\")\n",
    "display(SCADANN_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Day_7</th>\n",
       "      <td>0.094406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_8</th>\n",
       "      <td>0.020979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_9</th>\n",
       "      <td>0.059441</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Participant_5\n",
       "Day_7      0.094406\n",
       "Day_8      0.020979\n",
       "Day_9      0.059441"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "diff_df = SCADANN_df-TSD_df\n",
    "diff_df = diff_df.drop('Day_'+index_participant_list[0])\n",
    "display(diff_df)\n",
    "diff_df.to_csv(save_TSD+'/diff_results/across_day_loc1_lump7_diff.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Overall_Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TSD</th>\n",
       "      <td>0.806381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DANN</th>\n",
       "      <td>0.829108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SCADANN</th>\n",
       "      <td>0.850087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Overall_Acc\n",
       "TSD         0.806381\n",
       "DANN        0.829108\n",
       "SCADANN     0.850087"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_acc_df = pd.DataFrame([TSD_acc_overall, DANN_acc_overall, SCADANN_acc_overall],\n",
    "                             index = [\"TSD\", \"DANN\", \"SCADANN\"],\n",
    "                             columns = [\"Overall_Acc\"])\n",
    "overall_acc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1800x1800 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(25,25))\n",
    "acc_list = [TSD_df, DANN_df, SCADANN_df, overall_acc_df]\n",
    "title_list = [\"TSD\", \"DANN\", \"SCADANN\", \"Overall\"]\n",
    "for idx, ax in enumerate(axes.reshape(-1)): \n",
    "    acc_list[idx].transpose().plot.bar(ax = ax, rot=0)\n",
    "    ax.set_title(title_list[idx])\n",
    "    ax.set_ylim([0, 1.0])\n",
    "    for p in ax.patches:\n",
    "        ax.annotate(str(np.round(p.get_height(),2)), (p.get_x()+p.get_width()/2., p.get_height()),\n",
    "                    ha='center', va='center', xytext=(0, 8),textcoords='offset points')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
