{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "sys.path.insert(0,'../../LongTermEMG-master')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LongTermClassificationMain.PrepareAndLoadDataLongTerm.prepare_dataset_utils import butter_bandpass_filter, \\\n",
    "    show_filtered_signal, load_timestamps_from_participant, get_angles_from_positions_3d_arm\n",
    "from LongTermClassificationMain.PrepareAndLoadDataLongTerm import feature_extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['longterm_dataset_3DC.zip', 'longterm_dataset_3DC', 'README.md', 'LongTermClassificationMain', 'datasets', 'TransferLearning', '.idea']\n"
     ]
    }
   ],
   "source": [
    "os.chdir(\"/home/laiy/gitrepos/msr_final/LongTermEMG-master/LongTermClassificationMain/PrepareAndLoadDataLongTerm\")\n",
    "print(os.listdir(\"../../\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_participant_training_1_to_skip = [\"Participant0/Training1\", \"Participant0/Evaluation2\", \"Participant0/Evaluation3\",\n",
    "                                       \"Participant2/Training1\", \"Participant2/Evaluation2\", \"Participant2/Evaluation3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_set_name = \"TSD_features_set\"\n",
    "feature_set_function = feature_extraction.getTSD\n",
    "path=\"../../datasets/longterm_dataset_3DC\"\n",
    "number_of_gestures=11\n",
    "number_of_cycles=4\n",
    "window_size=150 # 150ms frame with 100 overlap\n",
    "size_non_overlap=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_examples(emg_examples, feature_set_function, window_size=150, size_non_overlap=50):\n",
    "    ''' \n",
    "    emg_examples: list of emg signals, each member represent one recording of a 10 channel emg\n",
    "    \n",
    "    feature_set_function:\n",
    "        shape(formated_example) = (97, 10, 150)\n",
    "        shape(featured_example) = (97, 385)\n",
    "        might not need this for myo\n",
    "    \n",
    "    window_size, size_non_verlap: 150ms frame with 100ms overlap    \n",
    "        ignoring leftover examples at the end\n",
    "        \n",
    "    '''\n",
    "    examples_to_calculate_features_set_from = []\n",
    "    example = []\n",
    "    for emg_vector in emg_examples:\n",
    "        if len(example) == 0:\n",
    "            example = emg_vector\n",
    "        else:\n",
    "            example = np.row_stack((example, emg_vector))\n",
    "        \n",
    "        if len(example) >= window_size:\n",
    "            # The example is of the shape TIME x CHANNEL. Make it of the shape CHANNEL x TIME\n",
    "            example = example.transpose()\n",
    "            # Go over each channel and bandpass filter it between 20 and 495 Hz.\n",
    "            example_filtered = []\n",
    "            for channel in example:\n",
    "                channel_filtered = butter_bandpass_filter(channel, lowcut=20, highcut=495, fs=1000, order=4)\n",
    "                # show_filtered_signal(channel, channel_filtered)\n",
    "                example_filtered.append(channel_filtered)\n",
    "            # Add the filtered example to the list of examples to return and transpose the example array again to go\n",
    "            # back to TIME x CHANNEL\n",
    "            examples_to_calculate_features_set_from.append(example_filtered)\n",
    "            example = example.transpose()\n",
    "            # Remove part of the data of the example according to the size_non_overlap variable\n",
    "            example = example[size_non_overlap:]\n",
    "    \n",
    "#     print(\"   @ formatting example = \", np.shape(examples_to_calculate_features_set_from))\n",
    "    \n",
    "    examples_features_set_calculated = feature_extraction.get_dataset_with_features_set(\n",
    "        dataset=examples_to_calculate_features_set_from, features_set_function=feature_set_function)\n",
    "#     print(\"   @ featured example = \", np.shape(examples_features_set_calculated))\n",
    "\n",
    "    return examples_features_set_calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from handcrafted_features_prepare_from_from_raw_dataset import get_highest_average_emg_window\n",
    "\n",
    "def read_files_to_format_training_session(path_folder_examples, feature_set_function,\n",
    "                                          number_of_cycles, number_of_gestures, window_size,\n",
    "                                          size_non_overlap):\n",
    "    '''\n",
    "    when #cycle = 1, examples are recorded at highest activation\n",
    "        \n",
    "    '''\n",
    "    examples_training, labels_training = [], []\n",
    "    # Check if the folder is empty, if so, skip it\n",
    "    # path_folder_examples = path + \"/\" + folder_participant + \"/\" + training_directory + \"/EMG\"\n",
    "    if len(os.listdir(path_folder_examples)) == 0:\n",
    "        return [], [], []\n",
    "#     print(os.listdir(path_folder_examples))\n",
    "#--------  filename = `3dc_EMG_gesture_<#cycle>_<#gesture>.txt`-------------\n",
    "    highest_activation_per_gesture = []\n",
    "    for cycle in range(number_of_cycles):\n",
    "        # path_folder_examples = path + folder_participant + \"/\" + training_directory + \"/EMG\"\n",
    "        # This one instance, the participant only recorded one cycle of training. Skip it\n",
    "        for participant_session_to_skip in list_participant_training_1_to_skip:\n",
    "            if participant_session_to_skip in path_folder_examples:\n",
    "                return [], [], []\n",
    "        path_emg = path_folder_examples + \"/3dc_EMG_gesture_%d_\" % cycle\n",
    "        examples, labels = [], []\n",
    "        for gesture_index in range(number_of_gestures):\n",
    "            examples_to_format = []\n",
    "            read_file = path_emg + '%d.txt' % gesture_index\n",
    "#             print(\"READ FILE \", read_file)\n",
    "            \n",
    "            for line in open(read_file):\n",
    "                #  strip() remove the \"\\n\" character, split separate the data in a list. np.float\n",
    "                #  transform each element of the list from a str to a float\n",
    "                emg_signal = np.float32(line.strip().split(\",\"))\n",
    "                examples_to_format.append(emg_signal)\n",
    "#             print(\"   example emg from file (to format)\", np.shape(examples_to_format))\n",
    "            \n",
    "            if cycle == 1:  # This cycle is the second cycle and correspond to the highest effort baseline. Record it.\n",
    "                if gesture_index == 0:\n",
    "                    highest_activation_per_gesture.append(0)\n",
    "                else:\n",
    "                    highest_activation_per_gesture.append(get_highest_average_emg_window(\n",
    "                        examples_to_format, window_for_moving_average=window_size))\n",
    "                    \n",
    "#             print(\"   start formating\")\n",
    "            examples_formatted = format_examples(examples_to_format,\n",
    "                                                 feature_set_function=feature_set_function, window_size=window_size,\n",
    "                                                 size_non_overlap=size_non_overlap)\n",
    "            examples.extend(examples_formatted)\n",
    "            labels.extend(np.ones(len(examples_formatted)) * gesture_index)\n",
    "#             print(\"--- format complete for gesture \", gesture_index, \" #examples = \", len(examples))\n",
    "            \n",
    "        print(\"   SHAPE SESSION \", cycle, \" EXAMPLES: \", np.shape(examples))\n",
    "        examples_training.append(examples)\n",
    "        labels_training.append(labels)\n",
    "        print(\"   SHAPE ALL SESSION EXAMPLES: \", np.shape(examples_training))\n",
    "\n",
    "    return examples_training, labels_training, highest_activation_per_gesture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_training_sessions_datasets, labels_training_sessions_datasets = [], []\n",
    "highest_activation_participants = []\n",
    "examples_evaluation_sessions_datasets, labels_evaluation_sessions_datasets, timestamps_emg_evaluation = [], [], []\n",
    "angles_with_timestamps_emg_evaluation = []\n",
    "\n",
    "training_datetimes, evaluation_datetimes = [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Each participant has four training sessions, each session has four cycles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sessions_directories'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['Evaluation3',\n",
       " 'Training3',\n",
       " 'Training1',\n",
       " 'Training5',\n",
       " 'Evaluation8',\n",
       " 'Evaluation0',\n",
       " 'Evaluation6',\n",
       " 'Evaluation7',\n",
       " 'Training0',\n",
       " 'Evaluation2',\n",
       " 'Training4',\n",
       " 'Evaluation9',\n",
       " 'Training2',\n",
       " 'Evaluation1',\n",
       " 'Evaluation4',\n",
       " 'Evaluation5']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current dir =  Evaluation3\n",
      "current dir =  Training3\n",
      "   SHAPE SESSION  0  EXAMPLES:  (1009, 385)\n",
      "   SHAPE ALL SESSION EXAMPLES:  (1, 1009, 385)\n",
      "   SHAPE SESSION  1  EXAMPLES:  (1052, 385)\n",
      "   SHAPE ALL SESSION EXAMPLES:  (2,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laiy/.local/lib/python3.8/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   SHAPE SESSION  2  EXAMPLES:  (949, 385)\n",
      "   SHAPE ALL SESSION EXAMPLES:  (3,)\n",
      "   SHAPE SESSION  3  EXAMPLES:  (1014, 385)\n",
      "   SHAPE ALL SESSION EXAMPLES:  (4,)\n",
      "@ traning sessions =  (1, 4)\n",
      "current dir =  Training1\n",
      "current dir =  Training5\n",
      "current dir =  Evaluation8\n",
      "current dir =  Evaluation0\n",
      "current dir =  Evaluation6\n",
      "current dir =  Evaluation7\n",
      "current dir =  Training0\n",
      "   SHAPE SESSION  0  EXAMPLES:  (1064, 385)\n",
      "   SHAPE ALL SESSION EXAMPLES:  (1, 1064, 385)\n",
      "   SHAPE SESSION  1  EXAMPLES:  (1046, 385)\n",
      "   SHAPE ALL SESSION EXAMPLES:  (2,)\n",
      "   SHAPE SESSION  2  EXAMPLES:  (1063, 385)\n",
      "   SHAPE ALL SESSION EXAMPLES:  (3,)\n",
      "   SHAPE SESSION  3  EXAMPLES:  (1017, 385)\n",
      "   SHAPE ALL SESSION EXAMPLES:  (4,)\n",
      "@ traning sessions =  (2, 4)\n",
      "current dir =  Evaluation2\n",
      "current dir =  Training4\n",
      "   SHAPE SESSION  0  EXAMPLES:  (1067, 385)\n",
      "   SHAPE ALL SESSION EXAMPLES:  (1, 1067, 385)\n",
      "   SHAPE SESSION  1  EXAMPLES:  (1087, 385)\n",
      "   SHAPE ALL SESSION EXAMPLES:  (2,)\n",
      "   SHAPE SESSION  2  EXAMPLES:  (1067, 385)\n",
      "   SHAPE ALL SESSION EXAMPLES:  (3,)\n",
      "   SHAPE SESSION  3  EXAMPLES:  (1068, 385)\n",
      "   SHAPE ALL SESSION EXAMPLES:  (4,)\n",
      "@ traning sessions =  (3, 4)\n",
      "current dir =  Evaluation9\n",
      "current dir =  Training2\n",
      "   SHAPE SESSION  0  EXAMPLES:  (1014, 385)\n",
      "   SHAPE ALL SESSION EXAMPLES:  (1, 1014, 385)\n",
      "   SHAPE SESSION  1  EXAMPLES:  (964, 385)\n",
      "   SHAPE ALL SESSION EXAMPLES:  (2,)\n",
      "   SHAPE SESSION  2  EXAMPLES:  (1072, 385)\n",
      "   SHAPE ALL SESSION EXAMPLES:  (3,)\n",
      "   SHAPE SESSION  3  EXAMPLES:  (1049, 385)\n",
      "   SHAPE ALL SESSION EXAMPLES:  (4,)\n",
      "@ traning sessions =  (4, 4)\n",
      "current dir =  Evaluation1\n",
      "current dir =  Evaluation4\n",
      "current dir =  Evaluation5\n"
     ]
    }
   ],
   "source": [
    "for index_participant in range(1):\n",
    "    folder_participant = \"/Participant\" + str(index_participant)\n",
    "    sessions_directories = os.listdir(path + folder_participant)\n",
    "    display(\"sessions_directories\", sessions_directories)\n",
    "    \n",
    "#     Don't need datetime\n",
    "#     training_datetime, evaluation_datetime = load_timestamps_from_participant(path + folder_participant)\n",
    "#     training_datetimes.append(training_datetime)\n",
    "#     display(\"training_datetime \", training_datetime)\n",
    "#     evaluation_datetimes.append(evaluation_datetime)\n",
    "#     display(\"evaluation_datetime \", evaluation_datetime)\n",
    "\n",
    "    examples_participant_training_sessions, labels_participant_training_sessions = [], []\n",
    "    highest_activation_per_session = []\n",
    "    examples_participant_evaluation_sessions, labels_participant_evaluation_sessions = [], []\n",
    "    timestamps_evaluation_participant_sessions, angles_with_timestamps_participant_sessions = [], []\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Don't need Evaluation part for now, only process Training \n",
    "    \"\"\"\n",
    "\n",
    "    for session_directory in sessions_directories:\n",
    "        print(\"current dir = \", session_directory)\n",
    "\n",
    "        if \"Training\" in session_directory:\n",
    "            path_folder_examples = path + \"/\" + folder_participant + \"/\" + session_directory + \"/EMG\"\n",
    "            examples_training, labels_training, highest_activation_per_gesture = \\\n",
    "                read_files_to_format_training_session(path_folder_examples=path_folder_examples,\n",
    "                                                      number_of_cycles=number_of_cycles,\n",
    "                                                      number_of_gestures=number_of_gestures,\n",
    "                                                      window_size=window_size,\n",
    "                                                      size_non_overlap=size_non_overlap,\n",
    "                                                      feature_set_function=feature_set_function)\n",
    "            if len(examples_training) > 0:\n",
    "                # These instances, the participant only recorded one cycle of training. Skip it\n",
    "                skip_it = False\n",
    "                for participant_session_to_skip in list_participant_training_1_to_skip:\n",
    "                    if participant_session_to_skip in path_folder_examples:\n",
    "                        skip_it = True\n",
    "                if skip_it is False:\n",
    "                    examples_participant_training_sessions.append(examples_training)\n",
    "                    labels_participant_training_sessions.append(labels_training)\n",
    "                    highest_activation_per_session.append(highest_activation_per_gesture)\n",
    "\n",
    "                    print(\"@ traning sessions = \", np.shape(examples_participant_training_sessions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1009,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(labels_training_sessions_datasets[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traning examples  (4, 4)\n",
      "all traning examples  (1, 4, 4)\n",
      "traning labels  (4, 4)\n",
      "all traning labels  (1, 4, 4)\n"
     ]
    }
   ],
   "source": [
    "# store processed data to dictionary\n",
    "print('traning examples ', np.shape(examples_participant_training_sessions))\n",
    "examples_training_sessions_datasets = []\n",
    "examples_training_sessions_datasets.append(examples_participant_training_sessions)\n",
    "print('all traning examples ', np.shape(examples_training_sessions_datasets))\n",
    "\n",
    "print('traning labels ', np.shape(labels_participant_training_sessions))\n",
    "labels_training_sessions_datasets = []\n",
    "labels_training_sessions_datasets.append(labels_participant_training_sessions)\n",
    "print('all traning labels ', np.shape(labels_training_sessions_datasets))\n",
    "\n",
    "dataset_dictionnary = {\"examples_training\": np.array(examples_training_sessions_datasets, dtype=object),\n",
    "                       \"labels_training\": np.array(labels_training_sessions_datasets, dtype=object)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store dictionary to pickle\n",
    "training_session_dataset_dictionnary = {}\n",
    "training_session_dataset_dictionnary[\"examples_training\"] = dataset_dictionnary[\"examples_training\"]\n",
    "training_session_dataset_dictionnary[\"labels_training\"] = dataset_dictionnary[\"labels_training\"]\n",
    "\n",
    "os.chdir(\"/home/laiy/gitrepos/msr_final/LongTermEMG-master/LongTermClassificationMain/PrepareAndLoadDataLongTerm\")\n",
    "with open(\"../Processed_datasets/%s_training_session.pickle\" % features_set_name, 'wb') as f:\n",
    "    pickle.dump(training_session_dataset_dictionnary, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train TSD_DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['self_learning', 'utils_training_and_evaluation.py', 'training_loops_preparations.py', 'ForTrainingSessions', 'test_polar_plot.py', '__pycache__', 'ForEvaluationSessions']\n",
      "traning examples  (1, 4, 4)\n",
      "traning labels  (1, 4, 4)\n"
     ]
    }
   ],
   "source": [
    "os.chdir(\"/home/laiy/gitrepos/msr_final/LongTermEMG-master/LongTermClassificationMain/TrainingsAndEvaluations/ForTrainingSessions/TSD_DNN\")\n",
    "print(os.listdir(\"../../\"))\n",
    "with open(\"../../../Processed_datasets/TSD_features_set_training_session.pickle\", 'rb') as f:\n",
    "    dataset_training = pickle.load(file=f)\n",
    "\n",
    "examples_datasets_train = dataset_training['examples_training']\n",
    "print('traning examples ', np.shape(examples_datasets_train))\n",
    "labels_datasets_train = dataset_training['labels_training']\n",
    "print('traning labels ', np.shape(labels_datasets_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TSD_features_set'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_set_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
