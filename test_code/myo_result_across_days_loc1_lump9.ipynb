{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results of TSD, DANN, SCADANN models across 10 days of inward rotation starting at Day_0~8 for Subject_4\n",
    "\n",
    "Library used can be downloaded from https://github.com/aonai/long_term_EMG_myo   \n",
    "&emsp; Original by UlysseCoteAllard https://github.com/UlysseCoteAllard/LongTermEMG   \n",
    "Dataset recorded by https://github.com/Suguru55/Wearable_Sensor_Long-term_sEMG_Dataset   \n",
    "Extended robot project can be found in https://github.com/aonai/myo_robot_arm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "* weights for TSD are total of 50 training models, 10 for each day\n",
    "* weights for DANN and SCADANN are total of 45 trianing models, 9 for each day\n",
    "\n",
    "\n",
    "* training examples should have shape (1, 2,)\n",
    "* first session has shape (36, 572, 252)\n",
    "* the following sessions have shape (4, 572, 252)\n",
    "* training labels should have shape (1, 2,)\n",
    "\n",
    "\n",
    "* location 0, 1, and 2 corresponds to neutral position, inward rotation, and outward rotation respectively\n",
    "* session mentioned below are days, so number of sessions is 10\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "import torch\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_dir = \"/home/laiy/gitrepos/msr_final/LongTermEMG_myo\"\n",
    "os.chdir(code_dir)\n",
    "from PrepareAndLoadData.process_data import read_data_training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Prepare Data\n",
    "use `switch=2` to train across days and individually on wearing location 0 (`session_in_include=[0]`)\n",
    "\n",
    "### specify the directories used for running the code:\n",
    "* `code_diar`: path to long_term_EMG_myo library\n",
    "* `data_dir`: where raw dataset is loaded; raw data is in csv format\n",
    "* `processed_data_dir`: where processed dataset is loaded; processed data is in npy pickle format\n",
    "    * processed data should be a ndarray of shape   \n",
    "    (controlling_factor_1 x controlling_factor_2 x num_sessions_per_gesture x #examples_window*#mov(26*22=572) x processed_channel_shape(252 for TSD, (4,8,10) for ConvNet)\n",
    "* `path_<model_name>`: where model weights are saved\n",
    "    * weights should be saved in folder `/Weights/<model_name>`. Each folder has subfolders containing weights for the first controlling factor.\n",
    "    * weights for base model (TSD or ConvNet) contain m set of training model\n",
    "    * weights for DANN and SCADANN contain m-1 set of trianing model (these models are trianed based on TSD, so they do not have a best_state_0.pt model). \n",
    "* `save_<model_name>`: where model results are saved\n",
    "    * each result for testing a model on a group of dataset is saved in folder `results`. Each result has corresponding \n",
    "        * `<model_name>.txt` includes predictions, ground truths, array of accuracies for each participant and each session, and overall accuracy\n",
    "        * `predictions_<model_name>.npy` includes array of accuracies, ground truths, predictions, and model outputs (probability array for each prediction)\n",
    "        * remember to make blank files in these names before saving\n",
    "\n",
    "\n",
    "\n",
    "* use `read_data_training` to process raw dataset\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/home/laiy/gitrepos/msr_final/Wearable_Sensor_Long-term_sEMG_Dataset/data\"\n",
    "processed_data_dir = \"/home/laiy/gitrepos/msr_final/LongTermEMG_myo/Processed_datasets_all_across_day_loc_1_lump9\"\n",
    "code_dir = \"/home/laiy/gitrepos/msr_final/LongTermEMG_myo\"\n",
    "save_dir = \"/home/laiy/gitrepos/msr_final/LongTermEMG_myo/Results\"\n",
    "\n",
    "path_TSD =\"/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump9/TSD\"\n",
    "save_TSD = \"/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/results\"\n",
    "\n",
    "path_DANN =\"/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump9/DANN\"\n",
    "save_DANN = \"/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/results\"\n",
    "\n",
    "path_SCADANN =\"/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump9/SCADANN\"\n",
    "save_SCADANN = \"/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing Training datasets...\n",
      "session  1  --- process data in days  [2, 5, 6, 16, 17, 18, 22, 24, 25, 28]\n",
      "index_participant_list  [5]\n",
      "READ  Sub 5 _Loc 1 _Day 2\n",
      "examples_per_session =  (1, 4, 572, 252)\n",
      "READ  Sub 5 _Loc 1 _Day 5\n",
      "Include day  5  in first dataset  (4, 572, 252)\n",
      "examples of first session =  (8, 572, 252)\n",
      "examples_per_session =  (1, 8, 572, 252)\n",
      "READ  Sub 5 _Loc 1 _Day 6\n",
      "Include day  6  in first dataset  (8, 572, 252)\n",
      "examples of first session =  (12, 572, 252)\n",
      "examples_per_session =  (1, 12, 572, 252)\n",
      "READ  Sub 5 _Loc 1 _Day 16\n",
      "Include day  16  in first dataset  (12, 572, 252)\n",
      "examples of first session =  (16, 572, 252)\n",
      "examples_per_session =  (1, 16, 572, 252)\n",
      "READ  Sub 5 _Loc 1 _Day 17\n",
      "Include day  17  in first dataset  (16, 572, 252)\n",
      "examples of first session =  (20, 572, 252)\n",
      "examples_per_session =  (1, 20, 572, 252)\n",
      "READ  Sub 5 _Loc 1 _Day 18\n",
      "Include day  18  in first dataset  (20, 572, 252)\n",
      "examples of first session =  (24, 572, 252)\n",
      "examples_per_session =  (1, 24, 572, 252)\n",
      "READ  Sub 5 _Loc 1 _Day 22\n",
      "Include day  22  in first dataset  (24, 572, 252)\n",
      "examples of first session =  (28, 572, 252)\n",
      "examples_per_session =  (1, 28, 572, 252)\n",
      "READ  Sub 5 _Loc 1 _Day 24\n",
      "Include day  24  in first dataset  (28, 572, 252)\n",
      "examples of first session =  (32, 572, 252)\n",
      "examples_per_session =  (1, 32, 572, 252)\n",
      "READ  Sub 5 _Loc 1 _Day 25\n",
      "Include day  25  in first dataset  (32, 572, 252)\n",
      "examples of first session =  (36, 572, 252)\n",
      "examples_per_session =  (1, 36, 572, 252)\n",
      "READ  Sub 5 _Loc 1 _Day 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laiy/.local/lib/python3.8/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "examples_per_session =  (2,)\n",
      "@ traning sessions =  (1, 2)\n",
      "traning examples  (1, 2)\n",
      "traning labels  (1, 2)\n",
      "all traning examples  (1, 2)\n",
      "all traning labels  (1, 2)\n"
     ]
    }
   ],
   "source": [
    "read_data_training(path=data_dir, store_path = processed_data_dir,  \n",
    "                   sessions_to_include =[1], switch=2, include_in_first=9,\n",
    "                   start_at_participant=5, num_participant=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traning examples  (1, 2)\n",
      "traning labels  (1, 2)\n"
     ]
    }
   ],
   "source": [
    "# check stored pickle \n",
    "with open(processed_data_dir + \"/training_session.pickle\", 'rb') as f:\n",
    "    dataset_training = pickle.load(file=f)\n",
    "\n",
    "examples_datasets_train = dataset_training['examples_training']\n",
    "print('traning examples ', np.shape(examples_datasets_train))\n",
    "labels_datasets_train = dataset_training['labels_training']\n",
    "print('traning labels ', np.shape(labels_datasets_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  examples_per_session =  (36, 572, 252)\n",
      "0  labels_per_session =  (36, 572)\n",
      "1  examples_per_session =  (4, 572, 252)\n",
      "1  labels_per_session =  (4, 572)\n"
     ]
    }
   ],
   "source": [
    "for idx, examples_per_session in enumerate (examples_datasets_train[0]):\n",
    "    print(idx, \" examples_per_session = \", np.shape(examples_per_session))\n",
    "    print(idx, \" labels_per_session = \", np.shape(labels_datasets_train[0][idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify params used for training and testing\n",
    "\n",
    "During training and testing, processed datasets are first put into pytorch dataloders, then feed to the model trainer; following are params for TSD model and dataloaders\n",
    "\n",
    "* `num_kernels`: list of integers defining number of neurons used in each linear layer (linear block has `dropout`=0.5)\n",
    "* `number_of_cycles_total`: number of trails performed for each session (assuming that all session have the same trail size)\n",
    "    * 4 for myo across day training\n",
    "* `number_of_classes`: total number of gestures performed in dataset\n",
    "    * 22 for myo\n",
    "* `batch_size`: number of examples stored in each batch\n",
    "* `feature_vector_input_length`: length of input array or each processed signal; i.e. size of one training example \n",
    "    * 252 for TSD\n",
    "* `learning_rate`= 0.002515\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number_of_cycle_for_first_training  36\n",
      "number_of_cycles_total  4\n"
     ]
    }
   ],
   "source": [
    "num_kernels=[200, 200, 200]                                \n",
    "number_of_cycle_for_first_training = np.shape(examples_datasets_train[0][0])[0]               \n",
    "number_of_cycles_total=np.shape(examples_datasets_train[-1][-1])[0]               \n",
    "print(\"number_of_cycle_for_first_training \", number_of_cycle_for_first_training)\n",
    "print(\"number_of_cycles_total \", number_of_cycles_total)\n",
    "number_of_classes=22\n",
    "batch_size=128          \n",
    "feature_vector_input_length=252                     \n",
    "learning_rate=0.002515"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. TSD_DNN\n",
    "* `train_fine_tuning`: used to train data using a base model (TSD or ConvNet)\n",
    "    * running this function will save num_sessions sets of TSD model weights (each is fine tuned based on the previous training)  \n",
    "    \n",
    "* `test_standard_model_on_training_sessions`: test model result\n",
    "\n",
    "\n",
    "### check if dataloaders are loaded correctly:\n",
    "* each participant has shape (num_session x 40 x 572 x 252)\n",
    "* each session has shape (40 x 572 x 252)\n",
    "* put these data into on group ends up with shape (40*572=22880, 252)\n",
    "    * shuffle on group of data and put into dataloaders\n",
    "    * each participant should have num_sessions sets of dataloaders, each correspond to one session\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TrainingsAndEvaluations.ForTrainingSessions.train_tsd_dnn_standard import \\\n",
    "            test_standard_model_on_training_sessions, train_fine_tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET one participant_examples  (2,)\n",
      "   GET one training_index_examples  (36, 572, 252)  at  0\n",
      "   GOT one group XY  (20592, 252)    (20592,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (18532, 252)    (18532,)\n",
      "       one group XY valid (2060, 252)    (2060, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  1\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "dataloaders: \n",
      "   train  (1, 2)\n",
      "   valid  (1, 2)\n",
      "   test  (1, 0)\n",
      "START TRAINING\n",
      "Participant:  0\n",
      "Session:  0\n",
      "<generator object Module.parameters at 0x7efd220e25f0>\n",
      "Epoch 0/499\n",
      "----------\n",
      "train Loss: 0.00802420 Acc: 0.67643229\n",
      "val Loss: 0.00021155 Acc: 0.84757282\n",
      "New best validation loss: 0.00021154751766075207\n",
      "Epoch 1 of 500 took 0.947s\n",
      "Epoch 1/499\n",
      "----------\n",
      "train Loss: 0.00473713 Acc: 0.79052734\n",
      "val Loss: 0.00018928 Acc: 0.85582524\n",
      "Epoch 2 of 500 took 0.936s\n",
      "Epoch 2/499\n",
      "----------\n",
      "train Loss: 0.00415745 Acc: 0.81467014\n",
      "val Loss: 0.00015388 Acc: 0.88786408\n",
      "Epoch 3 of 500 took 0.937s\n",
      "Epoch 3/499\n",
      "----------\n",
      "train Loss: 0.00387324 Acc: 0.82242839\n",
      "val Loss: 0.00014083 Acc: 0.8961165\n",
      "Epoch 4 of 500 took 0.930s\n",
      "Epoch 4/499\n",
      "----------\n",
      "train Loss: 0.00357393 Acc: 0.83734809\n",
      "val Loss: 0.00015022 Acc: 0.88737864\n",
      "Epoch 5 of 500 took 0.934s\n",
      "Epoch 5/499\n",
      "----------\n",
      "train Loss: 0.00347349 Acc: 0.84217665\n",
      "val Loss: 0.00012853 Acc: 0.90097087\n",
      "Epoch 6 of 500 took 0.935s\n",
      "Epoch 6/499\n",
      "----------\n",
      "train Loss: 0.00325085 Acc: 0.85487196\n",
      "val Loss: 0.00012778 Acc: 0.90582524\n",
      "Epoch 7 of 500 took 0.931s\n",
      "Epoch 7/499\n",
      "----------\n",
      "train Loss: 0.00314031 Acc: 0.85519748\n",
      "val Loss: 0.00012572 Acc: 0.90436893\n",
      "Epoch 8 of 500 took 0.945s\n",
      "Epoch 8/499\n",
      "----------\n",
      "train Loss: 0.00315387 Acc: 0.85677083\n",
      "val Loss: 0.00012275 Acc: 0.90825243\n",
      "Epoch 9 of 500 took 0.933s\n",
      "Epoch 9/499\n",
      "----------\n",
      "train Loss: 0.00301617 Acc: 0.86132812\n",
      "val Loss: 0.00011128 Acc: 0.91456311\n",
      "New best validation loss: 0.00011128300746667733\n",
      "Epoch 10 of 500 took 0.935s\n",
      "Epoch 10/499\n",
      "----------\n",
      "train Loss: 0.00293104 Acc: 0.86691623\n",
      "val Loss: 0.00011399 Acc: 0.91019417\n",
      "Epoch 11 of 500 took 0.980s\n",
      "Epoch 11/499\n",
      "----------\n",
      "train Loss: 0.00284036 Acc: 0.87044271\n",
      "val Loss: 0.00014266 Acc: 0.89368932\n",
      "Epoch 12 of 500 took 0.959s\n",
      "Epoch 12/499\n",
      "----------\n",
      "train Loss: 0.00276508 Acc: 0.87489149\n",
      "val Loss: 0.00010565 Acc: 0.9184466\n",
      "Epoch 13 of 500 took 0.947s\n",
      "Epoch 13/499\n",
      "----------\n",
      "train Loss: 0.00279140 Acc: 0.87310113\n",
      "val Loss: 0.00010890 Acc: 0.9131068\n",
      "Epoch 14 of 500 took 0.939s\n",
      "Epoch 14/499\n",
      "----------\n",
      "train Loss: 0.00267207 Acc: 0.87972005\n",
      "val Loss: 0.00010781 Acc: 0.91990291\n",
      "Epoch 15 of 500 took 0.937s\n",
      "Epoch 15/499\n",
      "----------\n",
      "train Loss: 0.00260311 Acc: 0.88264974\n",
      "val Loss: 0.00010152 Acc: 0.91990291\n",
      "Epoch 16 of 500 took 0.938s\n",
      "Epoch 16/499\n",
      "----------\n",
      "train Loss: 0.00260727 Acc: 0.8811849\n",
      "val Loss: 0.00011961 Acc: 0.90728155\n",
      "Epoch 17 of 500 took 0.938s\n",
      "Epoch 17/499\n",
      "----------\n",
      "train Loss: 0.00254490 Acc: 0.88346354\n",
      "val Loss: 0.00009851 Acc: 0.92330097\n",
      "Epoch 18 of 500 took 0.946s\n",
      "Epoch 18/499\n",
      "----------\n",
      "train Loss: 0.00252472 Acc: 0.88699002\n",
      "val Loss: 0.00010238 Acc: 0.92427184\n",
      "Epoch 19 of 500 took 0.942s\n",
      "Epoch 19/499\n",
      "----------\n",
      "train Loss: 0.00251775 Acc: 0.8821072\n",
      "val Loss: 0.00009767 Acc: 0.92621359\n",
      "Epoch 20 of 500 took 0.931s\n",
      "Epoch 20/499\n",
      "----------\n",
      "train Loss: 0.00252778 Acc: 0.8835178\n",
      "val Loss: 0.00009297 Acc: 0.92475728\n",
      "Epoch 21 of 500 took 0.935s\n",
      "\n",
      "Training complete in 0m 20s\n",
      "Best val loss: 0.000111\n",
      "Session:  1\n",
      "<generator object Module.parameters at 0x7efd220b19e0>\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump9/TSD/participant_0/best_state_0.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump9/TSD/participant_0/best_state_0.pt' (epoch 10)\n",
      "Epoch 0/499\n",
      "----------\n",
      "train Loss: 0.00331000 Acc: 0.84277344\n",
      "val Loss: 0.00102523 Acc: 0.91266376\n",
      "New best validation loss: 0.001025230538376554\n",
      "Epoch 1 of 500 took 0.109s\n",
      "Epoch 1/499\n",
      "----------\n",
      "train Loss: 0.00266718 Acc: 0.87695312\n",
      "val Loss: 0.00103834 Acc: 0.89956332\n",
      "Epoch 2 of 500 took 0.108s\n",
      "Epoch 2/499\n",
      "----------\n",
      "train Loss: 0.00229606 Acc: 0.89453125\n",
      "val Loss: 0.00081838 Acc: 0.92139738\n",
      "New best validation loss: 0.000818384201245537\n",
      "Epoch 3 of 500 took 0.111s\n",
      "Epoch 3/499\n",
      "----------\n",
      "train Loss: 0.00210115 Acc: 0.90527344\n",
      "val Loss: 0.00098970 Acc: 0.90393013\n",
      "Epoch 4 of 500 took 0.133s\n",
      "Epoch 4/499\n",
      "----------\n",
      "train Loss: 0.00208459 Acc: 0.90136719\n",
      "val Loss: 0.00095729 Acc: 0.89956332\n",
      "Epoch 5 of 500 took 0.128s\n",
      "Epoch 5/499\n",
      "----------\n",
      "train Loss: 0.00196221 Acc: 0.90917969\n",
      "val Loss: 0.00075688 Acc: 0.93449782\n",
      "Epoch 6 of 500 took 0.134s\n",
      "Epoch 6/499\n",
      "----------\n",
      "train Loss: 0.00194685 Acc: 0.91308594\n",
      "val Loss: 0.00064692 Acc: 0.96069869\n",
      "New best validation loss: 0.0006469194675637124\n",
      "Epoch 7 of 500 took 0.130s\n",
      "Epoch 7/499\n",
      "----------\n",
      "train Loss: 0.00171088 Acc: 0.91943359\n",
      "val Loss: 0.00062646 Acc: 0.95196507\n",
      "Epoch 8 of 500 took 0.143s\n",
      "Epoch 8/499\n",
      "----------\n",
      "train Loss: 0.00154610 Acc: 0.92480469\n",
      "val Loss: 0.00224922 Acc: 0.83406114\n",
      "Epoch 9 of 500 took 0.137s\n",
      "Epoch 9/499\n",
      "----------\n",
      "train Loss: 0.00177507 Acc: 0.91308594\n",
      "val Loss: 0.00059026 Acc: 0.9650655\n",
      "Epoch 10 of 500 took 0.144s\n",
      "Epoch 10/499\n",
      "----------\n",
      "train Loss: 0.00153121 Acc: 0.93115234\n",
      "val Loss: 0.00052141 Acc: 0.95633188\n",
      "New best validation loss: 0.0005214119321914739\n",
      "Epoch 11 of 500 took 0.128s\n",
      "Epoch 11/499\n",
      "----------\n",
      "train Loss: 0.00156497 Acc: 0.92480469\n",
      "val Loss: 0.00074551 Acc: 0.930131\n",
      "Epoch 12 of 500 took 0.133s\n",
      "Epoch 12/499\n",
      "----------\n",
      "train Loss: 0.00151702 Acc: 0.92675781\n",
      "val Loss: 0.00107311 Acc: 0.91266376\n",
      "Epoch 13 of 500 took 0.128s\n",
      "Epoch 13/499\n",
      "----------\n",
      "train Loss: 0.00128919 Acc: 0.93554688\n",
      "val Loss: 0.00051938 Acc: 0.95196507\n",
      "Epoch 14 of 500 took 0.138s\n",
      "Epoch 14/499\n",
      "----------\n",
      "train Loss: 0.00142445 Acc: 0.93994141\n",
      "val Loss: 0.00046270 Acc: 0.96943231\n",
      "Epoch 15 of 500 took 0.135s\n",
      "Epoch 15/499\n",
      "----------\n",
      "train Loss: 0.00147037 Acc: 0.93652344\n",
      "val Loss: 0.00054403 Acc: 0.94323144\n",
      "Epoch 16 of 500 took 0.127s\n",
      "Epoch 16/499\n",
      "----------\n",
      "train Loss: 0.00129009 Acc: 0.93896484\n",
      "val Loss: 0.00060547 Acc: 0.95196507\n",
      "Epoch 17 of 500 took 0.128s\n",
      "Epoch 17/499\n",
      "----------\n",
      "train Loss: 0.00134884 Acc: 0.94091797\n",
      "val Loss: 0.00071363 Acc: 0.92576419\n",
      "Epoch 18 of 500 took 0.138s\n",
      "Epoch 18/499\n",
      "----------\n",
      "train Loss: 0.00114981 Acc: 0.95214844\n",
      "val Loss: 0.00045600 Acc: 0.9650655\n",
      "Epoch 19 of 500 took 0.126s\n",
      "Epoch 19/499\n",
      "----------\n",
      "train Loss: 0.00122691 Acc: 0.94287109\n",
      "val Loss: 0.00086080 Acc: 0.930131\n",
      "Epoch 20 of 500 took 0.134s\n",
      "Epoch 20/499\n",
      "----------\n",
      "train Loss: 0.00128926 Acc: 0.94384766\n",
      "val Loss: 0.00048070 Acc: 0.9650655\n",
      "Epoch 21 of 500 took 0.119s\n",
      "Epoch 21/499\n",
      "----------\n",
      "train Loss: 0.00121633 Acc: 0.94580078\n",
      "val Loss: 0.00047028 Acc: 0.9650655\n",
      "Epoch 22 of 500 took 0.108s\n",
      "\n",
      "Training complete in 0m 3s\n",
      "Best val loss: 0.000521\n"
     ]
    }
   ],
   "source": [
    "train_fine_tuning(examples_datasets_train, labels_datasets_train,\n",
    "                  num_kernels=num_kernels, path_weight_to_save_to=path_TSD,\n",
    "                  number_of_classes=number_of_classes, \n",
    "                  number_of_cycles_total=number_of_cycles_total,\n",
    "                  number_of_cycle_for_first_training = number_of_cycle_for_first_training,\n",
    "                  batch_size=batch_size,\n",
    "                  feature_vector_input_length=feature_vector_input_length,\n",
    "                  learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET one participant_examples  (2,)\n",
      "   GET one training_index_examples  (36, 572, 252)  at  0\n",
      "   GOT one group XY  (20592, 252)    (20592,)\n",
      "       one group XY test  (5148, 252)    (5148, 252)\n",
      "       one group XY train (18532, 252)    (18532,)\n",
      "       one group XY valid (2060, 252)    (2060, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  1\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "dataloaders: \n",
      "   train  (1, 2)\n",
      "   valid  (1, 2)\n",
      "   test  (1, 2)\n",
      "0  SESSION   data =  5148\n",
      "Participant:  0  Accuracy:  0.9054001554001554\n",
      "1  SESSION   data =  572\n",
      "Participant:  0  Accuracy:  0.8548951048951049\n",
      "ACCURACY PARTICIPANT  0 :  [0.9054001554001554, 0.8548951048951049]\n",
      "[array([0.90540016, 0.8548951 ])]\n",
      "OVERALL ACCURACY: 0.8801476301476301\n"
     ]
    }
   ],
   "source": [
    "algo_name = \"standard_TSD\"\n",
    "test_standard_model_on_training_sessions(examples_datasets_train, labels_datasets_train,\n",
    "                                  num_neurons=num_kernels, use_only_first_training=True,\n",
    "                                  path_weights=path_TSD,\n",
    "                                  feature_vector_input_length=feature_vector_input_length,\n",
    "                                  save_path = save_TSD, algo_name=algo_name,\n",
    "                                  number_of_cycles_total=number_of_cycles_total,\n",
    "                                  number_of_cycle_for_first_training = number_of_cycle_for_first_training,\n",
    "                                  number_of_classes=number_of_classes, cycle_for_test=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Day_0~8</th>\n",
       "      <td>0.9054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_9</th>\n",
       "      <td>0.854895</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Participant_5\n",
       "Day_0~8        0.9054\n",
       "Day_9        0.854895"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_filename = save_TSD + '/predictions_' + algo_name + \"_no_retraining.npy\"\n",
    "results = np.load(results_filename, allow_pickle=True)\n",
    "TSD_acc = results[0]\n",
    "TSD_acc_overall = np.mean(TSD_acc)\n",
    "index_participant_list = ['0~8', 9]\n",
    "TSD_df = pd.DataFrame(TSD_acc.transpose(), \n",
    "                       index = [f'Day_{i}' for i in index_participant_list],\n",
    "                        columns = ['Participant_5'])\n",
    "TSD_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "TSD_df.transpose().plot.bar(rot=0, figsize=(10,5))\n",
    "plt.title(\"TSD Accuracies\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TrainingsAndEvaluations.ForTrainingSessions.utils import get_gesture_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ground_truths  =  (1, 2)\n",
      "predictions =  (1, 2)\n",
      "index_participant_list  ['0~8', 9]\n",
      "accuracies_gestures =  (22, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Loc1_Sub5_Day0~8-&gt;0~8</th>\n",
       "      <th>Loc1_Sub5_Day0~8-&gt;9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M1</td>\n",
       "      <td>0.897436</td>\n",
       "      <td>0.961538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M2</td>\n",
       "      <td>0.935897</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M3</td>\n",
       "      <td>0.952991</td>\n",
       "      <td>0.538462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M4</td>\n",
       "      <td>0.867521</td>\n",
       "      <td>0.192308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>M5</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.961538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>M6</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>M7</td>\n",
       "      <td>0.982906</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>M8</td>\n",
       "      <td>0.918803</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>M9</td>\n",
       "      <td>0.867521</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>M10</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>M11</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.769231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>M12</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.384615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>M13</td>\n",
       "      <td>0.880342</td>\n",
       "      <td>0.961538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>M14</td>\n",
       "      <td>0.794872</td>\n",
       "      <td>0.576923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>M15</td>\n",
       "      <td>0.786325</td>\n",
       "      <td>0.846154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>M16</td>\n",
       "      <td>0.970085</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>M17</td>\n",
       "      <td>0.978632</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>M18</td>\n",
       "      <td>0.970085</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>M19</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.961538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>M20</td>\n",
       "      <td>0.880342</td>\n",
       "      <td>0.884615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>M21</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Mean</td>\n",
       "      <td>0.905400</td>\n",
       "      <td>0.854895</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Loc1_Sub5_Day0~8->0~8  Loc1_Sub5_Day0~8->9\n",
       "0          M0               1.000000             1.000000\n",
       "1          M1               0.897436             0.961538\n",
       "2          M2               0.935897             0.923077\n",
       "3          M3               0.952991             0.538462\n",
       "4          M4               0.867521             0.192308\n",
       "5          M5               0.961538             0.961538\n",
       "6          M6               0.961538             1.000000\n",
       "7          M7               0.982906             1.000000\n",
       "8          M8               0.918803             0.923077\n",
       "9          M9               0.867521             0.923077\n",
       "10        M10               0.884615             1.000000\n",
       "11        M11               0.769231             0.769231\n",
       "12        M12               0.769231             0.384615\n",
       "13        M13               0.880342             0.961538\n",
       "14        M14               0.794872             0.576923\n",
       "15        M15               0.786325             0.846154\n",
       "16        M16               0.970085             1.000000\n",
       "17        M17               0.978632             1.000000\n",
       "18        M18               0.970085             1.000000\n",
       "19        M19               1.000000             0.961538\n",
       "20        M20               0.880342             0.884615\n",
       "21        M21               0.888889             1.000000\n",
       "22       Mean               0.905400             0.854895"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truths = results[1]\n",
    "predictions = results[2]\n",
    "print(\"ground_truths  = \", np.shape(ground_truths))\n",
    "print(\"predictions = \", np.shape(predictions))\n",
    "m_name = \"Loc1_Sub\"\n",
    "n_name = \"Day0~8->\"\n",
    "df = get_gesture_accuracies(ground_truths, predictions, number_of_classes=number_of_classes, \n",
    "                            m_name=m_name, n_name=n_name, path=save_TSD, algo_name=algo_name,\n",
    "                           index_participant_list_customized=index_participant_list,\n",
    "                           lump_day_at_participant=5)\n",
    "df = pd.read_csv(save_TSD+'/'+algo_name+'.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. DANN\n",
    "* `train_DANN`: train DANN model using the first set of training weights from base model\n",
    "    * num_sessions-1 sets of training weights will be saved\n",
    "* `test_DANN_on_training_sessions`: test DANN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TrainingsAndEvaluations.ForTrainingSessions.train_tsd_dnn_DA import train_DANN, test_DANN_on_training_sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET one participant_examples  (2,)\n",
      "   GET one training_index_examples  (36, 572, 252)  at  0\n",
      "   GOT one group XY  (20592, 252)    (20592,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (18532, 252)    (18532,)\n",
      "       one group XY valid (2060, 252)    (2060, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  1\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "dataloaders: \n",
      "   train  (1, 2)\n",
      "   valid  (1, 2)\n",
      "   test  (1, 0)\n",
      "SHAPE SESSIONS:  (2,)\n",
      "()\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump9/TSD/participant_0/best_state_0.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump9/TSD/participant_0/best_state_0.pt' (epoch 10)\n",
      "STARTING TRAINING\n",
      "Accuracy source 0.856934, main loss classifier 0.270607, source classification loss 0.387267, loss domain distinction 0.259161, accuracy domain distinction 0.491211\n",
      "VALIDATION Loss: 0.28190431 Acc: 0.89466019\n",
      "New best validation loss:  0.28190430998802185\n",
      "Epoch 1 of 500 took 0.297s\n",
      "Accuracy source 0.853027, main loss classifier 0.281853, source classification loss 0.417991, loss domain distinction 0.192376, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.24187019 Acc: 0.90679612\n",
      "New best validation loss:  0.2418701946735382\n",
      "Epoch 2 of 500 took 0.244s\n",
      "Accuracy source 0.874023, main loss classifier 0.251250, source classification loss 0.359417, loss domain distinction 0.188891, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.22367443 Acc: 0.91893204\n",
      "New best validation loss:  0.22367443144321442\n",
      "Epoch 3 of 500 took 0.244s\n",
      "Accuracy source 0.845703, main loss classifier 0.275733, source classification loss 0.409419, loss domain distinction 0.187885, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.27636197 Acc: 0.90582524\n",
      "Epoch 4 of 500 took 0.243s\n",
      "Accuracy source 0.858887, main loss classifier 0.270392, source classification loss 0.398885, loss domain distinction 0.188041, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.27448213 Acc: 0.90145631\n",
      "Epoch 5 of 500 took 0.245s\n",
      "Accuracy source 0.854004, main loss classifier 0.282833, source classification loss 0.424180, loss domain distinction 0.188460, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.24260861 Acc: 0.91116505\n",
      "Epoch    16: reducing learning rate of group 0 to 5.0300e-04.\n",
      "Epoch 6 of 500 took 0.243s\n",
      "Accuracy source 0.871582, main loss classifier 0.251289, source classification loss 0.365193, loss domain distinction 0.186802, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.25927213 Acc: 0.90631068\n",
      "Epoch 7 of 500 took 0.244s\n",
      "Accuracy source 0.865723, main loss classifier 0.256214, source classification loss 0.375535, loss domain distinction 0.184781, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.22773044 Acc: 0.91456311\n",
      "Epoch 8 of 500 took 0.241s\n",
      "Accuracy source 0.862793, main loss classifier 0.257327, source classification loss 0.378733, loss domain distinction 0.183372, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.21428542 Acc: 0.92330097\n",
      "New best validation loss:  0.21428541839122772\n",
      "Epoch 9 of 500 took 0.261s\n",
      "Accuracy source 0.878906, main loss classifier 0.237805, source classification loss 0.338402, loss domain distinction 0.184663, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.21609874 Acc: 0.92669903\n",
      "Epoch 10 of 500 took 0.244s\n",
      "Accuracy source 0.880859, main loss classifier 0.238436, source classification loss 0.340266, loss domain distinction 0.183572, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.20267323 Acc: 0.92912621\n",
      "New best validation loss:  0.20267322659492493\n",
      "Epoch 11 of 500 took 0.243s\n",
      "Accuracy source 0.881348, main loss classifier 0.236837, source classification loss 0.337657, loss domain distinction 0.184275, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.23364732 Acc: 0.91407767\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0060e-04.\n",
      "Epoch 12 of 500 took 0.242s\n",
      "Accuracy source 0.879883, main loss classifier 0.238775, source classification loss 0.342520, loss domain distinction 0.183893, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.21281770 Acc: 0.92912621\n",
      "Epoch 13 of 500 took 0.294s\n",
      "Accuracy source 0.878418, main loss classifier 0.238842, source classification loss 0.342006, loss domain distinction 0.183007, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.19834411 Acc: 0.93349515\n",
      "New best validation loss:  0.19834411144256592\n",
      "Epoch 14 of 500 took 0.242s\n",
      "Accuracy source 0.872070, main loss classifier 0.231109, source classification loss 0.326988, loss domain distinction 0.182674, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.21261370 Acc: 0.92718447\n",
      "Epoch 15 of 500 took 0.243s\n",
      "Accuracy source 0.864746, main loss classifier 0.247800, source classification loss 0.359934, loss domain distinction 0.183449, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.20710190 Acc: 0.9276699\n",
      "Epoch 16 of 500 took 0.248s\n",
      "Accuracy source 0.887207, main loss classifier 0.231745, source classification loss 0.327901, loss domain distinction 0.183090, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.19645037 Acc: 0.9315534\n",
      "New best validation loss:  0.1964503675699234\n",
      "Epoch 17 of 500 took 0.243s\n",
      "Accuracy source 0.870605, main loss classifier 0.240009, source classification loss 0.344483, loss domain distinction 0.182877, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.19515868 Acc: 0.93252427\n",
      "New best validation loss:  0.1951586753129959\n",
      "Epoch 18 of 500 took 0.242s\n",
      "Accuracy source 0.883789, main loss classifier 0.234447, source classification loss 0.333681, loss domain distinction 0.183605, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.24488118 Acc: 0.91019417\n",
      "Epoch 19 of 500 took 0.241s\n",
      "Accuracy source 0.893066, main loss classifier 0.216297, source classification loss 0.296594, loss domain distinction 0.184304, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.19802320 Acc: 0.93398058\n",
      "Epoch 20 of 500 took 0.252s\n",
      "Accuracy source 0.874512, main loss classifier 0.241179, source classification loss 0.347665, loss domain distinction 0.181716, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.21126783 Acc: 0.92281553\n",
      "Epoch 21 of 500 took 0.241s\n",
      "Accuracy source 0.885254, main loss classifier 0.237393, source classification loss 0.339011, loss domain distinction 0.184403, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.21220040 Acc: 0.92330097\n",
      "Epoch 22 of 500 took 0.239s\n",
      "Accuracy source 0.889160, main loss classifier 0.221828, source classification loss 0.308085, loss domain distinction 0.184358, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.21423824 Acc: 0.92378641\n",
      "Epoch 23 of 500 took 0.240s\n",
      "Accuracy source 0.889648, main loss classifier 0.228589, source classification loss 0.321769, loss domain distinction 0.182413, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.22020537 Acc: 0.92135922\n",
      "Epoch 24 of 500 took 0.246s\n",
      "Accuracy source 0.879395, main loss classifier 0.235653, source classification loss 0.335816, loss domain distinction 0.184272, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.20978633 Acc: 0.9223301\n",
      "Epoch 25 of 500 took 0.293s\n",
      "Accuracy source 0.895020, main loss classifier 0.224445, source classification loss 0.313599, loss domain distinction 0.182903, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.19998311 Acc: 0.93203883\n",
      "Epoch 26 of 500 took 0.242s\n",
      "Accuracy source 0.873535, main loss classifier 0.251730, source classification loss 0.367750, loss domain distinction 0.183192, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.22569305 Acc: 0.91699029\n",
      "Epoch 27 of 500 took 0.240s\n",
      "Accuracy source 0.899902, main loss classifier 0.218364, source classification loss 0.301165, loss domain distinction 0.184521, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.23100828 Acc: 0.91796117\n",
      "Epoch 28 of 500 took 0.242s\n",
      "Accuracy source 0.869629, main loss classifier 0.250617, source classification loss 0.365205, loss domain distinction 0.184155, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.19284841 Acc: 0.9368932\n",
      "New best validation loss:  0.19284841418266296\n",
      "Epoch 29 of 500 took 0.243s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy source 0.890625, main loss classifier 0.221667, source classification loss 0.308507, loss domain distinction 0.184284, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.19585155 Acc: 0.93349515\n",
      "Epoch 30 of 500 took 0.243s\n",
      "Accuracy source 0.887695, main loss classifier 0.230619, source classification loss 0.325897, loss domain distinction 0.183122, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.19707111 Acc: 0.93398058\n",
      "Epoch 31 of 500 took 0.239s\n",
      "Accuracy source 0.895020, main loss classifier 0.219503, source classification loss 0.303306, loss domain distinction 0.184149, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.20394354 Acc: 0.92912621\n",
      "Epoch 32 of 500 took 0.245s\n",
      "Accuracy source 0.873535, main loss classifier 0.240908, source classification loss 0.345963, loss domain distinction 0.183895, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.20858526 Acc: 0.92669903\n",
      "Epoch 33 of 500 took 0.240s\n",
      "Accuracy source 0.888184, main loss classifier 0.225183, source classification loss 0.314798, loss domain distinction 0.182858, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.19051619 Acc: 0.93495146\n",
      "New best validation loss:  0.19051618874073029\n",
      "Epoch 34 of 500 took 0.243s\n",
      "Accuracy source 0.885742, main loss classifier 0.223861, source classification loss 0.312036, loss domain distinction 0.183266, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.19197288 Acc: 0.93495146\n",
      "Epoch 35 of 500 took 0.239s\n",
      "Accuracy source 0.883789, main loss classifier 0.230137, source classification loss 0.325259, loss domain distinction 0.183891, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.22188492 Acc: 0.91796117\n",
      "Epoch 36 of 500 took 0.252s\n",
      "Accuracy source 0.888672, main loss classifier 0.222380, source classification loss 0.309454, loss domain distinction 0.182821, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.21972656 Acc: 0.92038835\n",
      "Epoch 37 of 500 took 0.291s\n",
      "Accuracy source 0.883789, main loss classifier 0.229669, source classification loss 0.324747, loss domain distinction 0.182264, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.19210956 Acc: 0.93592233\n",
      "Epoch 38 of 500 took 0.241s\n",
      "Accuracy source 0.899414, main loss classifier 0.216312, source classification loss 0.297224, loss domain distinction 0.183303, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.20226860 Acc: 0.93106796\n",
      "Epoch 39 of 500 took 0.241s\n",
      "Accuracy source 0.889160, main loss classifier 0.216985, source classification loss 0.298825, loss domain distinction 0.183542, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.20315240 Acc: 0.92524272\n",
      "Epoch 40 of 500 took 0.269s\n",
      "Accuracy source 0.890625, main loss classifier 0.223788, source classification loss 0.312008, loss domain distinction 0.183822, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.19210294 Acc: 0.9315534\n",
      "Epoch 41 of 500 took 0.240s\n",
      "Accuracy source 0.887207, main loss classifier 0.223966, source classification loss 0.312833, loss domain distinction 0.183382, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.18976812 Acc: 0.93300971\n",
      "New best validation loss:  0.18976812064647675\n",
      "Epoch 42 of 500 took 0.243s\n",
      "Accuracy source 0.881348, main loss classifier 0.229344, source classification loss 0.323502, loss domain distinction 0.182378, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.21502145 Acc: 0.92718447\n",
      "Epoch 43 of 500 took 0.259s\n",
      "Accuracy source 0.889648, main loss classifier 0.213430, source classification loss 0.292101, loss domain distinction 0.182411, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.21268113 Acc: 0.92621359\n",
      "Epoch 44 of 500 took 0.246s\n",
      "Accuracy source 0.884766, main loss classifier 0.234703, source classification loss 0.334643, loss domain distinction 0.181337, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.19617324 Acc: 0.93398058\n",
      "Epoch 45 of 500 took 0.241s\n",
      "Accuracy source 0.878906, main loss classifier 0.227572, source classification loss 0.319764, loss domain distinction 0.183197, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.19906856 Acc: 0.92912621\n",
      "Epoch 46 of 500 took 0.242s\n",
      "Accuracy source 0.887695, main loss classifier 0.232838, source classification loss 0.330570, loss domain distinction 0.182754, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.18901639 Acc: 0.93495146\n",
      "New best validation loss:  0.18901638686656952\n",
      "Epoch 47 of 500 took 0.242s\n",
      "Accuracy source 0.892090, main loss classifier 0.225313, source classification loss 0.315356, loss domain distinction 0.182764, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.19915432 Acc: 0.93203883\n",
      "Epoch 48 of 500 took 0.261s\n",
      "Accuracy source 0.883789, main loss classifier 0.223478, source classification loss 0.312050, loss domain distinction 0.184467, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.20578392 Acc: 0.92427184\n",
      "Epoch 49 of 500 took 0.242s\n",
      "Accuracy source 0.885254, main loss classifier 0.230427, source classification loss 0.325511, loss domain distinction 0.183332, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.20462462 Acc: 0.92961165\n",
      "Epoch 50 of 500 took 0.289s\n",
      "Accuracy source 0.889160, main loss classifier 0.222858, source classification loss 0.310678, loss domain distinction 0.182242, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.22418329 Acc: 0.9184466\n",
      "Epoch 51 of 500 took 0.241s\n",
      "Accuracy source 0.894043, main loss classifier 0.219770, source classification loss 0.304280, loss domain distinction 0.183218, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.21824351 Acc: 0.92087379\n",
      "Epoch 52 of 500 took 0.244s\n",
      "Accuracy source 0.889648, main loss classifier 0.220513, source classification loss 0.306372, loss domain distinction 0.182356, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.20412295 Acc: 0.93106796\n",
      "Epoch 53 of 500 took 0.240s\n",
      "Accuracy source 0.890137, main loss classifier 0.222377, source classification loss 0.309730, loss domain distinction 0.183165, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.19730482 Acc: 0.93252427\n",
      "Epoch 54 of 500 took 0.242s\n",
      "Accuracy source 0.879395, main loss classifier 0.225793, source classification loss 0.316321, loss domain distinction 0.183359, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.20300996 Acc: 0.92621359\n",
      "Epoch 55 of 500 took 0.239s\n",
      "Accuracy source 0.899414, main loss classifier 0.213573, source classification loss 0.292155, loss domain distinction 0.182734, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.19815655 Acc: 0.92961165\n",
      "Epoch 56 of 500 took 0.248s\n",
      "Accuracy source 0.895020, main loss classifier 0.212296, source classification loss 0.289299, loss domain distinction 0.183318, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.19704959 Acc: 0.93398058\n",
      "Epoch 57 of 500 took 0.241s\n",
      "Accuracy source 0.887695, main loss classifier 0.217619, source classification loss 0.300029, loss domain distinction 0.184122, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.21563730 Acc: 0.92184466\n",
      "Training complete in 0m 14s\n"
     ]
    }
   ],
   "source": [
    "train_DANN(examples_datasets_train, labels_datasets_train, \n",
    "          num_kernels=num_kernels,\n",
    "          path_weights_fine_tuning=path_TSD,\n",
    "          number_of_classes=number_of_classes,\n",
    "          number_of_cycles_total = number_of_cycles_total,\n",
    "          number_of_cycle_for_first_training = number_of_cycle_for_first_training,\n",
    "          batch_size=batch_size,\n",
    "          feature_vector_input_length=feature_vector_input_length,\n",
    "          path_weights_to_save_to=path_DANN, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET one participant_examples  (2,)\n",
      "   GET one training_index_examples  (36, 572, 252)  at  0\n",
      "   GOT one group XY  (20592, 252)    (20592,)\n",
      "       one group XY test  (5148, 252)    (5148, 252)\n",
      "       one group XY train (18532, 252)    (18532,)\n",
      "       one group XY valid (2060, 252)    (2060, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  1\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "dataloaders: \n",
      "   train  (1, 2)\n",
      "   valid  (1, 2)\n",
      "   test  (1, 2)\n",
      "(2,)\n",
      "Participant ID:  0  Session ID:  0  Accuracy:  0.9054001554001554\n",
      "Participant ID:  0  Session ID:  1  Accuracy:  0.8513986013986014\n",
      "ACCURACY PARTICIPANT:  [0.9054001554001554, 0.8513986013986014]\n",
      "[[0.90540016 0.8513986 ]]\n",
      "[array([0.90540016, 0.8513986 ])]\n",
      "OVERALL ACCURACY: 0.8783993783993784\n"
     ]
    }
   ],
   "source": [
    "algo_name = \"DANN\"\n",
    "test_DANN_on_training_sessions(examples_datasets_train, labels_datasets_train,\n",
    "                              feature_vector_input_length=feature_vector_input_length,\n",
    "                              num_neurons=num_kernels, path_weights_DA=path_DANN,\n",
    "                              algo_name=algo_name, save_path = save_DANN, \n",
    "                              number_of_cycles_total=number_of_cycles_total,\n",
    "                              number_of_cycle_for_first_training = number_of_cycle_for_first_training,\n",
    "                              path_weights_normal=path_TSD, number_of_classes=number_of_classes,\n",
    "                              cycle_for_test=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Day_0~8</th>\n",
       "      <td>0.9054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_9</th>\n",
       "      <td>0.851399</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Participant_5\n",
       "Day_0~8        0.9054\n",
       "Day_9        0.851399"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_filename = save_DANN + '/predictions_' + algo_name + \".npy\"\n",
    "results = np.load(results_filename, allow_pickle=True)\n",
    "DANN_acc = results[0]\n",
    "DANN_acc_overall = np.mean(DANN_acc)\n",
    "DANN_df = pd.DataFrame(DANN_acc.transpose(), \n",
    "                       index = [f'Day_{i}' for i in index_participant_list],\n",
    "                        columns = ['Participant_5'])\n",
    "DANN_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "DANN_df.transpose().plot.bar(rot=0, figsize=(10,5))\n",
    "plt.title(\"DANN Accuracies\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ground_truths  =  (1, 2)\n",
      "predictions =  (1, 2)\n",
      "index_participant_list  ['0~8', 9]\n",
      "accuracies_gestures =  (22, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Loc1_Sub5_Day0~8-&gt;0~8</th>\n",
       "      <th>Loc1_Sub5_Day0~8-&gt;9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M1</td>\n",
       "      <td>0.897436</td>\n",
       "      <td>0.884615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M2</td>\n",
       "      <td>0.935897</td>\n",
       "      <td>0.961538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M3</td>\n",
       "      <td>0.952991</td>\n",
       "      <td>0.461538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M4</td>\n",
       "      <td>0.867521</td>\n",
       "      <td>0.576923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>M5</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.961538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>M6</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>M7</td>\n",
       "      <td>0.982906</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>M8</td>\n",
       "      <td>0.918803</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>M9</td>\n",
       "      <td>0.867521</td>\n",
       "      <td>0.807692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>M10</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.961538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>M11</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.846154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>M12</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.384615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>M13</td>\n",
       "      <td>0.880342</td>\n",
       "      <td>0.846154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>M14</td>\n",
       "      <td>0.794872</td>\n",
       "      <td>0.615385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>M15</td>\n",
       "      <td>0.786325</td>\n",
       "      <td>0.846154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>M16</td>\n",
       "      <td>0.970085</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>M17</td>\n",
       "      <td>0.978632</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>M18</td>\n",
       "      <td>0.970085</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>M19</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>M20</td>\n",
       "      <td>0.880342</td>\n",
       "      <td>0.653846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>M21</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Mean</td>\n",
       "      <td>0.905400</td>\n",
       "      <td>0.851399</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Loc1_Sub5_Day0~8->0~8  Loc1_Sub5_Day0~8->9\n",
       "0          M0               1.000000             1.000000\n",
       "1          M1               0.897436             0.884615\n",
       "2          M2               0.935897             0.961538\n",
       "3          M3               0.952991             0.461538\n",
       "4          M4               0.867521             0.576923\n",
       "5          M5               0.961538             0.961538\n",
       "6          M6               0.961538             1.000000\n",
       "7          M7               0.982906             1.000000\n",
       "8          M8               0.918803             0.923077\n",
       "9          M9               0.867521             0.807692\n",
       "10        M10               0.884615             0.961538\n",
       "11        M11               0.769231             0.846154\n",
       "12        M12               0.769231             0.384615\n",
       "13        M13               0.880342             0.846154\n",
       "14        M14               0.794872             0.615385\n",
       "15        M15               0.786325             0.846154\n",
       "16        M16               0.970085             1.000000\n",
       "17        M17               0.978632             1.000000\n",
       "18        M18               0.970085             1.000000\n",
       "19        M19               1.000000             1.000000\n",
       "20        M20               0.880342             0.653846\n",
       "21        M21               0.888889             1.000000\n",
       "22       Mean               0.905400             0.851399"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truths = results[1]\n",
    "predictions = results[2]\n",
    "print(\"ground_truths  = \", np.shape(ground_truths))\n",
    "print(\"predictions = \", np.shape(predictions))\n",
    "df = get_gesture_accuracies(ground_truths, predictions, number_of_classes=number_of_classes, \n",
    "                            m_name=m_name, n_name=n_name, path=save_TSD, algo_name=algo_name,\n",
    "                           index_participant_list_customized=index_participant_list,\n",
    "                           lump_day_at_participant=5)\n",
    "df = pd.read_csv(save_DANN+'/'+algo_name+'.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. SCADANN\n",
    "\n",
    "* `run_SCADANN_training_sessions`: train SCADANN model. The first session uses TSD model_0 wegits; others use DANN weights\n",
    "    * specify `percentage_same_gesture_stable` based on the performance of most pseudo labels: \n",
    "        * print accuracies out and check what percentage will optimize `ACCURACY MODEL` and `ACCURACY PSEUDO` without cutting out too much data \n",
    "    * num_sessions-1 sets of training weights will be saved\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TrainingsAndEvaluations.ForTrainingSessions.train_tsd_dnn_SCADANN import \\\n",
    "    run_SCADANN_training_sessions, test_network_SCADANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET one participant_examples  (2,)\n",
      "   GET one training_index_examples  (36, 572, 252)  at  0\n",
      "   GOT one group XY  (20592, 252)    (20592,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (20592, 252)    (20592,)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  1\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2288, 252)    (2288,)\n",
      "dataloaders: \n",
      "   train  (1, 2)\n",
      "   valid  (0,)\n",
      "   test  (1, 0)\n",
      "participants_train =  1\n",
      "Optimizer =  <generator object Module.parameters at 0x7efd22158b30>\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump9/DANN/participant_0/best_state_1.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump9/DANN/participant_0/best_state_1.pt' (epoch 47)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump9/TSD/participant_0/best_state_0.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump9/TSD/participant_0/best_state_0.pt' (epoch 10)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump9/DANN/participant_0/best_state_1.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump9/DANN/participant_0/best_state_1.pt' (epoch 47)\n",
      "==== models_array =  (2,)  @ session  1\n",
      "HANDLING NEW SESSION  0\n",
      "HANDLING NEW SESSION  1\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.5769230769230769  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  2\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.8461538461538461  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.8846153846153846  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  15\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  3\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  13\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.2777777777777778  len before:  26   len after:  18\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.46153846153846156  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.868006993006993   Accuracy pseudo: 0.9763593380614657  len pseudo:  2115    len predictions 2288\n",
      "STARTING TRAINING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laiy/.local/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/laiy/.local/lib/python3.8/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy total 0.863281, main loss classifier 0.488760, source accuracy 0.881611 source classification loss 0.330979, target accuracy 0.844952 target loss 0.418861 accuracy domain distinction 0.500000 loss domain distinction 1.138399,\n",
      "VALIDATION Loss: 0.27781433 Acc: 0.91962175\n",
      "New best validation loss:  0.2778143286705017\n",
      "Epoch 2 of 500 took 0.399s\n",
      "Accuracy total 0.870192, main loss classifier 0.477677, source accuracy 0.880409 source classification loss 0.338472, target accuracy 0.859976 target loss 0.404925 accuracy domain distinction 0.500000 loss domain distinction 1.059784,\n",
      "VALIDATION Loss: 0.25161982 Acc: 0.89834515\n",
      "New best validation loss:  0.2516198179551533\n",
      "Epoch 3 of 500 took 0.402s\n",
      "Accuracy total 0.871995, main loss classifier 0.462476, source accuracy 0.871995 source classification loss 0.335463, target accuracy 0.871995 target loss 0.381775 accuracy domain distinction 0.500000 loss domain distinction 1.038567,\n",
      "VALIDATION Loss: 0.24462104 Acc: 0.90780142\n",
      "New best validation loss:  0.2446210395012583\n",
      "Epoch 4 of 500 took 0.411s\n",
      "Accuracy total 0.867488, main loss classifier 0.465162, source accuracy 0.871394 source classification loss 0.346659, target accuracy 0.863582 target loss 0.375789 accuracy domain distinction 0.500000 loss domain distinction 1.039381,\n",
      "VALIDATION Loss: 0.25439362 Acc: 0.90780142\n",
      "Epoch 5 of 500 took 0.404s\n",
      "Accuracy total 0.871995, main loss classifier 0.458562, source accuracy 0.874399 source classification loss 0.355839, target accuracy 0.869591 target loss 0.354728 accuracy domain distinction 0.500000 loss domain distinction 1.032783,\n",
      "VALIDATION Loss: 0.23540162 Acc: 0.90070922\n",
      "New best validation loss:  0.23540161656481878\n",
      "Epoch 6 of 500 took 0.346s\n",
      "Accuracy total 0.871394, main loss classifier 0.466629, source accuracy 0.873197 source classification loss 0.373668, target accuracy 0.869591 target loss 0.352350 accuracy domain distinction 0.500000 loss domain distinction 1.036202,\n",
      "VALIDATION Loss: 0.28169402 Acc: 0.89834515\n",
      "Epoch 7 of 500 took 0.342s\n",
      "Accuracy total 0.877103, main loss classifier 0.455646, source accuracy 0.867188 source classification loss 0.366113, target accuracy 0.887019 target loss 0.338896 accuracy domain distinction 0.500000 loss domain distinction 1.031415,\n",
      "VALIDATION Loss: 0.24612554 Acc: 0.91725768\n",
      "Epoch 8 of 500 took 0.341s\n",
      "Accuracy total 0.885216, main loss classifier 0.437858, source accuracy 0.885817 source classification loss 0.332388, target accuracy 0.884615 target loss 0.336673 accuracy domain distinction 0.500000 loss domain distinction 1.033277,\n",
      "VALIDATION Loss: 0.21177472 Acc: 0.92434988\n",
      "New best validation loss:  0.21177471535546438\n",
      "Epoch 9 of 500 took 0.345s\n",
      "Accuracy total 0.887921, main loss classifier 0.430505, source accuracy 0.885817 source classification loss 0.316331, target accuracy 0.890024 target loss 0.337977 accuracy domain distinction 0.500000 loss domain distinction 1.033516,\n",
      "VALIDATION Loss: 0.21753859 Acc: 0.91962175\n",
      "Epoch 10 of 500 took 0.342s\n",
      "Accuracy total 0.876502, main loss classifier 0.447536, source accuracy 0.870192 source classification loss 0.353885, target accuracy 0.882812 target loss 0.333992 accuracy domain distinction 0.500000 loss domain distinction 1.035977,\n",
      "VALIDATION Loss: 0.20266227 Acc: 0.94089835\n",
      "New best validation loss:  0.20266226891960418\n",
      "Epoch 11 of 500 took 0.343s\n",
      "Accuracy total 0.886719, main loss classifier 0.431005, source accuracy 0.883413 source classification loss 0.332447, target accuracy 0.890024 target loss 0.323294 accuracy domain distinction 0.500000 loss domain distinction 1.031349,\n",
      "VALIDATION Loss: 0.23723715 Acc: 0.91016548\n",
      "Epoch 12 of 500 took 0.348s\n",
      "Accuracy total 0.881911, main loss classifier 0.441919, source accuracy 0.867788 source classification loss 0.349090, target accuracy 0.896034 target loss 0.328319 accuracy domain distinction 0.500000 loss domain distinction 1.032148,\n",
      "VALIDATION Loss: 0.21432423 Acc: 0.94326241\n",
      "Epoch 13 of 500 took 0.341s\n",
      "Accuracy total 0.879808, main loss classifier 0.443152, source accuracy 0.872596 source classification loss 0.361286, target accuracy 0.887019 target loss 0.318225 accuracy domain distinction 0.500000 loss domain distinction 1.033960,\n",
      "VALIDATION Loss: 0.20990749 Acc: 0.92671395\n",
      "Epoch 14 of 500 took 0.342s\n",
      "Accuracy total 0.891226, main loss classifier 0.418871, source accuracy 0.894832 source classification loss 0.306376, target accuracy 0.887620 target loss 0.324423 accuracy domain distinction 0.500000 loss domain distinction 1.034722,\n",
      "VALIDATION Loss: 0.19406845 Acc: 0.94089835\n",
      "New best validation loss:  0.19406844675540924\n",
      "Epoch 15 of 500 took 0.347s\n",
      "Accuracy total 0.882512, main loss classifier 0.434311, source accuracy 0.865986 source classification loss 0.353348, target accuracy 0.899038 target loss 0.308590 accuracy domain distinction 0.500000 loss domain distinction 1.033421,\n",
      "VALIDATION Loss: 0.21684249 Acc: 0.93380615\n",
      "Epoch 16 of 500 took 0.346s\n",
      "Accuracy total 0.881611, main loss classifier 0.448059, source accuracy 0.872596 source classification loss 0.372617, target accuracy 0.890625 target loss 0.316553 accuracy domain distinction 0.500000 loss domain distinction 1.034739,\n",
      "VALIDATION Loss: 0.21344711 Acc: 0.93617021\n",
      "Epoch 17 of 500 took 0.340s\n",
      "Accuracy total 0.884315, main loss classifier 0.430959, source accuracy 0.873197 source classification loss 0.343802, target accuracy 0.895433 target loss 0.312277 accuracy domain distinction 0.500000 loss domain distinction 1.029199,\n",
      "VALIDATION Loss: 0.22698891 Acc: 0.91489362\n",
      "Epoch 18 of 500 took 0.351s\n",
      "Accuracy total 0.878906, main loss classifier 0.432846, source accuracy 0.866587 source classification loss 0.345402, target accuracy 0.891226 target loss 0.313923 accuracy domain distinction 0.500000 loss domain distinction 1.031839,\n",
      "VALIDATION Loss: 0.22953827 Acc: 0.91962175\n",
      "Epoch 19 of 500 took 0.341s\n",
      "Accuracy total 0.884014, main loss classifier 0.429491, source accuracy 0.875601 source classification loss 0.348336, target accuracy 0.892428 target loss 0.303768 accuracy domain distinction 0.500000 loss domain distinction 1.034394,\n",
      "VALIDATION Loss: 0.20934869 Acc: 0.93144208\n",
      "Epoch 20 of 500 took 0.341s\n",
      "Accuracy total 0.897536, main loss classifier 0.399909, source accuracy 0.893630 source classification loss 0.302361, target accuracy 0.901442 target loss 0.291667 accuracy domain distinction 0.500000 loss domain distinction 1.028945,\n",
      "VALIDATION Loss: 0.21027406 Acc: 0.92198582\n",
      "Epoch    20: reducing learning rate of group 0 to 2.0120e-05.\n",
      "Epoch 21 of 500 took 0.346s\n",
      "Accuracy total 0.890325, main loss classifier 0.421245, source accuracy 0.876202 source classification loss 0.343761, target accuracy 0.904447 target loss 0.292392 accuracy domain distinction 0.500000 loss domain distinction 1.031681,\n",
      "VALIDATION Loss: 0.19563464 Acc: 0.93617021\n",
      "Epoch 22 of 500 took 0.340s\n",
      "Accuracy total 0.884315, main loss classifier 0.430891, source accuracy 0.877404 source classification loss 0.349209, target accuracy 0.891226 target loss 0.306567 accuracy domain distinction 0.500000 loss domain distinction 1.030034,\n",
      "VALIDATION Loss: 0.25658778 Acc: 0.90543735\n",
      "Epoch 23 of 500 took 0.341s\n",
      "Accuracy total 0.887019, main loss classifier 0.425872, source accuracy 0.875000 source classification loss 0.343704, target accuracy 0.899038 target loss 0.301871 accuracy domain distinction 0.500000 loss domain distinction 1.030847,\n",
      "VALIDATION Loss: 0.19476758 Acc: 0.93853428\n",
      "Epoch 24 of 500 took 0.345s\n",
      "Accuracy total 0.888822, main loss classifier 0.412554, source accuracy 0.882812 source classification loss 0.333046, target accuracy 0.894832 target loss 0.286424 accuracy domain distinction 0.500000 loss domain distinction 1.028188,\n",
      "VALIDATION Loss: 0.24957695 Acc: 0.91962175\n",
      "Epoch 25 of 500 took 0.339s\n",
      "Accuracy total 0.882812, main loss classifier 0.437297, source accuracy 0.876202 source classification loss 0.353203, target accuracy 0.889423 target loss 0.316657 accuracy domain distinction 0.500000 loss domain distinction 1.023665,\n",
      "VALIDATION Loss: 0.21015825 Acc: 0.93144208\n",
      "Epoch 26 of 500 took 0.344s\n",
      "Training complete in 0m 9s\n",
      "['participant_0']\n"
     ]
    }
   ],
   "source": [
    "percentage_same_gesture_stable = 0.75 \n",
    "run_SCADANN_training_sessions(examples_datasets=examples_datasets_train, labels_datasets=labels_datasets_train,\n",
    "                              num_kernels=num_kernels, feature_vector_input_length=feature_vector_input_length,\n",
    "                              path_weights_to_save_to=path_SCADANN,\n",
    "                              path_weights_Adversarial_training=path_DANN,\n",
    "                              path_weights_Normal_training=path_TSD,\n",
    "                              number_of_cycles_total = number_of_cycles_total, \n",
    "                              number_of_cycle_for_first_training = number_of_cycle_for_first_training,\n",
    "                              number_of_classes=number_of_classes,\n",
    "                              learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET one participant_examples  (2,)\n",
      "   GET one training_index_examples  (36, 572, 252)  at  0\n",
      "   GOT one group XY  (20592, 252)    (20592,)\n",
      "       one group XY test  (5148, 252)    (5148, 252)\n",
      "       one group XY train (18532, 252)    (18532,)\n",
      "       one group XY valid (2060, 252)    (2060, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  1\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "dataloaders: \n",
      "   train  (1, 2)\n",
      "   valid  (1, 2)\n",
      "   test  (1, 2)\n",
      "Participant:  0  Accuracy:  0.9054001554001554\n",
      "Participant:  0  Accuracy:  0.8916083916083916\n",
      "ACCURACY PARTICIPANT:  [0.9054001554001554, 0.8916083916083916]\n",
      "[[0.90540016 0.89160839]]\n",
      "[array([0.90540016, 0.89160839])]\n",
      "OVERALL ACCURACY: 0.8985042735042734\n"
     ]
    }
   ],
   "source": [
    "algo_name = \"SCADANN\"\n",
    "test_network_SCADANN(examples_datasets_train=examples_datasets_train, labels_datasets_train=labels_datasets_train,\n",
    "                     num_neurons=num_kernels, feature_vector_input_length=feature_vector_input_length,\n",
    "                     path_weights_SCADANN =path_SCADANN, path_weights_normal=path_TSD,\n",
    "                     algo_name=algo_name, cycle_test=3, number_of_cycles_total=number_of_cycles_total,\n",
    "                     number_of_cycle_for_first_training = number_of_cycle_for_first_training,\n",
    "                     number_of_classes=number_of_classes, save_path = save_SCADANN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Day_0~8</th>\n",
       "      <td>0.9054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_9</th>\n",
       "      <td>0.891608</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Participant_5\n",
       "Day_0~8        0.9054\n",
       "Day_9        0.891608"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_filename = save_SCADANN + '/predictions_' + algo_name + \".npy\"\n",
    "results = np.load(results_filename, allow_pickle=True)\n",
    "SCADANN_acc = results[0]\n",
    "SCADANN_acc_overall = np.mean(SCADANN_acc)\n",
    "SCADANN_df = pd.DataFrame(SCADANN_acc.transpose(), \n",
    "                       index = [f'Day_{i}' for i in index_participant_list],\n",
    "                        columns = ['Participant_5'])\n",
    "SCADANN_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "SCADANN_df.transpose().plot.bar(rot=0, figsize=(10,5))\n",
    "plt.title(\"SCADANN Accuracies\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ground_truths  =  (1, 2)\n",
      "predictions =  (1, 2)\n",
      "index_participant_list  ['0~8', 9]\n",
      "accuracies_gestures =  (22, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Loc1_Sub5_Day0~8-&gt;0~8</th>\n",
       "      <th>Loc1_Sub5_Day0~8-&gt;9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M1</td>\n",
       "      <td>0.897436</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M2</td>\n",
       "      <td>0.935897</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M3</td>\n",
       "      <td>0.952991</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M4</td>\n",
       "      <td>0.867521</td>\n",
       "      <td>0.653846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>M5</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>M6</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>M7</td>\n",
       "      <td>0.982906</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>M8</td>\n",
       "      <td>0.918803</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>M9</td>\n",
       "      <td>0.867521</td>\n",
       "      <td>0.807692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>M10</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.961538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>M11</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>M12</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.423077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>M13</td>\n",
       "      <td>0.880342</td>\n",
       "      <td>0.884615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>M14</td>\n",
       "      <td>0.794872</td>\n",
       "      <td>0.769231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>M15</td>\n",
       "      <td>0.786325</td>\n",
       "      <td>0.846154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>M16</td>\n",
       "      <td>0.970085</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>M17</td>\n",
       "      <td>0.978632</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>M18</td>\n",
       "      <td>0.970085</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>M19</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>M20</td>\n",
       "      <td>0.880342</td>\n",
       "      <td>0.846154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>M21</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Mean</td>\n",
       "      <td>0.905400</td>\n",
       "      <td>0.891608</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Loc1_Sub5_Day0~8->0~8  Loc1_Sub5_Day0~8->9\n",
       "0          M0               1.000000             1.000000\n",
       "1          M1               0.897436             1.000000\n",
       "2          M2               0.935897             1.000000\n",
       "3          M3               0.952991             0.500000\n",
       "4          M4               0.867521             0.653846\n",
       "5          M5               0.961538             1.000000\n",
       "6          M6               0.961538             1.000000\n",
       "7          M7               0.982906             1.000000\n",
       "8          M8               0.918803             0.923077\n",
       "9          M9               0.867521             0.807692\n",
       "10        M10               0.884615             0.961538\n",
       "11        M11               0.769231             1.000000\n",
       "12        M12               0.769231             0.423077\n",
       "13        M13               0.880342             0.884615\n",
       "14        M14               0.794872             0.769231\n",
       "15        M15               0.786325             0.846154\n",
       "16        M16               0.970085             1.000000\n",
       "17        M17               0.978632             1.000000\n",
       "18        M18               0.970085             1.000000\n",
       "19        M19               1.000000             1.000000\n",
       "20        M20               0.880342             0.846154\n",
       "21        M21               0.888889             1.000000\n",
       "22       Mean               0.905400             0.891608"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truths = results[1]\n",
    "predictions = results[2]\n",
    "print(\"ground_truths  = \", np.shape(ground_truths))\n",
    "print(\"predictions = \", np.shape(predictions))\n",
    "df = get_gesture_accuracies(ground_truths, predictions, number_of_classes=number_of_classes, \n",
    "                            m_name=m_name, n_name=n_name, path=save_TSD, algo_name=algo_name,\n",
    "                           index_participant_list_customized=index_participant_list,\n",
    "                           lump_day_at_participant=5)\n",
    "df = pd.read_csv(save_SCADANN+'/'+algo_name+'.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "Suppose there is a ndarray of NxM dataloaders, then N group of models will be trained, and each group will have M model. Each group is independent of the other, and each model within a group is dependent on its previous training weights.\n",
    "\n",
    "In general, overall accuracies of SCADANN are better than DANN, and DANN is better than TSD.\n",
    "Occasionally accuracies of SCADANN end up a little smaller than DANN, reasons may be lack of datasets put into training model (fixed) and non-optimal percentage_same_gesture_sable (fixed). Code should be reproducible if processed dataset sticks to the shape defined above.  \n",
    "\n",
    "The amount of increase in accuracies from DANN to SCADANN looks random. But if the base model is better at classifying one session, then its corresponding SCADANN is also better at classifying the same session. Given such result, to obtain the best performance from SCADANN, a good model trained with good data should be the starting point.\n",
    "\n",
    "* What to check if sth goes wrong:\n",
    "    * percentage_same_gesture_sable\n",
    "    * number of cycles or sessions\n",
    "    * shape of dataloaders (combination of train, test, valid should include all dataset)\n",
    "    * shape of procssed datasets\n",
    "    * directory paths of weights and results\n",
    "    * if weights are stored or loaded correcltyTSD_acc_overall_one = np.mean(TSD_acc, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TSD\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Day_0~8</th>\n",
       "      <td>0.9054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_9</th>\n",
       "      <td>0.854895</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Participant_5\n",
       "Day_0~8        0.9054\n",
       "Day_9        0.854895"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DANN\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Day_0~8</th>\n",
       "      <td>0.9054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_9</th>\n",
       "      <td>0.851399</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Participant_5\n",
       "Day_0~8        0.9054\n",
       "Day_9        0.851399"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCADANN\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Day_0~8</th>\n",
       "      <td>0.9054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_9</th>\n",
       "      <td>0.891608</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Participant_5\n",
       "Day_0~8        0.9054\n",
       "Day_9        0.891608"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"TSD\")\n",
    "display(TSD_df)\n",
    "print(\"DANN\")\n",
    "display(DANN_df)\n",
    "print(\"SCADANN\")\n",
    "display(SCADANN_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Day_9</th>\n",
       "      <td>0.036713</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Participant_5\n",
       "Day_9      0.036713"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "diff_df = SCADANN_df-TSD_df\n",
    "diff_df = diff_df.drop('Day_'+index_participant_list[0])\n",
    "display(diff_df)\n",
    "diff_df.to_csv(save_TSD+'/diff_results/across_day_loc1_lump9_diff.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Overall_Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TSD</th>\n",
       "      <td>0.880148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DANN</th>\n",
       "      <td>0.878399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SCADANN</th>\n",
       "      <td>0.898504</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Overall_Acc\n",
       "TSD         0.880148\n",
       "DANN        0.878399\n",
       "SCADANN     0.898504"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_acc_df = pd.DataFrame([TSD_acc_overall, DANN_acc_overall, SCADANN_acc_overall],\n",
    "                             index = [\"TSD\", \"DANN\", \"SCADANN\"],\n",
    "                             columns = [\"Overall_Acc\"])\n",
    "overall_acc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1800x1800 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(25,25))\n",
    "acc_list = [TSD_df, DANN_df, SCADANN_df, overall_acc_df]\n",
    "title_list = [\"TSD\", \"DANN\", \"SCADANN\", \"Overall\"]\n",
    "for idx, ax in enumerate(axes.reshape(-1)): \n",
    "    acc_list[idx].transpose().plot.bar(ax = ax, rot=0)\n",
    "    ax.set_title(title_list[idx])\n",
    "    ax.set_ylim([0, 1.0])\n",
    "    for p in ax.patches:\n",
    "        ax.annotate(str(np.round(p.get_height(),2)), (p.get_x()+p.get_width()/2., p.get_height()),\n",
    "                    ha='center', va='center', xytext=(0, 8),textcoords='offset points')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
