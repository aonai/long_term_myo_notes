{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import pickle\n",
    "import numpy as np  \n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.insert(0,'../../LongTermEMG-master')\n",
    "os.chdir(\"/home/laiy/gitrepos/msr_final/LongTermEMG-master/LongTermClassificationMain/TrainingsAndEvaluations/ForTrainingSessions/TSD_DNN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traning examples  (1, 4, 4)\n",
      "one group example  (1009, 385)\n",
      "traning labels  (1, 4, 4)\n",
      "one group label  (1009,)\n"
     ]
    }
   ],
   "source": [
    "with open(\"../../../Processed_datasets/TSD_features_set_training_session.pickle\", 'rb') as f:\n",
    "    dataset_training = pickle.load(file=f)\n",
    "\n",
    "examples_datasets_train = dataset_training['examples_training']\n",
    "print('traning examples ', np.shape(examples_datasets_train))\n",
    "print(\"one group example \", np.shape(examples_datasets_train[0][0][0]))\n",
    "labels_datasets_train = dataset_training['labels_training']\n",
    "print('traning labels ', np.shape(labels_datasets_train))\n",
    "print(\"one group label \", np.shape(labels_datasets_train[0][0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_size=None        \n",
    "num_kernels=[200, 200, 200]                        # model layer size \n",
    "number_of_cycle_for_first_training=4               # #session\n",
    "number_of_cycles_rest_of_training=4     \n",
    "path_weight_to_save_to=\"Weights_TSD/TSD_DANN\"      \n",
    "gestures_to_remove=None                    \n",
    "number_of_classes=11\n",
    "batch_size=128          \n",
    "spectrogram_model=False\n",
    "feature_vector_input_length=385                     # size of one example \n",
    "learning_rate=0.002515"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "def get_dataloader(examples_datasets, labels_datasets, number_of_cycle_for_first_training=2,\n",
    "                   number_of_cycles_rest_of_training=2, batch_size=128,\n",
    "                   drop_last=True, shuffle=True,\n",
    "                   number_of_cycles_total=4, validation_set_ratio=0.1, get_validation_set=True, cycle_for_test=None,\n",
    "                   ignore_first=False, gestures_to_remove=None):\n",
    "    \"\"\"\n",
    "    X = signals\n",
    "    Y = labels\n",
    "    \n",
    "    examples_datasets_train\n",
    "    labels_datasets_train\n",
    "    number_of_cycle_for_first_training:  #session for each training group \n",
    "    number_of_cycles_rest_of_training: totally four training groups\n",
    "    batch_size (TensorDataset param)\n",
    "    \n",
    "    drop_last (TensorDataset param): when True, drop last batch if its size < batch_size (only used for taining)\n",
    "    shuffle (TensorDataset param)\n",
    "    \n",
    "    number_of_cycles_total\n",
    "    \n",
    "    validation_set_ratio (train_test_split param): \n",
    "        ratio of validation set out of entire training data, the rest are training set \n",
    "    get_validation_set: return validation dataloader if True\n",
    "    \n",
    "    cycle_for_test: specify which cycle will be used for testing; no test dataloader returned if None\n",
    "    \n",
    "    ignore_first: ignore max_activatio training session (num=1) if True\n",
    "    gestures_to_remove: None\n",
    "    \n",
    "    \"\"\"\n",
    "    participants_dataloaders, participants_dataloaders_validation, participants_dataloaders_test = [], [], []\n",
    "\n",
    "    for participant_examples, participant_labels in zip(examples_datasets, labels_datasets):\n",
    "        print(\"GET one participant_examples \", np.shape(participant_examples))\n",
    "        dataloaders_trainings = []\n",
    "        dataloaders_validations = []\n",
    "        dataloaders_testing = []\n",
    "        \n",
    "        k = 0\n",
    "        for training_index_examples, training_index_labels in zip(participant_examples, participant_labels):\n",
    "            print(\"   GET one training_index_examples \", np.shape(training_index_examples), \" at \", k)\n",
    "            cycles_to_add_to_train = number_of_cycle_for_first_training\n",
    "            if k > 0:\n",
    "                cycles_to_add_to_train = number_of_cycles_rest_of_training\n",
    "                \n",
    "            X_associated_with_training_i, Y_associated_with_training_i = [], []\n",
    "            X_test_associated_with_training_i, Y_test_associated_with_training_i = [], []\n",
    "            for cycle in range(number_of_cycles_total):\n",
    "                if (ignore_first is True and cycle != 1) or ignore_first is False:\n",
    "                    examples_cycles = training_index_examples[cycle]\n",
    "                    labels_cycles = training_index_labels[cycle]\n",
    "                    print(\"      GET one examples_cycles \", np.shape(examples_cycles), \" at \", cycle)\n",
    "                    \n",
    "                    if gestures_to_remove is not None:\n",
    "                        examples_cycles, labels_cycles = remove_list_gestures_from_cycle(examples_cycles, labels_cycles,\n",
    "                                                                                         gestures_to_remove=\n",
    "                                                                                         gestures_to_remove)\n",
    "                    if cycle < cycles_to_add_to_train:\n",
    "                        X_associated_with_training_i.extend(examples_cycles)\n",
    "                        Y_associated_with_training_i.extend(labels_cycles)\n",
    "                    elif cycle_for_test is None:\n",
    "                        X_test_associated_with_training_i.extend(examples_cycles)\n",
    "                        Y_test_associated_with_training_i.extend(labels_cycles)\n",
    "                    elif cycle == cycle_for_test:\n",
    "                        X_test_associated_with_training_i.extend(examples_cycles)\n",
    "                        Y_test_associated_with_training_i.extend(labels_cycles)\n",
    "\n",
    "            print(\"   GOT one group XY \", np.shape(X_associated_with_training_i), \"  \",  np.shape(Y_associated_with_training_i))\n",
    "            print(\"       one group XY test \", np.shape(X_test_associated_with_training_i), \"  \",  np.shape(X_test_associated_with_training_i))\n",
    "            k += 1\n",
    "\n",
    "            if get_validation_set:\n",
    "                # Shuffle X and Y and separate them in a train and validation set.\n",
    "                X, X_valid, Y, Y_valid = train_test_split(X_associated_with_training_i, Y_associated_with_training_i,\n",
    "                                                          test_size=validation_set_ratio, shuffle=True)\n",
    "            else:\n",
    "                X, Y = X_associated_with_training_i, Y_associated_with_training_i\n",
    "            print(\"       one group XY train\", np.shape(X), \"  \",  np.shape(Y))\n",
    "            print(\"       one group XY valid\", np.shape(X_valid), \"  \",  np.shape(X_valid))\n",
    "            \n",
    "            # trainning dataloader\n",
    "            train = TensorDataset(torch.from_numpy(np.array(X, dtype=np.float32)),\n",
    "                                  torch.from_numpy(np.array(Y, dtype=np.int64)))\n",
    "            trainloader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=shuffle,\n",
    "                                                      drop_last=drop_last)\n",
    "            dataloaders_trainings.append(trainloader)\n",
    "            \n",
    "            # validation dataloader\n",
    "            if get_validation_set:\n",
    "                validation = TensorDataset(torch.from_numpy(np.array(X_valid, dtype=np.float32)),\n",
    "                                           torch.from_numpy(np.array(Y_valid, dtype=np.int64)))\n",
    "                validationloader = torch.utils.data.DataLoader(validation, batch_size=len(X_valid), shuffle=shuffle,\n",
    "                                                               drop_last=False)\n",
    "                dataloaders_validations.append(validationloader)\n",
    "\n",
    "            # testing dataloader \n",
    "            if len(X_test_associated_with_training_i) > 0:\n",
    "                test = TensorDataset(torch.from_numpy(np.array(X_test_associated_with_training_i, dtype=np.float32)),\n",
    "                                     torch.from_numpy(np.array(Y_test_associated_with_training_i, dtype=np.int64)))\n",
    "                testLoader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False,\n",
    "                                                         drop_last=False)\n",
    "                dataloaders_testing.append(testLoader)\n",
    "\n",
    "        participants_dataloaders.append(dataloaders_trainings)\n",
    "        if get_validation_set:\n",
    "            participants_dataloaders_validation.append(dataloaders_validations)\n",
    "        participants_dataloaders_test.append(dataloaders_testing)\n",
    "        print(\"dataloaders: \")\n",
    "        print(\"   train \", np.shape(participants_dataloaders))\n",
    "        print(\"   valid \", np.shape(participants_dataloaders_validation))\n",
    "        print(\"   test \", np.shape(participants_dataloaders_test))\n",
    "\n",
    "    return participants_dataloaders, participants_dataloaders_validation, participants_dataloaders_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataloaders_training_sessions(examples_datasets_train, labels_datasets_train,\n",
    "                                       number_of_cycle_for_first_training=2, number_of_cycles_rest_of_training=2,\n",
    "                                       batch_size=128, drop_last=True, shuffle=True, get_validation_set=True,\n",
    "                                       cycle_for_test=None, ignore_first=False, gestures_to_remove=None):\n",
    "    \"\"\"\n",
    "    examples_datasets_train\n",
    "    labels_datasets_train\n",
    "    number_of_cycle_for_first_training:  #session for each training group \n",
    "    number_of_cycles_rest_of_training: totally four training groups\n",
    "    batch_size\n",
    "    \n",
    "    drop_last\n",
    "    shuffle\n",
    "    get_validation_set: return validation dataloader if True\n",
    "    cycle_for_test: specify which cycle will be used for testing; no test dataloader returned if None\n",
    "    \n",
    "    ignore_first: ignore max_activatio training session (num=1) if True\n",
    "    gestures_to_remove: None\n",
    "    \n",
    "    Return: \n",
    "        fource training dataloader and four validation dataloader \n",
    "    \"\"\"\n",
    "    train, validation, test = get_dataloader(examples_datasets_train, labels_datasets_train,\n",
    "                                             number_of_cycle_for_first_training=number_of_cycle_for_first_training,\n",
    "                                             number_of_cycles_rest_of_training=number_of_cycles_rest_of_training, \n",
    "                                             batch_size=batch_size,\n",
    "                                             drop_last=drop_last, shuffle=shuffle,\n",
    "                                             get_validation_set=get_validation_set,\n",
    "                                             cycle_for_test=cycle_for_test,\n",
    "                                             ignore_first=ignore_first,\n",
    "                                             gestures_to_remove=gestures_to_remove)\n",
    "\n",
    "    return train, validation, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET one participant_examples  (4, 4)\n",
      "   GET one training_index_examples  (4,)  at  0\n",
      "      GET one examples_cycles  (1009, 385)  at  0\n",
      "      GET one examples_cycles  (949, 385)  at  2\n",
      "      GET one examples_cycles  (1014, 385)  at  3\n",
      "   GOT one group XY  (2972, 385)    (2972,)\n",
      "   GOT one group XY test  (0,)    (0,)\n",
      "       one group XY train (2674, 385)    (2674,)\n",
      "       one group XY valid (298, 385)    (298, 385)\n",
      "   GET one training_index_examples  (4,)  at  1\n",
      "      GET one examples_cycles  (1064, 385)  at  0\n",
      "      GET one examples_cycles  (1063, 385)  at  2\n",
      "      GET one examples_cycles  (1017, 385)  at  3\n",
      "   GOT one group XY  (3144, 385)    (3144,)\n",
      "   GOT one group XY test  (0,)    (0,)\n",
      "       one group XY train (2829, 385)    (2829,)\n",
      "       one group XY valid (315, 385)    (315, 385)\n",
      "   GET one training_index_examples  (4,)  at  2\n",
      "      GET one examples_cycles  (1067, 385)  at  0\n",
      "      GET one examples_cycles  (1067, 385)  at  2\n",
      "      GET one examples_cycles  (1068, 385)  at  3\n",
      "   GOT one group XY  (3202, 385)    (3202,)\n",
      "   GOT one group XY test  (0,)    (0,)\n",
      "       one group XY train (2881, 385)    (2881,)\n",
      "       one group XY valid (321, 385)    (321, 385)\n",
      "   GET one training_index_examples  (4,)  at  3\n",
      "      GET one examples_cycles  (1014, 385)  at  0\n",
      "      GET one examples_cycles  (1072, 385)  at  2\n",
      "      GET one examples_cycles  (1049, 385)  at  3\n",
      "   GOT one group XY  (3135, 385)    (3135,)\n",
      "   GOT one group XY test  (0,)    (0,)\n",
      "       one group XY train (2821, 385)    (2821,)\n",
      "       one group XY valid (314, 385)    (314, 385)\n",
      "dataloaders: \n",
      "   train  (1, 4)\n",
      "   valid  (1, 4)\n",
      "   valid  (1, 0)\n"
     ]
    }
   ],
   "source": [
    "participants_train, participants_validation, _ = load_dataloaders_training_sessions(\n",
    "    examples_datasets_train, labels_datasets_train, batch_size=batch_size,\n",
    "    number_of_cycle_for_first_training=number_of_cycle_for_first_training,\n",
    "    number_of_cycles_rest_of_training=number_of_cycles_rest_of_training, \n",
    "    gestures_to_remove=gestures_to_remove,\n",
    "    ignore_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of last tain dataloader for last participant  2821\n",
      "size of last valid dataloader for last participant  314\n",
      "torch.Size([128, 385]) --- torch.Size([128])\n",
      "torch.Size([385]) --- tensor(8)\n"
     ]
    }
   ],
   "source": [
    "print(\"size of last tain dataloader for last participant \", len(participants_train[-1][-1].dataset))\n",
    "print(\"size of last valid dataloader for last participant \", len(participants_validation[-1][-1].dataset))\n",
    "\n",
    "dataiter = iter(participants_train[-1][-1])\n",
    "signals, labels = dataiter.next()\n",
    "\n",
    "print(signals.shape, '---', labels.shape)   # batch_size = 128\n",
    "print(signals[-1].shape, '---', labels[-1])\n",
    "\n",
    "# each dataloader corresponds to one model training session "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Participant:  0\n",
      "Session:  0\n",
      "Number Parameters:  162013\n",
      "Epoch 0/499\n",
      "----------\n",
      "train Loss: 0.01063568 Acc: 0.58085937\n",
      "val Loss: 0.00678623 Acc: 0.40604027\n",
      "New best validation loss: 0.006786228826382016\n",
      "Epoch 1 of 500 took 0.245s\n",
      "Epoch 1/499\n",
      "----------\n",
      "train Loss: 0.00527268 Acc: 0.80546875\n",
      "val Loss: 0.01377121 Acc: 0.23154362\n",
      "Epoch 2 of 500 took 0.172s\n",
      "Epoch 2/499\n",
      "----------\n",
      "train Loss: 0.00405301 Acc: 0.85429687\n",
      "val Loss: 0.01203972 Acc: 0.2852349\n",
      "Epoch 3 of 500 took 0.178s\n",
      "Epoch 3/499\n",
      "----------\n",
      "train Loss: 0.00344460 Acc: 0.865625\n",
      "val Loss: 0.00422998 Acc: 0.70805369\n",
      "New best validation loss: 0.004229978426990893\n",
      "Epoch 4 of 500 took 0.181s\n",
      "Epoch 4/499\n",
      "----------\n",
      "train Loss: 0.00292405 Acc: 0.89140625\n",
      "val Loss: 0.00942227 Acc: 0.4295302\n",
      "Epoch 5 of 500 took 0.199s\n",
      "Epoch 5/499\n",
      "----------\n",
      "train Loss: 0.00295011 Acc: 0.88320312\n",
      "val Loss: 0.00851000 Acc: 0.56040268\n",
      "Epoch 6 of 500 took 0.222s\n",
      "Epoch 6/499\n",
      "----------\n",
      "train Loss: 0.00260905 Acc: 0.903125\n",
      "val Loss: 0.00462683 Acc: 0.70805369\n",
      "Epoch 7 of 500 took 0.185s\n",
      "Epoch 7/499\n",
      "----------\n",
      "train Loss: 0.00250209 Acc: 0.9015625\n",
      "val Loss: 0.00186152 Acc: 0.82214765\n",
      "New best validation loss: 0.0018615238618530682\n",
      "Epoch 8 of 500 took 0.186s\n",
      "Epoch 8/499\n",
      "----------\n",
      "train Loss: 0.00230440 Acc: 0.91289062\n",
      "val Loss: 0.00826833 Acc: 0.44966443\n",
      "Epoch 9 of 500 took 0.174s\n",
      "Epoch 9/499\n",
      "----------\n",
      "train Loss: 0.00206195 Acc: 0.92578125\n",
      "val Loss: 0.00685988 Acc: 0.58053691\n",
      "Epoch 10 of 500 took 0.207s\n",
      "Epoch 10/499\n",
      "----------\n",
      "train Loss: 0.00200667 Acc: 0.92265625\n",
      "val Loss: 0.00130867 Acc: 0.89261745\n",
      "New best validation loss: 0.0013086660796363882\n",
      "Epoch 11 of 500 took 0.210s\n",
      "Epoch 11/499\n",
      "----------\n",
      "train Loss: 0.00187795 Acc: 0.92734375\n",
      "val Loss: 0.00511714 Acc: 0.71812081\n",
      "Epoch 12 of 500 took 0.175s\n",
      "Epoch 12/499\n",
      "----------\n",
      "train Loss: 0.00196672 Acc: 0.92148438\n",
      "val Loss: 0.01227627 Acc: 0.48322148\n",
      "Epoch 13 of 500 took 0.179s\n",
      "Epoch 13/499\n",
      "----------\n",
      "train Loss: 0.00179063 Acc: 0.93476563\n",
      "val Loss: 0.00196470 Acc: 0.82550336\n",
      "Epoch 14 of 500 took 0.172s\n",
      "Epoch 14/499\n",
      "----------\n",
      "train Loss: 0.00178433 Acc: 0.93320313\n",
      "val Loss: 0.00144772 Acc: 0.87919463\n",
      "Epoch 15 of 500 took 0.205s\n",
      "Epoch 15/499\n",
      "----------\n",
      "train Loss: 0.00152775 Acc: 0.94257813\n",
      "val Loss: 0.00096703 Acc: 0.89932886\n",
      "New best validation loss: 0.0009670315572879459\n",
      "Epoch 16 of 500 took 0.216s\n",
      "Epoch 16/499\n",
      "----------\n",
      "train Loss: 0.00154855 Acc: 0.9390625\n",
      "val Loss: 0.00361053 Acc: 0.75167785\n",
      "Epoch 17 of 500 took 0.189s\n",
      "Epoch 17/499\n",
      "----------\n",
      "train Loss: 0.00179858 Acc: 0.9265625\n",
      "val Loss: 0.00327572 Acc: 0.77181208\n",
      "Epoch 18 of 500 took 0.180s\n",
      "Epoch 18/499\n",
      "----------\n",
      "train Loss: 0.00151152 Acc: 0.94414062\n",
      "val Loss: 0.00848294 Acc: 0.40268456\n",
      "Epoch 19 of 500 took 0.173s\n",
      "Epoch 19/499\n",
      "----------\n",
      "train Loss: 0.00148461 Acc: 0.94609375\n",
      "val Loss: 0.00481484 Acc: 0.60738255\n",
      "Epoch 20 of 500 took 0.189s\n",
      "Epoch 20/499\n",
      "----------\n",
      "train Loss: 0.00150329 Acc: 0.94492188\n",
      "val Loss: 0.01786782 Acc: 0.29530201\n",
      "Epoch 21 of 500 took 0.225s\n",
      "Epoch 21/499\n",
      "----------\n",
      "train Loss: 0.00127018 Acc: 0.94921875\n",
      "val Loss: 0.01242038 Acc: 0.43959732\n",
      "Epoch    22: reducing learning rate of group 0 to 5.0300e-04.\n",
      "Epoch 22 of 500 took 0.199s\n",
      "Epoch 22/499\n",
      "----------\n",
      "train Loss: 0.00111041 Acc: 0.95976562\n",
      "val Loss: 0.00048932 Acc: 0.97651007\n",
      "New best validation loss: 0.0004893167806951791\n",
      "Epoch 23 of 500 took 0.178s\n",
      "Epoch 23/499\n",
      "----------\n",
      "train Loss: 0.00097032 Acc: 0.96132812\n",
      "val Loss: 0.00037069 Acc: 0.97315436\n",
      "New best validation loss: 0.0003706851361581943\n",
      "Epoch 24 of 500 took 0.186s\n",
      "Epoch 24/499\n",
      "----------\n",
      "train Loss: 0.00091573 Acc: 0.9671875\n",
      "val Loss: 0.00041176 Acc: 0.96644295\n",
      "Epoch 25 of 500 took 0.187s\n",
      "Epoch 25/499\n",
      "----------\n",
      "train Loss: 0.00098100 Acc: 0.9640625\n",
      "val Loss: 0.00041777 Acc: 0.95973154\n",
      "Epoch 26 of 500 took 0.180s\n",
      "Epoch 26/499\n",
      "----------\n",
      "train Loss: 0.00085419 Acc: 0.96992188\n",
      "val Loss: 0.00046859 Acc: 0.95973154\n",
      "Epoch 27 of 500 took 0.185s\n",
      "Epoch 27/499\n",
      "----------\n",
      "train Loss: 0.00084492 Acc: 0.96640625\n",
      "val Loss: 0.00042077 Acc: 0.96308725\n",
      "Epoch 28 of 500 took 0.192s\n",
      "Epoch 28/499\n",
      "----------\n",
      "train Loss: 0.00083230 Acc: 0.96835938\n",
      "val Loss: 0.00085580 Acc: 0.9261745\n",
      "Epoch 29 of 500 took 0.180s\n",
      "Epoch 29/499\n",
      "----------\n",
      "train Loss: 0.00080879 Acc: 0.97226563\n",
      "val Loss: 0.00042105 Acc: 0.97315436\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0060e-04.\n",
      "Epoch 30 of 500 took 0.192s\n",
      "Epoch 30/499\n",
      "----------\n",
      "train Loss: 0.00077418 Acc: 0.971875\n",
      "val Loss: 0.00038737 Acc: 0.97315436\n",
      "Epoch 31 of 500 took 0.187s\n",
      "Epoch 31/499\n",
      "----------\n",
      "train Loss: 0.00082639 Acc: 0.96875\n",
      "val Loss: 0.00039317 Acc: 0.96644295\n",
      "Epoch 32 of 500 took 0.191s\n",
      "Epoch 32/499\n",
      "----------\n",
      "train Loss: 0.00079265 Acc: 0.9671875\n",
      "val Loss: 0.00036500 Acc: 0.97315436\n",
      "New best validation loss: 0.00036499566779840713\n",
      "Epoch 33 of 500 took 0.205s\n",
      "Epoch 33/499\n",
      "----------\n",
      "train Loss: 0.00082734 Acc: 0.9703125\n",
      "val Loss: 0.00035239 Acc: 0.96308725\n",
      "New best validation loss: 0.0003523934857557284\n",
      "Epoch 34 of 500 took 0.188s\n",
      "Epoch 34/499\n",
      "----------\n",
      "train Loss: 0.00074394 Acc: 0.97070312\n",
      "val Loss: 0.00030155 Acc: 0.97651007\n",
      "New best validation loss: 0.00030154917364152485\n",
      "Epoch 35 of 500 took 0.230s\n",
      "Epoch 35/499\n",
      "----------\n",
      "train Loss: 0.00076535 Acc: 0.97421875\n",
      "val Loss: 0.00047100 Acc: 0.94966443\n",
      "Epoch 36 of 500 took 0.241s\n",
      "Epoch 36/499\n",
      "----------\n",
      "train Loss: 0.00066373 Acc: 0.97148437\n",
      "val Loss: 0.00036616 Acc: 0.97315436\n",
      "Epoch 37 of 500 took 0.243s\n",
      "Epoch 37/499\n",
      "----------\n",
      "train Loss: 0.00082739 Acc: 0.96953125\n",
      "val Loss: 0.00032429 Acc: 0.97651007\n",
      "Epoch 38 of 500 took 0.181s\n",
      "Epoch 38/499\n",
      "----------\n",
      "train Loss: 0.00072239 Acc: 0.97148437\n",
      "val Loss: 0.00037480 Acc: 0.97315436\n",
      "Epoch 39 of 500 took 0.178s\n",
      "Epoch 39/499\n",
      "----------\n",
      "train Loss: 0.00080682 Acc: 0.97148437\n",
      "val Loss: 0.00033871 Acc: 0.97315436\n",
      "Epoch 40 of 500 took 0.171s\n",
      "Epoch 40/499\n",
      "----------\n",
      "train Loss: 0.00070451 Acc: 0.97460938\n",
      "val Loss: 0.00034138 Acc: 0.97315436\n",
      "Epoch    41: reducing learning rate of group 0 to 2.0120e-05.\n",
      "Epoch 41 of 500 took 0.177s\n",
      "Epoch 41/499\n",
      "----------\n",
      "train Loss: 0.00081278 Acc: 0.96953125\n",
      "val Loss: 0.00043441 Acc: 0.95973154\n",
      "Epoch 42 of 500 took 0.172s\n",
      "Epoch 42/499\n",
      "----------\n",
      "train Loss: 0.00081659 Acc: 0.97070312\n",
      "val Loss: 0.00034979 Acc: 0.96979866\n",
      "Epoch 43 of 500 took 0.181s\n",
      "Epoch 43/499\n",
      "----------\n",
      "train Loss: 0.00075585 Acc: 0.96953125\n",
      "val Loss: 0.00042341 Acc: 0.97315436\n",
      "Epoch 44 of 500 took 0.171s\n",
      "Epoch 44/499\n",
      "----------\n",
      "train Loss: 0.00076796 Acc: 0.97070312\n",
      "val Loss: 0.00034715 Acc: 0.96644295\n",
      "Epoch 45 of 500 took 0.194s\n",
      "Epoch 45/499\n",
      "----------\n",
      "train Loss: 0.00077561 Acc: 0.97304687\n",
      "val Loss: 0.00031755 Acc: 0.97651007\n",
      "Epoch 46 of 500 took 0.208s\n",
      "\n",
      "Training complete in 0m 9s\n",
      "Best val loss: 0.000302\n",
      "Session:  1\n",
      "Number Parameters:  162013\n",
      "=> loading checkpoint 'Weights_TSD/TSD_DANN/participant_0/best_state_0.pt'\n",
      "=> loaded checkpoint 'Weights_TSD/TSD_DANN/participant_0/best_state_0.pt' (epoch 35)\n",
      "Epoch 0/499\n",
      "----------\n",
      "train Loss: 0.00522827 Acc: 0.80681818\n",
      "val Loss: 0.00413761 Acc: 0.73015873\n",
      "New best validation loss: 0.004137612146044535\n",
      "Epoch 1 of 500 took 0.197s\n",
      "Epoch 1/499\n",
      "----------\n",
      "train Loss: 0.00277913 Acc: 0.88991477\n",
      "val Loss: 0.00395150 Acc: 0.65714286\n",
      "New best validation loss: 0.003951504116966611\n",
      "Epoch 2 of 500 took 0.192s\n",
      "Epoch 2/499\n",
      "----------\n",
      "train Loss: 0.00241847 Acc: 0.90305398\n",
      "val Loss: 0.00162506 Acc: 0.82222222\n",
      "New best validation loss: 0.0016250581968398322\n",
      "Epoch 3 of 500 took 0.203s\n",
      "Epoch 3/499\n",
      "----------\n",
      "train Loss: 0.00215306 Acc: 0.9140625\n",
      "val Loss: 0.00805544 Acc: 0.55555556\n",
      "Epoch 4 of 500 took 0.193s\n",
      "Epoch 4/499\n",
      "----------\n",
      "train Loss: 0.00216878 Acc: 0.91512784\n",
      "val Loss: 0.01012613 Acc: 0.53015873\n",
      "Epoch 5 of 500 took 0.197s\n",
      "Epoch 5/499\n",
      "----------\n",
      "train Loss: 0.00181589 Acc: 0.9243608\n",
      "val Loss: 0.00204344 Acc: 0.80634921\n",
      "Epoch 6 of 500 took 0.207s\n",
      "Epoch 6/499\n",
      "----------\n",
      "train Loss: 0.00172673 Acc: 0.93110795\n",
      "val Loss: 0.00313318 Acc: 0.6952381\n",
      "Epoch 7 of 500 took 0.193s\n",
      "Epoch 7/499\n",
      "----------\n",
      "train Loss: 0.00161097 Acc: 0.93039773\n",
      "val Loss: 0.00974483 Acc: 0.62222222\n",
      "Epoch 8 of 500 took 0.194s\n",
      "Epoch 8/499\n",
      "----------\n",
      "train Loss: 0.00164095 Acc: 0.92684659\n",
      "val Loss: 0.00207994 Acc: 0.78412698\n",
      "Epoch     9: reducing learning rate of group 0 to 5.0300e-04.\n",
      "Epoch 9 of 500 took 0.189s\n",
      "Epoch 9/499\n",
      "----------\n",
      "train Loss: 0.00135555 Acc: 0.94460227\n",
      "val Loss: 0.00111774 Acc: 0.87619048\n",
      "New best validation loss: 0.0011177388448563833\n",
      "Epoch 10 of 500 took 0.194s\n",
      "Epoch 10/499\n",
      "----------\n",
      "train Loss: 0.00120520 Acc: 0.94886364\n",
      "val Loss: 0.00066313 Acc: 0.93968254\n",
      "New best validation loss: 0.0006631255149841308\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 of 500 took 0.195s\n",
      "Epoch 11/499\n",
      "----------\n",
      "train Loss: 0.00111865 Acc: 0.95987216\n",
      "val Loss: 0.00059376 Acc: 0.94285714\n",
      "New best validation loss: 0.0005937591431632875\n",
      "Epoch 12 of 500 took 0.228s\n",
      "Epoch 12/499\n",
      "----------\n",
      "train Loss: 0.00096822 Acc: 0.9634233\n",
      "val Loss: 0.00098074 Acc: 0.90793651\n",
      "Epoch 13 of 500 took 0.211s\n",
      "Epoch 13/499\n",
      "----------\n",
      "train Loss: 0.00108505 Acc: 0.95703125\n",
      "val Loss: 0.00069757 Acc: 0.93015873\n",
      "Epoch 14 of 500 took 0.222s\n",
      "Epoch 14/499\n",
      "----------\n",
      "train Loss: 0.00102909 Acc: 0.95809659\n",
      "val Loss: 0.00050530 Acc: 0.95873016\n",
      "New best validation loss: 0.0005052990383572049\n",
      "Epoch 15 of 500 took 0.225s\n",
      "Epoch 15/499\n",
      "----------\n",
      "train Loss: 0.00099450 Acc: 0.95774148\n",
      "val Loss: 0.00077577 Acc: 0.91428571\n",
      "Epoch 16 of 500 took 0.271s\n",
      "Epoch 16/499\n",
      "----------\n",
      "train Loss: 0.00096871 Acc: 0.9609375\n",
      "val Loss: 0.00060676 Acc: 0.94920635\n",
      "Epoch 17 of 500 took 0.253s\n",
      "Epoch 17/499\n",
      "----------\n",
      "train Loss: 0.00102020 Acc: 0.9609375\n",
      "val Loss: 0.00109824 Acc: 0.8952381\n",
      "Epoch 18 of 500 took 0.247s\n",
      "Epoch 18/499\n",
      "----------\n",
      "train Loss: 0.00100898 Acc: 0.96058239\n",
      "val Loss: 0.00102953 Acc: 0.90793651\n",
      "Epoch 19 of 500 took 0.236s\n",
      "Epoch 19/499\n",
      "----------\n",
      "train Loss: 0.00095088 Acc: 0.96519886\n",
      "val Loss: 0.00047764 Acc: 0.95555556\n",
      "New best validation loss: 0.0004776442807818216\n",
      "Epoch 20 of 500 took 0.233s\n",
      "Epoch 20/499\n",
      "----------\n",
      "train Loss: 0.00091095 Acc: 0.95880682\n",
      "val Loss: 0.00120223 Acc: 0.86349206\n",
      "Epoch 21 of 500 took 0.193s\n",
      "Epoch 21/499\n",
      "----------\n",
      "train Loss: 0.00081673 Acc: 0.9662642\n",
      "val Loss: 0.00045239 Acc: 0.95873016\n",
      "New best validation loss: 0.00045238974548521494\n",
      "Epoch 22 of 500 took 0.205s\n",
      "Epoch 22/499\n",
      "----------\n",
      "train Loss: 0.00089564 Acc: 0.96448864\n",
      "val Loss: 0.00056396 Acc: 0.95873016\n",
      "Epoch 23 of 500 took 0.197s\n",
      "Epoch 23/499\n",
      "----------\n",
      "train Loss: 0.00082614 Acc: 0.96732955\n",
      "val Loss: 0.00052417 Acc: 0.95238095\n",
      "Epoch 24 of 500 took 0.253s\n",
      "Epoch 24/499\n",
      "----------\n",
      "train Loss: 0.00081885 Acc: 0.9634233\n",
      "val Loss: 0.00048494 Acc: 0.96825397\n",
      "Epoch 25 of 500 took 0.202s\n",
      "Epoch 25/499\n",
      "----------\n",
      "train Loss: 0.00083114 Acc: 0.96484375\n",
      "val Loss: 0.00062260 Acc: 0.94920635\n",
      "Epoch 26 of 500 took 0.213s\n",
      "Epoch 26/499\n",
      "----------\n",
      "train Loss: 0.00080780 Acc: 0.96697443\n",
      "val Loss: 0.00115773 Acc: 0.8952381\n",
      "Epoch 27 of 500 took 0.191s\n",
      "Epoch 27/499\n",
      "----------\n",
      "train Loss: 0.00083656 Acc: 0.96306818\n",
      "val Loss: 0.00052378 Acc: 0.95238095\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0060e-04.\n",
      "Epoch 28 of 500 took 0.195s\n",
      "Epoch 28/499\n",
      "----------\n",
      "train Loss: 0.00080635 Acc: 0.97017045\n",
      "val Loss: 0.00047005 Acc: 0.95873016\n",
      "Epoch 29 of 500 took 0.202s\n",
      "Epoch 29/499\n",
      "----------\n",
      "train Loss: 0.00076115 Acc: 0.97017045\n",
      "val Loss: 0.00044471 Acc: 0.95873016\n",
      "New best validation loss: 0.0004447095923953586\n",
      "Epoch 30 of 500 took 0.191s\n",
      "Epoch 30/499\n",
      "----------\n",
      "train Loss: 0.00072540 Acc: 0.96910511\n",
      "val Loss: 0.00051509 Acc: 0.96190476\n",
      "Epoch 31 of 500 took 0.197s\n",
      "Epoch 31/499\n",
      "----------\n",
      "train Loss: 0.00076618 Acc: 0.96768466\n",
      "val Loss: 0.00046827 Acc: 0.96190476\n",
      "Epoch 32 of 500 took 0.227s\n",
      "Epoch 32/499\n",
      "----------\n",
      "train Loss: 0.00074678 Acc: 0.96661932\n",
      "val Loss: 0.00055046 Acc: 0.95555556\n",
      "Epoch 33 of 500 took 0.196s\n",
      "Epoch 33/499\n",
      "----------\n",
      "train Loss: 0.00073957 Acc: 0.96981534\n",
      "val Loss: 0.00051216 Acc: 0.95873016\n",
      "Epoch 34 of 500 took 0.245s\n",
      "Epoch 34/499\n",
      "----------\n",
      "train Loss: 0.00074495 Acc: 0.97194602\n",
      "val Loss: 0.00050256 Acc: 0.95555556\n",
      "Epoch 35 of 500 took 0.258s\n",
      "Epoch 35/499\n",
      "----------\n",
      "train Loss: 0.00077198 Acc: 0.96732955\n",
      "val Loss: 0.00061881 Acc: 0.95555556\n",
      "Epoch    36: reducing learning rate of group 0 to 2.0120e-05.\n",
      "Epoch 36 of 500 took 0.229s\n",
      "Epoch 36/499\n",
      "----------\n",
      "train Loss: 0.00080031 Acc: 0.96768466\n",
      "val Loss: 0.00048602 Acc: 0.95555556\n",
      "Epoch 37 of 500 took 0.237s\n",
      "Epoch 37/499\n",
      "----------\n",
      "train Loss: 0.00071969 Acc: 0.97265625\n",
      "val Loss: 0.00045675 Acc: 0.96507937\n",
      "Epoch 38 of 500 took 0.255s\n",
      "Epoch 38/499\n",
      "----------\n",
      "train Loss: 0.00069717 Acc: 0.97194602\n",
      "val Loss: 0.00050938 Acc: 0.96190476\n",
      "Epoch 39 of 500 took 0.256s\n",
      "Epoch 39/499\n",
      "----------\n",
      "train Loss: 0.00063507 Acc: 0.97585227\n",
      "val Loss: 0.00051740 Acc: 0.95238095\n",
      "Epoch 40 of 500 took 0.205s\n",
      "Epoch 40/499\n",
      "----------\n",
      "train Loss: 0.00072477 Acc: 0.97017045\n",
      "val Loss: 0.00046420 Acc: 0.96507937\n",
      "Epoch 41 of 500 took 0.240s\n",
      "\n",
      "Training complete in 0m 9s\n",
      "Best val loss: 0.000445\n",
      "Session:  2\n",
      "Number Parameters:  162013\n",
      "=> loading checkpoint 'Weights_TSD/TSD_DANN/participant_0/best_state_1.pt'\n",
      "=> loaded checkpoint 'Weights_TSD/TSD_DANN/participant_0/best_state_1.pt' (epoch 30)\n",
      "Epoch 0/499\n",
      "----------\n",
      "train Loss: 0.00279900 Acc: 0.89808239\n",
      "val Loss: 0.00229783 Acc: 0.76012461\n",
      "New best validation loss: 0.002297826830843156\n",
      "Epoch 1 of 500 took 0.261s\n",
      "Epoch 1/499\n",
      "----------\n",
      "train Loss: 0.00125107 Acc: 0.94921875\n",
      "val Loss: 0.00080737 Acc: 0.91588785\n",
      "New best validation loss: 0.0008073650602239686\n",
      "Epoch 2 of 500 took 0.237s\n",
      "Epoch 2/499\n",
      "----------\n",
      "train Loss: 0.00123693 Acc: 0.94566761\n",
      "val Loss: 0.00423081 Acc: 0.75077882\n",
      "Epoch 3 of 500 took 0.203s\n",
      "Epoch 3/499\n",
      "----------\n",
      "train Loss: 0.00109423 Acc: 0.95276989\n",
      "val Loss: 0.00210145 Acc: 0.82554517\n",
      "Epoch 4 of 500 took 0.213s\n",
      "Epoch 4/499\n",
      "----------\n",
      "train Loss: 0.00095585 Acc: 0.95951705\n",
      "val Loss: 0.00118882 Acc: 0.90654206\n",
      "Epoch 5 of 500 took 0.217s\n",
      "Epoch 5/499\n",
      "----------\n",
      "train Loss: 0.00081060 Acc: 0.96910511\n",
      "val Loss: 0.01269854 Acc: 0.59501558\n",
      "Epoch 6 of 500 took 0.198s\n",
      "Epoch 6/499\n",
      "----------\n",
      "train Loss: 0.00068059 Acc: 0.97159091\n",
      "val Loss: 0.00375672 Acc: 0.73831776\n",
      "Epoch 7 of 500 took 0.202s\n",
      "Epoch 7/499\n",
      "----------\n",
      "train Loss: 0.00081272 Acc: 0.96697443\n",
      "val Loss: 0.00379916 Acc: 0.72897196\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0300e-04.\n",
      "Epoch 8 of 500 took 0.205s\n",
      "Epoch 8/499\n",
      "----------\n",
      "train Loss: 0.00057900 Acc: 0.97691761\n",
      "val Loss: 0.00030518 Acc: 0.96261682\n",
      "New best validation loss: 0.00030518049298788527\n",
      "Epoch 9 of 500 took 0.214s\n",
      "Epoch 9/499\n",
      "----------\n",
      "train Loss: 0.00063507 Acc: 0.9740767\n",
      "val Loss: 0.00021347 Acc: 0.97507788\n",
      "New best validation loss: 0.00021347454589475354\n",
      "Epoch 10 of 500 took 0.232s\n",
      "Epoch 10/499\n",
      "----------\n",
      "train Loss: 0.00053677 Acc: 0.98224432\n",
      "val Loss: 0.00029642 Acc: 0.96573209\n",
      "Epoch 11 of 500 took 0.198s\n",
      "Epoch 11/499\n",
      "----------\n",
      "train Loss: 0.00042355 Acc: 0.98792614\n",
      "val Loss: 0.00021609 Acc: 0.98130841\n",
      "Epoch 12 of 500 took 0.244s\n",
      "Epoch 12/499\n",
      "----------\n",
      "train Loss: 0.00052122 Acc: 0.98259943\n",
      "val Loss: 0.00026113 Acc: 0.97507788\n",
      "Epoch 13 of 500 took 0.248s\n",
      "Epoch 13/499\n",
      "----------\n",
      "train Loss: 0.00046292 Acc: 0.9818892\n",
      "val Loss: 0.00022536 Acc: 0.97507788\n",
      "Epoch 14 of 500 took 0.240s\n",
      "Epoch 14/499\n",
      "----------\n",
      "train Loss: 0.00045518 Acc: 0.98011364\n",
      "val Loss: 0.00021925 Acc: 0.97819315\n",
      "Epoch 15 of 500 took 0.214s\n",
      "Epoch 15/499\n",
      "----------\n",
      "train Loss: 0.00038467 Acc: 0.98508523\n",
      "val Loss: 0.00024096 Acc: 0.97507788\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0060e-04.\n",
      "Epoch 16 of 500 took 0.243s\n",
      "Epoch 16/499\n",
      "----------\n",
      "train Loss: 0.00039908 Acc: 0.98579545\n",
      "val Loss: 0.00019546 Acc: 0.97196262\n",
      "New best validation loss: 0.0001954644732757521\n",
      "Epoch 17 of 500 took 0.209s\n",
      "Epoch 17/499\n",
      "----------\n",
      "train Loss: 0.00034188 Acc: 0.98828125\n",
      "val Loss: 0.00018141 Acc: 0.97819315\n",
      "New best validation loss: 0.00018141028758521393\n",
      "Epoch 18 of 500 took 0.210s\n",
      "Epoch 18/499\n",
      "----------\n",
      "train Loss: 0.00037330 Acc: 0.9868608\n",
      "val Loss: 0.00020489 Acc: 0.98130841\n",
      "Epoch 19 of 500 took 0.204s\n",
      "Epoch 19/499\n",
      "----------\n",
      "train Loss: 0.00035549 Acc: 0.98721591\n",
      "val Loss: 0.00015668 Acc: 0.98753894\n",
      "New best validation loss: 0.00015668407361084055\n",
      "Epoch 20 of 500 took 0.219s\n",
      "Epoch 20/499\n",
      "----------\n",
      "train Loss: 0.00044393 Acc: 0.9818892\n",
      "val Loss: 0.00016450 Acc: 0.98130841\n",
      "Epoch 21 of 500 took 0.231s\n",
      "Epoch 21/499\n",
      "----------\n",
      "train Loss: 0.00041599 Acc: 0.98615057\n",
      "val Loss: 0.00017554 Acc: 0.97819315\n",
      "Epoch 22 of 500 took 0.250s\n",
      "Epoch 22/499\n",
      "----------\n",
      "train Loss: 0.00034120 Acc: 0.9868608\n",
      "val Loss: 0.00016738 Acc: 0.98130841\n",
      "Epoch 23 of 500 took 0.228s\n",
      "Epoch 23/499\n",
      "----------\n",
      "train Loss: 0.00032729 Acc: 0.98757102\n",
      "val Loss: 0.00022940 Acc: 0.97196262\n",
      "Epoch 24 of 500 took 0.194s\n",
      "Epoch 24/499\n",
      "----------\n",
      "train Loss: 0.00039298 Acc: 0.98401989\n",
      "val Loss: 0.00016942 Acc: 0.97819315\n",
      "Epoch 25 of 500 took 0.250s\n",
      "Epoch 25/499\n",
      "----------\n",
      "train Loss: 0.00034839 Acc: 0.98757102\n",
      "val Loss: 0.00018707 Acc: 0.97819315\n",
      "Epoch    26: reducing learning rate of group 0 to 2.0120e-05.\n",
      "Epoch 26 of 500 took 0.193s\n",
      "Epoch 26/499\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.00032449 Acc: 0.98757102\n",
      "val Loss: 0.00015051 Acc: 0.98442368\n",
      "New best validation loss: 0.00015051144166527508\n",
      "Epoch 27 of 500 took 0.193s\n",
      "Epoch 27/499\n",
      "----------\n",
      "train Loss: 0.00034255 Acc: 0.98757102\n",
      "val Loss: 0.00016081 Acc: 0.98753894\n",
      "Epoch 28 of 500 took 0.214s\n",
      "Epoch 28/499\n",
      "----------\n",
      "train Loss: 0.00034944 Acc: 0.98544034\n",
      "val Loss: 0.00017903 Acc: 0.98130841\n",
      "Epoch 29 of 500 took 0.229s\n",
      "Epoch 29/499\n",
      "----------\n",
      "train Loss: 0.00039481 Acc: 0.9818892\n",
      "val Loss: 0.00013989 Acc: 0.98753894\n",
      "New best validation loss: 0.00013988796981324287\n",
      "Epoch 30 of 500 took 0.226s\n",
      "Epoch 30/499\n",
      "----------\n",
      "train Loss: 0.00033752 Acc: 0.9868608\n",
      "val Loss: 0.00024055 Acc: 0.96884735\n",
      "Epoch 31 of 500 took 0.236s\n",
      "Epoch 31/499\n",
      "----------\n",
      "train Loss: 0.00035991 Acc: 0.98615057\n",
      "val Loss: 0.00017178 Acc: 0.98442368\n",
      "Epoch 32 of 500 took 0.236s\n",
      "Epoch 32/499\n",
      "----------\n",
      "train Loss: 0.00043015 Acc: 0.98508523\n",
      "val Loss: 0.00014516 Acc: 0.99065421\n",
      "Epoch 33 of 500 took 0.193s\n",
      "Epoch 33/499\n",
      "----------\n",
      "train Loss: 0.00032217 Acc: 0.98473011\n",
      "val Loss: 0.00015192 Acc: 0.98442368\n",
      "Epoch 34 of 500 took 0.243s\n",
      "Epoch 34/499\n",
      "----------\n",
      "train Loss: 0.00034567 Acc: 0.9868608\n",
      "val Loss: 0.00019296 Acc: 0.98753894\n",
      "Epoch 35 of 500 took 0.226s\n",
      "Epoch 35/499\n",
      "----------\n",
      "train Loss: 0.00033041 Acc: 0.98828125\n",
      "val Loss: 0.00015106 Acc: 0.98130841\n",
      "Epoch    36: reducing learning rate of group 0 to 4.0240e-06.\n",
      "Epoch 36 of 500 took 0.243s\n",
      "Epoch 36/499\n",
      "----------\n",
      "train Loss: 0.00031149 Acc: 0.99005682\n",
      "val Loss: 0.00017186 Acc: 0.98442368\n",
      "Epoch 37 of 500 took 0.240s\n",
      "Epoch 37/499\n",
      "----------\n",
      "train Loss: 0.00034737 Acc: 0.98579545\n",
      "val Loss: 0.00018113 Acc: 0.98442368\n",
      "Epoch 38 of 500 took 0.218s\n",
      "Epoch 38/499\n",
      "----------\n",
      "train Loss: 0.00039193 Acc: 0.984375\n",
      "val Loss: 0.00019821 Acc: 0.97819315\n",
      "Epoch 39 of 500 took 0.232s\n",
      "Epoch 39/499\n",
      "----------\n",
      "train Loss: 0.00036841 Acc: 0.98544034\n",
      "val Loss: 0.00016989 Acc: 0.97819315\n",
      "Epoch 40 of 500 took 0.207s\n",
      "Epoch 40/499\n",
      "----------\n",
      "train Loss: 0.00037994 Acc: 0.98721591\n",
      "val Loss: 0.00018517 Acc: 0.97819315\n",
      "Epoch 41 of 500 took 0.240s\n",
      "\n",
      "Training complete in 0m 9s\n",
      "Best val loss: 0.000140\n",
      "Session:  3\n",
      "Number Parameters:  162013\n",
      "=> loading checkpoint 'Weights_TSD/TSD_DANN/participant_0/best_state_2.pt'\n",
      "=> loaded checkpoint 'Weights_TSD/TSD_DANN/participant_0/best_state_2.pt' (epoch 30)\n",
      "Epoch 0/499\n",
      "----------\n",
      "train Loss: 0.00282248 Acc: 0.90980114\n",
      "val Loss: 0.00144816 Acc: 0.86624204\n",
      "New best validation loss: 0.0014481612831164317\n",
      "Epoch 1 of 500 took 0.250s\n",
      "Epoch 1/499\n",
      "----------\n",
      "train Loss: 0.00151757 Acc: 0.94921875\n",
      "val Loss: 0.02088249 Acc: 0.31847134\n",
      "Epoch 2 of 500 took 0.214s\n",
      "Epoch 2/499\n",
      "----------\n",
      "train Loss: 0.00118932 Acc: 0.95241477\n",
      "val Loss: 0.01547461 Acc: 0.32802548\n",
      "Epoch 3 of 500 took 0.202s\n",
      "Epoch 3/499\n",
      "----------\n",
      "train Loss: 0.00102029 Acc: 0.9634233\n",
      "val Loss: 0.00505166 Acc: 0.7133758\n",
      "Epoch 4 of 500 took 0.203s\n",
      "Epoch 4/499\n",
      "----------\n",
      "train Loss: 0.00123565 Acc: 0.95987216\n",
      "val Loss: 0.00107614 Acc: 0.91401274\n",
      "New best validation loss: 0.0010761444925502608\n",
      "Epoch 5 of 500 took 0.202s\n",
      "Epoch 5/499\n",
      "----------\n",
      "train Loss: 0.00076867 Acc: 0.9765625\n",
      "val Loss: 0.00105949 Acc: 0.86942675\n",
      "New best validation loss: 0.0010594901195756948\n",
      "Epoch 6 of 500 took 0.276s\n",
      "Epoch 6/499\n",
      "----------\n",
      "train Loss: 0.00076562 Acc: 0.96946023\n",
      "val Loss: 0.00253345 Acc: 0.80573248\n",
      "Epoch 7 of 500 took 0.190s\n",
      "Epoch 7/499\n",
      "----------\n",
      "train Loss: 0.00078787 Acc: 0.97088068\n",
      "val Loss: 0.00039169 Acc: 0.97133758\n",
      "New best validation loss: 0.00039169353664301\n",
      "Epoch 8 of 500 took 0.230s\n",
      "Epoch 8/499\n",
      "----------\n",
      "train Loss: 0.00085176 Acc: 0.96910511\n",
      "val Loss: 0.00211288 Acc: 0.84713376\n",
      "Epoch 9 of 500 took 0.254s\n",
      "Epoch 9/499\n",
      "----------\n",
      "train Loss: 0.00053558 Acc: 0.98046875\n",
      "val Loss: 0.00078729 Acc: 0.92356688\n",
      "Epoch 10 of 500 took 0.244s\n",
      "Epoch 10/499\n",
      "----------\n",
      "train Loss: 0.00069569 Acc: 0.97265625\n",
      "val Loss: 0.00924116 Acc: 0.52547771\n",
      "Epoch 11 of 500 took 0.234s\n",
      "Epoch 11/499\n",
      "----------\n",
      "train Loss: 0.00061552 Acc: 0.97727273\n",
      "val Loss: 0.00422781 Acc: 0.7133758\n",
      "Epoch 12 of 500 took 0.195s\n",
      "Epoch 12/499\n",
      "----------\n",
      "train Loss: 0.00066783 Acc: 0.97372159\n",
      "val Loss: 0.03724567 Acc: 0.35350318\n",
      "Epoch 13 of 500 took 0.191s\n",
      "Epoch 13/499\n",
      "----------\n",
      "train Loss: 0.00075035 Acc: 0.97301136\n",
      "val Loss: 0.00086002 Acc: 0.91719745\n",
      "Epoch    14: reducing learning rate of group 0 to 5.0300e-04.\n",
      "Epoch 14 of 500 took 0.194s\n",
      "Epoch 14/499\n",
      "----------\n",
      "train Loss: 0.00060654 Acc: 0.97620739\n",
      "val Loss: 0.00033714 Acc: 0.97452229\n",
      "New best validation loss: 0.00033713537891199636\n",
      "Epoch 15 of 500 took 0.191s\n",
      "Epoch 15/499\n",
      "----------\n",
      "train Loss: 0.00045393 Acc: 0.9818892\n",
      "val Loss: 0.00026592 Acc: 0.97770701\n",
      "New best validation loss: 0.00026592396342071\n",
      "Epoch 16 of 500 took 0.191s\n",
      "Epoch 16/499\n",
      "----------\n",
      "train Loss: 0.00052170 Acc: 0.98082386\n",
      "val Loss: 0.00025722 Acc: 0.97770701\n",
      "New best validation loss: 0.00025722474619081824\n",
      "Epoch 17 of 500 took 0.194s\n",
      "Epoch 17/499\n",
      "----------\n",
      "train Loss: 0.00045757 Acc: 0.98046875\n",
      "val Loss: 0.00025438 Acc: 0.98089172\n",
      "New best validation loss: 0.0002543778176520281\n",
      "Epoch 18 of 500 took 0.193s\n",
      "Epoch 18/499\n",
      "----------\n",
      "train Loss: 0.00041661 Acc: 0.98650568\n",
      "val Loss: 0.00043925 Acc: 0.97133758\n",
      "Epoch 19 of 500 took 0.193s\n",
      "Epoch 19/499\n",
      "----------\n",
      "train Loss: 0.00036305 Acc: 0.98615057\n",
      "val Loss: 0.00036784 Acc: 0.97452229\n",
      "Epoch 20 of 500 took 0.193s\n",
      "Epoch 20/499\n",
      "----------\n",
      "train Loss: 0.00036407 Acc: 0.98579545\n",
      "val Loss: 0.00040093 Acc: 0.96815287\n",
      "Epoch 21 of 500 took 0.218s\n",
      "Epoch 21/499\n",
      "----------\n",
      "train Loss: 0.00034552 Acc: 0.98721591\n",
      "val Loss: 0.00022636 Acc: 0.98089172\n",
      "New best validation loss: 0.00022636190246624552\n",
      "Epoch 22 of 500 took 0.220s\n",
      "Epoch 22/499\n",
      "----------\n",
      "train Loss: 0.00031153 Acc: 0.98792614\n",
      "val Loss: 0.00034753 Acc: 0.97133758\n",
      "Epoch 23 of 500 took 0.227s\n",
      "Epoch 23/499\n",
      "----------\n",
      "train Loss: 0.00040052 Acc: 0.98544034\n",
      "val Loss: 0.00062811 Acc: 0.94585987\n",
      "Epoch 24 of 500 took 0.226s\n",
      "Epoch 24/499\n",
      "----------\n",
      "train Loss: 0.00035363 Acc: 0.98508523\n",
      "val Loss: 0.00032144 Acc: 0.97133758\n",
      "Epoch 25 of 500 took 0.205s\n",
      "Epoch 25/499\n",
      "----------\n",
      "train Loss: 0.00031145 Acc: 0.98792614\n",
      "val Loss: 0.00024283 Acc: 0.97770701\n",
      "Epoch 26 of 500 took 0.210s\n",
      "Epoch 26/499\n",
      "----------\n",
      "train Loss: 0.00033492 Acc: 0.9868608\n",
      "val Loss: 0.00027154 Acc: 0.97770701\n",
      "Epoch 27 of 500 took 0.196s\n",
      "Epoch 27/499\n",
      "----------\n",
      "train Loss: 0.00029847 Acc: 0.98757102\n",
      "val Loss: 0.00030031 Acc: 0.97452229\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0060e-04.\n",
      "Epoch 28 of 500 took 0.223s\n",
      "Epoch 28/499\n",
      "----------\n",
      "train Loss: 0.00027365 Acc: 0.99005682\n",
      "val Loss: 0.00026923 Acc: 0.97770701\n",
      "Epoch 29 of 500 took 0.258s\n",
      "Epoch 29/499\n",
      "----------\n",
      "train Loss: 0.00031086 Acc: 0.98721591\n",
      "val Loss: 0.00026586 Acc: 0.97452229\n",
      "Epoch 30 of 500 took 0.234s\n",
      "Epoch 30/499\n",
      "----------\n",
      "train Loss: 0.00032406 Acc: 0.98615057\n",
      "val Loss: 0.00025536 Acc: 0.98089172\n",
      "Epoch 31 of 500 took 0.196s\n",
      "Epoch 31/499\n",
      "----------\n",
      "train Loss: 0.00024364 Acc: 0.99041193\n",
      "val Loss: 0.00023914 Acc: 0.98089172\n",
      "Epoch 32 of 500 took 0.208s\n",
      "Epoch 32/499\n",
      "----------\n",
      "train Loss: 0.00025771 Acc: 0.99076705\n",
      "val Loss: 0.00026473 Acc: 0.98089172\n",
      "Epoch 33 of 500 took 0.207s\n",
      "\n",
      "Training complete in 0m 7s\n",
      "Best val loss: 0.000226\n"
     ]
    }
   ],
   "source": [
    "os.chdir(\"/home/laiy/gitrepos/msr_final/LongTermEMG-master\")\n",
    "from LongTermClassificationMain.Models.TSD_neural_network import TSD_Network\n",
    "from LongTermClassificationMain.TrainingsAndEvaluations.training_loops_preparations import load_checkpoint\n",
    "from LongTermClassificationMain.Models.model_training import train_model_standard\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "os.chdir(\"/home/laiy/gitrepos/msr_final/LongTermEMG-master/LongTermClassificationMain/TrainingsAndEvaluations/ForTrainingSessions/TSD_DNN\")\n",
    "\n",
    "for participant_i in range(len(participants_train)):\n",
    "    print(\"Participant: \", participant_i)\n",
    "    for session_j in range(0, len(participants_train[participant_i])):\n",
    "        print(\"Session: \", session_j)\n",
    "        # Define Model\n",
    "        model = TSD_Network(number_of_class=number_of_classes, num_neurons=num_kernels,\n",
    "                                feature_vector_input_length=feature_vector_input_length)\n",
    "\n",
    "        # Define Loss functions\n",
    "        cross_entropy_loss_classes = nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "        # Define Optimizer\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
    "\n",
    "        # Define Scheduler\n",
    "        precision = 1e-8\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, mode='min', factor=.2, patience=5,\n",
    "                                                         verbose=True, eps=precision)\n",
    "\n",
    "        if session_j > 0:\n",
    "            # Fine-tune from the previous training\n",
    "            model, _, _, start_epoch = load_checkpoint(\n",
    "                model=model, optimizer=None, scheduler=None,\n",
    "                filename=path_weight_to_save_to +\n",
    "                         \"/participant_%d/best_state_%d.pt\" %\n",
    "                         (participant_i, session_j - 1))\n",
    "\n",
    "        best_state = train_model_standard(model=model, criterion=cross_entropy_loss_classes, optimizer=optimizer,\n",
    "                                          scheduler=scheduler,\n",
    "                                          dataloaders={\"train\": participants_train[participant_i][session_j],\n",
    "                                                       \"val\": participants_validation[participant_i][session_j]},\n",
    "                                          precision=precision, patience=10, patience_increase=10)\n",
    "\n",
    "        if not os.path.exists(path_weight_to_save_to + \"/participant_%d\" % participant_i):\n",
    "            os.makedirs(path_weight_to_save_to + \"/participant_%d\" % participant_i)\n",
    "        torch.save(best_state, f=path_weight_to_save_to +\n",
    "                                 \"/participant_%d/best_state_%d.pt\"\n",
    "                                 % (participant_i, session_j))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on Train Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET one participant_examples  (4, 4)\n",
      "   GET one training_index_examples  (4,)  at  0\n",
      "      GET one examples_cycles  (1009, 385)  at  0\n",
      "      GET one examples_cycles  (1052, 385)  at  1\n",
      "      GET one examples_cycles  (949, 385)  at  2\n",
      "      GET one examples_cycles  (1014, 385)  at  3\n",
      "   GOT one group XY  (2061, 385)    (2061,)\n",
      "       one group XY test  (1014, 385)    (1014, 385)\n",
      "       one group XY train (1854, 385)    (1854,)\n",
      "       one group XY valid (207, 385)    (207, 385)\n",
      "   GET one training_index_examples  (4,)  at  1\n",
      "      GET one examples_cycles  (1064, 385)  at  0\n",
      "      GET one examples_cycles  (1046, 385)  at  1\n",
      "      GET one examples_cycles  (1063, 385)  at  2\n",
      "      GET one examples_cycles  (1017, 385)  at  3\n",
      "   GOT one group XY  (2110, 385)    (2110,)\n",
      "       one group XY test  (1017, 385)    (1017, 385)\n",
      "       one group XY train (1899, 385)    (1899,)\n",
      "       one group XY valid (211, 385)    (211, 385)\n",
      "   GET one training_index_examples  (4,)  at  2\n",
      "      GET one examples_cycles  (1067, 385)  at  0\n",
      "      GET one examples_cycles  (1087, 385)  at  1\n",
      "      GET one examples_cycles  (1067, 385)  at  2\n",
      "      GET one examples_cycles  (1068, 385)  at  3\n",
      "   GOT one group XY  (2154, 385)    (2154,)\n",
      "       one group XY test  (1068, 385)    (1068, 385)\n",
      "       one group XY train (1938, 385)    (1938,)\n",
      "       one group XY valid (216, 385)    (216, 385)\n",
      "   GET one training_index_examples  (4,)  at  3\n",
      "      GET one examples_cycles  (1014, 385)  at  0\n",
      "      GET one examples_cycles  (964, 385)  at  1\n",
      "      GET one examples_cycles  (1072, 385)  at  2\n",
      "      GET one examples_cycles  (1049, 385)  at  3\n",
      "   GOT one group XY  (1978, 385)    (1978,)\n",
      "       one group XY test  (1049, 385)    (1049, 385)\n",
      "       one group XY train (1780, 385)    (1780,)\n",
      "       one group XY valid (198, 385)    (198, 385)\n",
      "dataloaders: \n",
      "   train  (1, 4)\n",
      "   valid  (1, 4)\n",
      "   test  (1, 4)\n"
     ]
    }
   ],
   "source": [
    "cycle_for_test=3    # use third session for testing\n",
    "_, _, participants_test = load_dataloaders_training_sessions(examples_datasets_train, labels_datasets_train,\n",
    "                                                                 batch_size=512, cycle_for_test=cycle_for_test,\n",
    "                                                                 gestures_to_remove=gestures_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of last test dataloader for last participant  1049\n",
      "torch.Size([512, 385]) --- torch.Size([512])\n",
      "torch.Size([385]) --- tensor(5)\n"
     ]
    }
   ],
   "source": [
    "print(\"size of last test dataloader for last participant \", len(participants_test[-1][-1].dataset))\n",
    "\n",
    "dataiter = iter(participants_test[-1][-1])\n",
    "signals, labels = dataiter.next()\n",
    "\n",
    "print(signals.shape, '---', labels.shape)   # batch_size = 128*4 = 512\n",
    "print(signals[-1].shape, '---', labels[-1])\n",
    "\n",
    "# each dataloader corresponds to one model training session "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number Parameters:  162013\n",
      "0  SESSION   data =  1014\n",
      "   one batch  torch.Size([512])\n",
      "   one batch  torch.Size([502])\n",
      "Participant:  0  Accuracy:  0.9990138067061144\n",
      "1  SESSION   data =  1017\n",
      "   one batch  torch.Size([512])\n",
      "   one batch  torch.Size([505])\n",
      "Participant:  0  Accuracy:  0.6047197640117994\n",
      "2  SESSION   data =  1068\n",
      "   one batch  torch.Size([512])\n",
      "   one batch  torch.Size([512])\n",
      "   one batch  torch.Size([44])\n",
      "Participant:  0  Accuracy:  0.6713483146067416\n",
      "3  SESSION   data =  1049\n",
      "   one batch  torch.Size([512])\n",
      "   one batch  torch.Size([512])\n",
      "   one batch  torch.Size([25])\n",
      "Participant:  0  Accuracy:  0.5958055290753098\n",
      "ACCURACY PARTICIPANT  0 :  [0.9990138067061144, 0.6047197640117994, 0.6713483146067416, 0.5958055290753098]\n"
     ]
    }
   ],
   "source": [
    "num_neurons = [200, 200, 200]   \n",
    "use_only_first_training=True\n",
    "path_weights=path_weight_to_save_to\n",
    "algo_name = \"standard_TSD\"\n",
    "\"\"\"\n",
    "num_neurons (TSD model params)\n",
    "feature_vector_input_length: size of one example (=385)\n",
    "\n",
    "path_weights: where to load weights from\n",
    "algo_name: where to save results\n",
    "\n",
    "use_only_first_training: \n",
    "    load weights from first trainning session only if True; \n",
    "    otherwise use fine tunning weights (trained from all four sessions)\n",
    "cycle_for_test: which session to use for testing\n",
    "\"\"\"\n",
    "\n",
    "model_outputs = []\n",
    "predictions = []\n",
    "ground_truths = []\n",
    "accuracies = []\n",
    "\n",
    "for participant_index, dataset_test in enumerate(participants_test):\n",
    "    model_outputs_participant = []\n",
    "    predictions_participant = []\n",
    "    ground_truth_participant = []\n",
    "    accuracies_participant = []\n",
    "    \n",
    "    # get model\n",
    "    model = TSD_Network(number_of_class=number_of_classes, feature_vector_input_length=feature_vector_input_length,\n",
    "                        num_neurons=num_neurons)\n",
    "    \n",
    "    # start test\n",
    "    for session_index, training_session_test_data in enumerate(dataset_test):\n",
    "        print(session_index, \" SESSION   data = \", len(training_session_test_data.dataset))\n",
    "        if use_only_first_training:\n",
    "            best_state = torch.load(\n",
    "                path_weights + \"/participant_%d/best_state_%d.pt\" %\n",
    "                (participant_index, 0))\n",
    "        else:\n",
    "            best_state = torch.load(\n",
    "                path_weights + \"/participant_%d/best_state_%d.pt\" %\n",
    "                (participant_index, session_index))\n",
    "        best_weights = best_state['state_dict']\n",
    "        \n",
    "        # load trained model\n",
    "        model.load_state_dict(best_weights)\n",
    "        \n",
    "        # compare prediction and ground truth\n",
    "        predictions_training_session = []\n",
    "        ground_truth_training_sesssion = []\n",
    "        model_outputs_session = []\n",
    "        with torch.no_grad():\n",
    "            # https://stackoverflow.com/questions/60018578/what-does-model-eval-do-in-pytorch\n",
    "            model.eval()\n",
    "            for inputs, labels in training_session_test_data:\n",
    "                inputs = inputs\n",
    "                output = model(inputs)\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "                model_outputs_session.extend(torch.softmax(output, dim=1).cpu().numpy())\n",
    "                predictions_training_session.extend(predicted.cpu().numpy())\n",
    "                ground_truth_training_sesssion.extend(labels.numpy())\n",
    "                print(\"   one batch \", predicted.shape)\n",
    "        print(\"Participant: \", participant_index, \" Accuracy: \",\n",
    "              np.mean(np.array(predictions_training_session) == np.array(ground_truth_training_sesssion)))\n",
    "        predictions_participant.append(predictions_training_session)\n",
    "        model_outputs_participant.append(model_outputs_session)\n",
    "        ground_truth_participant.append(ground_truth_training_sesssion)\n",
    "        accuracies_participant.append(np.mean(np.array(predictions_training_session) ==\n",
    "                                              np.array(ground_truth_training_sesssion)))\n",
    "    accuracies.append(np.array(accuracies_participant))\n",
    "    predictions.append(predictions_participant)\n",
    "    model_outputs.append(model_outputs_participant)\n",
    "    ground_truths.append(ground_truth_participant)\n",
    "    print(\"ACCURACY PARTICIPANT \", participant_index, \": \", accuracies_participant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9990138067061144, 0.6047197640117994, 0.6713483146067416, 0.5958055290753098]\n",
      "OVERALL ACCURACY: 0.7177218535999913\n"
     ]
    }
   ],
   "source": [
    "accuracies_to_display = []\n",
    "for accuracies_from_participant in np.array(accuracies).flatten():\n",
    "    accuracies_to_display.append(accuracies_from_participant)\n",
    "print(accuracies_to_display)\n",
    "print(\"OVERALL ACCURACY: \" + str(np.mean(accuracies_to_display)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_to_open = \"results_tsd/\" + algo_name + \"_no_retraining.txt\"\n",
    "np.save(\"results_tsd/predictions_\" + algo_name + \"_no_retraining\", np.array((ground_truths, \n",
    "                                                                            predictions, \n",
    "                                                                            model_outputs), dtype=object))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Session_0</th>\n",
       "      <th>Session_1</th>\n",
       "      <th>Session_2</th>\n",
       "      <th>Session_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Participant_0</th>\n",
       "      <td>0.999014</td>\n",
       "      <td>0.60472</td>\n",
       "      <td>0.671348</td>\n",
       "      <td>0.595806</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Session_0  Session_1  Session_2  Session_3\n",
       "Participant_0   0.999014    0.60472   0.671348   0.595806"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_filename = \"results_tsd/predictions_\" + algo_name + \"_no_retraining.npy\"\n",
    "results = np.load(results_filename, allow_pickle=True)\n",
    "ground_truths = results[0]\n",
    "predictions = results[1]\n",
    "\n",
    "TSD_acc = np.zeros(ground_truths.shape)\n",
    "for i, ground in np.ndenumerate(ground_truths):\n",
    "    acc = np.mean(np.array(ground) == np.array(predictions[i]))\n",
    "    TSD_acc[i] = acc\n",
    "TSD_acc_overall = np.mean(TSD_acc)\n",
    "TSD_df = pd.DataFrame(TSD_acc, \n",
    "                       columns = [f'Session_{i}' for i in range(ground_truths.shape[1])],\n",
    "                        index = [f'Participant_{j}' for j in range(ground_truths.shape[0])])\n",
    "TSD_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "TSD_df.transpose().plot.bar(rot=0, figsize=(10,5))\n",
    "plt.title(\"TSD Accuracies\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
