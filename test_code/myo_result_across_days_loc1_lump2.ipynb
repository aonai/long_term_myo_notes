{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results of TSD, DANN, SCADANN models across 10 days of inward rotation starting at Day_0~1 for Subject_4\n",
    "\n",
    "Library used can be downloaded from https://github.com/aonai/long_term_EMG_myo   \n",
    "&emsp; Original by UlysseCoteAllard https://github.com/UlysseCoteAllard/LongTermEMG   \n",
    "Dataset recorded by https://github.com/Suguru55/Wearable_Sensor_Long-term_sEMG_Dataset   \n",
    "Extended robot project can be found in https://github.com/aonai/myo_robot_arm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "* weights for TSD are total of 50 training models, 10 for each day\n",
    "* weights for DANN and SCADANN are total of 45 trianing models, 9 for each day\n",
    "\n",
    "\n",
    "* training examples should have shape (1, 9,)\n",
    "* first session has shape (8, 572, 252)\n",
    "* the following sessions have shape (4, 572, 252)\n",
    "* training labels should have shape (1, 9,)\n",
    "\n",
    "\n",
    "* location 0, 1, and 2 corresponds to neutral position, inward rotation, and outward rotation respectively\n",
    "* session mentioned below are days, so number of sessions is 10\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "import torch\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_dir = \"/home/laiy/gitrepos/msr_final/LongTermEMG_myo\"\n",
    "os.chdir(code_dir)\n",
    "from PrepareAndLoadData.process_data import read_data_training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Prepare Data\n",
    "use `switch=2` to train across days and individually on wearing location 0 (`session_in_include=[0]`)\n",
    "\n",
    "### specify the directories used for running the code:\n",
    "* `code_diar`: path to long_term_EMG_myo library\n",
    "* `data_dir`: where raw dataset is loaded; raw data is in csv format\n",
    "* `processed_data_dir`: where processed dataset is loaded; processed data is in npy pickle format\n",
    "    * processed data should be a ndarray of shape   \n",
    "    (controlling_factor_1 x controlling_factor_2 x num_sessions_per_gesture x #examples_window*#mov(26*22=572) x processed_channel_shape(252 for TSD, (4,8,10) for ConvNet)\n",
    "* `path_<model_name>`: where model weights are saved\n",
    "    * weights should be saved in folder `/Weights/<model_name>`. Each folder has subfolders containing weights for the first controlling factor.\n",
    "    * weights for base model (TSD or ConvNet) contain m set of training model\n",
    "    * weights for DANN and SCADANN contain m-1 set of trianing model (these models are trianed based on TSD, so they do not have a best_state_0.pt model). \n",
    "* `save_<model_name>`: where model results are saved\n",
    "    * each result for testing a model on a group of dataset is saved in folder `results`. Each result has corresponding \n",
    "        * `<model_name>.txt` includes predictions, ground truths, array of accuracies for each participant and each session, and overall accuracy\n",
    "        * `predictions_<model_name>.npy` includes array of accuracies, ground truths, predictions, and model outputs (probability array for each prediction)\n",
    "        * remember to make blank files in these names before saving\n",
    "\n",
    "\n",
    "\n",
    "* use `read_data_training` to process raw dataset\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/home/laiy/gitrepos/msr_final/Wearable_Sensor_Long-term_sEMG_Dataset/data\"\n",
    "processed_data_dir = \"/home/laiy/gitrepos/msr_final/LongTermEMG_myo/Processed_datasets_all_across_day_loc_1_lump2\"\n",
    "code_dir = \"/home/laiy/gitrepos/msr_final/LongTermEMG_myo\"\n",
    "save_dir = \"/home/laiy/gitrepos/msr_final/LongTermEMG_myo/Results\"\n",
    "\n",
    "path_TSD =\"/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/TSD\"\n",
    "save_TSD = \"/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/results\"\n",
    "\n",
    "path_DANN =\"/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/DANN\"\n",
    "save_DANN = \"/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/results\"\n",
    "\n",
    "path_SCADANN =\"/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/SCADANN\"\n",
    "save_SCADANN = \"/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# read_data_training(path=data_dir, store_path = processed_data_dir,  \n",
    "#                    sessions_to_include =[1], switch=2, include_in_first=2,\n",
    "#                    start_at_participant=5, num_participant=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traning examples  (1, 9)\n",
      "traning labels  (1, 9)\n"
     ]
    }
   ],
   "source": [
    "# check stored pickle \n",
    "with open(processed_data_dir + \"/training_session.pickle\", 'rb') as f:\n",
    "    dataset_training = pickle.load(file=f)\n",
    "\n",
    "examples_datasets_train = dataset_training['examples_training']\n",
    "print('traning examples ', np.shape(examples_datasets_train))\n",
    "labels_datasets_train = dataset_training['labels_training']\n",
    "print('traning labels ', np.shape(labels_datasets_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  examples_per_session =  (8, 572, 252)\n",
      "0  labels_per_session =  (8, 572)\n",
      "1  examples_per_session =  (4, 572, 252)\n",
      "1  labels_per_session =  (4, 572)\n",
      "2  examples_per_session =  (4, 572, 252)\n",
      "2  labels_per_session =  (4, 572)\n",
      "3  examples_per_session =  (4, 572, 252)\n",
      "3  labels_per_session =  (4, 572)\n",
      "4  examples_per_session =  (4, 572, 252)\n",
      "4  labels_per_session =  (4, 572)\n",
      "5  examples_per_session =  (4, 572, 252)\n",
      "5  labels_per_session =  (4, 572)\n",
      "6  examples_per_session =  (4, 572, 252)\n",
      "6  labels_per_session =  (4, 572)\n",
      "7  examples_per_session =  (4, 572, 252)\n",
      "7  labels_per_session =  (4, 572)\n",
      "8  examples_per_session =  (4, 572, 252)\n",
      "8  labels_per_session =  (4, 572)\n"
     ]
    }
   ],
   "source": [
    "for idx, examples_per_session in enumerate (examples_datasets_train[0]):\n",
    "    print(idx, \" examples_per_session = \", np.shape(examples_per_session))\n",
    "    print(idx, \" labels_per_session = \", np.shape(labels_datasets_train[0][idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify params used for training and testing\n",
    "\n",
    "During training and testing, processed datasets are first put into pytorch dataloders, then feed to the model trainer; following are params for TSD model and dataloaders\n",
    "\n",
    "* `num_kernels`: list of integers defining number of neurons used in each linear layer (linear block has `dropout`=0.5)\n",
    "* `number_of_cycles_total`: number of trails performed for each session (assuming that all session have the same trail size)\n",
    "    * 4 for myo across day training\n",
    "* `number_of_classes`: total number of gestures performed in dataset\n",
    "    * 22 for myo\n",
    "* `batch_size`: number of examples stored in each batch\n",
    "* `feature_vector_input_length`: length of input array or each processed signal; i.e. size of one training example \n",
    "    * 252 for TSD\n",
    "* `learning_rate`= 0.002515\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number_of_cycle_for_first_training  8\n",
      "number_of_cycles_total  4\n"
     ]
    }
   ],
   "source": [
    "num_kernels=[200, 200, 200]                                \n",
    "number_of_cycle_for_first_training = np.shape(examples_datasets_train[0][0])[0]               \n",
    "number_of_cycles_total=np.shape(examples_datasets_train[-1][-1])[0]               \n",
    "print(\"number_of_cycle_for_first_training \", number_of_cycle_for_first_training)\n",
    "print(\"number_of_cycles_total \", number_of_cycles_total)\n",
    "number_of_classes=22\n",
    "batch_size=128          \n",
    "feature_vector_input_length=252                     \n",
    "learning_rate=0.002515"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. TSD_DNN\n",
    "* `train_fine_tuning`: used to train data using a base model (TSD or ConvNet)\n",
    "    * running this function will save num_sessions sets of TSD model weights (each is fine tuned based on the previous training)  \n",
    "    \n",
    "* `test_standard_model_on_training_sessions`: test model result\n",
    "\n",
    "\n",
    "### check if dataloaders are loaded correctly:\n",
    "* each participant has shape (num_session x 40 x 572 x 252)\n",
    "* each session has shape (40 x 572 x 252)\n",
    "* put these data into on group ends up with shape (40*572=22880, 252)\n",
    "    * shuffle on group of data and put into dataloaders\n",
    "    * each participant should have num_sessions sets of dataloaders, each correspond to one session\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TrainingsAndEvaluations.ForTrainingSessions.train_tsd_dnn_standard import \\\n",
    "            test_standard_model_on_training_sessions, train_fine_tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_fine_tuning(examples_datasets_train, labels_datasets_train,\n",
    "#                   num_kernels=num_kernels, path_weight_to_save_to=path_TSD,\n",
    "#                   number_of_classes=number_of_classes, \n",
    "#                   number_of_cycles_total=number_of_cycles_total,\n",
    "#                   number_of_cycle_for_first_training = number_of_cycle_for_first_training,\n",
    "#                   batch_size=batch_size,\n",
    "#                   feature_vector_input_length=feature_vector_input_length,\n",
    "#                   learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET one participant_examples  (9,)\n",
      "   GET one training_index_examples  (8, 572, 252)  at  0\n",
      "   GOT one group XY  (4576, 252)    (4576,)\n",
      "       one group XY test  (1144, 252)    (1144, 252)\n",
      "       one group XY train (4118, 252)    (4118,)\n",
      "       one group XY valid (458, 252)    (458, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  1\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  2\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  3\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  4\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  5\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  6\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  7\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  8\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "dataloaders: \n",
      "   train  (1, 9)\n",
      "   valid  (1, 9)\n",
      "   test  (1, 9)\n",
      "0  SESSION   data =  1144\n",
      "Participant:  0  Accuracy:  0.9527972027972028\n",
      "1  SESSION   data =  572\n",
      "Participant:  0  Accuracy:  0.8251748251748252\n",
      "2  SESSION   data =  572\n",
      "Participant:  0  Accuracy:  0.6800699300699301\n",
      "3  SESSION   data =  572\n",
      "Participant:  0  Accuracy:  0.6993006993006993\n",
      "4  SESSION   data =  572\n",
      "Participant:  0  Accuracy:  0.5734265734265734\n",
      "5  SESSION   data =  572\n",
      "Participant:  0  Accuracy:  0.6048951048951049\n",
      "6  SESSION   data =  572\n",
      "Participant:  0  Accuracy:  0.40034965034965037\n",
      "7  SESSION   data =  572\n",
      "Participant:  0  Accuracy:  0.4458041958041958\n",
      "8  SESSION   data =  572\n",
      "Participant:  0  Accuracy:  0.40734265734265734\n",
      "ACCURACY PARTICIPANT  0 :  [0.9527972027972028, 0.8251748251748252, 0.6800699300699301, 0.6993006993006993, 0.5734265734265734, 0.6048951048951049, 0.40034965034965037, 0.4458041958041958, 0.40734265734265734]\n",
      "[array([0.9527972 , 0.82517483, 0.68006993, 0.6993007 , 0.57342657,\n",
      "       0.6048951 , 0.40034965, 0.4458042 , 0.40734266])]\n",
      "OVERALL ACCURACY: 0.621017871017871\n"
     ]
    }
   ],
   "source": [
    "algo_name = \"standard_TSD\"\n",
    "test_standard_model_on_training_sessions(examples_datasets_train, labels_datasets_train,\n",
    "                                  num_neurons=num_kernels, use_only_first_training=True,\n",
    "                                  path_weights=path_TSD,\n",
    "                                  feature_vector_input_length=feature_vector_input_length,\n",
    "                                  save_path = save_TSD, algo_name=algo_name,\n",
    "                                  number_of_cycles_total=number_of_cycles_total,\n",
    "                                  number_of_cycle_for_first_training = number_of_cycle_for_first_training,\n",
    "                                  number_of_classes=number_of_classes, cycle_for_test=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Day_0~1</th>\n",
       "      <td>0.952797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_2</th>\n",
       "      <td>0.825175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_3</th>\n",
       "      <td>0.68007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_4</th>\n",
       "      <td>0.699301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_5</th>\n",
       "      <td>0.573427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_6</th>\n",
       "      <td>0.604895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_7</th>\n",
       "      <td>0.40035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_8</th>\n",
       "      <td>0.445804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_9</th>\n",
       "      <td>0.407343</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Participant_5\n",
       "Day_0~1      0.952797\n",
       "Day_2        0.825175\n",
       "Day_3         0.68007\n",
       "Day_4        0.699301\n",
       "Day_5        0.573427\n",
       "Day_6        0.604895\n",
       "Day_7         0.40035\n",
       "Day_8        0.445804\n",
       "Day_9        0.407343"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_filename = save_TSD + '/predictions_' + algo_name + \"_no_retraining.npy\"\n",
    "results = np.load(results_filename, allow_pickle=True)\n",
    "TSD_acc = results[0]\n",
    "TSD_acc_overall = np.mean(TSD_acc)\n",
    "index_participant_list = ['0~1', 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "TSD_df = pd.DataFrame(TSD_acc.transpose(), \n",
    "                       index = [f'Day_{i}' for i in index_participant_list],\n",
    "                        columns = ['Participant_5'])\n",
    "TSD_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "TSD_df.transpose().plot.bar(rot=0, figsize=(10,5))\n",
    "plt.title(\"TSD Accuracies\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TrainingsAndEvaluations.ForTrainingSessions.utils import get_gesture_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ground_truths  =  (1, 9)\n",
      "predictions =  (1, 9)\n",
      "index_participant_list  ['0~1', 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "accuracies_gestures =  (22, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Loc1_Sub5_Day0~1-&gt;0~1</th>\n",
       "      <th>Loc1_Sub5_Day0~1-&gt;2</th>\n",
       "      <th>Loc1_Sub5_Day0~1-&gt;3</th>\n",
       "      <th>Loc1_Sub5_Day0~1-&gt;4</th>\n",
       "      <th>Loc1_Sub5_Day0~1-&gt;5</th>\n",
       "      <th>Loc1_Sub5_Day0~1-&gt;6</th>\n",
       "      <th>Loc1_Sub5_Day0~1-&gt;7</th>\n",
       "      <th>Loc1_Sub5_Day0~1-&gt;8</th>\n",
       "      <th>Loc1_Sub5_Day0~1-&gt;9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M1</td>\n",
       "      <td>0.980769</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>0.384615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M2</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.384615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M4</td>\n",
       "      <td>0.788462</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>M5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>0.269231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>M6</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.769231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>M7</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>0.653846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>M8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>M9</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.576923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>M10</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>M11</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>M12</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.076923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>M13</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.346154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>M14</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>M15</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>M16</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>M17</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>M18</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.192308</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.423077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>M19</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>M20</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.884615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>M21</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.346154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Mean</td>\n",
       "      <td>0.952797</td>\n",
       "      <td>0.825175</td>\n",
       "      <td>0.680070</td>\n",
       "      <td>0.699301</td>\n",
       "      <td>0.573427</td>\n",
       "      <td>0.604895</td>\n",
       "      <td>0.400350</td>\n",
       "      <td>0.445804</td>\n",
       "      <td>0.407343</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Loc1_Sub5_Day0~1->0~1  Loc1_Sub5_Day0~1->2  \\\n",
       "0          M0               1.000000             1.000000   \n",
       "1          M1               0.980769             1.000000   \n",
       "2          M2               0.961538             0.576923   \n",
       "3          M3               1.000000             0.807692   \n",
       "4          M4               0.788462             0.846154   \n",
       "5          M5               1.000000             0.961538   \n",
       "6          M6               1.000000             1.000000   \n",
       "7          M7               1.000000             0.538462   \n",
       "8          M8               1.000000             0.769231   \n",
       "9          M9               1.000000             0.961538   \n",
       "10        M10               0.884615             1.000000   \n",
       "11        M11               1.000000             0.807692   \n",
       "12        M12               0.846154             0.692308   \n",
       "13        M13               0.846154             0.730769   \n",
       "14        M14               0.884615             0.384615   \n",
       "15        M15               0.846154             0.807692   \n",
       "16        M16               1.000000             1.000000   \n",
       "17        M17               1.000000             1.000000   \n",
       "18        M18               1.000000             1.000000   \n",
       "19        M19               1.000000             1.000000   \n",
       "20        M20               0.923077             0.653846   \n",
       "21        M21               1.000000             0.615385   \n",
       "22       Mean               0.952797             0.825175   \n",
       "\n",
       "    Loc1_Sub5_Day0~1->3  Loc1_Sub5_Day0~1->4  Loc1_Sub5_Day0~1->5  \\\n",
       "0              1.000000             1.000000             1.000000   \n",
       "1              0.384615             0.769231             0.961538   \n",
       "2              0.538462             0.769231             0.500000   \n",
       "3              0.038462             0.423077             0.038462   \n",
       "4              0.000000             0.153846             0.000000   \n",
       "5              0.846154             1.000000             0.846154   \n",
       "6              0.230769             1.000000             0.576923   \n",
       "7              1.000000             0.961538             0.615385   \n",
       "8              1.000000             1.000000             1.000000   \n",
       "9              0.769231             0.807692             0.692308   \n",
       "10             0.153846             0.538462             0.576923   \n",
       "11             0.807692             0.076923             0.000000   \n",
       "12             0.769231             0.461538             0.538462   \n",
       "13             0.961538             0.923077             1.000000   \n",
       "14             0.076923             0.038462             0.038462   \n",
       "15             0.730769             0.000000             0.038462   \n",
       "16             1.000000             1.000000             0.615385   \n",
       "17             1.000000             0.961538             1.000000   \n",
       "18             1.000000             0.923077             0.500000   \n",
       "19             0.961538             0.923077             0.307692   \n",
       "20             1.000000             0.923077             0.923077   \n",
       "21             0.692308             0.730769             0.846154   \n",
       "22             0.680070             0.699301             0.573427   \n",
       "\n",
       "    Loc1_Sub5_Day0~1->6  Loc1_Sub5_Day0~1->7  Loc1_Sub5_Day0~1->8  \\\n",
       "0              1.000000             1.000000             1.000000   \n",
       "1              0.500000             0.153846             0.269231   \n",
       "2              0.461538             0.000000             0.461538   \n",
       "3              0.000000             0.000000             0.000000   \n",
       "4              0.000000             0.000000             0.000000   \n",
       "5              0.884615             0.000000             0.653846   \n",
       "6              0.384615             0.230769             0.115385   \n",
       "7              1.000000             0.730769             0.653846   \n",
       "8              1.000000             1.000000             1.000000   \n",
       "9              0.884615             0.730769             0.769231   \n",
       "10             0.269231             0.000000             0.038462   \n",
       "11             0.538462             0.000000             0.038462   \n",
       "12             0.269231             0.153846             0.000000   \n",
       "13             1.000000             0.846154             1.000000   \n",
       "14             0.038462             0.000000             0.000000   \n",
       "15             0.000000             0.000000             0.000000   \n",
       "16             1.000000             0.961538             1.000000   \n",
       "17             1.000000             1.000000             1.000000   \n",
       "18             0.461538             0.192308             0.076923   \n",
       "19             0.884615             0.038462             0.038462   \n",
       "20             0.769231             0.961538             0.769231   \n",
       "21             0.961538             0.807692             0.923077   \n",
       "22             0.604895             0.400350             0.445804   \n",
       "\n",
       "    Loc1_Sub5_Day0~1->9  \n",
       "0              1.000000  \n",
       "1              0.384615  \n",
       "2              0.384615  \n",
       "3              0.000000  \n",
       "4              0.000000  \n",
       "5              0.269231  \n",
       "6              0.769231  \n",
       "7              0.653846  \n",
       "8              1.000000  \n",
       "9              0.576923  \n",
       "10             0.000000  \n",
       "11             0.000000  \n",
       "12             0.076923  \n",
       "13             0.346154  \n",
       "14             0.000000  \n",
       "15             0.000000  \n",
       "16             0.846154  \n",
       "17             1.000000  \n",
       "18             0.423077  \n",
       "19             0.000000  \n",
       "20             0.884615  \n",
       "21             0.346154  \n",
       "22             0.407343  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truths = results[1]\n",
    "predictions = results[2]\n",
    "print(\"ground_truths  = \", np.shape(ground_truths))\n",
    "print(\"predictions = \", np.shape(predictions))\n",
    "m_name = \"Loc1_Sub\"\n",
    "n_name = \"Day0~1->\"\n",
    "df = get_gesture_accuracies(ground_truths, predictions, number_of_classes=number_of_classes, \n",
    "                            m_name=m_name, n_name=n_name, path=save_TSD, algo_name=algo_name,\n",
    "                           index_participant_list_customized=index_participant_list,\n",
    "                           lump_day_at_participant=5)\n",
    "df = pd.read_csv(save_TSD+'/'+algo_name+'.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. DANN\n",
    "* `train_DANN`: train DANN model using the first set of training weights from base model\n",
    "    * num_sessions-1 sets of training weights will be saved\n",
    "* `test_DANN_on_training_sessions`: test DANN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TrainingsAndEvaluations.ForTrainingSessions.train_tsd_dnn_DA import train_DANN, test_DANN_on_training_sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET one participant_examples  (9,)\n",
      "   GET one training_index_examples  (8, 572, 252)  at  0\n",
      "   GOT one group XY  (4576, 252)    (4576,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (4118, 252)    (4118,)\n",
      "       one group XY valid (458, 252)    (458, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  1\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  2\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  3\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  4\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  5\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  6\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  7\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  8\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "dataloaders: \n",
      "   train  (1, 9)\n",
      "   valid  (1, 9)\n",
      "   test  (1, 0)\n",
      "SHAPE SESSIONS:  (9,)\n",
      "()\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/TSD/participant_0/best_state_0.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/TSD/participant_0/best_state_0.pt' (epoch 27)\n",
      "STARTING TRAINING\n",
      "Accuracy source 0.924805, main loss classifier 0.184310, source classification loss 0.203905, loss domain distinction 0.468698, accuracy domain distinction 0.492432\n",
      "VALIDATION Loss: 0.09826083 Acc: 0.96724891\n",
      "New best validation loss:  0.09826082736253738\n",
      "Epoch 1 of 500 took 0.262s\n",
      "Accuracy source 0.925293, main loss classifier 0.174062, source classification loss 0.195985, loss domain distinction 0.248058, accuracy domain distinction 0.498047\n",
      "VALIDATION Loss: 0.10361965 Acc: 0.96069869\n",
      "Epoch 2 of 500 took 0.275s\n",
      "Accuracy source 0.931641, main loss classifier 0.168773, source classification loss 0.188221, loss domain distinction 0.206954, accuracy domain distinction 0.499023\n",
      "VALIDATION Loss: 0.11433166 Acc: 0.95633188\n",
      "Epoch 3 of 500 took 0.248s\n",
      "Accuracy source 0.930664, main loss classifier 0.170018, source classification loss 0.192146, loss domain distinction 0.200012, accuracy domain distinction 0.499512\n",
      "VALIDATION Loss: 0.09743787 Acc: 0.96069869\n",
      "New best validation loss:  0.09743787348270416\n",
      "Epoch 4 of 500 took 0.304s\n",
      "Accuracy source 0.934570, main loss classifier 0.168156, source classification loss 0.189354, loss domain distinction 0.196384, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.08310597 Acc: 0.96724891\n",
      "New best validation loss:  0.08310596644878387\n",
      "Epoch 5 of 500 took 0.375s\n",
      "Accuracy source 0.939941, main loss classifier 0.158369, source classification loss 0.172362, loss domain distinction 0.193556, accuracy domain distinction 0.500244\n",
      "VALIDATION Loss: 0.09199450 Acc: 0.9628821\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0060e-04.\n",
      "Epoch 6 of 500 took 0.432s\n",
      "Accuracy source 0.939453, main loss classifier 0.161765, source classification loss 0.180330, loss domain distinction 0.190152, accuracy domain distinction 0.500244\n",
      "VALIDATION Loss: 0.09536783 Acc: 0.96069869\n",
      "Epoch 7 of 500 took 0.410s\n",
      "Accuracy source 0.933105, main loss classifier 0.163137, source classification loss 0.183079, loss domain distinction 0.190696, accuracy domain distinction 0.499756\n",
      "VALIDATION Loss: 0.11863700 Acc: 0.96069869\n",
      "Epoch 8 of 500 took 0.279s\n",
      "Accuracy source 0.939453, main loss classifier 0.157096, source classification loss 0.172136, loss domain distinction 0.188946, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.09594692 Acc: 0.96069869\n",
      "Epoch 9 of 500 took 0.282s\n",
      "Accuracy source 0.941406, main loss classifier 0.154118, source classification loss 0.164393, loss domain distinction 0.194116, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.08298697 Acc: 0.96943231\n",
      "New best validation loss:  0.08298696577548981\n",
      "Epoch 10 of 500 took 0.354s\n",
      "Accuracy source 0.937012, main loss classifier 0.156931, source classification loss 0.171893, loss domain distinction 0.188741, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.10762484 Acc: 0.95633188\n",
      "Epoch 11 of 500 took 0.275s\n",
      "Accuracy source 0.945801, main loss classifier 0.151473, source classification loss 0.160636, loss domain distinction 0.192536, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.11451600 Acc: 0.94978166\n",
      "Epoch 12 of 500 took 0.420s\n",
      "Accuracy source 0.937500, main loss classifier 0.163358, source classification loss 0.183321, loss domain distinction 0.191327, accuracy domain distinction 0.499512\n",
      "VALIDATION Loss: 0.09996275 Acc: 0.96724891\n",
      "Epoch 13 of 500 took 0.335s\n",
      "Accuracy source 0.939453, main loss classifier 0.156956, source classification loss 0.170444, loss domain distinction 0.190576, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.08696846 Acc: 0.96724891\n",
      "Epoch 14 of 500 took 0.279s\n",
      "Accuracy source 0.936035, main loss classifier 0.163818, source classification loss 0.185307, loss domain distinction 0.188776, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.10218544 Acc: 0.96069869\n",
      "Epoch 15 of 500 took 0.271s\n",
      "Accuracy source 0.941895, main loss classifier 0.150844, source classification loss 0.159211, loss domain distinction 0.192541, accuracy domain distinction 0.500244\n",
      "VALIDATION Loss: 0.08080689 Acc: 0.9650655\n",
      "New best validation loss:  0.08080688863992691\n",
      "Epoch 16 of 500 took 0.443s\n",
      "Accuracy source 0.938477, main loss classifier 0.158424, source classification loss 0.173669, loss domain distinction 0.191238, accuracy domain distinction 0.500244\n",
      "VALIDATION Loss: 0.07991531 Acc: 0.96943231\n",
      "New best validation loss:  0.07991531491279602\n",
      "Epoch 17 of 500 took 0.301s\n",
      "Accuracy source 0.947754, main loss classifier 0.152194, source classification loss 0.162992, loss domain distinction 0.185785, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.10095334 Acc: 0.9628821\n",
      "Epoch 18 of 500 took 0.290s\n",
      "Accuracy source 0.950195, main loss classifier 0.144169, source classification loss 0.147199, loss domain distinction 0.184927, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.07193633 Acc: 0.97161572\n",
      "New best validation loss:  0.07193633168935776\n",
      "Epoch 19 of 500 took 0.293s\n",
      "Accuracy source 0.943848, main loss classifier 0.151534, source classification loss 0.160171, loss domain distinction 0.190633, accuracy domain distinction 0.500244\n",
      "VALIDATION Loss: 0.11774543 Acc: 0.95633188\n",
      "Epoch 20 of 500 took 0.303s\n",
      "Accuracy source 0.944824, main loss classifier 0.153375, source classification loss 0.165315, loss domain distinction 0.189769, accuracy domain distinction 0.500244\n",
      "VALIDATION Loss: 0.11462505 Acc: 0.95851528\n",
      "Epoch 21 of 500 took 0.377s\n",
      "Accuracy source 0.946289, main loss classifier 0.149786, source classification loss 0.157273, loss domain distinction 0.187546, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.08428671 Acc: 0.96943231\n",
      "Epoch 22 of 500 took 0.367s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy source 0.931641, main loss classifier 0.163203, source classification loss 0.185883, loss domain distinction 0.185558, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.07425328 Acc: 0.97161572\n",
      "Epoch 23 of 500 took 0.426s\n",
      "Accuracy source 0.937012, main loss classifier 0.159712, source classification loss 0.178805, loss domain distinction 0.187798, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.10240833 Acc: 0.95851528\n",
      "Epoch 24 of 500 took 0.317s\n",
      "Accuracy source 0.947266, main loss classifier 0.149247, source classification loss 0.157248, loss domain distinction 0.187769, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.11176700 Acc: 0.95633188\n",
      "Epoch 25 of 500 took 0.455s\n",
      "Accuracy source 0.944824, main loss classifier 0.153602, source classification loss 0.166413, loss domain distinction 0.186134, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.09110668 Acc: 0.9628821\n",
      "Epoch 26 of 500 took 0.348s\n",
      "Accuracy source 0.944824, main loss classifier 0.145241, source classification loss 0.149727, loss domain distinction 0.187560, accuracy domain distinction 0.499756\n",
      "VALIDATION Loss: 0.10023161 Acc: 0.9628821\n",
      "Epoch 27 of 500 took 0.405s\n",
      "Accuracy source 0.939453, main loss classifier 0.155291, source classification loss 0.168553, loss domain distinction 0.190442, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.08014490 Acc: 0.96724891\n",
      "Epoch 28 of 500 took 0.285s\n",
      "Accuracy source 0.945801, main loss classifier 0.153394, source classification loss 0.166591, loss domain distinction 0.186948, accuracy domain distinction 0.499756\n",
      "VALIDATION Loss: 0.07624138 Acc: 0.96943231\n",
      "Epoch 29 of 500 took 0.241s\n",
      "Accuracy source 0.941895, main loss classifier 0.148201, source classification loss 0.156718, loss domain distinction 0.185192, accuracy domain distinction 0.500244\n",
      "VALIDATION Loss: 0.09120568 Acc: 0.95414847\n",
      "Training complete in 0m 10s\n",
      "()\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/TSD/participant_0/best_state_0.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/TSD/participant_0/best_state_0.pt' (epoch 27)\n",
      "STARTING TRAINING\n",
      "Accuracy source 0.937988, main loss classifier 0.181426, source classification loss 0.197475, loss domain distinction 0.476189, accuracy domain distinction 0.485352\n",
      "VALIDATION Loss: 0.17035869 Acc: 0.92576419\n",
      "New best validation loss:  0.17035868763923645\n",
      "Epoch 1 of 500 took 0.286s\n",
      "Accuracy source 0.919434, main loss classifier 0.182176, source classification loss 0.214295, loss domain distinction 0.242387, accuracy domain distinction 0.495605\n",
      "VALIDATION Loss: 0.14110386 Acc: 0.93231441\n",
      "New best validation loss:  0.1411038637161255\n",
      "Epoch 2 of 500 took 0.260s\n",
      "Accuracy source 0.934082, main loss classifier 0.167627, source classification loss 0.186550, loss domain distinction 0.208347, accuracy domain distinction 0.499023\n",
      "VALIDATION Loss: 0.22023743 Acc: 0.90829694\n",
      "Epoch 3 of 500 took 0.421s\n",
      "Accuracy source 0.932129, main loss classifier 0.169466, source classification loss 0.190457, loss domain distinction 0.202706, accuracy domain distinction 0.499512\n",
      "VALIDATION Loss: 0.16594154 Acc: 0.92139738\n",
      "Epoch 4 of 500 took 0.391s\n",
      "Accuracy source 0.926270, main loss classifier 0.169474, source classification loss 0.193360, loss domain distinction 0.194598, accuracy domain distinction 0.499512\n",
      "VALIDATION Loss: 0.14629319 Acc: 0.94104803\n",
      "Epoch 5 of 500 took 0.288s\n",
      "Accuracy source 0.940430, main loss classifier 0.158174, source classification loss 0.170375, loss domain distinction 0.195519, accuracy domain distinction 0.500488\n",
      "VALIDATION Loss: 0.13094875 Acc: 0.94323144\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0060e-04.\n",
      "New best validation loss:  0.1309487521648407\n",
      "Epoch 6 of 500 took 0.295s\n",
      "Accuracy source 0.934570, main loss classifier 0.165305, source classification loss 0.187023, loss domain distinction 0.190443, accuracy domain distinction 0.500732\n",
      "VALIDATION Loss: 0.14557454 Acc: 0.93886463\n",
      "Epoch 7 of 500 took 0.326s\n",
      "Accuracy source 0.937988, main loss classifier 0.162414, source classification loss 0.182097, loss domain distinction 0.189845, accuracy domain distinction 0.500244\n",
      "VALIDATION Loss: 0.10854857 Acc: 0.95851528\n",
      "New best validation loss:  0.10854857414960861\n",
      "Epoch 8 of 500 took 0.351s\n",
      "Accuracy source 0.938477, main loss classifier 0.160282, source classification loss 0.177854, loss domain distinction 0.192932, accuracy domain distinction 0.499268\n",
      "VALIDATION Loss: 0.11689822 Acc: 0.94978166\n",
      "Epoch 9 of 500 took 0.250s\n",
      "Accuracy source 0.930176, main loss classifier 0.157925, source classification loss 0.172667, loss domain distinction 0.190085, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.11529164 Acc: 0.95196507\n",
      "Epoch 10 of 500 took 0.242s\n",
      "Accuracy source 0.928711, main loss classifier 0.169205, source classification loss 0.195138, loss domain distinction 0.190039, accuracy domain distinction 0.500244\n",
      "VALIDATION Loss: 0.10548934 Acc: 0.95633188\n",
      "New best validation loss:  0.1054893359541893\n",
      "Epoch 11 of 500 took 0.249s\n",
      "Accuracy source 0.940430, main loss classifier 0.157775, source classification loss 0.172151, loss domain distinction 0.192628, accuracy domain distinction 0.499756\n",
      "VALIDATION Loss: 0.14717475 Acc: 0.93231441\n",
      "Epoch 12 of 500 took 0.278s\n",
      "Accuracy source 0.939453, main loss classifier 0.160925, source classification loss 0.179677, loss domain distinction 0.189080, accuracy domain distinction 0.499756\n",
      "VALIDATION Loss: 0.13665335 Acc: 0.94104803\n",
      "Epoch 13 of 500 took 0.251s\n",
      "Accuracy source 0.930664, main loss classifier 0.160701, source classification loss 0.178433, loss domain distinction 0.190203, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.13174908 Acc: 0.94323144\n",
      "Epoch 14 of 500 took 0.245s\n",
      "Accuracy source 0.939453, main loss classifier 0.153016, source classification loss 0.162398, loss domain distinction 0.193236, accuracy domain distinction 0.500244\n",
      "VALIDATION Loss: 0.12558751 Acc: 0.95414847\n",
      "Epoch 15 of 500 took 0.237s\n",
      "Accuracy source 0.942383, main loss classifier 0.148018, source classification loss 0.154182, loss domain distinction 0.188869, accuracy domain distinction 0.500244\n",
      "VALIDATION Loss: 0.10599732 Acc: 0.95851528\n",
      "Epoch 16 of 500 took 0.263s\n",
      "Accuracy source 0.943848, main loss classifier 0.150680, source classification loss 0.159407, loss domain distinction 0.187598, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.14987791 Acc: 0.93231441\n",
      "Epoch 17 of 500 took 0.246s\n",
      "Accuracy source 0.937012, main loss classifier 0.160131, source classification loss 0.178950, loss domain distinction 0.187082, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.10224880 Acc: 0.9628821\n",
      "New best validation loss:  0.10224879533052444\n",
      "Epoch 18 of 500 took 0.251s\n",
      "Accuracy source 0.936035, main loss classifier 0.156137, source classification loss 0.169859, loss domain distinction 0.189918, accuracy domain distinction 0.500244\n",
      "VALIDATION Loss: 0.12058966 Acc: 0.94104803\n",
      "Epoch 19 of 500 took 0.233s\n",
      "Accuracy source 0.938965, main loss classifier 0.157757, source classification loss 0.173924, loss domain distinction 0.188855, accuracy domain distinction 0.500244\n",
      "VALIDATION Loss: 0.10600111 Acc: 0.95414847\n",
      "Epoch 20 of 500 took 0.245s\n",
      "Accuracy source 0.947754, main loss classifier 0.153689, source classification loss 0.165798, loss domain distinction 0.187797, accuracy domain distinction 0.500244\n",
      "VALIDATION Loss: 0.10756504 Acc: 0.95414847\n",
      "Epoch 21 of 500 took 0.250s\n",
      "Accuracy source 0.941406, main loss classifier 0.153637, source classification loss 0.165809, loss domain distinction 0.187898, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.08410276 Acc: 0.97161572\n",
      "New best validation loss:  0.08410276472568512\n",
      "Epoch 22 of 500 took 0.263s\n",
      "Accuracy source 0.941406, main loss classifier 0.155722, source classification loss 0.170810, loss domain distinction 0.185878, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.13890761 Acc: 0.93886463\n",
      "Epoch 23 of 500 took 0.264s\n",
      "Accuracy source 0.943848, main loss classifier 0.152002, source classification loss 0.163296, loss domain distinction 0.187897, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.11903045 Acc: 0.94978166\n",
      "Epoch 24 of 500 took 0.231s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy source 0.944824, main loss classifier 0.150521, source classification loss 0.159346, loss domain distinction 0.188001, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.10088512 Acc: 0.9628821\n",
      "Epoch 25 of 500 took 0.229s\n",
      "Accuracy source 0.934082, main loss classifier 0.161898, source classification loss 0.183971, loss domain distinction 0.185666, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.09584443 Acc: 0.95414847\n",
      "Epoch 26 of 500 took 0.231s\n",
      "Accuracy source 0.946289, main loss classifier 0.145989, source classification loss 0.152052, loss domain distinction 0.187349, accuracy domain distinction 0.499756\n",
      "VALIDATION Loss: 0.11828817 Acc: 0.95414847\n",
      "Epoch 27 of 500 took 0.233s\n",
      "Accuracy source 0.949707, main loss classifier 0.148390, source classification loss 0.156012, loss domain distinction 0.188078, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.17503989 Acc: 0.9279476\n",
      "Epoch 28 of 500 took 0.229s\n",
      "Accuracy source 0.938477, main loss classifier 0.155809, source classification loss 0.170621, loss domain distinction 0.187522, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.10828614 Acc: 0.95851528\n",
      "Epoch 29 of 500 took 0.231s\n",
      "Accuracy source 0.945801, main loss classifier 0.148324, source classification loss 0.156537, loss domain distinction 0.185493, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.12868685 Acc: 0.93668122\n",
      "Epoch 30 of 500 took 0.231s\n",
      "Accuracy source 0.955566, main loss classifier 0.144306, source classification loss 0.147927, loss domain distinction 0.189317, accuracy domain distinction 0.499756\n",
      "VALIDATION Loss: 0.14570846 Acc: 0.93231441\n",
      "Epoch 31 of 500 took 0.233s\n",
      "Accuracy source 0.948242, main loss classifier 0.150219, source classification loss 0.160468, loss domain distinction 0.185535, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.14256249 Acc: 0.93449782\n",
      "Epoch 32 of 500 took 0.231s\n",
      "Accuracy source 0.939453, main loss classifier 0.157001, source classification loss 0.173778, loss domain distinction 0.186742, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.12311181 Acc: 0.94541485\n",
      "Training complete in 0m 9s\n",
      "()\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/TSD/participant_0/best_state_0.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/TSD/participant_0/best_state_0.pt' (epoch 27)\n",
      "STARTING TRAINING\n",
      "Accuracy source 0.927246, main loss classifier 0.185293, source classification loss 0.205173, loss domain distinction 0.472737, accuracy domain distinction 0.481201\n",
      "VALIDATION Loss: 0.13711758 Acc: 0.94541485\n",
      "New best validation loss:  0.13711757957935333\n",
      "Epoch 1 of 500 took 0.232s\n",
      "Accuracy source 0.924805, main loss classifier 0.180006, source classification loss 0.208900, loss domain distinction 0.243088, accuracy domain distinction 0.495117\n",
      "VALIDATION Loss: 0.12592372 Acc: 0.94978166\n",
      "New best validation loss:  0.12592372298240662\n",
      "Epoch 2 of 500 took 0.237s\n",
      "Accuracy source 0.924316, main loss classifier 0.175655, source classification loss 0.201453, loss domain distinction 0.211270, accuracy domain distinction 0.497559\n",
      "VALIDATION Loss: 0.14976750 Acc: 0.94759825\n",
      "Epoch 3 of 500 took 0.236s\n",
      "Accuracy source 0.936523, main loss classifier 0.164023, source classification loss 0.180613, loss domain distinction 0.198599, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.17529096 Acc: 0.93668122\n",
      "Epoch 4 of 500 took 0.230s\n",
      "Accuracy source 0.934570, main loss classifier 0.165257, source classification loss 0.185387, loss domain distinction 0.195385, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.10675437 Acc: 0.9628821\n",
      "New best validation loss:  0.106754370033741\n",
      "Epoch 5 of 500 took 0.233s\n",
      "Accuracy source 0.940430, main loss classifier 0.162631, source classification loss 0.179703, loss domain distinction 0.194927, accuracy domain distinction 0.499756\n",
      "VALIDATION Loss: 0.20256811 Acc: 0.92139738\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0060e-04.\n",
      "Epoch 6 of 500 took 0.232s\n",
      "Accuracy source 0.936035, main loss classifier 0.159878, source classification loss 0.177059, loss domain distinction 0.188105, accuracy domain distinction 0.499512\n",
      "VALIDATION Loss: 0.15189038 Acc: 0.94541485\n",
      "Epoch 7 of 500 took 0.230s\n",
      "Accuracy source 0.934082, main loss classifier 0.167577, source classification loss 0.192558, loss domain distinction 0.189287, accuracy domain distinction 0.499756\n",
      "VALIDATION Loss: 0.18419847 Acc: 0.92358079\n",
      "Epoch 8 of 500 took 0.227s\n",
      "Accuracy source 0.940430, main loss classifier 0.153292, source classification loss 0.163993, loss domain distinction 0.188632, accuracy domain distinction 0.500244\n",
      "VALIDATION Loss: 0.15258975 Acc: 0.93886463\n",
      "Epoch 9 of 500 took 0.251s\n",
      "Accuracy source 0.935059, main loss classifier 0.160890, source classification loss 0.177911, loss domain distinction 0.191075, accuracy domain distinction 0.499756\n",
      "VALIDATION Loss: 0.14430384 Acc: 0.93886463\n",
      "Epoch 10 of 500 took 0.240s\n",
      "Accuracy source 0.939453, main loss classifier 0.153524, source classification loss 0.162815, loss domain distinction 0.191262, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.10854660 Acc: 0.95633188\n",
      "Epoch 11 of 500 took 0.230s\n",
      "Accuracy source 0.938965, main loss classifier 0.158347, source classification loss 0.174275, loss domain distinction 0.188198, accuracy domain distinction 0.500244\n",
      "VALIDATION Loss: 0.13490567 Acc: 0.93668122\n",
      "Epoch 12 of 500 took 0.229s\n",
      "Accuracy source 0.936523, main loss classifier 0.156154, source classification loss 0.171024, loss domain distinction 0.188831, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.12216264 Acc: 0.94759825\n",
      "Epoch 13 of 500 took 0.228s\n",
      "Accuracy source 0.934082, main loss classifier 0.158806, source classification loss 0.175903, loss domain distinction 0.185869, accuracy domain distinction 0.499756\n",
      "VALIDATION Loss: 0.16094564 Acc: 0.93231441\n",
      "Epoch 14 of 500 took 0.241s\n",
      "Accuracy source 0.932617, main loss classifier 0.160984, source classification loss 0.179078, loss domain distinction 0.192170, accuracy domain distinction 0.499512\n",
      "VALIDATION Loss: 0.12441076 Acc: 0.94541485\n",
      "Epoch 15 of 500 took 0.238s\n",
      "Accuracy source 0.940430, main loss classifier 0.152411, source classification loss 0.163920, loss domain distinction 0.184753, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.12872230 Acc: 0.94759825\n",
      "Training complete in 0m 4s\n",
      "()\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/TSD/participant_0/best_state_0.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/TSD/participant_0/best_state_0.pt' (epoch 27)\n",
      "STARTING TRAINING\n",
      "Accuracy source 0.933105, main loss classifier 0.180562, source classification loss 0.193921, loss domain distinction 0.485316, accuracy domain distinction 0.473633\n",
      "VALIDATION Loss: 0.16180636 Acc: 0.92139738\n",
      "New best validation loss:  0.1618063598871231\n",
      "Epoch 1 of 500 took 0.286s\n",
      "Accuracy source 0.932617, main loss classifier 0.170159, source classification loss 0.191544, loss domain distinction 0.237586, accuracy domain distinction 0.500977\n",
      "VALIDATION Loss: 0.20409709 Acc: 0.90829694\n",
      "Epoch 2 of 500 took 0.237s\n",
      "Accuracy source 0.927734, main loss classifier 0.180804, source classification loss 0.212892, loss domain distinction 0.206519, accuracy domain distinction 0.498779\n",
      "VALIDATION Loss: 0.18293662 Acc: 0.92358079\n",
      "Epoch 3 of 500 took 0.277s\n",
      "Accuracy source 0.934082, main loss classifier 0.166854, source classification loss 0.185824, loss domain distinction 0.200382, accuracy domain distinction 0.499756\n",
      "VALIDATION Loss: 0.13736783 Acc: 0.94759825\n",
      "New best validation loss:  0.1373678296804428\n",
      "Epoch 4 of 500 took 0.242s\n",
      "Accuracy source 0.934570, main loss classifier 0.167616, source classification loss 0.188356, loss domain distinction 0.195970, accuracy domain distinction 0.500244\n",
      "VALIDATION Loss: 0.14933714 Acc: 0.94323144\n",
      "Epoch 5 of 500 took 0.230s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy source 0.930664, main loss classifier 0.162714, source classification loss 0.180803, loss domain distinction 0.190807, accuracy domain distinction 0.499756\n",
      "VALIDATION Loss: 0.13641451 Acc: 0.94323144\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0060e-04.\n",
      "New best validation loss:  0.1364145129919052\n",
      "Epoch 6 of 500 took 0.238s\n",
      "Accuracy source 0.935547, main loss classifier 0.163700, source classification loss 0.184633, loss domain distinction 0.192362, accuracy domain distinction 0.499756\n",
      "VALIDATION Loss: 0.15995201 Acc: 0.92139738\n",
      "Epoch 7 of 500 took 0.257s\n",
      "Accuracy source 0.942383, main loss classifier 0.162872, source classification loss 0.182529, loss domain distinction 0.192376, accuracy domain distinction 0.499512\n",
      "VALIDATION Loss: 0.16706367 Acc: 0.93449782\n",
      "Epoch 8 of 500 took 0.293s\n",
      "Accuracy source 0.937988, main loss classifier 0.153361, source classification loss 0.164338, loss domain distinction 0.188261, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.25821635 Acc: 0.89737991\n",
      "Epoch 9 of 500 took 0.231s\n",
      "Accuracy source 0.937988, main loss classifier 0.159003, source classification loss 0.175533, loss domain distinction 0.190328, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.17047513 Acc: 0.93449782\n",
      "Epoch 10 of 500 took 0.273s\n",
      "Accuracy source 0.934570, main loss classifier 0.161184, source classification loss 0.179861, loss domain distinction 0.188656, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.22290920 Acc: 0.91266376\n",
      "Epoch 11 of 500 took 0.287s\n",
      "Accuracy source 0.934082, main loss classifier 0.161653, source classification loss 0.181088, loss domain distinction 0.187606, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.16567174 Acc: 0.93449782\n",
      "Epoch 12 of 500 took 0.241s\n",
      "Accuracy source 0.940918, main loss classifier 0.153759, source classification loss 0.165246, loss domain distinction 0.189112, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.13419141 Acc: 0.94323144\n",
      "New best validation loss:  0.13419140875339508\n",
      "Epoch 13 of 500 took 0.301s\n",
      "Accuracy source 0.933594, main loss classifier 0.164190, source classification loss 0.186384, loss domain distinction 0.188649, accuracy domain distinction 0.500244\n",
      "VALIDATION Loss: 0.18661995 Acc: 0.91921397\n",
      "Epoch 14 of 500 took 0.235s\n",
      "Accuracy source 0.938477, main loss classifier 0.158792, source classification loss 0.174681, loss domain distinction 0.188201, accuracy domain distinction 0.499756\n",
      "VALIDATION Loss: 0.16349193 Acc: 0.94104803\n",
      "Epoch 15 of 500 took 0.235s\n",
      "Accuracy source 0.947754, main loss classifier 0.151217, source classification loss 0.160387, loss domain distinction 0.188400, accuracy domain distinction 0.499512\n",
      "VALIDATION Loss: 0.19136010 Acc: 0.92576419\n",
      "Epoch 16 of 500 took 0.231s\n",
      "Accuracy source 0.943359, main loss classifier 0.152341, source classification loss 0.164451, loss domain distinction 0.183750, accuracy domain distinction 0.500488\n",
      "VALIDATION Loss: 0.14725059 Acc: 0.9279476\n",
      "Epoch 17 of 500 took 0.233s\n",
      "Accuracy source 0.951172, main loss classifier 0.144008, source classification loss 0.146742, loss domain distinction 0.187201, accuracy domain distinction 0.500244\n",
      "VALIDATION Loss: 0.17403333 Acc: 0.93231441\n",
      "Epoch 18 of 500 took 0.231s\n",
      "Accuracy source 0.945312, main loss classifier 0.155783, source classification loss 0.170693, loss domain distinction 0.186880, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.14700888 Acc: 0.930131\n",
      "Epoch 19 of 500 took 0.235s\n",
      "Accuracy source 0.936035, main loss classifier 0.156753, source classification loss 0.172583, loss domain distinction 0.184815, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.15773574 Acc: 0.930131\n",
      "Epoch 20 of 500 took 0.238s\n",
      "Accuracy source 0.944824, main loss classifier 0.148048, source classification loss 0.155323, loss domain distinction 0.186280, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.22520636 Acc: 0.89956332\n",
      "Epoch 21 of 500 took 0.232s\n",
      "Accuracy source 0.939941, main loss classifier 0.155817, source classification loss 0.171229, loss domain distinction 0.188100, accuracy domain distinction 0.499756\n",
      "VALIDATION Loss: 0.12708439 Acc: 0.95196507\n",
      "New best validation loss:  0.1270843893289566\n",
      "Epoch 22 of 500 took 0.235s\n",
      "Accuracy source 0.948730, main loss classifier 0.150891, source classification loss 0.161307, loss domain distinction 0.186185, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.15505707 Acc: 0.93886463\n",
      "Epoch 23 of 500 took 0.231s\n",
      "Accuracy source 0.946289, main loss classifier 0.151590, source classification loss 0.161874, loss domain distinction 0.187489, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.14094539 Acc: 0.94978166\n",
      "Epoch 24 of 500 took 0.234s\n",
      "Accuracy source 0.940430, main loss classifier 0.159086, source classification loss 0.177906, loss domain distinction 0.185204, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.15617016 Acc: 0.94323144\n",
      "Epoch 25 of 500 took 0.229s\n",
      "Accuracy source 0.944824, main loss classifier 0.149093, source classification loss 0.157308, loss domain distinction 0.186183, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.13608681 Acc: 0.94541485\n",
      "Epoch 26 of 500 took 0.234s\n",
      "Accuracy source 0.948242, main loss classifier 0.150548, source classification loss 0.161569, loss domain distinction 0.185151, accuracy domain distinction 0.500244\n",
      "VALIDATION Loss: 0.12182483 Acc: 0.95414847\n",
      "New best validation loss:  0.12182483077049255\n",
      "Epoch 27 of 500 took 0.240s\n",
      "Accuracy source 0.949219, main loss classifier 0.143048, source classification loss 0.145319, loss domain distinction 0.188600, accuracy domain distinction 0.499512\n",
      "VALIDATION Loss: 0.12597755 Acc: 0.94541485\n",
      "Epoch 28 of 500 took 0.239s\n",
      "Accuracy source 0.938477, main loss classifier 0.157698, source classification loss 0.174760, loss domain distinction 0.186199, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.19357231 Acc: 0.91703057\n",
      "Epoch 29 of 500 took 0.231s\n",
      "Accuracy source 0.940918, main loss classifier 0.149727, source classification loss 0.159608, loss domain distinction 0.185655, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.13494703 Acc: 0.94978166\n",
      "Epoch 30 of 500 took 0.249s\n",
      "Accuracy source 0.941406, main loss classifier 0.152989, source classification loss 0.165897, loss domain distinction 0.185094, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.15082781 Acc: 0.93886463\n",
      "Epoch 31 of 500 took 0.259s\n",
      "Accuracy source 0.942871, main loss classifier 0.149313, source classification loss 0.158119, loss domain distinction 0.187059, accuracy domain distinction 0.500244\n",
      "VALIDATION Loss: 0.14937532 Acc: 0.93449782\n",
      "Epoch 32 of 500 took 0.273s\n",
      "Accuracy source 0.949219, main loss classifier 0.150102, source classification loss 0.159171, loss domain distinction 0.187265, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.15994434 Acc: 0.93449782\n",
      "Epoch 33 of 500 took 0.267s\n",
      "Accuracy source 0.948730, main loss classifier 0.148477, source classification loss 0.157467, loss domain distinction 0.184741, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.12063286 Acc: 0.93668122\n",
      "New best validation loss:  0.12063286453485489\n",
      "Epoch 34 of 500 took 0.255s\n",
      "Accuracy source 0.942383, main loss classifier 0.150324, source classification loss 0.160560, loss domain distinction 0.187187, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.12753558 Acc: 0.95196507\n",
      "Epoch 35 of 500 took 0.252s\n",
      "Accuracy source 0.952637, main loss classifier 0.142013, source classification loss 0.144281, loss domain distinction 0.186409, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.19263680 Acc: 0.92576419\n",
      "Epoch 36 of 500 took 0.262s\n",
      "Accuracy source 0.946289, main loss classifier 0.151596, source classification loss 0.163878, loss domain distinction 0.184965, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.19680385 Acc: 0.92358079\n",
      "Epoch 37 of 500 took 0.276s\n",
      "Accuracy source 0.947266, main loss classifier 0.148408, source classification loss 0.158347, loss domain distinction 0.183397, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.17399146 Acc: 0.9279476\n",
      "Epoch 38 of 500 took 0.246s\n",
      "Accuracy source 0.949219, main loss classifier 0.143375, source classification loss 0.147819, loss domain distinction 0.184245, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.13144350 Acc: 0.94759825\n",
      "Epoch 39 of 500 took 0.237s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy source 0.941895, main loss classifier 0.150453, source classification loss 0.160880, loss domain distinction 0.188627, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.15612380 Acc: 0.93449782\n",
      "Epoch 40 of 500 took 0.234s\n",
      "Accuracy source 0.945312, main loss classifier 0.149224, source classification loss 0.159992, loss domain distinction 0.184661, accuracy domain distinction 0.499756\n",
      "VALIDATION Loss: 0.13948172 Acc: 0.94541485\n",
      "Epoch 41 of 500 took 0.230s\n",
      "Accuracy source 0.950684, main loss classifier 0.137669, source classification loss 0.137735, loss domain distinction 0.182865, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.14330809 Acc: 0.93886463\n",
      "Epoch 42 of 500 took 0.275s\n",
      "Accuracy source 0.947266, main loss classifier 0.149810, source classification loss 0.160951, loss domain distinction 0.183924, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.18112440 Acc: 0.9279476\n",
      "Epoch 43 of 500 took 0.277s\n",
      "Accuracy source 0.953613, main loss classifier 0.136623, source classification loss 0.135143, loss domain distinction 0.183347, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.17090654 Acc: 0.93449782\n",
      "Epoch 44 of 500 took 0.245s\n",
      "Accuracy source 0.952637, main loss classifier 0.140543, source classification loss 0.142548, loss domain distinction 0.185398, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.11344660 Acc: 0.94541485\n",
      "New best validation loss:  0.11344660073518753\n",
      "Epoch 45 of 500 took 0.229s\n",
      "Accuracy source 0.953125, main loss classifier 0.141867, source classification loss 0.146012, loss domain distinction 0.183611, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.12018529 Acc: 0.94759825\n",
      "Epoch 46 of 500 took 0.231s\n",
      "Accuracy source 0.947754, main loss classifier 0.150720, source classification loss 0.163053, loss domain distinction 0.184208, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.15133746 Acc: 0.93886463\n",
      "Epoch 47 of 500 took 0.235s\n",
      "Accuracy source 0.941895, main loss classifier 0.149110, source classification loss 0.158920, loss domain distinction 0.185824, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.20232981 Acc: 0.91484716\n",
      "Epoch 48 of 500 took 0.230s\n",
      "Accuracy source 0.946777, main loss classifier 0.147967, source classification loss 0.157516, loss domain distinction 0.185274, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.15855125 Acc: 0.930131\n",
      "Epoch 49 of 500 took 0.234s\n",
      "Accuracy source 0.949219, main loss classifier 0.139897, source classification loss 0.141686, loss domain distinction 0.184166, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.12657352 Acc: 0.94104803\n",
      "Epoch 50 of 500 took 0.231s\n",
      "Accuracy source 0.940918, main loss classifier 0.148942, source classification loss 0.160447, loss domain distinction 0.182478, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.14309567 Acc: 0.92576419\n",
      "Epoch 51 of 500 took 0.230s\n",
      "Accuracy source 0.946777, main loss classifier 0.144582, source classification loss 0.150503, loss domain distinction 0.185984, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.15848988 Acc: 0.93449782\n",
      "Epoch 52 of 500 took 0.229s\n",
      "Accuracy source 0.947754, main loss classifier 0.143086, source classification loss 0.148760, loss domain distinction 0.183156, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.16739270 Acc: 0.94104803\n",
      "Epoch 53 of 500 took 0.234s\n",
      "Accuracy source 0.954590, main loss classifier 0.139224, source classification loss 0.140910, loss domain distinction 0.182642, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.14838155 Acc: 0.94323144\n",
      "Epoch 54 of 500 took 0.230s\n",
      "Accuracy source 0.946777, main loss classifier 0.143792, source classification loss 0.149702, loss domain distinction 0.185775, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.10452711 Acc: 0.96069869\n",
      "New best validation loss:  0.10452710837125778\n",
      "Epoch 55 of 500 took 0.232s\n",
      "Accuracy source 0.949219, main loss classifier 0.138312, source classification loss 0.138558, loss domain distinction 0.185815, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.15828851 Acc: 0.93449782\n",
      "Epoch 56 of 500 took 0.258s\n",
      "Accuracy source 0.943359, main loss classifier 0.144679, source classification loss 0.151444, loss domain distinction 0.182942, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.15798315 Acc: 0.9279476\n",
      "Epoch 57 of 500 took 0.233s\n",
      "Accuracy source 0.955078, main loss classifier 0.138953, source classification loss 0.139351, loss domain distinction 0.184418, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.13213123 Acc: 0.94323144\n",
      "Epoch 58 of 500 took 0.230s\n",
      "Accuracy source 0.948730, main loss classifier 0.135153, source classification loss 0.132413, loss domain distinction 0.184284, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.14133440 Acc: 0.93668122\n",
      "Epoch 59 of 500 took 0.230s\n",
      "Accuracy source 0.947266, main loss classifier 0.143711, source classification loss 0.150817, loss domain distinction 0.182426, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.12885813 Acc: 0.93886463\n",
      "Epoch 60 of 500 took 0.230s\n",
      "Accuracy source 0.952637, main loss classifier 0.138678, source classification loss 0.139583, loss domain distinction 0.184319, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.26656201 Acc: 0.89082969\n",
      "Epoch 61 of 500 took 0.229s\n",
      "Accuracy source 0.953613, main loss classifier 0.133588, source classification loss 0.130135, loss domain distinction 0.181417, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.18915717 Acc: 0.91484716\n",
      "Epoch 62 of 500 took 0.234s\n",
      "Accuracy source 0.950195, main loss classifier 0.138352, source classification loss 0.139286, loss domain distinction 0.184731, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.14986965 Acc: 0.930131\n",
      "Epoch 63 of 500 took 0.230s\n",
      "Accuracy source 0.943848, main loss classifier 0.145525, source classification loss 0.153338, loss domain distinction 0.183624, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.18545374 Acc: 0.92139738\n",
      "Epoch 64 of 500 took 0.260s\n",
      "Accuracy source 0.947266, main loss classifier 0.143824, source classification loss 0.151545, loss domain distinction 0.181522, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.21577577 Acc: 0.91703057\n",
      "Epoch 65 of 500 took 0.230s\n",
      "Accuracy source 0.954590, main loss classifier 0.135993, source classification loss 0.135518, loss domain distinction 0.182829, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.18032978 Acc: 0.92358079\n",
      "Training complete in 0m 16s\n",
      "()\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/TSD/participant_0/best_state_0.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/TSD/participant_0/best_state_0.pt' (epoch 27)\n",
      "STARTING TRAINING\n",
      "Accuracy source 0.932129, main loss classifier 0.184844, source classification loss 0.203597, loss domain distinction 0.478365, accuracy domain distinction 0.475098\n",
      "VALIDATION Loss: 0.21569347 Acc: 0.91703057\n",
      "New best validation loss:  0.21569347381591797\n",
      "Epoch 1 of 500 took 0.230s\n",
      "Accuracy source 0.937500, main loss classifier 0.163617, source classification loss 0.179211, loss domain distinction 0.236893, accuracy domain distinction 0.499268\n",
      "VALIDATION Loss: 0.18798321 Acc: 0.92139738\n",
      "New best validation loss:  0.1879832148551941\n",
      "Epoch 2 of 500 took 0.232s\n",
      "Accuracy source 0.929199, main loss classifier 0.161980, source classification loss 0.175576, loss domain distinction 0.207129, accuracy domain distinction 0.500732\n",
      "VALIDATION Loss: 0.24232657 Acc: 0.90611354\n",
      "Epoch 3 of 500 took 0.240s\n",
      "Accuracy source 0.923340, main loss classifier 0.174743, source classification loss 0.202663, loss domain distinction 0.195382, accuracy domain distinction 0.500488\n",
      "VALIDATION Loss: 0.19560355 Acc: 0.92358079\n",
      "Epoch 4 of 500 took 0.234s\n",
      "Accuracy source 0.947266, main loss classifier 0.161055, source classification loss 0.176643, loss domain distinction 0.194867, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.33073083 Acc: 0.88209607\n",
      "Epoch 5 of 500 took 0.233s\n",
      "Accuracy source 0.934570, main loss classifier 0.160711, source classification loss 0.176799, loss domain distinction 0.192587, accuracy domain distinction 0.499512\n",
      "VALIDATION Loss: 0.22582783 Acc: 0.90829694\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0060e-04.\n",
      "Epoch 6 of 500 took 0.242s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy source 0.935547, main loss classifier 0.160022, source classification loss 0.176754, loss domain distinction 0.189082, accuracy domain distinction 0.500244\n",
      "VALIDATION Loss: 0.23249531 Acc: 0.90829694\n",
      "Epoch 7 of 500 took 0.231s\n",
      "Accuracy source 0.936523, main loss classifier 0.165952, source classification loss 0.189991, loss domain distinction 0.190260, accuracy domain distinction 0.500732\n",
      "VALIDATION Loss: 0.30136690 Acc: 0.88646288\n",
      "Epoch 8 of 500 took 0.235s\n",
      "Accuracy source 0.942383, main loss classifier 0.152399, source classification loss 0.163730, loss domain distinction 0.187150, accuracy domain distinction 0.500244\n",
      "VALIDATION Loss: 0.32147467 Acc: 0.8930131\n",
      "Epoch 9 of 500 took 0.231s\n",
      "Accuracy source 0.943359, main loss classifier 0.158877, source classification loss 0.174556, loss domain distinction 0.189095, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.22521220 Acc: 0.90829694\n",
      "Epoch 10 of 500 took 0.229s\n",
      "Accuracy source 0.934570, main loss classifier 0.164116, source classification loss 0.187376, loss domain distinction 0.187689, accuracy domain distinction 0.500488\n",
      "VALIDATION Loss: 0.31119046 Acc: 0.87991266\n",
      "Epoch 11 of 500 took 0.231s\n",
      "Accuracy source 0.932617, main loss classifier 0.163686, source classification loss 0.185667, loss domain distinction 0.185195, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.36407414 Acc: 0.87336245\n",
      "Epoch 12 of 500 took 0.230s\n",
      "Accuracy source 0.940918, main loss classifier 0.153487, source classification loss 0.165682, loss domain distinction 0.188103, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.15355015 Acc: 0.94104803\n",
      "New best validation loss:  0.1535501480102539\n",
      "Epoch 13 of 500 took 0.235s\n",
      "Accuracy source 0.945312, main loss classifier 0.148853, source classification loss 0.157126, loss domain distinction 0.184601, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.29753512 Acc: 0.89082969\n",
      "Epoch 14 of 500 took 0.230s\n",
      "Accuracy source 0.936523, main loss classifier 0.152984, source classification loss 0.164093, loss domain distinction 0.187974, accuracy domain distinction 0.499512\n",
      "VALIDATION Loss: 0.23298024 Acc: 0.90829694\n",
      "Epoch 15 of 500 took 0.229s\n",
      "Accuracy source 0.947754, main loss classifier 0.149448, source classification loss 0.156182, loss domain distinction 0.190308, accuracy domain distinction 0.499512\n",
      "VALIDATION Loss: 0.28394589 Acc: 0.88646288\n",
      "Epoch 16 of 500 took 0.230s\n",
      "Accuracy source 0.937500, main loss classifier 0.159824, source classification loss 0.178614, loss domain distinction 0.185362, accuracy domain distinction 0.500244\n",
      "VALIDATION Loss: 0.24328999 Acc: 0.91048035\n",
      "Epoch 17 of 500 took 0.234s\n",
      "Accuracy source 0.933594, main loss classifier 0.160628, source classification loss 0.179803, loss domain distinction 0.187215, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.21985154 Acc: 0.91048035\n",
      "Epoch 18 of 500 took 0.230s\n",
      "Accuracy source 0.940430, main loss classifier 0.153425, source classification loss 0.165872, loss domain distinction 0.186154, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.31623322 Acc: 0.89082969\n",
      "Epoch 19 of 500 took 0.230s\n",
      "Accuracy source 0.945801, main loss classifier 0.149233, source classification loss 0.157726, loss domain distinction 0.186031, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.27016595 Acc: 0.90393013\n",
      "Epoch 20 of 500 took 0.228s\n",
      "Accuracy source 0.945312, main loss classifier 0.150117, source classification loss 0.158876, loss domain distinction 0.185798, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.26725531 Acc: 0.90393013\n",
      "Epoch 21 of 500 took 0.230s\n",
      "Accuracy source 0.937988, main loss classifier 0.150612, source classification loss 0.160658, loss domain distinction 0.185825, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.22175537 Acc: 0.91703057\n",
      "Epoch 22 of 500 took 0.232s\n",
      "Accuracy source 0.935059, main loss classifier 0.157520, source classification loss 0.175244, loss domain distinction 0.185394, accuracy domain distinction 0.499756\n",
      "VALIDATION Loss: 0.21141234 Acc: 0.91266376\n",
      "Epoch 23 of 500 took 0.229s\n",
      "Accuracy source 0.942871, main loss classifier 0.149069, source classification loss 0.157402, loss domain distinction 0.188924, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.21389306 Acc: 0.90393013\n",
      "Training complete in 0m 6s\n",
      "()\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/TSD/participant_0/best_state_0.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/TSD/participant_0/best_state_0.pt' (epoch 27)\n",
      "STARTING TRAINING\n",
      "Accuracy source 0.926270, main loss classifier 0.190313, source classification loss 0.214947, loss domain distinction 0.482528, accuracy domain distinction 0.471680\n",
      "VALIDATION Loss: 0.43267485 Acc: 0.8558952\n",
      "New best validation loss:  0.4326748549938202\n",
      "Epoch 1 of 500 took 0.232s\n",
      "Accuracy source 0.934082, main loss classifier 0.169835, source classification loss 0.190496, loss domain distinction 0.242115, accuracy domain distinction 0.497314\n",
      "VALIDATION Loss: 0.46650726 Acc: 0.84934498\n",
      "Epoch 2 of 500 took 0.234s\n",
      "Accuracy source 0.927734, main loss classifier 0.173086, source classification loss 0.197829, loss domain distinction 0.208850, accuracy domain distinction 0.500488\n",
      "VALIDATION Loss: 0.39669460 Acc: 0.86462882\n",
      "New best validation loss:  0.3966946005821228\n",
      "Epoch 3 of 500 took 0.235s\n",
      "Accuracy source 0.937500, main loss classifier 0.168792, source classification loss 0.189800, loss domain distinction 0.198457, accuracy domain distinction 0.498779\n",
      "VALIDATION Loss: 0.52036661 Acc: 0.84497817\n",
      "Epoch 4 of 500 took 0.229s\n",
      "Accuracy source 0.939453, main loss classifier 0.167348, source classification loss 0.189059, loss domain distinction 0.195175, accuracy domain distinction 0.499512\n",
      "VALIDATION Loss: 0.52829951 Acc: 0.83406114\n",
      "Epoch 5 of 500 took 0.230s\n",
      "Accuracy source 0.944336, main loss classifier 0.158427, source classification loss 0.172484, loss domain distinction 0.192535, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.35722712 Acc: 0.87117904\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0060e-04.\n",
      "New best validation loss:  0.3572271168231964\n",
      "Epoch 6 of 500 took 0.235s\n",
      "Accuracy source 0.928223, main loss classifier 0.170759, source classification loss 0.199003, loss domain distinction 0.189451, accuracy domain distinction 0.500244\n",
      "VALIDATION Loss: 0.44467512 Acc: 0.8580786\n",
      "Epoch 7 of 500 took 0.229s\n",
      "Accuracy source 0.947754, main loss classifier 0.162035, source classification loss 0.181465, loss domain distinction 0.189165, accuracy domain distinction 0.499756\n",
      "VALIDATION Loss: 0.45040438 Acc: 0.8558952\n",
      "Epoch 8 of 500 took 0.230s\n",
      "Accuracy source 0.937012, main loss classifier 0.157777, source classification loss 0.172951, loss domain distinction 0.189809, accuracy domain distinction 0.499756\n",
      "VALIDATION Loss: 0.35253176 Acc: 0.87772926\n",
      "New best validation loss:  0.352531760931015\n",
      "Epoch 9 of 500 took 0.232s\n",
      "Accuracy source 0.935059, main loss classifier 0.161056, source classification loss 0.180972, loss domain distinction 0.184508, accuracy domain distinction 0.499512\n",
      "VALIDATION Loss: 0.33366236 Acc: 0.87117904\n",
      "New best validation loss:  0.33366236090660095\n",
      "Epoch 10 of 500 took 0.231s\n",
      "Accuracy source 0.939453, main loss classifier 0.160161, source classification loss 0.177487, loss domain distinction 0.189660, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.44652677 Acc: 0.86026201\n",
      "Epoch 11 of 500 took 0.233s\n",
      "Accuracy source 0.944336, main loss classifier 0.156172, source classification loss 0.168563, loss domain distinction 0.190335, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.45952699 Acc: 0.8558952\n",
      "Epoch 12 of 500 took 0.232s\n",
      "Accuracy source 0.942383, main loss classifier 0.159633, source classification loss 0.178903, loss domain distinction 0.186680, accuracy domain distinction 0.500488\n",
      "VALIDATION Loss: 0.46228489 Acc: 0.8558952\n",
      "Epoch 13 of 500 took 0.246s\n",
      "Accuracy source 0.930664, main loss classifier 0.161144, source classification loss 0.180737, loss domain distinction 0.186349, accuracy domain distinction 0.500244\n",
      "VALIDATION Loss: 0.36463484 Acc: 0.86899563\n",
      "Epoch 14 of 500 took 0.230s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy source 0.938477, main loss classifier 0.158885, source classification loss 0.176298, loss domain distinction 0.185679, accuracy domain distinction 0.499756\n",
      "VALIDATION Loss: 0.47445181 Acc: 0.84716157\n",
      "Epoch 15 of 500 took 0.241s\n",
      "Accuracy source 0.938965, main loss classifier 0.150111, source classification loss 0.160315, loss domain distinction 0.185120, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.44753006 Acc: 0.85371179\n",
      "Epoch 16 of 500 took 0.247s\n",
      "Accuracy source 0.941406, main loss classifier 0.151191, source classification loss 0.161806, loss domain distinction 0.183577, accuracy domain distinction 0.500488\n",
      "VALIDATION Loss: 0.54868448 Acc: 0.82751092\n",
      "Epoch 17 of 500 took 0.230s\n",
      "Accuracy source 0.941895, main loss classifier 0.148965, source classification loss 0.158364, loss domain distinction 0.183981, accuracy domain distinction 0.500244\n",
      "VALIDATION Loss: 0.40982744 Acc: 0.86244541\n",
      "Epoch 18 of 500 took 0.231s\n",
      "Accuracy source 0.940430, main loss classifier 0.159791, source classification loss 0.180815, loss domain distinction 0.185377, accuracy domain distinction 0.499756\n",
      "VALIDATION Loss: 0.52059150 Acc: 0.83842795\n",
      "Epoch 19 of 500 took 0.233s\n",
      "Accuracy source 0.936035, main loss classifier 0.158170, source classification loss 0.175672, loss domain distinction 0.186632, accuracy domain distinction 0.500488\n",
      "VALIDATION Loss: 0.33266807 Acc: 0.87117904\n",
      "New best validation loss:  0.3326680660247803\n",
      "Epoch 20 of 500 took 0.233s\n",
      "Accuracy source 0.936035, main loss classifier 0.157803, source classification loss 0.176120, loss domain distinction 0.184526, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.52382135 Acc: 0.83624454\n",
      "Epoch 21 of 500 took 0.234s\n",
      "Accuracy source 0.946289, main loss classifier 0.147447, source classification loss 0.154447, loss domain distinction 0.184531, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.45636874 Acc: 0.8580786\n",
      "Epoch 22 of 500 took 0.235s\n",
      "Accuracy source 0.949219, main loss classifier 0.146356, source classification loss 0.151704, loss domain distinction 0.184562, accuracy domain distinction 0.500244\n",
      "VALIDATION Loss: 0.44900852 Acc: 0.8558952\n",
      "Epoch 23 of 500 took 0.229s\n",
      "Accuracy source 0.945801, main loss classifier 0.149979, source classification loss 0.159372, loss domain distinction 0.184056, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.44090316 Acc: 0.8580786\n",
      "Epoch 24 of 500 took 0.233s\n",
      "Accuracy source 0.951172, main loss classifier 0.144416, source classification loss 0.149018, loss domain distinction 0.186238, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.36671200 Acc: 0.87336245\n",
      "Epoch 25 of 500 took 0.231s\n",
      "Accuracy source 0.938965, main loss classifier 0.153276, source classification loss 0.166044, loss domain distinction 0.185193, accuracy domain distinction 0.499756\n",
      "VALIDATION Loss: 0.39211604 Acc: 0.86899563\n",
      "Epoch 26 of 500 took 0.231s\n",
      "Accuracy source 0.938965, main loss classifier 0.154210, source classification loss 0.168628, loss domain distinction 0.184716, accuracy domain distinction 0.499512\n",
      "VALIDATION Loss: 0.45923060 Acc: 0.85371179\n",
      "Epoch 27 of 500 took 0.230s\n",
      "Accuracy source 0.931641, main loss classifier 0.167183, source classification loss 0.195239, loss domain distinction 0.182912, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.34887123 Acc: 0.88427948\n",
      "Epoch 28 of 500 took 0.234s\n",
      "Accuracy source 0.949707, main loss classifier 0.143545, source classification loss 0.148740, loss domain distinction 0.181183, accuracy domain distinction 0.500244\n",
      "VALIDATION Loss: 0.30411485 Acc: 0.88209607\n",
      "New best validation loss:  0.30411484837532043\n",
      "Epoch 29 of 500 took 0.232s\n",
      "Accuracy source 0.946777, main loss classifier 0.146268, source classification loss 0.152648, loss domain distinction 0.184191, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.39710122 Acc: 0.8580786\n",
      "Epoch 30 of 500 took 0.230s\n",
      "Accuracy source 0.950195, main loss classifier 0.143799, source classification loss 0.147016, loss domain distinction 0.186536, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.43584415 Acc: 0.86026201\n",
      "Epoch 31 of 500 took 0.230s\n",
      "Accuracy source 0.945312, main loss classifier 0.146342, source classification loss 0.153909, loss domain distinction 0.184550, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.57829559 Acc: 0.83842795\n",
      "Epoch 32 of 500 took 0.233s\n",
      "Accuracy source 0.949219, main loss classifier 0.141777, source classification loss 0.145946, loss domain distinction 0.179846, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.32804713 Acc: 0.88209607\n",
      "Epoch 33 of 500 took 0.230s\n",
      "Accuracy source 0.946777, main loss classifier 0.150353, source classification loss 0.162185, loss domain distinction 0.184731, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.52196193 Acc: 0.85152838\n",
      "Epoch 34 of 500 took 0.230s\n",
      "Accuracy source 0.947266, main loss classifier 0.142886, source classification loss 0.148000, loss domain distinction 0.183334, accuracy domain distinction 0.499756\n",
      "VALIDATION Loss: 0.49203020 Acc: 0.84061135\n",
      "Epoch 35 of 500 took 0.230s\n",
      "Accuracy source 0.951660, main loss classifier 0.138494, source classification loss 0.139538, loss domain distinction 0.179053, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.40056306 Acc: 0.86681223\n",
      "Epoch 36 of 500 took 0.229s\n",
      "Accuracy source 0.939941, main loss classifier 0.152992, source classification loss 0.168056, loss domain distinction 0.183593, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.49118352 Acc: 0.84934498\n",
      "Epoch 37 of 500 took 0.234s\n",
      "Accuracy source 0.939453, main loss classifier 0.149449, source classification loss 0.160210, loss domain distinction 0.184161, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.45424703 Acc: 0.85371179\n",
      "Epoch 38 of 500 took 0.231s\n",
      "Accuracy source 0.943848, main loss classifier 0.146367, source classification loss 0.154688, loss domain distinction 0.183684, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.37555894 Acc: 0.86899563\n",
      "Epoch 39 of 500 took 0.231s\n",
      "Accuracy source 0.939941, main loss classifier 0.152419, source classification loss 0.166079, loss domain distinction 0.182821, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.52390575 Acc: 0.85152838\n",
      "Training complete in 0m 9s\n",
      "()\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/TSD/participant_0/best_state_0.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/TSD/participant_0/best_state_0.pt' (epoch 27)\n",
      "STARTING TRAINING\n",
      "Accuracy source 0.921387, main loss classifier 0.183731, source classification loss 0.205104, loss domain distinction 0.468315, accuracy domain distinction 0.482422\n",
      "VALIDATION Loss: 0.31699067 Acc: 0.88209607\n",
      "New best validation loss:  0.3169906735420227\n",
      "Epoch 1 of 500 took 0.266s\n",
      "Accuracy source 0.931152, main loss classifier 0.170244, source classification loss 0.190953, loss domain distinction 0.242542, accuracy domain distinction 0.496582\n",
      "VALIDATION Loss: 0.32146993 Acc: 0.87117904\n",
      "Epoch 2 of 500 took 0.237s\n",
      "Accuracy source 0.935547, main loss classifier 0.168093, source classification loss 0.187406, loss domain distinction 0.205962, accuracy domain distinction 0.499756\n",
      "VALIDATION Loss: 0.34510455 Acc: 0.87336245\n",
      "Epoch 3 of 500 took 0.234s\n",
      "Accuracy source 0.930664, main loss classifier 0.165953, source classification loss 0.186425, loss domain distinction 0.195854, accuracy domain distinction 0.499756\n",
      "VALIDATION Loss: 0.38591194 Acc: 0.8558952\n",
      "Epoch 4 of 500 took 0.228s\n",
      "Accuracy source 0.936523, main loss classifier 0.160069, source classification loss 0.176119, loss domain distinction 0.191485, accuracy domain distinction 0.500488\n",
      "VALIDATION Loss: 0.34097290 Acc: 0.86899563\n",
      "Epoch 5 of 500 took 0.236s\n",
      "Accuracy source 0.938965, main loss classifier 0.165895, source classification loss 0.187538, loss domain distinction 0.190897, accuracy domain distinction 0.499756\n",
      "VALIDATION Loss: 0.31945553 Acc: 0.87336245\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0060e-04.\n",
      "Epoch 6 of 500 took 0.229s\n",
      "Accuracy source 0.935059, main loss classifier 0.157936, source classification loss 0.174116, loss domain distinction 0.188491, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.24141778 Acc: 0.89737991\n",
      "New best validation loss:  0.2414177805185318\n",
      "Epoch 7 of 500 took 0.232s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy source 0.939453, main loss classifier 0.161349, source classification loss 0.181656, loss domain distinction 0.188835, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.37903324 Acc: 0.86462882\n",
      "Epoch 8 of 500 took 0.229s\n",
      "Accuracy source 0.938477, main loss classifier 0.157267, source classification loss 0.171928, loss domain distinction 0.191023, accuracy domain distinction 0.500244\n",
      "VALIDATION Loss: 0.48165876 Acc: 0.84934498\n",
      "Epoch 9 of 500 took 0.231s\n",
      "Accuracy source 0.939941, main loss classifier 0.153569, source classification loss 0.164610, loss domain distinction 0.187941, accuracy domain distinction 0.500244\n",
      "VALIDATION Loss: 0.33302242 Acc: 0.87336245\n",
      "Epoch 10 of 500 took 0.231s\n",
      "Accuracy source 0.937500, main loss classifier 0.162092, source classification loss 0.180528, loss domain distinction 0.190560, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.39619333 Acc: 0.8580786\n",
      "Epoch 11 of 500 took 0.229s\n",
      "Accuracy source 0.939453, main loss classifier 0.157844, source classification loss 0.173739, loss domain distinction 0.184583, accuracy domain distinction 0.499756\n",
      "VALIDATION Loss: 0.41842458 Acc: 0.8580786\n",
      "Epoch 12 of 500 took 0.229s\n",
      "Accuracy source 0.936523, main loss classifier 0.155561, source classification loss 0.169808, loss domain distinction 0.189142, accuracy domain distinction 0.500488\n",
      "VALIDATION Loss: 0.38031894 Acc: 0.86462882\n",
      "Epoch 13 of 500 took 0.228s\n",
      "Accuracy source 0.942871, main loss classifier 0.153413, source classification loss 0.165277, loss domain distinction 0.187471, accuracy domain distinction 0.499512\n",
      "VALIDATION Loss: 0.35573062 Acc: 0.87554585\n",
      "Epoch 14 of 500 took 0.234s\n",
      "Accuracy source 0.947266, main loss classifier 0.150803, source classification loss 0.160885, loss domain distinction 0.183979, accuracy domain distinction 0.500244\n",
      "VALIDATION Loss: 0.28013000 Acc: 0.89519651\n",
      "Epoch 15 of 500 took 0.229s\n",
      "Accuracy source 0.937500, main loss classifier 0.157346, source classification loss 0.174143, loss domain distinction 0.183586, accuracy domain distinction 0.500488\n",
      "VALIDATION Loss: 0.40270227 Acc: 0.87117904\n",
      "Epoch 16 of 500 took 0.229s\n",
      "Accuracy source 0.932617, main loss classifier 0.166920, source classification loss 0.191136, loss domain distinction 0.187354, accuracy domain distinction 0.500244\n",
      "VALIDATION Loss: 0.42577583 Acc: 0.8558952\n",
      "Epoch 17 of 500 took 0.230s\n",
      "Accuracy source 0.937012, main loss classifier 0.163933, source classification loss 0.186452, loss domain distinction 0.188635, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.30490470 Acc: 0.88646288\n",
      "Training complete in 0m 4s\n",
      "()\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/TSD/participant_0/best_state_0.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/TSD/participant_0/best_state_0.pt' (epoch 27)\n",
      "STARTING TRAINING\n",
      "Accuracy source 0.923828, main loss classifier 0.182639, source classification loss 0.200627, loss domain distinction 0.479639, accuracy domain distinction 0.469727\n",
      "VALIDATION Loss: 0.33179453 Acc: 0.88209607\n",
      "New best validation loss:  0.33179453015327454\n",
      "Epoch 1 of 500 took 0.230s\n",
      "Accuracy source 0.928711, main loss classifier 0.176555, source classification loss 0.202705, loss domain distinction 0.244760, accuracy domain distinction 0.494629\n",
      "VALIDATION Loss: 0.41417554 Acc: 0.85371179\n",
      "Epoch 2 of 500 took 0.229s\n",
      "Accuracy source 0.923340, main loss classifier 0.177243, source classification loss 0.205473, loss domain distinction 0.206997, accuracy domain distinction 0.500488\n",
      "VALIDATION Loss: 0.28109854 Acc: 0.89519651\n",
      "New best validation loss:  0.28109854459762573\n",
      "Epoch 3 of 500 took 0.231s\n",
      "Accuracy source 0.927246, main loss classifier 0.172378, source classification loss 0.196592, loss domain distinction 0.200349, accuracy domain distinction 0.499756\n",
      "VALIDATION Loss: 0.37579831 Acc: 0.8580786\n",
      "Epoch 4 of 500 took 0.229s\n",
      "Accuracy source 0.935059, main loss classifier 0.160568, source classification loss 0.176182, loss domain distinction 0.194299, accuracy domain distinction 0.499512\n",
      "VALIDATION Loss: 0.30528197 Acc: 0.88646288\n",
      "Epoch 5 of 500 took 0.235s\n",
      "Accuracy source 0.931152, main loss classifier 0.163639, source classification loss 0.184649, loss domain distinction 0.188862, accuracy domain distinction 0.500244\n",
      "VALIDATION Loss: 0.40410474 Acc: 0.85371179\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0060e-04.\n",
      "Epoch 6 of 500 took 0.233s\n",
      "Accuracy source 0.942871, main loss classifier 0.153749, source classification loss 0.164134, loss domain distinction 0.190187, accuracy domain distinction 0.500244\n",
      "VALIDATION Loss: 0.41606894 Acc: 0.85152838\n",
      "Epoch 7 of 500 took 0.228s\n",
      "Accuracy source 0.938477, main loss classifier 0.157287, source classification loss 0.171987, loss domain distinction 0.192083, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.38664803 Acc: 0.8558952\n",
      "Epoch 8 of 500 took 0.227s\n",
      "Accuracy source 0.931152, main loss classifier 0.164418, source classification loss 0.186620, loss domain distinction 0.188812, accuracy domain distinction 0.499512\n",
      "VALIDATION Loss: 0.32086608 Acc: 0.88427948\n",
      "Epoch 9 of 500 took 0.239s\n",
      "Accuracy source 0.939453, main loss classifier 0.158607, source classification loss 0.175795, loss domain distinction 0.187211, accuracy domain distinction 0.500488\n",
      "VALIDATION Loss: 0.32435328 Acc: 0.87336245\n",
      "Epoch 10 of 500 took 0.241s\n",
      "Accuracy source 0.929199, main loss classifier 0.165218, source classification loss 0.188254, loss domain distinction 0.189871, accuracy domain distinction 0.499756\n",
      "VALIDATION Loss: 0.31130141 Acc: 0.86899563\n",
      "Epoch 11 of 500 took 0.230s\n",
      "Accuracy source 0.935547, main loss classifier 0.163431, source classification loss 0.184344, loss domain distinction 0.186817, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.42801932 Acc: 0.84934498\n",
      "Epoch 12 of 500 took 0.228s\n",
      "Accuracy source 0.943848, main loss classifier 0.154856, source classification loss 0.167699, loss domain distinction 0.190526, accuracy domain distinction 0.500488\n",
      "VALIDATION Loss: 0.33770764 Acc: 0.87772926\n",
      "Epoch 13 of 500 took 0.258s\n",
      "Accuracy source 0.936035, main loss classifier 0.160762, source classification loss 0.180602, loss domain distinction 0.185823, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.32562870 Acc: 0.86899563\n",
      "Training complete in 0m 3s\n"
     ]
    }
   ],
   "source": [
    "# train_DANN(examples_datasets_train, labels_datasets_train, \n",
    "#           num_kernels=num_kernels,\n",
    "#           path_weights_fine_tuning=path_TSD,\n",
    "#           number_of_classes=number_of_classes,\n",
    "#           number_of_cycles_total = number_of_cycles_total,\n",
    "#           number_of_cycle_for_first_training = number_of_cycle_for_first_training,\n",
    "#           batch_size=batch_size,\n",
    "#           feature_vector_input_length=feature_vector_input_length,\n",
    "#           path_weights_to_save_to=path_DANN, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET one participant_examples  (9,)\n",
      "   GET one training_index_examples  (8, 572, 252)  at  0\n",
      "   GOT one group XY  (4576, 252)    (4576,)\n",
      "       one group XY test  (1144, 252)    (1144, 252)\n",
      "       one group XY train (4118, 252)    (4118,)\n",
      "       one group XY valid (458, 252)    (458, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  1\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  2\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  3\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  4\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  5\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  6\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  7\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  8\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "dataloaders: \n",
      "   train  (1, 9)\n",
      "   valid  (1, 9)\n",
      "   test  (1, 9)\n",
      "(9,)\n",
      "Participant ID:  0  Session ID:  0  Accuracy:  0.9527972027972028\n",
      "Participant ID:  0  Session ID:  1  Accuracy:  0.8269230769230769\n",
      "Participant ID:  0  Session ID:  2  Accuracy:  0.7202797202797203\n",
      "Participant ID:  0  Session ID:  3  Accuracy:  0.7552447552447552\n",
      "Participant ID:  0  Session ID:  4  Accuracy:  0.6276223776223776\n",
      "Participant ID:  0  Session ID:  5  Accuracy:  0.6923076923076923\n",
      "Participant ID:  0  Session ID:  6  Accuracy:  0.4597902097902098\n",
      "Participant ID:  0  Session ID:  7  Accuracy:  0.49125874125874125\n",
      "Participant ID:  0  Session ID:  8  Accuracy:  0.43356643356643354\n",
      "ACCURACY PARTICIPANT:  [0.9527972027972028, 0.8269230769230769, 0.7202797202797203, 0.7552447552447552, 0.6276223776223776, 0.6923076923076923, 0.4597902097902098, 0.49125874125874125, 0.43356643356643354]\n",
      "[[0.9527972  0.82692308 0.72027972 0.75524476 0.62762238 0.69230769\n",
      "  0.45979021 0.49125874 0.43356643]]\n",
      "[array([0.9527972 , 0.82692308, 0.72027972, 0.75524476, 0.62762238,\n",
      "       0.69230769, 0.45979021, 0.49125874, 0.43356643])]\n",
      "OVERALL ACCURACY: 0.6621989121989121\n"
     ]
    }
   ],
   "source": [
    "algo_name = \"DANN\"\n",
    "test_DANN_on_training_sessions(examples_datasets_train, labels_datasets_train,\n",
    "                              feature_vector_input_length=feature_vector_input_length,\n",
    "                              num_neurons=num_kernels, path_weights_DA=path_DANN,\n",
    "                              algo_name=algo_name, save_path = save_DANN, \n",
    "                              number_of_cycles_total=number_of_cycles_total,\n",
    "                              number_of_cycle_for_first_training = number_of_cycle_for_first_training,\n",
    "                              path_weights_normal=path_TSD, number_of_classes=number_of_classes,\n",
    "                              cycle_for_test=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Day_0~1</th>\n",
       "      <td>0.952797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_2</th>\n",
       "      <td>0.826923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_3</th>\n",
       "      <td>0.72028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_4</th>\n",
       "      <td>0.755245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_5</th>\n",
       "      <td>0.627622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_6</th>\n",
       "      <td>0.692308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_7</th>\n",
       "      <td>0.45979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_8</th>\n",
       "      <td>0.491259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_9</th>\n",
       "      <td>0.433566</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Participant_5\n",
       "Day_0~1      0.952797\n",
       "Day_2        0.826923\n",
       "Day_3         0.72028\n",
       "Day_4        0.755245\n",
       "Day_5        0.627622\n",
       "Day_6        0.692308\n",
       "Day_7         0.45979\n",
       "Day_8        0.491259\n",
       "Day_9        0.433566"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_filename = save_DANN + '/predictions_' + algo_name + \".npy\"\n",
    "results = np.load(results_filename, allow_pickle=True)\n",
    "DANN_acc = results[0]\n",
    "DANN_acc_overall = np.mean(DANN_acc)\n",
    "DANN_df = pd.DataFrame(DANN_acc.transpose(), \n",
    "                       index = [f'Day_{i}' for i in index_participant_list],\n",
    "                        columns = ['Participant_5'])\n",
    "DANN_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "DANN_df.transpose().plot.bar(rot=0, figsize=(10,5))\n",
    "plt.title(\"DANN Accuracies\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ground_truths  =  (1, 9)\n",
      "predictions =  (1, 9)\n",
      "index_participant_list  ['0~1', 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "accuracies_gestures =  (22, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Loc1_Sub5_Day0~1-&gt;0~1</th>\n",
       "      <th>Loc1_Sub5_Day0~1-&gt;2</th>\n",
       "      <th>Loc1_Sub5_Day0~1-&gt;3</th>\n",
       "      <th>Loc1_Sub5_Day0~1-&gt;4</th>\n",
       "      <th>Loc1_Sub5_Day0~1-&gt;5</th>\n",
       "      <th>Loc1_Sub5_Day0~1-&gt;6</th>\n",
       "      <th>Loc1_Sub5_Day0~1-&gt;7</th>\n",
       "      <th>Loc1_Sub5_Day0~1-&gt;8</th>\n",
       "      <th>Loc1_Sub5_Day0~1-&gt;9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M1</td>\n",
       "      <td>0.980769</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>0.653846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M2</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.730769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.192308</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M4</td>\n",
       "      <td>0.788462</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>M5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.269231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>M6</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.192308</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>M7</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>0.692308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>M8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.423077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>M9</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.307692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>M10</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.192308</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.076923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>M11</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.153846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>M12</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>0.538462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>M13</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.346154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>M14</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.346154</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.192308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>M15</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.269231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>M16</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.692308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>M17</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.884615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>M18</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>0.423077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>M19</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.346154</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.192308</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.192308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>M20</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.384615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>M21</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.307692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Mean</td>\n",
       "      <td>0.952797</td>\n",
       "      <td>0.826923</td>\n",
       "      <td>0.720280</td>\n",
       "      <td>0.755245</td>\n",
       "      <td>0.627622</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.459790</td>\n",
       "      <td>0.491259</td>\n",
       "      <td>0.433566</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Loc1_Sub5_Day0~1->0~1  Loc1_Sub5_Day0~1->2  \\\n",
       "0          M0               1.000000             1.000000   \n",
       "1          M1               0.980769             1.000000   \n",
       "2          M2               0.961538             0.769231   \n",
       "3          M3               1.000000             0.769231   \n",
       "4          M4               0.788462             1.000000   \n",
       "5          M5               1.000000             0.961538   \n",
       "6          M6               1.000000             1.000000   \n",
       "7          M7               1.000000             0.307692   \n",
       "8          M8               1.000000             0.653846   \n",
       "9          M9               1.000000             1.000000   \n",
       "10        M10               0.884615             0.923077   \n",
       "11        M11               1.000000             0.846154   \n",
       "12        M12               0.846154             0.769231   \n",
       "13        M13               0.846154             0.807692   \n",
       "14        M14               0.884615             0.384615   \n",
       "15        M15               0.846154             0.807692   \n",
       "16        M16               1.000000             1.000000   \n",
       "17        M17               1.000000             1.000000   \n",
       "18        M18               1.000000             1.000000   \n",
       "19        M19               1.000000             1.000000   \n",
       "20        M20               0.923077             0.538462   \n",
       "21        M21               1.000000             0.653846   \n",
       "22       Mean               0.952797             0.826923   \n",
       "\n",
       "    Loc1_Sub5_Day0~1->3  Loc1_Sub5_Day0~1->4  Loc1_Sub5_Day0~1->5  \\\n",
       "0              1.000000             1.000000             1.000000   \n",
       "1              0.615385             0.807692             1.000000   \n",
       "2              0.576923             0.846154             0.538462   \n",
       "3              0.038462             0.653846             0.423077   \n",
       "4              0.000000             1.000000             0.000000   \n",
       "5              1.000000             0.807692             0.807692   \n",
       "6              0.384615             1.000000             0.653846   \n",
       "7              0.961538             0.807692             0.461538   \n",
       "8              1.000000             1.000000             1.000000   \n",
       "9              0.730769             0.615385             0.576923   \n",
       "10             0.192308             0.384615             0.653846   \n",
       "11             0.653846             0.115385             0.153846   \n",
       "12             0.884615             0.769231             0.538462   \n",
       "13             1.000000             1.000000             1.000000   \n",
       "14             0.346154             0.038462             0.307692   \n",
       "15             0.769231             0.230769             0.692308   \n",
       "16             1.000000             1.000000             0.615385   \n",
       "17             1.000000             0.961538             1.000000   \n",
       "18             1.000000             0.923077             0.423077   \n",
       "19             0.961538             1.000000             0.346154   \n",
       "20             0.961538             0.923077             0.769231   \n",
       "21             0.769231             0.730769             0.846154   \n",
       "22             0.720280             0.755245             0.627622   \n",
       "\n",
       "    Loc1_Sub5_Day0~1->6  Loc1_Sub5_Day0~1->7  Loc1_Sub5_Day0~1->8  \\\n",
       "0              1.000000             1.000000             0.846154   \n",
       "1              0.769231             0.615385             0.653846   \n",
       "2              0.653846             0.230769             0.576923   \n",
       "3              0.192308             0.000000             0.000000   \n",
       "4              0.000000             0.000000             0.000000   \n",
       "5              0.961538             0.038462             0.576923   \n",
       "6              0.384615             0.192308             0.384615   \n",
       "7              1.000000             0.576923             0.653846   \n",
       "8              0.923077             0.923077             0.615385   \n",
       "9              0.769231             0.230769             0.692308   \n",
       "10             0.384615             0.307692             0.384615   \n",
       "11             0.730769             0.538462             0.230769   \n",
       "12             0.692308             0.500000             0.269231   \n",
       "13             0.923077             0.769231             0.961538   \n",
       "14             0.269231             0.000000             0.000000   \n",
       "15             0.269231             0.000000             0.000000   \n",
       "16             1.000000             0.884615             1.000000   \n",
       "17             1.000000             1.000000             1.000000   \n",
       "18             0.846154             0.538462             0.269231   \n",
       "19             0.884615             0.192308             0.230769   \n",
       "20             0.653846             0.884615             0.461538   \n",
       "21             0.923077             0.692308             1.000000   \n",
       "22             0.692308             0.459790             0.491259   \n",
       "\n",
       "    Loc1_Sub5_Day0~1->9  \n",
       "0              1.000000  \n",
       "1              0.653846  \n",
       "2              0.730769  \n",
       "3              0.000000  \n",
       "4              0.000000  \n",
       "5              0.269231  \n",
       "6              1.000000  \n",
       "7              0.692308  \n",
       "8              0.423077  \n",
       "9              0.307692  \n",
       "10             0.076923  \n",
       "11             0.153846  \n",
       "12             0.538462  \n",
       "13             0.346154  \n",
       "14             0.192308  \n",
       "15             0.269231  \n",
       "16             0.692308  \n",
       "17             0.884615  \n",
       "18             0.423077  \n",
       "19             0.192308  \n",
       "20             0.384615  \n",
       "21             0.307692  \n",
       "22             0.433566  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truths = results[1]\n",
    "predictions = results[2]\n",
    "print(\"ground_truths  = \", np.shape(ground_truths))\n",
    "print(\"predictions = \", np.shape(predictions))\n",
    "df = get_gesture_accuracies(ground_truths, predictions, number_of_classes=number_of_classes, \n",
    "                            m_name=m_name, n_name=n_name, path=save_TSD, algo_name=algo_name,\n",
    "                           index_participant_list_customized=index_participant_list,\n",
    "                           lump_day_at_participant=5)\n",
    "df = pd.read_csv(save_DANN+'/'+algo_name+'.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. SCADANN\n",
    "\n",
    "* `run_SCADANN_training_sessions`: train SCADANN model. The first session uses TSD model_0 wegits; others use DANN weights\n",
    "    * specify `percentage_same_gesture_stable` based on the performance of most pseudo labels: \n",
    "        * print accuracies out and check what percentage will optimize `ACCURACY MODEL` and `ACCURACY PSEUDO` without cutting out too much data \n",
    "    * num_sessions-1 sets of training weights will be saved\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TrainingsAndEvaluations.ForTrainingSessions.train_tsd_dnn_SCADANN import \\\n",
    "    run_SCADANN_training_sessions, test_network_SCADANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET one participant_examples  (9,)\n",
      "   GET one training_index_examples  (8, 572, 252)  at  0\n",
      "   GOT one group XY  (4576, 252)    (4576,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (4576, 252)    (4576,)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  1\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2288, 252)    (2288,)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  2\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2288, 252)    (2288,)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  3\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2288, 252)    (2288,)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  4\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2288, 252)    (2288,)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  5\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2288, 252)    (2288,)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  6\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2288, 252)    (2288,)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  7\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2288, 252)    (2288,)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  8\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2288, 252)    (2288,)\n",
      "dataloaders: \n",
      "   train  (1, 9)\n",
      "   valid  (0,)\n",
      "   test  (1, 0)\n",
      "participants_train =  1\n",
      "Optimizer =  <generator object Module.parameters at 0x7fd3cce93200>\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/DANN/participant_0/best_state_1.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/DANN/participant_0/best_state_1.pt' (epoch 19)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/TSD/participant_0/best_state_0.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/TSD/participant_0/best_state_0.pt' (epoch 27)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/DANN/participant_0/best_state_1.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/DANN/participant_0/best_state_1.pt' (epoch 19)\n",
      "==== models_array =  (2,)  @ session  1\n",
      "HANDLING NEW SESSION  0\n",
      "HANDLING NEW SESSION  1\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.2222222222222222  len before:  26   len after:  9\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  13\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  4\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  0.9230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.5769230769230769  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  15\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.8846153846153846  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  13\n",
      "BEFORE:  0.8076923076923077   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  1.0  len before:  26   len after:  11\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.6538461538461539  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.8518356643356644   Accuracy pseudo: 0.9589622641509434  len pseudo:  2120    len predictions 2288\n",
      "STARTING TRAINING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laiy/.local/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/laiy/.local/lib/python3.8/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy total 0.892428, main loss classifier 0.438295, source accuracy 0.932091 source classification loss 0.175593, target accuracy 0.852764 target loss 0.462704 accuracy domain distinction 0.500901 loss domain distinction 1.191465,\n",
      "VALIDATION Loss: 0.35993299 Acc: 0.89386792\n",
      "New best validation loss:  0.3599329888820648\n",
      "Epoch 2 of 500 took 0.354s\n",
      "Accuracy total 0.895132, main loss classifier 0.408784, source accuracy 0.930889 source classification loss 0.186189, target accuracy 0.859375 target loss 0.409969 accuracy domain distinction 0.500300 loss domain distinction 1.107047,\n",
      "VALIDATION Loss: 0.33608604 Acc: 0.90330189\n",
      "New best validation loss:  0.3360860390322549\n",
      "Epoch 3 of 500 took 0.340s\n",
      "Accuracy total 0.897837, main loss classifier 0.421586, source accuracy 0.927885 source classification loss 0.204240, target accuracy 0.867788 target loss 0.419629 accuracy domain distinction 0.500601 loss domain distinction 1.096520,\n",
      "VALIDATION Loss: 0.31078851 Acc: 0.91037736\n",
      "New best validation loss:  0.31078851222991943\n",
      "Epoch 4 of 500 took 0.378s\n",
      "Accuracy total 0.911058, main loss classifier 0.386720, source accuracy 0.938702 source classification loss 0.178791, target accuracy 0.883413 target loss 0.377835 accuracy domain distinction 0.499399 loss domain distinction 1.084075,\n",
      "VALIDATION Loss: 0.31767414 Acc: 0.90566038\n",
      "Epoch 5 of 500 took 0.344s\n",
      "Accuracy total 0.897837, main loss classifier 0.400979, source accuracy 0.924279 source classification loss 0.186824, target accuracy 0.871394 target loss 0.400116 accuracy domain distinction 0.500000 loss domain distinction 1.075093,\n",
      "VALIDATION Loss: 0.27778076 Acc: 0.91273585\n",
      "New best validation loss:  0.2777807574187006\n",
      "Epoch 6 of 500 took 0.344s\n",
      "Accuracy total 0.905048, main loss classifier 0.390054, source accuracy 0.933894 source classification loss 0.186945, target accuracy 0.876202 target loss 0.377960 accuracy domain distinction 0.499700 loss domain distinction 1.076012,\n",
      "VALIDATION Loss: 0.30017934 Acc: 0.89858491\n",
      "Epoch 7 of 500 took 0.343s\n",
      "Accuracy total 0.899940, main loss classifier 0.398744, source accuracy 0.925481 source classification loss 0.196661, target accuracy 0.874399 target loss 0.387474 accuracy domain distinction 0.501202 loss domain distinction 1.066761,\n",
      "VALIDATION Loss: 0.28033411 Acc: 0.91745283\n",
      "Epoch 8 of 500 took 0.345s\n",
      "Accuracy total 0.910757, main loss classifier 0.370248, source accuracy 0.927885 source classification loss 0.197807, target accuracy 0.893630 target loss 0.327986 accuracy domain distinction 0.501202 loss domain distinction 1.073514,\n",
      "VALIDATION Loss: 0.26656271 Acc: 0.90566038\n",
      "New best validation loss:  0.26656270559344974\n",
      "Epoch 9 of 500 took 0.341s\n",
      "Accuracy total 0.905349, main loss classifier 0.367357, source accuracy 0.930889 source classification loss 0.184541, target accuracy 0.879808 target loss 0.337174 accuracy domain distinction 0.500601 loss domain distinction 1.064991,\n",
      "VALIDATION Loss: 0.28564372 Acc: 0.88915094\n",
      "Epoch 10 of 500 took 0.345s\n",
      "Accuracy total 0.909255, main loss classifier 0.374451, source accuracy 0.929688 source classification loss 0.197548, target accuracy 0.888822 target loss 0.336591 accuracy domain distinction 0.499700 loss domain distinction 1.073813,\n",
      "VALIDATION Loss: 0.21207859 Acc: 0.92216981\n",
      "New best validation loss:  0.21207858622074127\n",
      "Epoch 11 of 500 took 0.343s\n",
      "Accuracy total 0.909856, main loss classifier 0.360075, source accuracy 0.924880 source classification loss 0.201032, target accuracy 0.894832 target loss 0.305785 accuracy domain distinction 0.501202 loss domain distinction 1.066667,\n",
      "VALIDATION Loss: 0.25345233 Acc: 0.90330189\n",
      "Epoch 12 of 500 took 0.339s\n",
      "Accuracy total 0.910457, main loss classifier 0.359459, source accuracy 0.930288 source classification loss 0.189745, target accuracy 0.890625 target loss 0.314098 accuracy domain distinction 0.499399 loss domain distinction 1.075372,\n",
      "VALIDATION Loss: 0.29128965 Acc: 0.90801887\n",
      "Epoch 13 of 500 took 0.346s\n",
      "Accuracy total 0.918269, main loss classifier 0.351883, source accuracy 0.936899 source classification loss 0.175813, target accuracy 0.899639 target loss 0.313631 accuracy domain distinction 0.499700 loss domain distinction 1.071608,\n",
      "VALIDATION Loss: 0.22452259 Acc: 0.92688679\n",
      "Epoch 14 of 500 took 0.341s\n",
      "Accuracy total 0.902945, main loss classifier 0.377772, source accuracy 0.924279 source classification loss 0.199345, target accuracy 0.881611 target loss 0.341684 accuracy domain distinction 0.498798 loss domain distinction 1.072578,\n",
      "VALIDATION Loss: 0.24401728 Acc: 0.91745283\n",
      "Epoch 15 of 500 took 0.341s\n",
      "Accuracy total 0.910156, main loss classifier 0.361057, source accuracy 0.927284 source classification loss 0.195084, target accuracy 0.893029 target loss 0.315782 accuracy domain distinction 0.500000 loss domain distinction 1.056240,\n",
      "VALIDATION Loss: 0.20743160 Acc: 0.91745283\n",
      "New best validation loss:  0.20743160162653243\n",
      "Epoch 16 of 500 took 0.348s\n",
      "Accuracy total 0.908353, main loss classifier 0.357074, source accuracy 0.924880 source classification loss 0.194434, target accuracy 0.891827 target loss 0.308748 accuracy domain distinction 0.500000 loss domain distinction 1.054827,\n",
      "VALIDATION Loss: 0.21009344 Acc: 0.93867925\n",
      "Epoch 17 of 500 took 0.341s\n",
      "Accuracy total 0.912560, main loss classifier 0.356528, source accuracy 0.933293 source classification loss 0.187098, target accuracy 0.891827 target loss 0.313777 accuracy domain distinction 0.500300 loss domain distinction 1.060903,\n",
      "VALIDATION Loss: 0.24194734 Acc: 0.90801887\n",
      "Epoch 18 of 500 took 0.339s\n",
      "Accuracy total 0.913762, main loss classifier 0.361341, source accuracy 0.936899 source classification loss 0.188605, target accuracy 0.890625 target loss 0.322101 accuracy domain distinction 0.500000 loss domain distinction 1.059877,\n",
      "VALIDATION Loss: 0.31733334 Acc: 0.90566038\n",
      "Epoch 19 of 500 took 0.344s\n",
      "Accuracy total 0.916166, main loss classifier 0.355144, source accuracy 0.933293 source classification loss 0.190049, target accuracy 0.899038 target loss 0.307372 accuracy domain distinction 0.500300 loss domain distinction 1.064330,\n",
      "VALIDATION Loss: 0.19972869 Acc: 0.93632075\n",
      "New best validation loss:  0.19972868583032063\n",
      "Epoch 20 of 500 took 0.343s\n",
      "Accuracy total 0.917969, main loss classifier 0.343326, source accuracy 0.932091 source classification loss 0.175523, target accuracy 0.903846 target loss 0.302482 accuracy domain distinction 0.500000 loss domain distinction 1.043239,\n",
      "VALIDATION Loss: 0.22982369 Acc: 0.91745283\n",
      "Epoch 21 of 500 took 0.341s\n",
      "Accuracy total 0.919471, main loss classifier 0.351636, source accuracy 0.937500 source classification loss 0.174692, target accuracy 0.901442 target loss 0.317749 accuracy domain distinction 0.500000 loss domain distinction 1.054156,\n",
      "VALIDATION Loss: 0.21426968 Acc: 0.91981132\n",
      "Epoch 22 of 500 took 0.345s\n",
      "Accuracy total 0.919171, main loss classifier 0.348272, source accuracy 0.932091 source classification loss 0.181276, target accuracy 0.906250 target loss 0.303130 accuracy domain distinction 0.500300 loss domain distinction 1.060690,\n",
      "VALIDATION Loss: 0.21615183 Acc: 0.9245283\n",
      "Epoch 23 of 500 took 0.349s\n",
      "Accuracy total 0.918570, main loss classifier 0.343413, source accuracy 0.933894 source classification loss 0.190618, target accuracy 0.903245 target loss 0.284505 accuracy domain distinction 0.499700 loss domain distinction 1.058516,\n",
      "VALIDATION Loss: 0.21036464 Acc: 0.92688679\n",
      "Epoch 24 of 500 took 0.341s\n",
      "Accuracy total 0.924880, main loss classifier 0.326453, source accuracy 0.939904 source classification loss 0.175097, target accuracy 0.909856 target loss 0.267215 accuracy domain distinction 0.500000 loss domain distinction 1.052969,\n",
      "VALIDATION Loss: 0.21069337 Acc: 0.92688679\n",
      "Epoch 25 of 500 took 0.346s\n",
      "Accuracy total 0.913762, main loss classifier 0.343716, source accuracy 0.932692 source classification loss 0.177813, target accuracy 0.894832 target loss 0.299970 accuracy domain distinction 0.499700 loss domain distinction 1.048248,\n",
      "VALIDATION Loss: 0.19842730 Acc: 0.93160377\n",
      "New best validation loss:  0.19842730249677384\n",
      "Epoch 26 of 500 took 0.341s\n",
      "Accuracy total 0.914363, main loss classifier 0.344597, source accuracy 0.928486 source classification loss 0.190106, target accuracy 0.900240 target loss 0.288968 accuracy domain distinction 0.500000 loss domain distinction 1.050596,\n",
      "VALIDATION Loss: 0.20062375 Acc: 0.92924528\n",
      "Epoch 27 of 500 took 0.341s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy total 0.919772, main loss classifier 0.338995, source accuracy 0.943510 source classification loss 0.167355, target accuracy 0.896034 target loss 0.299098 accuracy domain distinction 0.500000 loss domain distinction 1.057680,\n",
      "VALIDATION Loss: 0.26561458 Acc: 0.90566038\n",
      "Epoch 28 of 500 took 0.345s\n",
      "Accuracy total 0.913462, main loss classifier 0.361487, source accuracy 0.924880 source classification loss 0.212206, target accuracy 0.902043 target loss 0.300391 accuracy domain distinction 0.500000 loss domain distinction 1.051886,\n",
      "VALIDATION Loss: 0.24413155 Acc: 0.91745283\n",
      "Epoch 29 of 500 took 0.341s\n",
      "Accuracy total 0.925180, main loss classifier 0.330619, source accuracy 0.942909 source classification loss 0.174332, target accuracy 0.907452 target loss 0.276817 accuracy domain distinction 0.500000 loss domain distinction 1.050448,\n",
      "VALIDATION Loss: 0.20454640 Acc: 0.92924528\n",
      "Epoch 30 of 500 took 0.340s\n",
      "Accuracy total 0.925180, main loss classifier 0.336110, source accuracy 0.939904 source classification loss 0.185960, target accuracy 0.910457 target loss 0.276584 accuracy domain distinction 0.500000 loss domain distinction 1.048379,\n",
      "VALIDATION Loss: 0.20987836 Acc: 0.91745283\n",
      "Epoch 31 of 500 took 0.377s\n",
      "Accuracy total 0.914663, main loss classifier 0.356837, source accuracy 0.924279 source classification loss 0.206275, target accuracy 0.905048 target loss 0.298022 accuracy domain distinction 0.500000 loss domain distinction 1.046882,\n",
      "VALIDATION Loss: 0.21788003 Acc: 0.92216981\n",
      "Epoch    31: reducing learning rate of group 0 to 2.0120e-05.\n",
      "Epoch 32 of 500 took 0.342s\n",
      "Accuracy total 0.916166, main loss classifier 0.336754, source accuracy 0.928486 source classification loss 0.187331, target accuracy 0.903846 target loss 0.276913 accuracy domain distinction 0.499700 loss domain distinction 1.046321,\n",
      "VALIDATION Loss: 0.18597045 Acc: 0.93867925\n",
      "New best validation loss:  0.1859704474253314\n",
      "Epoch 33 of 500 took 0.345s\n",
      "Accuracy total 0.924579, main loss classifier 0.326301, source accuracy 0.932692 source classification loss 0.194165, target accuracy 0.916466 target loss 0.249281 accuracy domain distinction 0.500000 loss domain distinction 1.045778,\n",
      "VALIDATION Loss: 0.17532805 Acc: 0.92688679\n",
      "New best validation loss:  0.17532804608345032\n",
      "Epoch 34 of 500 took 0.345s\n",
      "Accuracy total 0.921274, main loss classifier 0.333798, source accuracy 0.929087 source classification loss 0.204517, target accuracy 0.913462 target loss 0.252051 accuracy domain distinction 0.500000 loss domain distinction 1.055143,\n",
      "VALIDATION Loss: 0.21541117 Acc: 0.91745283\n",
      "Epoch 35 of 500 took 0.340s\n",
      "Accuracy total 0.926382, main loss classifier 0.324115, source accuracy 0.930288 source classification loss 0.191290, target accuracy 0.922476 target loss 0.246982 accuracy domain distinction 0.500000 loss domain distinction 1.049789,\n",
      "VALIDATION Loss: 0.24450845 Acc: 0.91745283\n",
      "Epoch 36 of 500 took 0.341s\n",
      "Accuracy total 0.921575, main loss classifier 0.323236, source accuracy 0.929688 source classification loss 0.192729, target accuracy 0.913462 target loss 0.244021 accuracy domain distinction 0.500000 loss domain distinction 1.048607,\n",
      "VALIDATION Loss: 0.17813914 Acc: 0.93396226\n",
      "Epoch 37 of 500 took 0.344s\n",
      "Accuracy total 0.925180, main loss classifier 0.322365, source accuracy 0.939303 source classification loss 0.174536, target accuracy 0.911058 target loss 0.263241 accuracy domain distinction 0.500000 loss domain distinction 1.034767,\n",
      "VALIDATION Loss: 0.17446062 Acc: 0.94339623\n",
      "New best validation loss:  0.17446062181677138\n",
      "Epoch 38 of 500 took 0.341s\n",
      "Accuracy total 0.920373, main loss classifier 0.339165, source accuracy 0.938101 source classification loss 0.196725, target accuracy 0.902644 target loss 0.272279 accuracy domain distinction 0.500000 loss domain distinction 1.046637,\n",
      "VALIDATION Loss: 0.21023293 Acc: 0.93160377\n",
      "Epoch 39 of 500 took 0.340s\n",
      "Accuracy total 0.919171, main loss classifier 0.336768, source accuracy 0.929087 source classification loss 0.200047, target accuracy 0.909255 target loss 0.264574 accuracy domain distinction 0.500000 loss domain distinction 1.044571,\n",
      "VALIDATION Loss: 0.20847618 Acc: 0.92924528\n",
      "Epoch 40 of 500 took 0.339s\n",
      "Accuracy total 0.924880, main loss classifier 0.314271, source accuracy 0.940505 source classification loss 0.174053, target accuracy 0.909255 target loss 0.245467 accuracy domain distinction 0.500000 loss domain distinction 1.045114,\n",
      "VALIDATION Loss: 0.19418953 Acc: 0.9245283\n",
      "Epoch 41 of 500 took 0.338s\n",
      "Accuracy total 0.924579, main loss classifier 0.325296, source accuracy 0.932692 source classification loss 0.184700, target accuracy 0.916466 target loss 0.255996 accuracy domain distinction 0.500000 loss domain distinction 1.049478,\n",
      "VALIDATION Loss: 0.19364814 Acc: 0.9245283\n",
      "Epoch 42 of 500 took 0.344s\n",
      "Accuracy total 0.923077, main loss classifier 0.325914, source accuracy 0.930889 source classification loss 0.194658, target accuracy 0.915264 target loss 0.248084 accuracy domain distinction 0.500000 loss domain distinction 1.045432,\n",
      "VALIDATION Loss: 0.18919086 Acc: 0.92924528\n",
      "Epoch 43 of 500 took 0.343s\n",
      "Accuracy total 0.922776, main loss classifier 0.330744, source accuracy 0.935697 source classification loss 0.186684, target accuracy 0.909856 target loss 0.266203 accuracy domain distinction 0.500000 loss domain distinction 1.043008,\n",
      "VALIDATION Loss: 0.20586086 Acc: 0.93160377\n",
      "Epoch    43: reducing learning rate of group 0 to 4.0240e-06.\n",
      "Epoch 44 of 500 took 0.339s\n",
      "Accuracy total 0.920974, main loss classifier 0.332924, source accuracy 0.931490 source classification loss 0.192819, target accuracy 0.910457 target loss 0.264202 accuracy domain distinction 0.500000 loss domain distinction 1.044138,\n",
      "VALIDATION Loss: 0.21396655 Acc: 0.93396226\n",
      "Epoch 45 of 500 took 0.343s\n",
      "Accuracy total 0.919471, main loss classifier 0.342874, source accuracy 0.929087 source classification loss 0.198300, target accuracy 0.909856 target loss 0.278614 accuracy domain distinction 0.500000 loss domain distinction 1.044170,\n",
      "VALIDATION Loss: 0.18704471 Acc: 0.92924528\n",
      "Epoch 46 of 500 took 0.340s\n",
      "Accuracy total 0.920072, main loss classifier 0.335270, source accuracy 0.933894 source classification loss 0.177007, target accuracy 0.906250 target loss 0.284296 accuracy domain distinction 0.500000 loss domain distinction 1.046187,\n",
      "VALIDATION Loss: 0.17892223 Acc: 0.93396226\n",
      "Epoch 47 of 500 took 0.340s\n",
      "Accuracy total 0.920673, main loss classifier 0.331098, source accuracy 0.939303 source classification loss 0.173723, target accuracy 0.902043 target loss 0.279613 accuracy domain distinction 0.500000 loss domain distinction 1.044308,\n",
      "VALIDATION Loss: 0.33861190 Acc: 0.90566038\n",
      "Epoch 48 of 500 took 0.345s\n",
      "Accuracy total 0.924579, main loss classifier 0.334443, source accuracy 0.935697 source classification loss 0.185826, target accuracy 0.913462 target loss 0.274546 accuracy domain distinction 0.500000 loss domain distinction 1.042570,\n",
      "VALIDATION Loss: 0.18776440 Acc: 0.92924528\n",
      "Epoch 49 of 500 took 0.340s\n",
      "Training complete in 0m 17s\n",
      "['participant_0']\n",
      "Optimizer =  <generator object Module.parameters at 0x7fd3c5406b30>\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/DANN/participant_0/best_state_2.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/DANN/participant_0/best_state_2.pt' (epoch 22)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/TSD/participant_0/best_state_0.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/TSD/participant_0/best_state_0.pt' (epoch 27)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/DANN/participant_0/best_state_1.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/DANN/participant_0/best_state_1.pt' (epoch 19)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/DANN/participant_0/best_state_2.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/DANN/participant_0/best_state_2.pt' (epoch 22)\n",
      "==== models_array =  (3,)  @ session  2\n",
      "HANDLING NEW SESSION  0\n",
      "HANDLING NEW SESSION  1\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.2222222222222222  len before:  26   len after:  9\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  13\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  4\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  0.9230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.5769230769230769  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  15\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.8846153846153846  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  13\n",
      "BEFORE:  0.8076923076923077   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  1.0  len before:  26   len after:  11\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.6538461538461539  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.8518356643356644   Accuracy pseudo: 0.9589622641509434  len pseudo:  2120    len predictions 2288\n",
      "HANDLING NEW SESSION  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.0  len before:  26   len after:  7\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.0  len before:  26   len after:  5\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.5  len before:  26   len after:  4\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.4230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  1\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  4\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  6\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.23076923076923078  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  8\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  9\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.0  len before:  26   len after:  17\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.5769230769230769  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  7\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.7777777777777778  len before:  26   len after:  9\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  18\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.6666666666666666  len before:  26   len after:  9\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.9230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.4230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.9230769230769231  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.7434440559440559   Accuracy pseudo: 0.8225646123260437  len pseudo:  2012    len predictions 2288\n",
      "STARTING TRAINING\n",
      "Accuracy total 0.835938, main loss classifier 0.671336, source accuracy 0.892500 source classification loss 0.322511, target accuracy 0.779375 target loss 0.782755 accuracy domain distinction 0.500000 loss domain distinction 1.187024,\n",
      "VALIDATION Loss: 0.48469904 Acc: 0.84119107\n",
      "New best validation loss:  0.48469904490879606\n",
      "Epoch 2 of 500 took 0.401s\n",
      "Accuracy total 0.866563, main loss classifier 0.564258, source accuracy 0.899375 source classification loss 0.292256, target accuracy 0.833750 target loss 0.615101 accuracy domain distinction 0.500313 loss domain distinction 1.105795,\n",
      "VALIDATION Loss: 0.41198063 Acc: 0.86104218\n",
      "New best validation loss:  0.4119806332247598\n",
      "Epoch 3 of 500 took 0.442s\n",
      "Accuracy total 0.869062, main loss classifier 0.541677, source accuracy 0.910000 source classification loss 0.284891, target accuracy 0.828125 target loss 0.583017 accuracy domain distinction 0.500313 loss domain distinction 1.077229,\n",
      "VALIDATION Loss: 0.43975012 Acc: 0.85111663\n",
      "Epoch 4 of 500 took 0.404s\n",
      "Accuracy total 0.865625, main loss classifier 0.525757, source accuracy 0.905625 source classification loss 0.291110, target accuracy 0.825625 target loss 0.546280 accuracy domain distinction 0.498750 loss domain distinction 1.070623,\n",
      "VALIDATION Loss: 0.37522326 Acc: 0.87593052\n",
      "New best validation loss:  0.3752232640981674\n",
      "Epoch 5 of 500 took 0.413s\n",
      "Accuracy total 0.872500, main loss classifier 0.492175, source accuracy 0.901250 source classification loss 0.276725, target accuracy 0.843750 target loss 0.493880 accuracy domain distinction 0.500000 loss domain distinction 1.068728,\n",
      "VALIDATION Loss: 0.31566237 Acc: 0.87841191\n",
      "New best validation loss:  0.31566236700330463\n",
      "Epoch 6 of 500 took 0.414s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy total 0.874687, main loss classifier 0.484760, source accuracy 0.910625 source classification loss 0.255283, target accuracy 0.838750 target loss 0.498833 accuracy domain distinction 0.500625 loss domain distinction 1.077017,\n",
      "VALIDATION Loss: 0.31126656 Acc: 0.90074442\n",
      "New best validation loss:  0.3112665648971285\n",
      "Epoch 7 of 500 took 0.367s\n",
      "Accuracy total 0.870000, main loss classifier 0.512876, source accuracy 0.901250 source classification loss 0.296733, target accuracy 0.838750 target loss 0.516347 accuracy domain distinction 0.498750 loss domain distinction 1.063358,\n",
      "VALIDATION Loss: 0.30268978 Acc: 0.88833747\n",
      "New best validation loss:  0.30268978221075876\n",
      "Epoch 8 of 500 took 0.331s\n",
      "Accuracy total 0.873125, main loss classifier 0.467586, source accuracy 0.898750 source classification loss 0.274009, target accuracy 0.847500 target loss 0.447301 accuracy domain distinction 0.498750 loss domain distinction 1.069305,\n",
      "VALIDATION Loss: 0.36622598 Acc: 0.86848635\n",
      "Epoch 9 of 500 took 0.325s\n",
      "Accuracy total 0.882188, main loss classifier 0.472198, source accuracy 0.912500 source classification loss 0.269774, target accuracy 0.851875 target loss 0.460755 accuracy domain distinction 0.500000 loss domain distinction 1.069333,\n",
      "VALIDATION Loss: 0.34143167 Acc: 0.87841191\n",
      "Epoch 10 of 500 took 0.327s\n",
      "Accuracy total 0.880313, main loss classifier 0.491677, source accuracy 0.911250 source classification loss 0.265041, target accuracy 0.849375 target loss 0.505332 accuracy domain distinction 0.500000 loss domain distinction 1.064900,\n",
      "VALIDATION Loss: 0.41992943 Acc: 0.86352357\n",
      "Epoch 11 of 500 took 0.329s\n",
      "Accuracy total 0.878750, main loss classifier 0.478366, source accuracy 0.911875 source classification loss 0.282507, target accuracy 0.845625 target loss 0.460739 accuracy domain distinction 0.500313 loss domain distinction 1.067438,\n",
      "VALIDATION Loss: 0.34254622 Acc: 0.87841191\n",
      "Epoch 12 of 500 took 0.327s\n",
      "Accuracy total 0.871875, main loss classifier 0.464619, source accuracy 0.898750 source classification loss 0.275615, target accuracy 0.845000 target loss 0.442734 accuracy domain distinction 0.500313 loss domain distinction 1.054445,\n",
      "VALIDATION Loss: 0.31681104 Acc: 0.89578164\n",
      "Epoch 13 of 500 took 0.330s\n",
      "Accuracy total 0.879062, main loss classifier 0.468764, source accuracy 0.908750 source classification loss 0.283539, target accuracy 0.849375 target loss 0.442480 accuracy domain distinction 0.500313 loss domain distinction 1.057536,\n",
      "VALIDATION Loss: 0.31935121 Acc: 0.89081886\n",
      "Epoch    13: reducing learning rate of group 0 to 2.0120e-05.\n",
      "Epoch 14 of 500 took 0.325s\n",
      "Accuracy total 0.882188, main loss classifier 0.447270, source accuracy 0.905625 source classification loss 0.258948, target accuracy 0.858750 target loss 0.425026 accuracy domain distinction 0.500313 loss domain distinction 1.052834,\n",
      "VALIDATION Loss: 0.33438457 Acc: 0.89330025\n",
      "Epoch 15 of 500 took 0.326s\n",
      "Accuracy total 0.882188, main loss classifier 0.469693, source accuracy 0.908125 source classification loss 0.266408, target accuracy 0.856250 target loss 0.461451 accuracy domain distinction 0.500000 loss domain distinction 1.057631,\n",
      "VALIDATION Loss: 0.29916170 Acc: 0.90074442\n",
      "New best validation loss:  0.2991616981370108\n",
      "Epoch 16 of 500 took 0.331s\n",
      "Accuracy total 0.883125, main loss classifier 0.451880, source accuracy 0.905000 source classification loss 0.261868, target accuracy 0.861250 target loss 0.430420 accuracy domain distinction 0.500000 loss domain distinction 1.057362,\n",
      "VALIDATION Loss: 0.26182310 Acc: 0.89826303\n",
      "New best validation loss:  0.26182309857436586\n",
      "Epoch 17 of 500 took 0.337s\n",
      "Accuracy total 0.876875, main loss classifier 0.450136, source accuracy 0.903750 source classification loss 0.264387, target accuracy 0.850000 target loss 0.425588 accuracy domain distinction 0.499688 loss domain distinction 1.051492,\n",
      "VALIDATION Loss: 0.32929776 Acc: 0.90322581\n",
      "Epoch 18 of 500 took 0.327s\n",
      "Accuracy total 0.883125, main loss classifier 0.441244, source accuracy 0.915000 source classification loss 0.253643, target accuracy 0.851250 target loss 0.417626 accuracy domain distinction 0.500625 loss domain distinction 1.056096,\n",
      "VALIDATION Loss: 0.29988959 Acc: 0.88337469\n",
      "Epoch 19 of 500 took 0.331s\n",
      "Accuracy total 0.885000, main loss classifier 0.443215, source accuracy 0.914375 source classification loss 0.242880, target accuracy 0.855625 target loss 0.433802 accuracy domain distinction 0.500000 loss domain distinction 1.048733,\n",
      "VALIDATION Loss: 0.30716539 Acc: 0.89081886\n",
      "Epoch 20 of 500 took 0.326s\n",
      "Accuracy total 0.889687, main loss classifier 0.449439, source accuracy 0.917500 source classification loss 0.250038, target accuracy 0.861875 target loss 0.437913 accuracy domain distinction 0.499688 loss domain distinction 1.054637,\n",
      "VALIDATION Loss: 0.28546887 Acc: 0.89826303\n",
      "Epoch 21 of 500 took 0.326s\n",
      "Accuracy total 0.885938, main loss classifier 0.457743, source accuracy 0.914375 source classification loss 0.257569, target accuracy 0.857500 target loss 0.447634 accuracy domain distinction 0.499688 loss domain distinction 1.051418,\n",
      "VALIDATION Loss: 0.34907259 Acc: 0.88833747\n",
      "Epoch 22 of 500 took 0.330s\n",
      "Accuracy total 0.885938, main loss classifier 0.434258, source accuracy 0.906250 source classification loss 0.251770, target accuracy 0.865625 target loss 0.405509 accuracy domain distinction 0.500625 loss domain distinction 1.056181,\n",
      "VALIDATION Loss: 0.31605449 Acc: 0.9057072\n",
      "Epoch    22: reducing learning rate of group 0 to 4.0240e-06.\n",
      "Epoch 23 of 500 took 0.331s\n",
      "Accuracy total 0.881250, main loss classifier 0.455858, source accuracy 0.910000 source classification loss 0.250526, target accuracy 0.852500 target loss 0.450918 accuracy domain distinction 0.499688 loss domain distinction 1.051355,\n",
      "VALIDATION Loss: 0.38180707 Acc: 0.89826303\n",
      "Epoch 24 of 500 took 0.328s\n",
      "Accuracy total 0.875938, main loss classifier 0.468938, source accuracy 0.903125 source classification loss 0.262308, target accuracy 0.848750 target loss 0.462484 accuracy domain distinction 0.500313 loss domain distinction 1.065425,\n",
      "VALIDATION Loss: 0.34274296 Acc: 0.87344913\n",
      "Epoch 25 of 500 took 0.328s\n",
      "Accuracy total 0.885625, main loss classifier 0.454893, source accuracy 0.906875 source classification loss 0.266573, target accuracy 0.864375 target loss 0.432743 accuracy domain distinction 0.500313 loss domain distinction 1.052350,\n",
      "VALIDATION Loss: 0.31814732 Acc: 0.89330025\n",
      "Epoch 26 of 500 took 0.336s\n",
      "Accuracy total 0.892813, main loss classifier 0.436474, source accuracy 0.920625 source classification loss 0.242386, target accuracy 0.865000 target loss 0.420669 accuracy domain distinction 0.500313 loss domain distinction 1.049470,\n",
      "VALIDATION Loss: 0.25287271 Acc: 0.91811414\n",
      "New best validation loss:  0.252872710781438\n",
      "Epoch 27 of 500 took 0.331s\n",
      "Accuracy total 0.882188, main loss classifier 0.442494, source accuracy 0.903125 source classification loss 0.281392, target accuracy 0.861250 target loss 0.392433 accuracy domain distinction 0.500313 loss domain distinction 1.055816,\n",
      "VALIDATION Loss: 0.33178492 Acc: 0.89081886\n",
      "Epoch 28 of 500 took 0.327s\n",
      "Accuracy total 0.880313, main loss classifier 0.454460, source accuracy 0.900000 source classification loss 0.286915, target accuracy 0.860625 target loss 0.411278 accuracy domain distinction 0.500313 loss domain distinction 1.053641,\n",
      "VALIDATION Loss: 0.29244620 Acc: 0.89330025\n",
      "Epoch 29 of 500 took 0.332s\n",
      "Accuracy total 0.872188, main loss classifier 0.478570, source accuracy 0.903125 source classification loss 0.289494, target accuracy 0.841250 target loss 0.456411 accuracy domain distinction 0.500000 loss domain distinction 1.056182,\n",
      "VALIDATION Loss: 0.26004033 Acc: 0.90818859\n",
      "Epoch 30 of 500 took 0.327s\n",
      "Accuracy total 0.888125, main loss classifier 0.435569, source accuracy 0.903750 source classification loss 0.270516, target accuracy 0.872500 target loss 0.391084 accuracy domain distinction 0.500313 loss domain distinction 1.047685,\n",
      "VALIDATION Loss: 0.30503062 Acc: 0.8808933\n",
      "Epoch 31 of 500 took 0.334s\n",
      "Accuracy total 0.892188, main loss classifier 0.435974, source accuracy 0.911250 source classification loss 0.264021, target accuracy 0.873125 target loss 0.397011 accuracy domain distinction 0.500000 loss domain distinction 1.054577,\n",
      "VALIDATION Loss: 0.33267668 Acc: 0.87593052\n",
      "Epoch 32 of 500 took 0.335s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy total 0.891250, main loss classifier 0.440251, source accuracy 0.913750 source classification loss 0.263144, target accuracy 0.868750 target loss 0.406724 accuracy domain distinction 0.500625 loss domain distinction 1.053172,\n",
      "VALIDATION Loss: 0.36157773 Acc: 0.86848635\n",
      "Epoch    32: reducing learning rate of group 0 to 8.0480e-07.\n",
      "Epoch 33 of 500 took 0.329s\n",
      "Accuracy total 0.879375, main loss classifier 0.458410, source accuracy 0.906250 source classification loss 0.265168, target accuracy 0.852500 target loss 0.441971 accuracy domain distinction 0.499688 loss domain distinction 1.048408,\n",
      "VALIDATION Loss: 0.30815374 Acc: 0.88585608\n",
      "Epoch 34 of 500 took 0.329s\n",
      "Accuracy total 0.879062, main loss classifier 0.452407, source accuracy 0.910000 source classification loss 0.243372, target accuracy 0.848125 target loss 0.449043 accuracy domain distinction 0.499688 loss domain distinction 1.061995,\n",
      "VALIDATION Loss: 0.32991060 Acc: 0.88585608\n",
      "Epoch 35 of 500 took 0.332s\n",
      "Accuracy total 0.885312, main loss classifier 0.455241, source accuracy 0.912500 source classification loss 0.267265, target accuracy 0.858125 target loss 0.432619 accuracy domain distinction 0.498750 loss domain distinction 1.052991,\n",
      "VALIDATION Loss: 0.25662316 Acc: 0.89578164\n",
      "Epoch 36 of 500 took 0.326s\n",
      "Accuracy total 0.887813, main loss classifier 0.439788, source accuracy 0.915000 source classification loss 0.244624, target accuracy 0.860625 target loss 0.423761 accuracy domain distinction 0.500313 loss domain distinction 1.055951,\n",
      "VALIDATION Loss: 0.26786442 Acc: 0.9057072\n",
      "Epoch 37 of 500 took 0.329s\n",
      "Accuracy total 0.884687, main loss classifier 0.440465, source accuracy 0.903750 source classification loss 0.262321, target accuracy 0.865625 target loss 0.407568 accuracy domain distinction 0.500313 loss domain distinction 1.055207,\n",
      "VALIDATION Loss: 0.26814543 Acc: 0.90818859\n",
      "Epoch 38 of 500 took 0.331s\n",
      "Training complete in 0m 13s\n",
      "['participant_0']\n",
      "Optimizer =  <generator object Module.parameters at 0x7fd3c5406b30>\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/DANN/participant_0/best_state_3.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/DANN/participant_0/best_state_3.pt' (epoch 5)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/TSD/participant_0/best_state_0.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/TSD/participant_0/best_state_0.pt' (epoch 27)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/DANN/participant_0/best_state_1.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/DANN/participant_0/best_state_1.pt' (epoch 19)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/DANN/participant_0/best_state_2.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/DANN/participant_0/best_state_2.pt' (epoch 22)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/DANN/participant_0/best_state_3.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/DANN/participant_0/best_state_3.pt' (epoch 5)\n",
      "==== models_array =  (4,)  @ session  3\n",
      "HANDLING NEW SESSION  0\n",
      "HANDLING NEW SESSION  1\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.2222222222222222  len before:  26   len after:  9\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  13\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  4\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  0.9230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.5769230769230769  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  15\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.8846153846153846  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  13\n",
      "BEFORE:  0.8076923076923077   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  1.0  len before:  26   len after:  11\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.6538461538461539  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.8518356643356644   Accuracy pseudo: 0.9589622641509434  len pseudo:  2120    len predictions 2288\n",
      "HANDLING NEW SESSION  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.0  len before:  26   len after:  7\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.0  len before:  26   len after:  5\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.5  len before:  26   len after:  4\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.4230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  1\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  4\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  6\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.23076923076923078  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  8\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  9\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.0  len before:  26   len after:  17\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.5769230769230769  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  7\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.7777777777777778  len before:  26   len after:  9\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  18\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.6666666666666666  len before:  26   len after:  9\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.9230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.4230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.9230769230769231  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.7434440559440559   Accuracy pseudo: 0.8225646123260437  len pseudo:  2012    len predictions 2288\n",
      "HANDLING NEW SESSION  3\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.5384615384615384  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.6538461538461539  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.46153846153846156  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  14\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  18\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.4230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.0  len before:  26   len after:  4\n",
      "BEFORE:  0.38461538461538464   AFTER:  1.0  len before:  26   len after:  12\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.0  len before:  26   len after:  5\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  8\n",
      "BEFORE:  0.5   AFTER:  0.6  len before:  26   len after:  10\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.4  len before:  26   len after:  10\n",
      "BEFORE:  0.11538461538461539   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.6923076923076923  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  15\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  5\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  8\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.11538461538461539   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  9\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  8\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  6\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  1.0  len before:  26   len after:  7\n",
      "BEFORE:  0.11538461538461539   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  8\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.7473776223776224   Accuracy pseudo: 0.8564102564102564  len pseudo:  1950    len predictions 2288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING TRAINING\n",
      "Accuracy total 0.851562, main loss classifier 0.599789, source accuracy 0.877604 source classification loss 0.382038, target accuracy 0.825521 target loss 0.587757 accuracy domain distinction 0.500326 loss domain distinction 1.148917,\n",
      "VALIDATION Loss: 0.30954473 Acc: 0.9025641\n",
      "New best validation loss:  0.3095447282705988\n",
      "Epoch 2 of 500 took 0.319s\n",
      "Accuracy total 0.866211, main loss classifier 0.497558, source accuracy 0.864583 source classification loss 0.363289, target accuracy 0.867839 target loss 0.411820 accuracy domain distinction 0.500651 loss domain distinction 1.100036,\n",
      "VALIDATION Loss: 0.23463452 Acc: 0.91794872\n",
      "New best validation loss:  0.2346345212842737\n",
      "Epoch 3 of 500 took 0.317s\n",
      "Accuracy total 0.873372, main loss classifier 0.497466, source accuracy 0.876953 source classification loss 0.376568, target accuracy 0.869792 target loss 0.400779 accuracy domain distinction 0.498047 loss domain distinction 1.087925,\n",
      "VALIDATION Loss: 0.27597829 Acc: 0.91282051\n",
      "Epoch 4 of 500 took 0.315s\n",
      "Accuracy total 0.868164, main loss classifier 0.489145, source accuracy 0.876953 source classification loss 0.351200, target accuracy 0.859375 target loss 0.410745 accuracy domain distinction 0.497721 loss domain distinction 1.081724,\n",
      "VALIDATION Loss: 0.20763922 Acc: 0.92564103\n",
      "New best validation loss:  0.20763922376292093\n",
      "Epoch 5 of 500 took 0.315s\n",
      "Accuracy total 0.880859, main loss classifier 0.459884, source accuracy 0.884766 source classification loss 0.347982, target accuracy 0.876953 target loss 0.357865 accuracy domain distinction 0.500326 loss domain distinction 1.069610,\n",
      "VALIDATION Loss: 0.22368805 Acc: 0.92307692\n",
      "Epoch 6 of 500 took 0.318s\n",
      "Accuracy total 0.867188, main loss classifier 0.476863, source accuracy 0.864583 source classification loss 0.357130, target accuracy 0.869792 target loss 0.383393 accuracy domain distinction 0.500000 loss domain distinction 1.066018,\n",
      "VALIDATION Loss: 0.20514354 Acc: 0.91025641\n",
      "New best validation loss:  0.20514354429074697\n",
      "Epoch 7 of 500 took 0.320s\n",
      "Accuracy total 0.887044, main loss classifier 0.438748, source accuracy 0.878906 source classification loss 0.363254, target accuracy 0.895182 target loss 0.301337 accuracy domain distinction 0.500000 loss domain distinction 1.064517,\n",
      "VALIDATION Loss: 0.22175887 Acc: 0.92051282\n",
      "Epoch 8 of 500 took 0.315s\n",
      "Accuracy total 0.890299, main loss classifier 0.427272, source accuracy 0.882161 source classification loss 0.326170, target accuracy 0.898438 target loss 0.316414 accuracy domain distinction 0.500000 loss domain distinction 1.059798,\n",
      "VALIDATION Loss: 0.21802470 Acc: 0.93076923\n",
      "Epoch 9 of 500 took 0.325s\n",
      "Accuracy total 0.888672, main loss classifier 0.420465, source accuracy 0.878255 source classification loss 0.338949, target accuracy 0.899089 target loss 0.290697 accuracy domain distinction 0.500000 loss domain distinction 1.056424,\n",
      "VALIDATION Loss: 0.24427031 Acc: 0.91538462\n",
      "Epoch 10 of 500 took 0.330s\n",
      "Accuracy total 0.883138, main loss classifier 0.440332, source accuracy 0.880208 source classification loss 0.344408, target accuracy 0.886068 target loss 0.326478 accuracy domain distinction 0.500000 loss domain distinction 1.048895,\n",
      "VALIDATION Loss: 0.17402323 Acc: 0.92051282\n",
      "New best validation loss:  0.17402322803224837\n",
      "Epoch 11 of 500 took 0.329s\n",
      "Accuracy total 0.889648, main loss classifier 0.434090, source accuracy 0.875000 source classification loss 0.352711, target accuracy 0.904297 target loss 0.305671 accuracy domain distinction 0.500000 loss domain distinction 1.048989,\n",
      "VALIDATION Loss: 0.19057481 Acc: 0.94615385\n",
      "Epoch 12 of 500 took 0.319s\n",
      "Accuracy total 0.891602, main loss classifier 0.419785, source accuracy 0.888021 source classification loss 0.319539, target accuracy 0.895182 target loss 0.309672 accuracy domain distinction 0.500000 loss domain distinction 1.051792,\n",
      "VALIDATION Loss: 0.21809394 Acc: 0.91282051\n",
      "Epoch 13 of 500 took 0.327s\n",
      "Accuracy total 0.891602, main loss classifier 0.415335, source accuracy 0.880208 source classification loss 0.319717, target accuracy 0.902995 target loss 0.301068 accuracy domain distinction 0.500000 loss domain distinction 1.049429,\n",
      "VALIDATION Loss: 0.21324480 Acc: 0.94358974\n",
      "Epoch 14 of 500 took 0.318s\n",
      "Accuracy total 0.895833, main loss classifier 0.397413, source accuracy 0.882161 source classification loss 0.321946, target accuracy 0.909505 target loss 0.263833 accuracy domain distinction 0.500000 loss domain distinction 1.045232,\n",
      "VALIDATION Loss: 0.20985653 Acc: 0.93076923\n",
      "Epoch 15 of 500 took 0.324s\n",
      "Accuracy total 0.901042, main loss classifier 0.401472, source accuracy 0.896484 source classification loss 0.318406, target accuracy 0.905599 target loss 0.276570 accuracy domain distinction 0.500000 loss domain distinction 1.039845,\n",
      "VALIDATION Loss: 0.18400306 Acc: 0.92307692\n",
      "Epoch 16 of 500 took 0.324s\n",
      "Accuracy total 0.907227, main loss classifier 0.391161, source accuracy 0.899089 source classification loss 0.315149, target accuracy 0.915365 target loss 0.257520 accuracy domain distinction 0.500000 loss domain distinction 1.048259,\n",
      "VALIDATION Loss: 0.17140312 Acc: 0.93076923\n",
      "New best validation loss:  0.17140311960663115\n",
      "Epoch 17 of 500 took 0.335s\n",
      "Accuracy total 0.892253, main loss classifier 0.411195, source accuracy 0.878906 source classification loss 0.332537, target accuracy 0.905599 target loss 0.280347 accuracy domain distinction 0.500000 loss domain distinction 1.047527,\n",
      "VALIDATION Loss: 0.15482008 Acc: 0.94615385\n",
      "New best validation loss:  0.1548200792499951\n",
      "Epoch 18 of 500 took 0.329s\n",
      "Accuracy total 0.900716, main loss classifier 0.394792, source accuracy 0.899089 source classification loss 0.292492, target accuracy 0.902344 target loss 0.288357 accuracy domain distinction 0.500000 loss domain distinction 1.043679,\n",
      "VALIDATION Loss: 0.27947319 Acc: 0.91025641\n",
      "Epoch 19 of 500 took 0.330s\n",
      "Accuracy total 0.893229, main loss classifier 0.385310, source accuracy 0.876302 source classification loss 0.313459, target accuracy 0.910156 target loss 0.247251 accuracy domain distinction 0.500000 loss domain distinction 1.049555,\n",
      "VALIDATION Loss: 0.22497375 Acc: 0.92564103\n",
      "Epoch 20 of 500 took 0.334s\n",
      "Accuracy total 0.899089, main loss classifier 0.383446, source accuracy 0.890625 source classification loss 0.300421, target accuracy 0.907552 target loss 0.257348 accuracy domain distinction 0.500000 loss domain distinction 1.045619,\n",
      "VALIDATION Loss: 0.29464790 Acc: 0.93846154\n",
      "Epoch 21 of 500 took 0.346s\n",
      "Accuracy total 0.898763, main loss classifier 0.392851, source accuracy 0.888021 source classification loss 0.309385, target accuracy 0.909505 target loss 0.267402 accuracy domain distinction 0.500000 loss domain distinction 1.044577,\n",
      "VALIDATION Loss: 0.16987922 Acc: 0.94871795\n",
      "Epoch 22 of 500 took 0.349s\n",
      "Accuracy total 0.900065, main loss classifier 0.393678, source accuracy 0.892578 source classification loss 0.313440, target accuracy 0.907552 target loss 0.265607 accuracy domain distinction 0.500000 loss domain distinction 1.041546,\n",
      "VALIDATION Loss: 0.18073086 Acc: 0.94871795\n",
      "Epoch 23 of 500 took 0.376s\n",
      "Accuracy total 0.911458, main loss classifier 0.367833, source accuracy 0.897135 source classification loss 0.300540, target accuracy 0.925781 target loss 0.225966 accuracy domain distinction 0.500000 loss domain distinction 1.045800,\n",
      "VALIDATION Loss: 0.15855782 Acc: 0.94871795\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0060e-04.\n",
      "Epoch 24 of 500 took 0.368s\n",
      "Accuracy total 0.904948, main loss classifier 0.374763, source accuracy 0.897786 source classification loss 0.287970, target accuracy 0.912109 target loss 0.255463 accuracy domain distinction 0.500000 loss domain distinction 1.030465,\n",
      "VALIDATION Loss: 0.25533731 Acc: 0.93589744\n",
      "Epoch 25 of 500 took 0.336s\n",
      "Accuracy total 0.912435, main loss classifier 0.357901, source accuracy 0.904948 source classification loss 0.256624, target accuracy 0.919922 target loss 0.253191 accuracy domain distinction 0.500000 loss domain distinction 1.029930,\n",
      "VALIDATION Loss: 0.14601162 Acc: 0.94615385\n",
      "New best validation loss:  0.14601162395306996\n",
      "Epoch 26 of 500 took 0.419s\n",
      "Accuracy total 0.914714, main loss classifier 0.343493, source accuracy 0.907552 source classification loss 0.255150, target accuracy 0.921875 target loss 0.225455 accuracy domain distinction 0.500000 loss domain distinction 1.031905,\n",
      "VALIDATION Loss: 0.16587571 Acc: 0.94615385\n",
      "Epoch 27 of 500 took 0.377s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy total 0.913086, main loss classifier 0.360575, source accuracy 0.901042 source classification loss 0.290754, target accuracy 0.925130 target loss 0.222757 accuracy domain distinction 0.500000 loss domain distinction 1.038191,\n",
      "VALIDATION Loss: 0.15717596 Acc: 0.94615385\n",
      "Epoch 28 of 500 took 0.321s\n",
      "Accuracy total 0.914062, main loss classifier 0.358383, source accuracy 0.899740 source classification loss 0.292560, target accuracy 0.928385 target loss 0.217486 accuracy domain distinction 0.500000 loss domain distinction 1.033608,\n",
      "VALIDATION Loss: 0.14584059 Acc: 0.94871795\n",
      "New best validation loss:  0.14584059161799295\n",
      "Epoch 29 of 500 took 0.327s\n",
      "Accuracy total 0.909831, main loss classifier 0.361788, source accuracy 0.890625 source classification loss 0.292880, target accuracy 0.929036 target loss 0.223587 accuracy domain distinction 0.500000 loss domain distinction 1.035549,\n",
      "VALIDATION Loss: 0.19729066 Acc: 0.93589744\n",
      "Epoch 30 of 500 took 0.368s\n",
      "Accuracy total 0.912760, main loss classifier 0.351157, source accuracy 0.895182 source classification loss 0.287250, target accuracy 0.930339 target loss 0.209059 accuracy domain distinction 0.500000 loss domain distinction 1.030028,\n",
      "VALIDATION Loss: 0.16376458 Acc: 0.94615385\n",
      "Epoch 31 of 500 took 0.356s\n",
      "Accuracy total 0.919271, main loss classifier 0.343208, source accuracy 0.910156 source classification loss 0.248987, target accuracy 0.928385 target loss 0.230989 accuracy domain distinction 0.500000 loss domain distinction 1.032196,\n",
      "VALIDATION Loss: 0.14791681 Acc: 0.94615385\n",
      "Epoch 32 of 500 took 0.315s\n",
      "Accuracy total 0.911133, main loss classifier 0.367995, source accuracy 0.905599 source classification loss 0.284066, target accuracy 0.916667 target loss 0.245165 accuracy domain distinction 0.500000 loss domain distinction 1.033795,\n",
      "VALIDATION Loss: 0.16884924 Acc: 0.93333333\n",
      "Epoch 33 of 500 took 0.369s\n",
      "Accuracy total 0.919271, main loss classifier 0.330976, source accuracy 0.917318 source classification loss 0.245296, target accuracy 0.921224 target loss 0.209919 accuracy domain distinction 0.500000 loss domain distinction 1.033684,\n",
      "VALIDATION Loss: 0.21983823 Acc: 0.94102564\n",
      "Epoch 34 of 500 took 0.316s\n",
      "Accuracy total 0.914388, main loss classifier 0.349466, source accuracy 0.899089 source classification loss 0.280989, target accuracy 0.929688 target loss 0.211427 accuracy domain distinction 0.500000 loss domain distinction 1.032573,\n",
      "VALIDATION Loss: 0.16727525 Acc: 0.95897436\n",
      "Epoch    34: reducing learning rate of group 0 to 2.0120e-05.\n",
      "Epoch 35 of 500 took 0.314s\n",
      "Accuracy total 0.920898, main loss classifier 0.338702, source accuracy 0.915365 source classification loss 0.260023, target accuracy 0.926432 target loss 0.212332 accuracy domain distinction 0.500000 loss domain distinction 1.025248,\n",
      "VALIDATION Loss: 0.11433751 Acc: 0.95897436\n",
      "New best validation loss:  0.11433750896581582\n",
      "Epoch 36 of 500 took 0.322s\n",
      "Accuracy total 0.912435, main loss classifier 0.360562, source accuracy 0.905599 source classification loss 0.286762, target accuracy 0.919271 target loss 0.228298 accuracy domain distinction 0.500000 loss domain distinction 1.030316,\n",
      "VALIDATION Loss: 0.15198957 Acc: 0.95897436\n",
      "Epoch 37 of 500 took 0.316s\n",
      "Accuracy total 0.914388, main loss classifier 0.348668, source accuracy 0.903646 source classification loss 0.277909, target accuracy 0.925130 target loss 0.213759 accuracy domain distinction 0.500000 loss domain distinction 1.028342,\n",
      "VALIDATION Loss: 0.15781734 Acc: 0.95384615\n",
      "Epoch 38 of 500 took 0.348s\n",
      "Accuracy total 0.913411, main loss classifier 0.361782, source accuracy 0.903646 source classification loss 0.283127, target accuracy 0.923177 target loss 0.233694 accuracy domain distinction 0.500000 loss domain distinction 1.033716,\n",
      "VALIDATION Loss: 0.14311075 Acc: 0.95128205\n",
      "Epoch 39 of 500 took 0.343s\n",
      "Accuracy total 0.918620, main loss classifier 0.342614, source accuracy 0.901042 source classification loss 0.280273, target accuracy 0.936198 target loss 0.200193 accuracy domain distinction 0.500000 loss domain distinction 1.023817,\n",
      "VALIDATION Loss: 0.14925581 Acc: 0.94358974\n",
      "Epoch 40 of 500 took 0.386s\n",
      "Accuracy total 0.920247, main loss classifier 0.346103, source accuracy 0.911458 source classification loss 0.271042, target accuracy 0.929036 target loss 0.216013 accuracy domain distinction 0.500000 loss domain distinction 1.025752,\n",
      "VALIDATION Loss: 0.13387296 Acc: 0.95128205\n",
      "Epoch 41 of 500 took 0.389s\n",
      "Accuracy total 0.910482, main loss classifier 0.359302, source accuracy 0.891927 source classification loss 0.310477, target accuracy 0.929036 target loss 0.201980 accuracy domain distinction 0.500000 loss domain distinction 1.030736,\n",
      "VALIDATION Loss: 0.15817973 Acc: 0.93333333\n",
      "Epoch    41: reducing learning rate of group 0 to 4.0240e-06.\n",
      "Epoch 42 of 500 took 0.369s\n",
      "Accuracy total 0.917969, main loss classifier 0.345621, source accuracy 0.908854 source classification loss 0.264470, target accuracy 0.927083 target loss 0.220363 accuracy domain distinction 0.500000 loss domain distinction 1.032046,\n",
      "VALIDATION Loss: 0.21307992 Acc: 0.93333333\n",
      "Epoch 43 of 500 took 0.351s\n",
      "Accuracy total 0.915365, main loss classifier 0.357492, source accuracy 0.906250 source classification loss 0.276994, target accuracy 0.924479 target loss 0.231821 accuracy domain distinction 0.500000 loss domain distinction 1.030844,\n",
      "VALIDATION Loss: 0.18776037 Acc: 0.95897436\n",
      "Epoch 44 of 500 took 0.328s\n",
      "Accuracy total 0.917318, main loss classifier 0.337896, source accuracy 0.908203 source classification loss 0.270979, target accuracy 0.926432 target loss 0.198036 accuracy domain distinction 0.500000 loss domain distinction 1.033883,\n",
      "VALIDATION Loss: 0.15817836 Acc: 0.93589744\n",
      "Epoch 45 of 500 took 0.325s\n",
      "Accuracy total 0.910807, main loss classifier 0.342386, source accuracy 0.911458 source classification loss 0.237514, target accuracy 0.910156 target loss 0.242332 accuracy domain distinction 0.500000 loss domain distinction 1.024631,\n",
      "VALIDATION Loss: 0.14619584 Acc: 0.95128205\n",
      "Epoch 46 of 500 took 0.317s\n",
      "Accuracy total 0.916667, main loss classifier 0.341448, source accuracy 0.905599 source classification loss 0.259447, target accuracy 0.927734 target loss 0.218304 accuracy domain distinction 0.500000 loss domain distinction 1.025728,\n",
      "VALIDATION Loss: 0.17549394 Acc: 0.93076923\n",
      "Epoch 47 of 500 took 0.313s\n",
      "Training complete in 0m 16s\n",
      "['participant_0']\n",
      "Optimizer =  <generator object Module.parameters at 0x7fd3cfed6ac0>\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/DANN/participant_0/best_state_4.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/DANN/participant_0/best_state_4.pt' (epoch 55)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/TSD/participant_0/best_state_0.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/TSD/participant_0/best_state_0.pt' (epoch 27)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/DANN/participant_0/best_state_1.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/DANN/participant_0/best_state_1.pt' (epoch 19)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/DANN/participant_0/best_state_2.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/DANN/participant_0/best_state_2.pt' (epoch 22)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/DANN/participant_0/best_state_3.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/DANN/participant_0/best_state_3.pt' (epoch 5)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/DANN/participant_0/best_state_4.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/DANN/participant_0/best_state_4.pt' (epoch 55)\n",
      "==== models_array =  (5,)  @ session  4\n",
      "HANDLING NEW SESSION  0\n",
      "HANDLING NEW SESSION  1\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.2222222222222222  len before:  26   len after:  9\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  13\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  4\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  0.9230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.5769230769230769  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  15\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.8846153846153846  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  13\n",
      "BEFORE:  0.8076923076923077   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  1.0  len before:  26   len after:  11\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.6538461538461539  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.8518356643356644   Accuracy pseudo: 0.9589622641509434  len pseudo:  2120    len predictions 2288\n",
      "HANDLING NEW SESSION  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.0  len before:  26   len after:  7\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.0  len before:  26   len after:  5\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.5  len before:  26   len after:  4\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.4230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  1\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  4\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  6\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.23076923076923078  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  8\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  9\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.0  len before:  26   len after:  17\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.5769230769230769  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  7\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.7777777777777778  len before:  26   len after:  9\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  18\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.6666666666666666  len before:  26   len after:  9\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.9230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.4230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.9230769230769231  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.7434440559440559   Accuracy pseudo: 0.8225646123260437  len pseudo:  2012    len predictions 2288\n",
      "HANDLING NEW SESSION  3\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.5384615384615384  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.6538461538461539  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.46153846153846156  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  14\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  18\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.4230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.0  len before:  26   len after:  4\n",
      "BEFORE:  0.38461538461538464   AFTER:  1.0  len before:  26   len after:  12\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.0  len before:  26   len after:  5\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  8\n",
      "BEFORE:  0.5   AFTER:  0.6  len before:  26   len after:  10\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.4  len before:  26   len after:  10\n",
      "BEFORE:  0.11538461538461539   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.6923076923076923  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  15\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  5\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  8\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.11538461538461539   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  9\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  8\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  6\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  1.0  len before:  26   len after:  7\n",
      "BEFORE:  0.11538461538461539   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  8\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.7473776223776224   Accuracy pseudo: 0.8564102564102564  len pseudo:  1950    len predictions 2288\n",
      "HANDLING NEW SESSION  4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  9\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.11538461538461539   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.2692307692307692  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  5\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  1.0  len before:  26   len after:  1\n",
      "BEFORE:  0.5   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  0.5   AFTER:  1.0  len before:  26   len after:  2\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  7\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  13\n",
      "BEFORE:  0.3076923076923077   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.5384615384615384  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  1.0  len before:  26   len after:  7\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.9230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.9375  len before:  26   len after:  16\n",
      "BEFORE:  0.8076923076923077   AFTER:  0.8461538461538461  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  5\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  17\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.34615384615384615  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.5384615384615384  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  15\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.7333333333333333  len before:  26   len after:  15\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.6538461538461539  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.3888888888888889  len before:  26   len after:  18\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.0  len before:  26   len after:  10\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.0  len before:  26   len after:  5\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.6437937062937062   Accuracy pseudo: 0.7436692506459949  len pseudo:  1935    len predictions 2288\n",
      "STARTING TRAINING\n",
      "Accuracy total 0.807292, main loss classifier 0.805683, source accuracy 0.840495 source classification loss 0.529058, target accuracy 0.774089 target loss 0.851044 accuracy domain distinction 0.500000 loss domain distinction 1.156324,\n",
      "VALIDATION Loss: 0.39307932 Acc: 0.86046512\n",
      "New best validation loss:  0.3930793223636491\n",
      "Epoch 2 of 500 took 0.368s\n",
      "Accuracy total 0.839518, main loss classifier 0.623012, source accuracy 0.871094 source classification loss 0.391225, target accuracy 0.807943 target loss 0.638700 accuracy domain distinction 0.499674 loss domain distinction 1.080500,\n",
      "VALIDATION Loss: 0.44593403 Acc: 0.87596899\n",
      "Epoch 3 of 500 took 0.319s\n",
      "Accuracy total 0.838542, main loss classifier 0.614294, source accuracy 0.863932 source classification loss 0.405811, target accuracy 0.813151 target loss 0.610101 accuracy domain distinction 0.499674 loss domain distinction 1.063380,\n",
      "VALIDATION Loss: 0.54411223 Acc: 0.84754522\n",
      "Epoch 4 of 500 took 0.313s\n",
      "Accuracy total 0.849609, main loss classifier 0.591693, source accuracy 0.872396 source classification loss 0.371124, target accuracy 0.826823 target loss 0.603979 accuracy domain distinction 0.499349 loss domain distinction 1.041416,\n",
      "VALIDATION Loss: 0.41352439 Acc: 0.84754522\n",
      "Epoch 5 of 500 took 0.320s\n",
      "Accuracy total 0.839193, main loss classifier 0.593278, source accuracy 0.860026 source classification loss 0.408171, target accuracy 0.818359 target loss 0.566824 accuracy domain distinction 0.499674 loss domain distinction 1.057802,\n",
      "VALIDATION Loss: 0.33260538 Acc: 0.87338501\n",
      "New best validation loss:  0.3326053832258497\n",
      "Epoch 6 of 500 took 0.322s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy total 0.841471, main loss classifier 0.581097, source accuracy 0.867188 source classification loss 0.406410, target accuracy 0.815755 target loss 0.545656 accuracy domain distinction 0.500000 loss domain distinction 1.050640,\n",
      "VALIDATION Loss: 0.60748805 Acc: 0.90956072\n",
      "Epoch 7 of 500 took 0.316s\n",
      "Accuracy total 0.855143, main loss classifier 0.570014, source accuracy 0.877604 source classification loss 0.397761, target accuracy 0.832682 target loss 0.535061 accuracy domain distinction 0.500000 loss domain distinction 1.036034,\n",
      "VALIDATION Loss: 0.37397589 Acc: 0.85271318\n",
      "Epoch 8 of 500 took 0.329s\n",
      "Accuracy total 0.860677, main loss classifier 0.542526, source accuracy 0.880859 source classification loss 0.359616, target accuracy 0.840495 target loss 0.516211 accuracy domain distinction 0.500000 loss domain distinction 1.046128,\n",
      "VALIDATION Loss: 0.56489828 Acc: 0.89922481\n",
      "Epoch 9 of 500 took 0.343s\n",
      "Accuracy total 0.857422, main loss classifier 0.546068, source accuracy 0.873698 source classification loss 0.384089, target accuracy 0.841146 target loss 0.498508 accuracy domain distinction 0.500000 loss domain distinction 1.047696,\n",
      "VALIDATION Loss: 0.32403277 Acc: 0.8630491\n",
      "New best validation loss:  0.32403276913932394\n",
      "Epoch 10 of 500 took 0.317s\n",
      "Accuracy total 0.870768, main loss classifier 0.509488, source accuracy 0.894531 source classification loss 0.339750, target accuracy 0.847005 target loss 0.471080 accuracy domain distinction 0.500000 loss domain distinction 1.040726,\n",
      "VALIDATION Loss: 0.28802446 Acc: 0.87338501\n",
      "New best validation loss:  0.2880244569054672\n",
      "Epoch 11 of 500 took 0.316s\n",
      "Accuracy total 0.871094, main loss classifier 0.501145, source accuracy 0.891927 source classification loss 0.335824, target accuracy 0.850260 target loss 0.458114 accuracy domain distinction 0.500000 loss domain distinction 1.041762,\n",
      "VALIDATION Loss: 0.29390852 Acc: 0.89405685\n",
      "Epoch 12 of 500 took 0.319s\n",
      "Accuracy total 0.865885, main loss classifier 0.532679, source accuracy 0.884766 source classification loss 0.359280, target accuracy 0.847005 target loss 0.496575 accuracy domain distinction 0.500326 loss domain distinction 1.047514,\n",
      "VALIDATION Loss: 0.34450735 Acc: 0.86563307\n",
      "Epoch 13 of 500 took 0.315s\n",
      "Accuracy total 0.863281, main loss classifier 0.524685, source accuracy 0.876953 source classification loss 0.382492, target accuracy 0.849609 target loss 0.458200 accuracy domain distinction 0.500000 loss domain distinction 1.043387,\n",
      "VALIDATION Loss: 0.22096832 Acc: 0.9121447\n",
      "New best validation loss:  0.2209683201674904\n",
      "Epoch 14 of 500 took 0.318s\n",
      "Accuracy total 0.862305, main loss classifier 0.512061, source accuracy 0.871745 source classification loss 0.379349, target accuracy 0.852865 target loss 0.435901 accuracy domain distinction 0.500000 loss domain distinction 1.044359,\n",
      "VALIDATION Loss: 0.23715841 Acc: 0.89664083\n",
      "Epoch 15 of 500 took 0.320s\n",
      "Accuracy total 0.858398, main loss classifier 0.518853, source accuracy 0.863932 source classification loss 0.419115, target accuracy 0.852865 target loss 0.409268 accuracy domain distinction 0.499674 loss domain distinction 1.046613,\n",
      "VALIDATION Loss: 0.26338847 Acc: 0.89922481\n",
      "Epoch 16 of 500 took 0.315s\n",
      "Accuracy total 0.869141, main loss classifier 0.500014, source accuracy 0.878906 source classification loss 0.352366, target accuracy 0.859375 target loss 0.438393 accuracy domain distinction 0.500000 loss domain distinction 1.046339,\n",
      "VALIDATION Loss: 0.22791949 Acc: 0.91472868\n",
      "Epoch 17 of 500 took 0.315s\n",
      "Accuracy total 0.865885, main loss classifier 0.494795, source accuracy 0.875651 source classification loss 0.358880, target accuracy 0.856120 target loss 0.421871 accuracy domain distinction 0.500000 loss domain distinction 1.044191,\n",
      "VALIDATION Loss: 0.35888499 Acc: 0.87338501\n",
      "Epoch 18 of 500 took 0.318s\n",
      "Accuracy total 0.860677, main loss classifier 0.505963, source accuracy 0.873047 source classification loss 0.351423, target accuracy 0.848307 target loss 0.453520 accuracy domain distinction 0.500000 loss domain distinction 1.034920,\n",
      "VALIDATION Loss: 0.48890443 Acc: 0.86821705\n",
      "Epoch 19 of 500 took 0.314s\n",
      "Accuracy total 0.876953, main loss classifier 0.484299, source accuracy 0.885417 source classification loss 0.347237, target accuracy 0.868490 target loss 0.413736 accuracy domain distinction 0.500000 loss domain distinction 1.038130,\n",
      "VALIDATION Loss: 0.23085395 Acc: 0.90180879\n",
      "Epoch    19: reducing learning rate of group 0 to 2.0120e-05.\n",
      "Epoch 20 of 500 took 0.315s\n",
      "Accuracy total 0.873047, main loss classifier 0.496511, source accuracy 0.888021 source classification loss 0.362212, target accuracy 0.858073 target loss 0.422616 accuracy domain distinction 0.500000 loss domain distinction 1.040971,\n",
      "VALIDATION Loss: 0.25770590 Acc: 0.89405685\n",
      "Epoch 21 of 500 took 0.319s\n",
      "Accuracy total 0.868815, main loss classifier 0.496495, source accuracy 0.879557 source classification loss 0.340684, target accuracy 0.858073 target loss 0.443805 accuracy domain distinction 0.500000 loss domain distinction 1.042513,\n",
      "VALIDATION Loss: 0.32866610 Acc: 0.89405685\n",
      "Epoch 22 of 500 took 0.319s\n",
      "Accuracy total 0.878581, main loss classifier 0.480330, source accuracy 0.893229 source classification loss 0.325159, target accuracy 0.863932 target loss 0.426554 accuracy domain distinction 0.500000 loss domain distinction 1.044737,\n",
      "VALIDATION Loss: 0.27361631 Acc: 0.9121447\n",
      "Epoch 23 of 500 took 0.313s\n",
      "Accuracy total 0.870768, main loss classifier 0.496154, source accuracy 0.876302 source classification loss 0.369237, target accuracy 0.865234 target loss 0.414415 accuracy domain distinction 0.500000 loss domain distinction 1.043280,\n",
      "VALIDATION Loss: 0.31376260 Acc: 0.87855297\n",
      "Epoch 24 of 500 took 0.314s\n",
      "Accuracy total 0.874349, main loss classifier 0.472395, source accuracy 0.896484 source classification loss 0.304155, target accuracy 0.852214 target loss 0.433395 accuracy domain distinction 0.500000 loss domain distinction 1.036199,\n",
      "VALIDATION Loss: 0.22806256 Acc: 0.9121447\n",
      "Epoch 25 of 500 took 0.319s\n",
      "Training complete in 0m 8s\n",
      "['participant_0']\n",
      "Optimizer =  <generator object Module.parameters at 0x7fd3cfed6ac0>\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/DANN/participant_0/best_state_5.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/DANN/participant_0/best_state_5.pt' (epoch 13)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/TSD/participant_0/best_state_0.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/TSD/participant_0/best_state_0.pt' (epoch 27)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/DANN/participant_0/best_state_1.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/DANN/participant_0/best_state_1.pt' (epoch 19)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/DANN/participant_0/best_state_2.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/DANN/participant_0/best_state_2.pt' (epoch 22)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/DANN/participant_0/best_state_3.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/DANN/participant_0/best_state_3.pt' (epoch 5)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/DANN/participant_0/best_state_4.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/DANN/participant_0/best_state_4.pt' (epoch 55)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/DANN/participant_0/best_state_5.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/DANN/participant_0/best_state_5.pt' (epoch 13)\n",
      "==== models_array =  (6,)  @ session  5\n",
      "HANDLING NEW SESSION  0\n",
      "HANDLING NEW SESSION  1\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.2222222222222222  len before:  26   len after:  9\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  13\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  4\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  0.9230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.5769230769230769  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  15\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.8846153846153846  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  13\n",
      "BEFORE:  0.8076923076923077   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  1.0  len before:  26   len after:  11\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.6538461538461539  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.8518356643356644   Accuracy pseudo: 0.9589622641509434  len pseudo:  2120    len predictions 2288\n",
      "HANDLING NEW SESSION  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.0  len before:  26   len after:  7\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.0  len before:  26   len after:  5\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.5  len before:  26   len after:  4\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.4230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  1\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  4\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  6\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.23076923076923078  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  8\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  9\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.0  len before:  26   len after:  17\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.5769230769230769  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  7\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.7777777777777778  len before:  26   len after:  9\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  18\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.6666666666666666  len before:  26   len after:  9\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.9230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.4230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.9230769230769231  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.7434440559440559   Accuracy pseudo: 0.8225646123260437  len pseudo:  2012    len predictions 2288\n",
      "HANDLING NEW SESSION  3\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.5384615384615384  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.6538461538461539  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.46153846153846156  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  14\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  18\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.4230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.0  len before:  26   len after:  4\n",
      "BEFORE:  0.38461538461538464   AFTER:  1.0  len before:  26   len after:  12\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.0  len before:  26   len after:  5\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  8\n",
      "BEFORE:  0.5   AFTER:  0.6  len before:  26   len after:  10\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.4  len before:  26   len after:  10\n",
      "BEFORE:  0.11538461538461539   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.6923076923076923  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  15\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  5\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  8\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.11538461538461539   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  9\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  8\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  6\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.7307692307692307  len before:  26   len after:  26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE:  0.38461538461538464   AFTER:  1.0  len before:  26   len after:  7\n",
      "BEFORE:  0.11538461538461539   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  8\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.7473776223776224   Accuracy pseudo: 0.8564102564102564  len pseudo:  1950    len predictions 2288\n",
      "HANDLING NEW SESSION  4\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  9\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.11538461538461539   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.2692307692307692  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  5\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  1.0  len before:  26   len after:  1\n",
      "BEFORE:  0.5   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  0.5   AFTER:  1.0  len before:  26   len after:  2\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  7\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  13\n",
      "BEFORE:  0.3076923076923077   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.5384615384615384  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  1.0  len before:  26   len after:  7\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.9230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.9375  len before:  26   len after:  16\n",
      "BEFORE:  0.8076923076923077   AFTER:  0.8461538461538461  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  5\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  17\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.34615384615384615  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.5384615384615384  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  15\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.7333333333333333  len before:  26   len after:  15\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.6538461538461539  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.3888888888888889  len before:  26   len after:  18\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.0  len before:  26   len after:  10\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.0  len before:  26   len after:  5\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.6437937062937062   Accuracy pseudo: 0.7436692506459949  len pseudo:  1935    len predictions 2288\n",
      "HANDLING NEW SESSION  5\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  16\n",
      "BEFORE:  0.07692307692307693   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  9\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  1.0  len before:  26   len after:  1\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.46153846153846156  len before:  26   len after:  26\n",
      "BEFORE:  0.11538461538461539   AFTER:  0.0  len before:  26   len after:  7\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.5384615384615384  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  1\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  9\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  9\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.6  len before:  26   len after:  10\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.4230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.11538461538461539   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.4230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.8846153846153846  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.4444444444444444  len before:  26   len after:  18\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  5\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  12\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.5384615384615384  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.0  len before:  26   len after:  3\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.6634615384615384   Accuracy pseudo: 0.7764883955600403  len pseudo:  1982    len predictions 2288\n",
      "STARTING TRAINING\n",
      "Accuracy total 0.805013, main loss classifier 0.769249, source accuracy 0.835286 source classification loss 0.517114, target accuracy 0.774740 target loss 0.779626 accuracy domain distinction 0.500000 loss domain distinction 1.208785,\n",
      "VALIDATION Loss: 0.48788275 Acc: 0.86146096\n",
      "New best validation loss:  0.4878827546324049\n",
      "Epoch 2 of 500 took 0.331s\n",
      "Accuracy total 0.830729, main loss classifier 0.687496, source accuracy 0.850911 source classification loss 0.495796, target accuracy 0.810547 target loss 0.657982 accuracy domain distinction 0.500977 loss domain distinction 1.106071,\n",
      "VALIDATION Loss: 0.46928540 Acc: 0.86397985\n",
      "New best validation loss:  0.46928539872169495\n",
      "Epoch 3 of 500 took 0.322s\n",
      "Accuracy total 0.831706, main loss classifier 0.645040, source accuracy 0.845703 source classification loss 0.481548, target accuracy 0.817708 target loss 0.588303 accuracy domain distinction 0.500000 loss domain distinction 1.101141,\n",
      "VALIDATION Loss: 0.39052173 Acc: 0.89168766\n",
      "New best validation loss:  0.39052173282418934\n",
      "Epoch 4 of 500 took 0.316s\n",
      "Accuracy total 0.831706, main loss classifier 0.641726, source accuracy 0.842448 source classification loss 0.509482, target accuracy 0.820964 target loss 0.557184 accuracy domain distinction 0.498698 loss domain distinction 1.083933,\n",
      "VALIDATION Loss: 0.46567852 Acc: 0.84382872\n",
      "Epoch 5 of 500 took 0.318s\n",
      "Accuracy total 0.847982, main loss classifier 0.594637, source accuracy 0.857422 source classification loss 0.444671, target accuracy 0.838542 target loss 0.529935 accuracy domain distinction 0.498372 loss domain distinction 1.073342,\n",
      "VALIDATION Loss: 0.37430429 Acc: 0.89672544\n",
      "New best validation loss:  0.3743042860712324\n",
      "Epoch 6 of 500 took 0.319s\n",
      "Accuracy total 0.845703, main loss classifier 0.591272, source accuracy 0.848307 source classification loss 0.440233, target accuracy 0.843099 target loss 0.527365 accuracy domain distinction 0.500977 loss domain distinction 1.074729,\n",
      "VALIDATION Loss: 0.39676178 Acc: 0.87657431\n",
      "Epoch 7 of 500 took 0.318s\n",
      "Accuracy total 0.843424, main loss classifier 0.589045, source accuracy 0.848307 source classification loss 0.455740, target accuracy 0.838542 target loss 0.507934 accuracy domain distinction 0.500326 loss domain distinction 1.072085,\n",
      "VALIDATION Loss: 0.56193086 Acc: 0.83627204\n",
      "Epoch 8 of 500 took 0.324s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy total 0.847656, main loss classifier 0.587259, source accuracy 0.855469 source classification loss 0.443180, target accuracy 0.839844 target loss 0.519493 accuracy domain distinction 0.501302 loss domain distinction 1.059232,\n",
      "VALIDATION Loss: 0.42041165 Acc: 0.90428212\n",
      "Epoch 9 of 500 took 0.316s\n",
      "Accuracy total 0.860352, main loss classifier 0.565402, source accuracy 0.868490 source classification loss 0.432227, target accuracy 0.852214 target loss 0.485932 accuracy domain distinction 0.500651 loss domain distinction 1.063232,\n",
      "VALIDATION Loss: 0.34408830 Acc: 0.90176322\n",
      "New best validation loss:  0.3440883031913212\n",
      "Epoch 10 of 500 took 0.328s\n",
      "Accuracy total 0.849284, main loss classifier 0.569778, source accuracy 0.859375 source classification loss 0.427876, target accuracy 0.839193 target loss 0.496215 accuracy domain distinction 0.499674 loss domain distinction 1.077317,\n",
      "VALIDATION Loss: 0.36953402 Acc: 0.88916877\n",
      "Epoch 11 of 500 took 0.319s\n",
      "Accuracy total 0.847005, main loss classifier 0.585147, source accuracy 0.847656 source classification loss 0.455098, target accuracy 0.846354 target loss 0.502358 accuracy domain distinction 0.500651 loss domain distinction 1.064187,\n",
      "VALIDATION Loss: 0.54983802 Acc: 0.85390428\n",
      "Epoch 12 of 500 took 0.316s\n",
      "Accuracy total 0.861328, main loss classifier 0.533846, source accuracy 0.872396 source classification loss 0.399442, target accuracy 0.850260 target loss 0.454417 accuracy domain distinction 0.500977 loss domain distinction 1.069168,\n",
      "VALIDATION Loss: 0.38481599 Acc: 0.87657431\n",
      "Epoch 13 of 500 took 0.315s\n",
      "Accuracy total 0.869141, main loss classifier 0.526371, source accuracy 0.882161 source classification loss 0.355170, target accuracy 0.856120 target loss 0.483252 accuracy domain distinction 0.500000 loss domain distinction 1.071600,\n",
      "VALIDATION Loss: 0.36816466 Acc: 0.8790932\n",
      "Epoch 14 of 500 took 0.319s\n",
      "Accuracy total 0.860026, main loss classifier 0.524482, source accuracy 0.874349 source classification loss 0.372906, target accuracy 0.845703 target loss 0.463488 accuracy domain distinction 0.499349 loss domain distinction 1.062850,\n",
      "VALIDATION Loss: 0.32927854 Acc: 0.89420655\n",
      "New best validation loss:  0.3292785350765501\n",
      "Epoch 15 of 500 took 0.316s\n",
      "Accuracy total 0.850586, main loss classifier 0.559616, source accuracy 0.847005 source classification loss 0.442851, target accuracy 0.854167 target loss 0.463432 accuracy domain distinction 0.500326 loss domain distinction 1.064738,\n",
      "VALIDATION Loss: 0.32235482 Acc: 0.92443325\n",
      "New best validation loss:  0.3223548190934317\n",
      "Epoch 16 of 500 took 0.318s\n",
      "Accuracy total 0.865560, main loss classifier 0.539793, source accuracy 0.869141 source classification loss 0.432042, target accuracy 0.861979 target loss 0.434089 accuracy domain distinction 0.500651 loss domain distinction 1.067275,\n",
      "VALIDATION Loss: 0.41450543 Acc: 0.89420655\n",
      "Epoch 17 of 500 took 0.320s\n",
      "Accuracy total 0.862305, main loss classifier 0.519944, source accuracy 0.875000 source classification loss 0.376005, target accuracy 0.849609 target loss 0.452030 accuracy domain distinction 0.500000 loss domain distinction 1.059261,\n",
      "VALIDATION Loss: 0.42934315 Acc: 0.90176322\n",
      "Epoch 18 of 500 took 0.317s\n",
      "Accuracy total 0.854818, main loss classifier 0.556644, source accuracy 0.854818 source classification loss 0.423822, target accuracy 0.854818 target loss 0.477670 accuracy domain distinction 0.500326 loss domain distinction 1.058979,\n",
      "VALIDATION Loss: 0.32888283 Acc: 0.91183879\n",
      "Epoch 19 of 500 took 0.316s\n",
      "Accuracy total 0.858073, main loss classifier 0.532157, source accuracy 0.853516 source classification loss 0.416169, target accuracy 0.862630 target loss 0.437237 accuracy domain distinction 0.500000 loss domain distinction 1.054540,\n",
      "VALIDATION Loss: 0.33405206 Acc: 0.89672544\n",
      "Epoch 20 of 500 took 0.317s\n",
      "Accuracy total 0.869466, main loss classifier 0.500293, source accuracy 0.867188 source classification loss 0.378190, target accuracy 0.871745 target loss 0.410976 accuracy domain distinction 0.501628 loss domain distinction 1.057100,\n",
      "VALIDATION Loss: 0.43246437 Acc: 0.89168766\n",
      "Epoch 21 of 500 took 0.319s\n",
      "Accuracy total 0.863281, main loss classifier 0.507514, source accuracy 0.866536 source classification loss 0.377375, target accuracy 0.860026 target loss 0.427235 accuracy domain distinction 0.500326 loss domain distinction 1.052090,\n",
      "VALIDATION Loss: 0.39468334 Acc: 0.89672544\n",
      "Epoch    21: reducing learning rate of group 0 to 2.0120e-05.\n",
      "Epoch 22 of 500 took 0.317s\n",
      "Accuracy total 0.857096, main loss classifier 0.544079, source accuracy 0.855469 source classification loss 0.453999, target accuracy 0.858724 target loss 0.422107 accuracy domain distinction 0.499349 loss domain distinction 1.060254,\n",
      "VALIDATION Loss: 0.40381568 Acc: 0.88413098\n",
      "Epoch 23 of 500 took 0.316s\n",
      "Accuracy total 0.860677, main loss classifier 0.502768, source accuracy 0.857422 source classification loss 0.390741, target accuracy 0.863932 target loss 0.404849 accuracy domain distinction 0.500326 loss domain distinction 1.049729,\n",
      "VALIDATION Loss: 0.28932183 Acc: 0.90428212\n",
      "New best validation loss:  0.28932182703699383\n",
      "Epoch 24 of 500 took 0.324s\n",
      "Accuracy total 0.858073, main loss classifier 0.520809, source accuracy 0.858073 source classification loss 0.397248, target accuracy 0.858073 target loss 0.432649 accuracy domain distinction 0.500000 loss domain distinction 1.058609,\n",
      "VALIDATION Loss: 0.35283494 Acc: 0.90428212\n",
      "Epoch 25 of 500 took 0.316s\n",
      "Accuracy total 0.861979, main loss classifier 0.533352, source accuracy 0.856120 source classification loss 0.451246, target accuracy 0.867839 target loss 0.404912 accuracy domain distinction 0.498698 loss domain distinction 1.052731,\n",
      "VALIDATION Loss: 0.35638023 Acc: 0.89420655\n",
      "Epoch 26 of 500 took 0.319s\n",
      "Accuracy total 0.868490, main loss classifier 0.493281, source accuracy 0.874349 source classification loss 0.347866, target accuracy 0.862630 target loss 0.428617 accuracy domain distinction 0.499023 loss domain distinction 1.050390,\n",
      "VALIDATION Loss: 0.31642843 Acc: 0.88664987\n",
      "Epoch 27 of 500 took 0.321s\n",
      "Accuracy total 0.869792, main loss classifier 0.487330, source accuracy 0.866536 source classification loss 0.393222, target accuracy 0.873047 target loss 0.371694 accuracy domain distinction 0.500326 loss domain distinction 1.048715,\n",
      "VALIDATION Loss: 0.33335313 Acc: 0.90680101\n",
      "Epoch 28 of 500 took 0.316s\n",
      "Accuracy total 0.862630, main loss classifier 0.501138, source accuracy 0.866536 source classification loss 0.384625, target accuracy 0.858724 target loss 0.408694 accuracy domain distinction 0.498372 loss domain distinction 1.044786,\n",
      "VALIDATION Loss: 0.30693758 Acc: 0.90176322\n",
      "Epoch 29 of 500 took 0.318s\n",
      "Accuracy total 0.866536, main loss classifier 0.505898, source accuracy 0.865234 source classification loss 0.385092, target accuracy 0.867839 target loss 0.415437 accuracy domain distinction 0.500977 loss domain distinction 1.056333,\n",
      "VALIDATION Loss: 0.28605400 Acc: 0.91687657\n",
      "New best validation loss:  0.2860539960009711\n",
      "Epoch 30 of 500 took 0.319s\n",
      "Accuracy total 0.867513, main loss classifier 0.509067, source accuracy 0.874349 source classification loss 0.381816, target accuracy 0.860677 target loss 0.425254 accuracy domain distinction 0.500000 loss domain distinction 1.055318,\n",
      "VALIDATION Loss: 0.33946998 Acc: 0.9093199\n",
      "Epoch 31 of 500 took 0.317s\n",
      "Accuracy total 0.875000, main loss classifier 0.522712, source accuracy 0.882161 source classification loss 0.401483, target accuracy 0.867839 target loss 0.433672 accuracy domain distinction 0.499674 loss domain distinction 1.051340,\n",
      "VALIDATION Loss: 0.39043289 Acc: 0.88161209\n",
      "Epoch 32 of 500 took 0.315s\n",
      "Accuracy total 0.861003, main loss classifier 0.512020, source accuracy 0.859375 source classification loss 0.421873, target accuracy 0.862630 target loss 0.391444 accuracy domain distinction 0.499349 loss domain distinction 1.053616,\n",
      "VALIDATION Loss: 0.33587689 Acc: 0.89420655\n",
      "Epoch 33 of 500 took 0.321s\n",
      "Accuracy total 0.862630, main loss classifier 0.514864, source accuracy 0.862630 source classification loss 0.425174, target accuracy 0.862630 target loss 0.395230 accuracy domain distinction 0.500977 loss domain distinction 1.046621,\n",
      "VALIDATION Loss: 0.31188324 Acc: 0.90680101\n",
      "Epoch 34 of 500 took 0.317s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy total 0.874023, main loss classifier 0.495959, source accuracy 0.885417 source classification loss 0.372512, target accuracy 0.862630 target loss 0.409302 accuracy domain distinction 0.500000 loss domain distinction 1.050525,\n",
      "VALIDATION Loss: 0.31944813 Acc: 0.90176322\n",
      "Epoch 35 of 500 took 0.315s\n",
      "Accuracy total 0.868490, main loss classifier 0.504084, source accuracy 0.869792 source classification loss 0.375833, target accuracy 0.867188 target loss 0.421702 accuracy domain distinction 0.500000 loss domain distinction 1.053167,\n",
      "VALIDATION Loss: 0.34277771 Acc: 0.89420655\n",
      "Epoch    35: reducing learning rate of group 0 to 4.0240e-06.\n",
      "Epoch 36 of 500 took 0.320s\n",
      "Accuracy total 0.872070, main loss classifier 0.496444, source accuracy 0.874349 source classification loss 0.387412, target accuracy 0.869792 target loss 0.396927 accuracy domain distinction 0.500977 loss domain distinction 1.042748,\n",
      "VALIDATION Loss: 0.39192492 Acc: 0.90428212\n",
      "Epoch 37 of 500 took 0.316s\n",
      "Accuracy total 0.876302, main loss classifier 0.462396, source accuracy 0.883464 source classification loss 0.335946, target accuracy 0.869141 target loss 0.379965 accuracy domain distinction 0.500651 loss domain distinction 1.044408,\n",
      "VALIDATION Loss: 0.32640262 Acc: 0.90680101\n",
      "Epoch 38 of 500 took 0.317s\n",
      "Accuracy total 0.866536, main loss classifier 0.521354, source accuracy 0.868490 source classification loss 0.395842, target accuracy 0.864583 target loss 0.437913 accuracy domain distinction 0.500000 loss domain distinction 1.044759,\n",
      "VALIDATION Loss: 0.31065941 Acc: 0.91183879\n",
      "Epoch 39 of 500 took 0.320s\n",
      "Accuracy total 0.883138, main loss classifier 0.478662, source accuracy 0.893880 source classification loss 0.342163, target accuracy 0.872396 target loss 0.404581 accuracy domain distinction 0.500326 loss domain distinction 1.052905,\n",
      "VALIDATION Loss: 0.36503648 Acc: 0.91183879\n",
      "Epoch 40 of 500 took 0.320s\n",
      "Accuracy total 0.862956, main loss classifier 0.511674, source accuracy 0.863932 source classification loss 0.377578, target accuracy 0.861979 target loss 0.435632 accuracy domain distinction 0.499674 loss domain distinction 1.050687,\n",
      "VALIDATION Loss: 0.30898173 Acc: 0.9093199\n",
      "Epoch 41 of 500 took 0.320s\n",
      "Training complete in 0m 13s\n",
      "['participant_0']\n",
      "Optimizer =  <generator object Module.parameters at 0x7fd3cfed6ac0>\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/DANN/participant_0/best_state_6.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/DANN/participant_0/best_state_6.pt' (epoch 29)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/TSD/participant_0/best_state_0.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/TSD/participant_0/best_state_0.pt' (epoch 27)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/DANN/participant_0/best_state_1.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/DANN/participant_0/best_state_1.pt' (epoch 19)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/DANN/participant_0/best_state_2.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/DANN/participant_0/best_state_2.pt' (epoch 22)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/DANN/participant_0/best_state_3.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/DANN/participant_0/best_state_3.pt' (epoch 5)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/DANN/participant_0/best_state_4.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/DANN/participant_0/best_state_4.pt' (epoch 55)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/DANN/participant_0/best_state_5.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/DANN/participant_0/best_state_5.pt' (epoch 13)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/DANN/participant_0/best_state_6.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/DANN/participant_0/best_state_6.pt' (epoch 29)\n",
      "==== models_array =  (7,)  @ session  6\n",
      "HANDLING NEW SESSION  0\n",
      "HANDLING NEW SESSION  1\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.2222222222222222  len before:  26   len after:  9\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  13\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  4\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  0.9230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.5769230769230769  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  15\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.8846153846153846  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  13\n",
      "BEFORE:  0.8076923076923077   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  1.0  len before:  26   len after:  11\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.6538461538461539  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.8518356643356644   Accuracy pseudo: 0.9589622641509434  len pseudo:  2120    len predictions 2288\n",
      "HANDLING NEW SESSION  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.0  len before:  26   len after:  7\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.0  len before:  26   len after:  5\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.5  len before:  26   len after:  4\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.4230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  1\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  4\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  6\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.23076923076923078  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  8\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  9\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.0  len before:  26   len after:  17\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.5769230769230769  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  7\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.7777777777777778  len before:  26   len after:  9\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  18\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.6666666666666666  len before:  26   len after:  9\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.9230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.4230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.9230769230769231  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.7434440559440559   Accuracy pseudo: 0.8225646123260437  len pseudo:  2012    len predictions 2288\n",
      "HANDLING NEW SESSION  3\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.5384615384615384  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.6538461538461539  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.46153846153846156  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  14\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  18\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.4230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.0  len before:  26   len after:  4\n",
      "BEFORE:  0.38461538461538464   AFTER:  1.0  len before:  26   len after:  12\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.0  len before:  26   len after:  5\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  8\n",
      "BEFORE:  0.5   AFTER:  0.6  len before:  26   len after:  10\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.4  len before:  26   len after:  10\n",
      "BEFORE:  0.11538461538461539   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.6923076923076923  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  15\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  5\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  8\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.11538461538461539   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  9\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  8\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  6\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  1.0  len before:  26   len after:  7\n",
      "BEFORE:  0.11538461538461539   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  8\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.7473776223776224   Accuracy pseudo: 0.8564102564102564  len pseudo:  1950    len predictions 2288\n",
      "HANDLING NEW SESSION  4\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  9\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.11538461538461539   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.2692307692307692  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  5\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  1.0  len before:  26   len after:  1\n",
      "BEFORE:  0.5   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  0.5   AFTER:  1.0  len before:  26   len after:  2\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  7\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  13\n",
      "BEFORE:  0.3076923076923077   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.5384615384615384  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  1.0  len before:  26   len after:  7\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.9230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.9375  len before:  26   len after:  16\n",
      "BEFORE:  0.8076923076923077   AFTER:  0.8461538461538461  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  5\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  17\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.34615384615384615  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.5384615384615384  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  15\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.7333333333333333  len before:  26   len after:  15\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.6538461538461539  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.3888888888888889  len before:  26   len after:  18\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.0  len before:  26   len after:  10\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.0  len before:  26   len after:  5\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.6437937062937062   Accuracy pseudo: 0.7436692506459949  len pseudo:  1935    len predictions 2288\n",
      "HANDLING NEW SESSION  5\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  16\n",
      "BEFORE:  0.07692307692307693   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  9\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  1.0  len before:  26   len after:  1\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.46153846153846156  len before:  26   len after:  26\n",
      "BEFORE:  0.11538461538461539   AFTER:  0.0  len before:  26   len after:  7\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.5384615384615384  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  1\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  9\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  9\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.6  len before:  26   len after:  10\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.4230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.11538461538461539   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.4230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.8846153846153846  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.4444444444444444  len before:  26   len after:  18\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  5\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  12\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.5384615384615384  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.0  len before:  26   len after:  3\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.6634615384615384   Accuracy pseudo: 0.7764883955600403  len pseudo:  1982    len predictions 2288\n",
      "HANDLING NEW SESSION  6\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.6538461538461539  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  7\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  17\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.0  len before:  26   len after:  12\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.0  len before:  26   len after:  6\n",
      "BEFORE:  0.11538461538461539   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  1.0  len before:  26   len after:  7\n",
      "BEFORE:  0.5   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.4230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  17\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.5769230769230769  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.0  len before:  26   len after:  7\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  11\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  6\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.8076923076923077   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  17\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  8\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.6923076923076923  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.0  len before:  26   len after:  15\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  13\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.4230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  9\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.6538461538461539  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.46153846153846156  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.5769230769230769  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.46153846153846156  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  5\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  10\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  9\n",
      "BEFORE:  0.3076923076923077   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  1.0  len before:  26   len after:  5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.6470588235294118  len before:  26   len after:  17\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.48732517482517484   Accuracy pseudo: 0.5804123711340207  len pseudo:  1940    len predictions 2288\n",
      "STARTING TRAINING\n",
      "Accuracy total 0.772786, main loss classifier 0.992845, source accuracy 0.836589 source classification loss 0.562818, target accuracy 0.708984 target loss 1.187133 accuracy domain distinction 0.499674 loss domain distinction 1.178694,\n",
      "VALIDATION Loss: 0.42696903 Acc: 0.8556701\n",
      "New best validation loss:  0.42696903007371084\n",
      "Epoch 2 of 500 took 0.319s\n",
      "Accuracy total 0.790039, main loss classifier 0.909145, source accuracy 0.840495 source classification loss 0.545779, target accuracy 0.739583 target loss 1.052586 accuracy domain distinction 0.499023 loss domain distinction 1.099619,\n",
      "VALIDATION Loss: 0.48722258 Acc: 0.81958763\n",
      "Epoch 3 of 500 took 0.317s\n",
      "Accuracy total 0.792318, main loss classifier 0.851387, source accuracy 0.840495 source classification loss 0.537552, target accuracy 0.744141 target loss 0.949570 accuracy domain distinction 0.500326 loss domain distinction 1.078258,\n",
      "VALIDATION Loss: 0.43503253 Acc: 0.82989691\n",
      "Epoch 4 of 500 took 0.316s\n",
      "Accuracy total 0.796549, main loss classifier 0.818935, source accuracy 0.847656 source classification loss 0.512103, target accuracy 0.745443 target loss 0.913374 accuracy domain distinction 0.500000 loss domain distinction 1.061962,\n",
      "VALIDATION Loss: 0.52575351 Acc: 0.8814433\n",
      "Epoch 5 of 500 took 0.316s\n",
      "Accuracy total 0.804688, main loss classifier 0.772292, source accuracy 0.843099 source classification loss 0.524195, target accuracy 0.766276 target loss 0.806685 accuracy domain distinction 0.500000 loss domain distinction 1.068514,\n",
      "VALIDATION Loss: 0.60510364 Acc: 0.84020619\n",
      "Epoch 6 of 500 took 0.321s\n",
      "Accuracy total 0.803060, main loss classifier 0.776426, source accuracy 0.838542 source classification loss 0.480614, target accuracy 0.767578 target loss 0.860921 accuracy domain distinction 0.500000 loss domain distinction 1.056590,\n",
      "VALIDATION Loss: 0.74349494 Acc: 0.7628866\n",
      "Epoch 7 of 500 took 0.316s\n",
      "Accuracy total 0.807292, main loss classifier 0.746494, source accuracy 0.833984 source classification loss 0.522243, target accuracy 0.780599 target loss 0.757131 accuracy domain distinction 0.499674 loss domain distinction 1.068072,\n",
      "VALIDATION Loss: 0.36035118 Acc: 0.88659794\n",
      "New best validation loss:  0.36035117506980896\n",
      "Epoch 8 of 500 took 0.385s\n",
      "Accuracy total 0.807943, main loss classifier 0.746864, source accuracy 0.845703 source classification loss 0.477282, target accuracy 0.770182 target loss 0.803170 accuracy domain distinction 0.500326 loss domain distinction 1.066378,\n",
      "VALIDATION Loss: 0.46261313 Acc: 0.81701031\n",
      "Epoch 9 of 500 took 0.318s\n",
      "Accuracy total 0.823568, main loss classifier 0.684106, source accuracy 0.850911 source classification loss 0.470768, target accuracy 0.796224 target loss 0.687688 accuracy domain distinction 0.499674 loss domain distinction 1.048787,\n",
      "VALIDATION Loss: 0.32375030 Acc: 0.87628866\n",
      "New best validation loss:  0.323750302195549\n",
      "Epoch 10 of 500 took 0.316s\n",
      "Accuracy total 0.823242, main loss classifier 0.650409, source accuracy 0.871094 source classification loss 0.400317, target accuracy 0.775391 target loss 0.690509 accuracy domain distinction 0.500326 loss domain distinction 1.049961,\n",
      "VALIDATION Loss: 0.35575144 Acc: 0.88402062\n",
      "Epoch 11 of 500 took 0.316s\n",
      "Accuracy total 0.814453, main loss classifier 0.685157, source accuracy 0.862630 source classification loss 0.412427, target accuracy 0.766276 target loss 0.745805 accuracy domain distinction 0.500000 loss domain distinction 1.060408,\n",
      "VALIDATION Loss: 0.41927651 Acc: 0.88917526\n",
      "Epoch 12 of 500 took 0.318s\n",
      "Accuracy total 0.812500, main loss classifier 0.712283, source accuracy 0.845703 source classification loss 0.452828, target accuracy 0.779297 target loss 0.763572 accuracy domain distinction 0.499349 loss domain distinction 1.040833,\n",
      "VALIDATION Loss: 0.32879371 Acc: 0.8685567\n",
      "Epoch 13 of 500 took 0.314s\n",
      "Accuracy total 0.827148, main loss classifier 0.663150, source accuracy 0.871745 source classification loss 0.423832, target accuracy 0.782552 target loss 0.693527 accuracy domain distinction 0.500326 loss domain distinction 1.044703,\n",
      "VALIDATION Loss: 0.28186778 Acc: 0.90463918\n",
      "New best validation loss:  0.2818677769973874\n",
      "Epoch 14 of 500 took 0.321s\n",
      "Accuracy total 0.814779, main loss classifier 0.679288, source accuracy 0.858724 source classification loss 0.439611, target accuracy 0.770833 target loss 0.709384 accuracy domain distinction 0.499349 loss domain distinction 1.047900,\n",
      "VALIDATION Loss: 0.31112095 Acc: 0.90463918\n",
      "Epoch 15 of 500 took 0.318s\n",
      "Accuracy total 0.813477, main loss classifier 0.668238, source accuracy 0.854167 source classification loss 0.432646, target accuracy 0.772786 target loss 0.694824 accuracy domain distinction 0.500000 loss domain distinction 1.045034,\n",
      "VALIDATION Loss: 0.32840891 Acc: 0.88659794\n",
      "Epoch 16 of 500 took 0.315s\n",
      "Accuracy total 0.829753, main loss classifier 0.658538, source accuracy 0.856120 source classification loss 0.441674, target accuracy 0.803385 target loss 0.666288 accuracy domain distinction 0.499674 loss domain distinction 1.045572,\n",
      "VALIDATION Loss: 0.42398010 Acc: 0.85309278\n",
      "Epoch 17 of 500 took 0.315s\n",
      "Accuracy total 0.817708, main loss classifier 0.696756, source accuracy 0.848307 source classification loss 0.439221, target accuracy 0.787109 target loss 0.745959 accuracy domain distinction 0.499674 loss domain distinction 1.041661,\n",
      "VALIDATION Loss: 0.63862384 Acc: 0.87628866\n",
      "Epoch 18 of 500 took 0.314s\n",
      "Accuracy total 0.839193, main loss classifier 0.656188, source accuracy 0.873047 source classification loss 0.426911, target accuracy 0.805339 target loss 0.675453 accuracy domain distinction 0.500000 loss domain distinction 1.050062,\n",
      "VALIDATION Loss: 0.29736757 Acc: 0.88659794\n",
      "Epoch 19 of 500 took 0.317s\n",
      "Accuracy total 0.832682, main loss classifier 0.639668, source accuracy 0.859375 source classification loss 0.422467, target accuracy 0.805990 target loss 0.649384 accuracy domain distinction 0.499674 loss domain distinction 1.037426,\n",
      "VALIDATION Loss: 0.30554838 Acc: 0.89948454\n",
      "Epoch    19: reducing learning rate of group 0 to 2.0120e-05.\n",
      "Epoch 20 of 500 took 0.316s\n",
      "Accuracy total 0.831055, main loss classifier 0.627271, source accuracy 0.864583 source classification loss 0.426331, target accuracy 0.797526 target loss 0.620710 accuracy domain distinction 0.500000 loss domain distinction 1.037506,\n",
      "VALIDATION Loss: 0.35025464 Acc: 0.88659794\n",
      "Epoch 21 of 500 took 0.314s\n",
      "Accuracy total 0.841146, main loss classifier 0.590361, source accuracy 0.867839 source classification loss 0.394724, target accuracy 0.814453 target loss 0.577511 accuracy domain distinction 0.500326 loss domain distinction 1.042440,\n",
      "VALIDATION Loss: 0.44708677 Acc: 0.87371134\n",
      "Epoch 22 of 500 took 0.320s\n",
      "Accuracy total 0.825195, main loss classifier 0.632656, source accuracy 0.848958 source classification loss 0.446534, target accuracy 0.801432 target loss 0.611063 accuracy domain distinction 0.500000 loss domain distinction 1.038578,\n",
      "VALIDATION Loss: 0.31751976 Acc: 0.88402062\n",
      "Epoch 23 of 500 took 0.319s\n",
      "Accuracy total 0.839518, main loss classifier 0.649359, source accuracy 0.863932 source classification loss 0.463054, target accuracy 0.815104 target loss 0.628588 accuracy domain distinction 0.500000 loss domain distinction 1.035377,\n",
      "VALIDATION Loss: 0.38026533 Acc: 0.87628866\n",
      "Epoch 24 of 500 took 0.317s\n",
      "Accuracy total 0.823893, main loss classifier 0.648035, source accuracy 0.844401 source classification loss 0.469525, target accuracy 0.803385 target loss 0.617917 accuracy domain distinction 0.500326 loss domain distinction 1.043141,\n",
      "VALIDATION Loss: 0.41795988 Acc: 0.86597938\n",
      "Epoch 25 of 500 took 0.317s\n",
      "Training complete in 0m 8s\n",
      "['participant_0']\n",
      "Optimizer =  <generator object Module.parameters at 0x7fd3c5406b30>\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/DANN/participant_0/best_state_7.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/DANN/participant_0/best_state_7.pt' (epoch 7)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/TSD/participant_0/best_state_0.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/TSD/participant_0/best_state_0.pt' (epoch 27)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/DANN/participant_0/best_state_1.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/DANN/participant_0/best_state_1.pt' (epoch 19)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/DANN/participant_0/best_state_2.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/DANN/participant_0/best_state_2.pt' (epoch 22)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/DANN/participant_0/best_state_3.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/DANN/participant_0/best_state_3.pt' (epoch 5)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/DANN/participant_0/best_state_4.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/DANN/participant_0/best_state_4.pt' (epoch 55)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/DANN/participant_0/best_state_5.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/DANN/participant_0/best_state_5.pt' (epoch 13)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/DANN/participant_0/best_state_6.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/DANN/participant_0/best_state_6.pt' (epoch 29)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/DANN/participant_0/best_state_7.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/DANN/participant_0/best_state_7.pt' (epoch 7)\n",
      "==== models_array =  (8,)  @ session  7\n",
      "HANDLING NEW SESSION  0\n",
      "HANDLING NEW SESSION  1\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.2222222222222222  len before:  26   len after:  9\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  13\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  4\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  0.9230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.5769230769230769  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  15\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.8846153846153846  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  13\n",
      "BEFORE:  0.8076923076923077   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  1.0  len before:  26   len after:  11\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.6538461538461539  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.8518356643356644   Accuracy pseudo: 0.9589622641509434  len pseudo:  2120    len predictions 2288\n",
      "HANDLING NEW SESSION  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.0  len before:  26   len after:  7\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.0  len before:  26   len after:  5\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.5  len before:  26   len after:  4\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.4230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  1\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  4\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  6\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.23076923076923078  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  8\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  9\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.0  len before:  26   len after:  17\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.5769230769230769  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  7\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.7777777777777778  len before:  26   len after:  9\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  18\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.6666666666666666  len before:  26   len after:  9\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.9230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.4230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.9230769230769231  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.7434440559440559   Accuracy pseudo: 0.8225646123260437  len pseudo:  2012    len predictions 2288\n",
      "HANDLING NEW SESSION  3\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.5384615384615384  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.6538461538461539  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.46153846153846156  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  14\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  18\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.4230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.0  len before:  26   len after:  4\n",
      "BEFORE:  0.38461538461538464   AFTER:  1.0  len before:  26   len after:  12\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.0  len before:  26   len after:  5\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  8\n",
      "BEFORE:  0.5   AFTER:  0.6  len before:  26   len after:  10\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.4  len before:  26   len after:  10\n",
      "BEFORE:  0.11538461538461539   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.6923076923076923  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  15\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  5\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  8\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.11538461538461539   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  9\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  8\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  6\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  1.0  len before:  26   len after:  7\n",
      "BEFORE:  0.11538461538461539   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  8\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.7473776223776224   Accuracy pseudo: 0.8564102564102564  len pseudo:  1950    len predictions 2288\n",
      "HANDLING NEW SESSION  4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  9\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.11538461538461539   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.2692307692307692  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  5\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  1.0  len before:  26   len after:  1\n",
      "BEFORE:  0.5   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  0.5   AFTER:  1.0  len before:  26   len after:  2\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  7\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  13\n",
      "BEFORE:  0.3076923076923077   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.5384615384615384  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  1.0  len before:  26   len after:  7\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.9230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.9375  len before:  26   len after:  16\n",
      "BEFORE:  0.8076923076923077   AFTER:  0.8461538461538461  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  5\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  17\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.34615384615384615  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.5384615384615384  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  15\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.7333333333333333  len before:  26   len after:  15\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.6538461538461539  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.3888888888888889  len before:  26   len after:  18\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.0  len before:  26   len after:  10\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.0  len before:  26   len after:  5\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.6437937062937062   Accuracy pseudo: 0.7436692506459949  len pseudo:  1935    len predictions 2288\n",
      "HANDLING NEW SESSION  5\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  16\n",
      "BEFORE:  0.07692307692307693   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  9\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  1.0  len before:  26   len after:  1\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.46153846153846156  len before:  26   len after:  26\n",
      "BEFORE:  0.11538461538461539   AFTER:  0.0  len before:  26   len after:  7\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.5384615384615384  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  1\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  9\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  9\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.6  len before:  26   len after:  10\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.4230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.11538461538461539   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.4230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.8846153846153846  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.4444444444444444  len before:  26   len after:  18\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  5\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  12\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.5384615384615384  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.0  len before:  26   len after:  3\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.6634615384615384   Accuracy pseudo: 0.7764883955600403  len pseudo:  1982    len predictions 2288\n",
      "HANDLING NEW SESSION  6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.6538461538461539  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  7\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  17\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.0  len before:  26   len after:  12\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.0  len before:  26   len after:  6\n",
      "BEFORE:  0.11538461538461539   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  1.0  len before:  26   len after:  7\n",
      "BEFORE:  0.5   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.4230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  17\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.5769230769230769  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.0  len before:  26   len after:  7\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  11\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  6\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.8076923076923077   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  17\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  8\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.6923076923076923  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.0  len before:  26   len after:  15\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  13\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.4230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  9\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.6538461538461539  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.46153846153846156  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.5769230769230769  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.46153846153846156  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  5\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  10\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  9\n",
      "BEFORE:  0.3076923076923077   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  1.0  len before:  26   len after:  5\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.6470588235294118  len before:  26   len after:  17\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.48732517482517484   Accuracy pseudo: 0.5804123711340207  len pseudo:  1940    len predictions 2288\n",
      "HANDLING NEW SESSION  7\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.6923076923076923  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.4230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  16\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.0  len before:  26   len after:  18\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.46153846153846156  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  1.0  len before:  26   len after:  1\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.46153846153846156   AFTER:  1.0  len before:  26   len after:  13\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  11\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.0  len before:  26   len after:  13\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  8\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.0  len before:  26   len after:  8\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.0  len before:  26   len after:  13\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.46153846153846156  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  5\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  10\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  18\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  1\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  12\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.6666666666666666  len before:  26   len after:  15\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  13\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.15384615384615385  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  9\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  10\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.6923076923076923  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.46153846153846156  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.9615384615384616  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.75  len before:  26   len after:  8\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.8461538461538461  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.0  len before:  26   len after:  1\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  7\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.0  len before:  26   len after:  3\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.0  len before:  26   len after:  10\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  18\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.7058823529411765  len before:  26   len after:  17\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.5113636363636364   Accuracy pseudo: 0.60687432867884  len pseudo:  1862    len predictions 2288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING TRAINING\n",
      "Accuracy total 0.755774, main loss classifier 0.990880, source accuracy 0.805027 source classification loss 0.721460, target accuracy 0.706522 target loss 1.015545 accuracy domain distinction 0.500000 loss domain distinction 1.223772,\n",
      "VALIDATION Loss: 0.63465704 Acc: 0.79356568\n",
      "New best validation loss:  0.6346570352713267\n",
      "Epoch 2 of 500 took 0.317s\n",
      "Accuracy total 0.802989, main loss classifier 0.782933, source accuracy 0.836957 source classification loss 0.567181, target accuracy 0.769022 target loss 0.770470 accuracy domain distinction 0.498981 loss domain distinction 1.141076,\n",
      "VALIDATION Loss: 0.50144913 Acc: 0.84718499\n",
      "New best validation loss:  0.5014491329590479\n",
      "Epoch 3 of 500 took 0.306s\n",
      "Accuracy total 0.800611, main loss classifier 0.792154, source accuracy 0.824728 source classification loss 0.598312, target accuracy 0.776495 target loss 0.763488 accuracy domain distinction 0.497962 loss domain distinction 1.112539,\n",
      "VALIDATION Loss: 0.49209863 Acc: 0.86327078\n",
      "New best validation loss:  0.4920986344416936\n",
      "Epoch 4 of 500 took 0.312s\n",
      "Accuracy total 0.811481, main loss classifier 0.746396, source accuracy 0.828125 source classification loss 0.601768, target accuracy 0.794837 target loss 0.672858 accuracy domain distinction 0.500679 loss domain distinction 1.090826,\n",
      "VALIDATION Loss: 0.47838219 Acc: 0.86327078\n",
      "New best validation loss:  0.4783821851015091\n",
      "Epoch 5 of 500 took 0.306s\n",
      "Accuracy total 0.806046, main loss classifier 0.744712, source accuracy 0.825408 source classification loss 0.578174, target accuracy 0.786685 target loss 0.692863 accuracy domain distinction 0.500000 loss domain distinction 1.091931,\n",
      "VALIDATION Loss: 0.49097711 Acc: 0.85254692\n",
      "Epoch 6 of 500 took 0.307s\n",
      "Accuracy total 0.820992, main loss classifier 0.697501, source accuracy 0.842391 source classification loss 0.517409, target accuracy 0.799592 target loss 0.661768 accuracy domain distinction 0.498302 loss domain distinction 1.079125,\n",
      "VALIDATION Loss: 0.47017892 Acc: 0.84986595\n",
      "New best validation loss:  0.47017891705036163\n",
      "Epoch 7 of 500 took 0.310s\n",
      "Accuracy total 0.821671, main loss classifier 0.730114, source accuracy 0.839674 source classification loss 0.532821, target accuracy 0.803668 target loss 0.708856 accuracy domain distinction 0.500340 loss domain distinction 1.092752,\n",
      "VALIDATION Loss: 0.47747936 Acc: 0.83378016\n",
      "Epoch 8 of 500 took 0.305s\n",
      "Accuracy total 0.813859, main loss classifier 0.704693, source accuracy 0.822011 source classification loss 0.566760, target accuracy 0.805707 target loss 0.625947 accuracy domain distinction 0.497962 loss domain distinction 1.083389,\n",
      "VALIDATION Loss: 0.42888332 Acc: 0.88739946\n",
      "New best validation loss:  0.42888331909974414\n",
      "Epoch 9 of 500 took 0.306s\n",
      "Accuracy total 0.814198, main loss classifier 0.684420, source accuracy 0.830163 source classification loss 0.537519, target accuracy 0.798234 target loss 0.615559 accuracy domain distinction 0.501359 loss domain distinction 1.078811,\n",
      "VALIDATION Loss: 0.42350129 Acc: 0.84986595\n",
      "New best validation loss:  0.42350128913919133\n",
      "Epoch 10 of 500 took 0.304s\n",
      "Accuracy total 0.830842, main loss classifier 0.664169, source accuracy 0.832201 source classification loss 0.549021, target accuracy 0.829484 target loss 0.566428 accuracy domain distinction 0.502038 loss domain distinction 1.064443,\n",
      "VALIDATION Loss: 0.44561513 Acc: 0.86327078\n",
      "Epoch 11 of 500 took 0.309s\n",
      "Accuracy total 0.824049, main loss classifier 0.663730, source accuracy 0.832880 source classification loss 0.519249, target accuracy 0.815217 target loss 0.592038 accuracy domain distinction 0.498641 loss domain distinction 1.080858,\n",
      "VALIDATION Loss: 0.40128031 Acc: 0.8766756\n",
      "New best validation loss:  0.40128030876318616\n",
      "Epoch 12 of 500 took 0.308s\n",
      "Accuracy total 0.828465, main loss classifier 0.651022, source accuracy 0.837636 source classification loss 0.519635, target accuracy 0.819293 target loss 0.569283 accuracy domain distinction 0.500340 loss domain distinction 1.065635,\n",
      "VALIDATION Loss: 0.41631314 Acc: 0.85522788\n",
      "Epoch 13 of 500 took 0.305s\n",
      "Accuracy total 0.827106, main loss classifier 0.675728, source accuracy 0.842391 source classification loss 0.540943, target accuracy 0.811821 target loss 0.595933 accuracy domain distinction 0.497962 loss domain distinction 1.072896,\n",
      "VALIDATION Loss: 0.54459250 Acc: 0.8230563\n",
      "Epoch 14 of 500 took 0.309s\n",
      "Accuracy total 0.831182, main loss classifier 0.648640, source accuracy 0.850543 source classification loss 0.484513, target accuracy 0.811821 target loss 0.601908 accuracy domain distinction 0.500000 loss domain distinction 1.054296,\n",
      "VALIDATION Loss: 0.48240792 Acc: 0.86595174\n",
      "Epoch 15 of 500 took 0.304s\n",
      "Accuracy total 0.833899, main loss classifier 0.637569, source accuracy 0.848505 source classification loss 0.498337, target accuracy 0.819293 target loss 0.562521 accuracy domain distinction 0.500000 loss domain distinction 1.071400,\n",
      "VALIDATION Loss: 0.40624102 Acc: 0.86327078\n",
      "Epoch 16 of 500 took 0.304s\n",
      "Accuracy total 0.824728, main loss classifier 0.691279, source accuracy 0.835598 source classification loss 0.572493, target accuracy 0.813859 target loss 0.597045 accuracy domain distinction 0.499321 loss domain distinction 1.065096,\n",
      "VALIDATION Loss: 0.50529080 Acc: 0.84182306\n",
      "Epoch 17 of 500 took 0.310s\n",
      "Accuracy total 0.838995, main loss classifier 0.639184, source accuracy 0.853261 source classification loss 0.518783, target accuracy 0.824728 target loss 0.547048 accuracy domain distinction 0.501359 loss domain distinction 1.062688,\n",
      "VALIDATION Loss: 0.37991512 Acc: 0.86058981\n",
      "New best validation loss:  0.37991512070099515\n",
      "Epoch 18 of 500 took 0.311s\n",
      "Accuracy total 0.832201, main loss classifier 0.626513, source accuracy 0.839674 source classification loss 0.503639, target accuracy 0.824728 target loss 0.538144 accuracy domain distinction 0.498641 loss domain distinction 1.056217,\n",
      "VALIDATION Loss: 0.48800040 Acc: 0.84986595\n",
      "Epoch 19 of 500 took 0.303s\n",
      "Accuracy total 0.833899, main loss classifier 0.650857, source accuracy 0.842391 source classification loss 0.515294, target accuracy 0.825408 target loss 0.573968 accuracy domain distinction 0.500000 loss domain distinction 1.062262,\n",
      "VALIDATION Loss: 0.44850518 Acc: 0.86863271\n",
      "Epoch 20 of 500 took 0.308s\n",
      "Accuracy total 0.826427, main loss classifier 0.672000, source accuracy 0.832880 source classification loss 0.546025, target accuracy 0.819973 target loss 0.587831 accuracy domain distinction 0.498302 loss domain distinction 1.050719,\n",
      "VALIDATION Loss: 0.43640649 Acc: 0.83914209\n",
      "Epoch 21 of 500 took 0.305s\n",
      "Accuracy total 0.825408, main loss classifier 0.652922, source accuracy 0.836277 source classification loss 0.551097, target accuracy 0.814538 target loss 0.544865 accuracy domain distinction 0.500000 loss domain distinction 1.049404,\n",
      "VALIDATION Loss: 0.44625020 Acc: 0.86595174\n",
      "Epoch 22 of 500 took 0.304s\n",
      "Accuracy total 0.849864, main loss classifier 0.564358, source accuracy 0.862092 source classification loss 0.428886, target accuracy 0.837636 target loss 0.487061 accuracy domain distinction 0.499660 loss domain distinction 1.063843,\n",
      "VALIDATION Loss: 0.38126776 Acc: 0.87131367\n",
      "Epoch 23 of 500 took 0.302s\n",
      "Accuracy total 0.835598, main loss classifier 0.606414, source accuracy 0.833560 source classification loss 0.497231, target accuracy 0.837636 target loss 0.505832 accuracy domain distinction 0.499660 loss domain distinction 1.048823,\n",
      "VALIDATION Loss: 0.37384951 Acc: 0.86863271\n",
      "New best validation loss:  0.3738495136300723\n",
      "Epoch 24 of 500 took 0.311s\n",
      "Accuracy total 0.836277, main loss classifier 0.633097, source accuracy 0.838315 source classification loss 0.558843, target accuracy 0.834239 target loss 0.496673 accuracy domain distinction 0.500340 loss domain distinction 1.053385,\n",
      "VALIDATION Loss: 0.41667004 Acc: 0.87131367\n",
      "Epoch 25 of 500 took 0.306s\n",
      "Accuracy total 0.851223, main loss classifier 0.579414, source accuracy 0.856658 source classification loss 0.456957, target accuracy 0.845788 target loss 0.490122 accuracy domain distinction 0.499660 loss domain distinction 1.058741,\n",
      "VALIDATION Loss: 0.40085926 Acc: 0.87131367\n",
      "Epoch 26 of 500 took 0.303s\n",
      "Accuracy total 0.845448, main loss classifier 0.596168, source accuracy 0.843750 source classification loss 0.492631, target accuracy 0.847147 target loss 0.487626 accuracy domain distinction 0.500679 loss domain distinction 1.060393,\n",
      "VALIDATION Loss: 0.46232987 Acc: 0.84986595\n",
      "Epoch 27 of 500 took 0.308s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy total 0.837636, main loss classifier 0.622620, source accuracy 0.843750 source classification loss 0.494122, target accuracy 0.831522 target loss 0.541121 accuracy domain distinction 0.500000 loss domain distinction 1.049984,\n",
      "VALIDATION Loss: 0.42906646 Acc: 0.8766756\n",
      "Epoch 28 of 500 took 0.303s\n",
      "Accuracy total 0.844769, main loss classifier 0.588414, source accuracy 0.852582 source classification loss 0.471083, target accuracy 0.836957 target loss 0.495021 accuracy domain distinction 0.500340 loss domain distinction 1.053617,\n",
      "VALIDATION Loss: 0.38936758 Acc: 0.8766756\n",
      "Epoch 29 of 500 took 0.304s\n",
      "Accuracy total 0.834579, main loss classifier 0.608822, source accuracy 0.843071 source classification loss 0.486518, target accuracy 0.826087 target loss 0.519650 accuracy domain distinction 0.499660 loss domain distinction 1.057376,\n",
      "VALIDATION Loss: 0.40007681 Acc: 0.87131367\n",
      "Epoch    29: reducing learning rate of group 0 to 2.0120e-05.\n",
      "Epoch 30 of 500 took 0.310s\n",
      "Accuracy total 0.836957, main loss classifier 0.611357, source accuracy 0.842391 source classification loss 0.523507, target accuracy 0.831522 target loss 0.488815 accuracy domain distinction 0.499660 loss domain distinction 1.051961,\n",
      "VALIDATION Loss: 0.37800885 Acc: 0.87935657\n",
      "Epoch 31 of 500 took 0.307s\n",
      "Accuracy total 0.845788, main loss classifier 0.593036, source accuracy 0.854620 source classification loss 0.462486, target accuracy 0.836957 target loss 0.512365 accuracy domain distinction 0.499660 loss domain distinction 1.056102,\n",
      "VALIDATION Loss: 0.39983915 Acc: 0.86058981\n",
      "Epoch 32 of 500 took 0.304s\n",
      "Accuracy total 0.831182, main loss classifier 0.634408, source accuracy 0.828125 source classification loss 0.572006, target accuracy 0.834239 target loss 0.486637 accuracy domain distinction 0.500000 loss domain distinction 1.050866,\n",
      "VALIDATION Loss: 0.33333290 Acc: 0.88203753\n",
      "New best validation loss:  0.333332896232605\n",
      "Epoch 33 of 500 took 0.308s\n",
      "Accuracy total 0.831522, main loss classifier 0.608077, source accuracy 0.839674 source classification loss 0.505132, target accuracy 0.823370 target loss 0.502705 accuracy domain distinction 0.500679 loss domain distinction 1.041585,\n",
      "VALIDATION Loss: 0.37004367 Acc: 0.89276139\n",
      "Epoch 34 of 500 took 0.310s\n",
      "Accuracy total 0.839334, main loss classifier 0.592928, source accuracy 0.845109 source classification loss 0.479559, target accuracy 0.833560 target loss 0.496023 accuracy domain distinction 0.500340 loss domain distinction 1.051371,\n",
      "VALIDATION Loss: 0.37791234 Acc: 0.87131367\n",
      "Epoch 35 of 500 took 0.304s\n",
      "Accuracy total 0.850204, main loss classifier 0.572986, source accuracy 0.853261 source classification loss 0.465686, target accuracy 0.847147 target loss 0.472767 accuracy domain distinction 0.500000 loss domain distinction 1.037594,\n",
      "VALIDATION Loss: 0.38955976 Acc: 0.85522788\n",
      "Epoch 36 of 500 took 0.302s\n",
      "Accuracy total 0.837296, main loss classifier 0.603692, source accuracy 0.843071 source classification loss 0.487562, target accuracy 0.831522 target loss 0.509486 accuracy domain distinction 0.500000 loss domain distinction 1.051679,\n",
      "VALIDATION Loss: 0.42576975 Acc: 0.85522788\n",
      "Epoch 37 of 500 took 0.309s\n",
      "Accuracy total 0.838995, main loss classifier 0.599921, source accuracy 0.851223 source classification loss 0.455668, target accuracy 0.826766 target loss 0.534708 accuracy domain distinction 0.499321 loss domain distinction 1.047328,\n",
      "VALIDATION Loss: 0.44165930 Acc: 0.84986595\n",
      "Epoch 38 of 500 took 0.303s\n",
      "Accuracy total 0.838315, main loss classifier 0.589464, source accuracy 0.843071 source classification loss 0.489974, target accuracy 0.833560 target loss 0.478501 accuracy domain distinction 0.500000 loss domain distinction 1.052263,\n",
      "VALIDATION Loss: 0.49845033 Acc: 0.84182306\n",
      "Epoch    38: reducing learning rate of group 0 to 4.0240e-06.\n",
      "Epoch 39 of 500 took 0.304s\n",
      "Accuracy total 0.841712, main loss classifier 0.576870, source accuracy 0.853940 source classification loss 0.438408, target accuracy 0.829484 target loss 0.503997 accuracy domain distinction 0.498981 loss domain distinction 1.056675,\n",
      "VALIDATION Loss: 0.51958073 Acc: 0.84450402\n",
      "Epoch 40 of 500 took 0.309s\n",
      "Accuracy total 0.844769, main loss classifier 0.572479, source accuracy 0.853261 source classification loss 0.448283, target accuracy 0.836277 target loss 0.487853 accuracy domain distinction 0.499321 loss domain distinction 1.044110,\n",
      "VALIDATION Loss: 0.40559556 Acc: 0.87399464\n",
      "Epoch 41 of 500 took 0.303s\n",
      "Accuracy total 0.848845, main loss classifier 0.575378, source accuracy 0.858696 source classification loss 0.462344, target accuracy 0.838995 target loss 0.479368 accuracy domain distinction 0.500000 loss domain distinction 1.045219,\n",
      "VALIDATION Loss: 0.39534340 Acc: 0.89008043\n",
      "Epoch 42 of 500 took 0.306s\n",
      "Accuracy total 0.841712, main loss classifier 0.576547, source accuracy 0.851902 source classification loss 0.459393, target accuracy 0.831522 target loss 0.485688 accuracy domain distinction 0.500000 loss domain distinction 1.040058,\n",
      "VALIDATION Loss: 0.39810261 Acc: 0.86863271\n",
      "Epoch 43 of 500 took 0.306s\n",
      "Accuracy total 0.846807, main loss classifier 0.570114, source accuracy 0.860054 source classification loss 0.452636, target accuracy 0.833560 target loss 0.475225 accuracy domain distinction 0.499321 loss domain distinction 1.061832,\n",
      "VALIDATION Loss: 0.37623996 Acc: 0.86327078\n",
      "Epoch 44 of 500 took 0.308s\n",
      "Training complete in 0m 13s\n",
      "['participant_0']\n",
      "Optimizer =  <generator object Module.parameters at 0x7fd3cfed6ac0>\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/DANN/participant_0/best_state_8.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/DANN/participant_0/best_state_8.pt' (epoch 3)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/TSD/participant_0/best_state_0.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/TSD/participant_0/best_state_0.pt' (epoch 27)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/DANN/participant_0/best_state_1.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/DANN/participant_0/best_state_1.pt' (epoch 19)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/DANN/participant_0/best_state_2.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/DANN/participant_0/best_state_2.pt' (epoch 22)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/DANN/participant_0/best_state_3.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/DANN/participant_0/best_state_3.pt' (epoch 5)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/DANN/participant_0/best_state_4.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/DANN/participant_0/best_state_4.pt' (epoch 55)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/DANN/participant_0/best_state_5.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/DANN/participant_0/best_state_5.pt' (epoch 13)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/DANN/participant_0/best_state_6.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/DANN/participant_0/best_state_6.pt' (epoch 29)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/DANN/participant_0/best_state_7.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/DANN/participant_0/best_state_7.pt' (epoch 7)\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/DANN/participant_0/best_state_8.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_across_day_loc_1_lump2/DANN/participant_0/best_state_8.pt' (epoch 3)\n",
      "==== models_array =  (9,)  @ session  8\n",
      "HANDLING NEW SESSION  0\n",
      "HANDLING NEW SESSION  1\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.2222222222222222  len before:  26   len after:  9\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  13\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  4\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  0.9230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.5769230769230769  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  15\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.8846153846153846  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  13\n",
      "BEFORE:  0.8076923076923077   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  1.0  len before:  26   len after:  11\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.6538461538461539  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.8518356643356644   Accuracy pseudo: 0.9589622641509434  len pseudo:  2120    len predictions 2288\n",
      "HANDLING NEW SESSION  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.0  len before:  26   len after:  7\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.0  len before:  26   len after:  5\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.5  len before:  26   len after:  4\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.4230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  1\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  4\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  6\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.23076923076923078  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  8\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  9\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.0  len before:  26   len after:  17\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.5769230769230769  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  7\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.7777777777777778  len before:  26   len after:  9\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  18\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.6666666666666666  len before:  26   len after:  9\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.9230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.4230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.9230769230769231  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.7434440559440559   Accuracy pseudo: 0.8225646123260437  len pseudo:  2012    len predictions 2288\n",
      "HANDLING NEW SESSION  3\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.5384615384615384  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.6538461538461539  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.46153846153846156  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  14\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  18\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.4230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.0  len before:  26   len after:  4\n",
      "BEFORE:  0.38461538461538464   AFTER:  1.0  len before:  26   len after:  12\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.0  len before:  26   len after:  5\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  8\n",
      "BEFORE:  0.5   AFTER:  0.6  len before:  26   len after:  10\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.4  len before:  26   len after:  10\n",
      "BEFORE:  0.11538461538461539   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.6923076923076923  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  15\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  5\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  8\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.11538461538461539   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  9\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  8\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  6\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  1.0  len before:  26   len after:  7\n",
      "BEFORE:  0.11538461538461539   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  8\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.7473776223776224   Accuracy pseudo: 0.8564102564102564  len pseudo:  1950    len predictions 2288\n",
      "HANDLING NEW SESSION  4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  9\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.11538461538461539   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.2692307692307692  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  5\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  1.0  len before:  26   len after:  1\n",
      "BEFORE:  0.5   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  0.5   AFTER:  1.0  len before:  26   len after:  2\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  7\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  13\n",
      "BEFORE:  0.3076923076923077   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.5384615384615384  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  1.0  len before:  26   len after:  7\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.9230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.9375  len before:  26   len after:  16\n",
      "BEFORE:  0.8076923076923077   AFTER:  0.8461538461538461  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  5\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  17\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.34615384615384615  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.5384615384615384  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  15\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.7333333333333333  len before:  26   len after:  15\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.6538461538461539  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.3888888888888889  len before:  26   len after:  18\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.0  len before:  26   len after:  10\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.0  len before:  26   len after:  5\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.6437937062937062   Accuracy pseudo: 0.7436692506459949  len pseudo:  1935    len predictions 2288\n",
      "HANDLING NEW SESSION  5\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  16\n",
      "BEFORE:  0.07692307692307693   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  9\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  1.0  len before:  26   len after:  1\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.46153846153846156  len before:  26   len after:  26\n",
      "BEFORE:  0.11538461538461539   AFTER:  0.0  len before:  26   len after:  7\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.5384615384615384  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  1\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  9\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  9\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.6  len before:  26   len after:  10\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.4230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.11538461538461539   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.4230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.8846153846153846  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.4444444444444444  len before:  26   len after:  18\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  5\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  12\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.5384615384615384  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.0  len before:  26   len after:  3\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.6634615384615384   Accuracy pseudo: 0.7764883955600403  len pseudo:  1982    len predictions 2288\n",
      "HANDLING NEW SESSION  6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.6538461538461539  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  7\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  17\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.0  len before:  26   len after:  12\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.0  len before:  26   len after:  6\n",
      "BEFORE:  0.11538461538461539   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  1.0  len before:  26   len after:  7\n",
      "BEFORE:  0.5   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.4230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  17\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.5769230769230769  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.0  len before:  26   len after:  7\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  11\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  6\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.8076923076923077   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  17\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  8\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.6923076923076923  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.0  len before:  26   len after:  15\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  13\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.4230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  9\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.6538461538461539  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.46153846153846156  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.5769230769230769  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.46153846153846156  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  5\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  10\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  9\n",
      "BEFORE:  0.3076923076923077   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  1.0  len before:  26   len after:  5\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.6470588235294118  len before:  26   len after:  17\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.48732517482517484   Accuracy pseudo: 0.5804123711340207  len pseudo:  1940    len predictions 2288\n",
      "HANDLING NEW SESSION  7\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.6923076923076923  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.4230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  16\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.0  len before:  26   len after:  18\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.46153846153846156  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  1.0  len before:  26   len after:  1\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.46153846153846156   AFTER:  1.0  len before:  26   len after:  13\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  11\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.0  len before:  26   len after:  13\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  8\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.0  len before:  26   len after:  8\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.0  len before:  26   len after:  13\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.46153846153846156  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  5\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  10\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  18\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  1\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  18\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  12\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.6666666666666666  len before:  26   len after:  15\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  13\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.15384615384615385  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  9\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  10\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.6923076923076923  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.46153846153846156  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.9615384615384616  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.75  len before:  26   len after:  8\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.8461538461538461  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.0  len before:  26   len after:  1\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  7\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.0  len before:  26   len after:  3\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.0  len before:  26   len after:  10\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  18\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.7058823529411765  len before:  26   len after:  17\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.5113636363636364   Accuracy pseudo: 0.60687432867884  len pseudo:  1862    len predictions 2288\n",
      "HANDLING NEW SESSION  8\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.6666666666666666  len before:  26   len after:  18\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.4117647058823529  len before:  26   len after:  17\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  15\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.5769230769230769  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.11538461538461539   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.11538461538461539   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.0  len before:  26   len after:  5\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  9\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  13\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.5384615384615384  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  8\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  16\n",
      "BEFORE:  0.11538461538461539   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  18\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  1.0  len before:  26   len after:  1\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  9\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  6\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  13\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  4\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  15\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.0  len before:  26   len after:  1\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  18\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  1\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.11538461538461539   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.0  len before:  26   len after:  13\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.5882352941176471  len before:  26   len after:  17\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  11\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  4\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.15384615384615385  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  18\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  6\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.8  len before:  26   len after:  5\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.0  len before:  26   len after:  1\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.7142857142857143  len before:  26   len after:  14\n",
      "ACCURACY MODEL:  0.44405594405594406   Accuracy pseudo: 0.5352112676056338  len pseudo:  1775    len predictions 2288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING TRAINING\n",
      "Accuracy total 0.782670, main loss classifier 0.917865, source accuracy 0.814631 source classification loss 0.671021, target accuracy 0.750710 target loss 0.930196 accuracy domain distinction 0.502486 loss domain distinction 1.172565,\n",
      "VALIDATION Loss: 0.41643783 Acc: 0.86760563\n",
      "New best validation loss:  0.4164378270506859\n",
      "Epoch 2 of 500 took 0.295s\n",
      "Accuracy total 0.784446, main loss classifier 0.822135, source accuracy 0.807528 source classification loss 0.644675, target accuracy 0.761364 target loss 0.779738 accuracy domain distinction 0.497159 loss domain distinction 1.099284,\n",
      "VALIDATION Loss: 0.44169663 Acc: 0.83380282\n",
      "Epoch 3 of 500 took 0.297s\n",
      "Accuracy total 0.799716, main loss classifier 0.770111, source accuracy 0.808239 source classification loss 0.612325, target accuracy 0.791193 target loss 0.708614 accuracy domain distinction 0.499290 loss domain distinction 1.096417,\n",
      "VALIDATION Loss: 0.46639840 Acc: 0.85633803\n",
      "Epoch 4 of 500 took 0.302s\n",
      "Accuracy total 0.810724, main loss classifier 0.706619, source accuracy 0.816761 source classification loss 0.550751, target accuracy 0.804688 target loss 0.645466 accuracy domain distinction 0.499645 loss domain distinction 1.085101,\n",
      "VALIDATION Loss: 0.59350737 Acc: 0.8\n",
      "Epoch 5 of 500 took 0.291s\n",
      "Accuracy total 0.813920, main loss classifier 0.685237, source accuracy 0.814631 source classification loss 0.552956, target accuracy 0.813210 target loss 0.602542 accuracy domain distinction 0.500355 loss domain distinction 1.074883,\n",
      "VALIDATION Loss: 0.41499635 Acc: 0.86478873\n",
      "New best validation loss:  0.4149963508049647\n",
      "Epoch 6 of 500 took 0.297s\n",
      "Accuracy total 0.813565, main loss classifier 0.697354, source accuracy 0.824574 source classification loss 0.573931, target accuracy 0.802557 target loss 0.607889 accuracy domain distinction 0.499645 loss domain distinction 1.064438,\n",
      "VALIDATION Loss: 0.46138829 Acc: 0.85633803\n",
      "Epoch 7 of 500 took 0.297s\n",
      "Accuracy total 0.828835, main loss classifier 0.629648, source accuracy 0.828125 source classification loss 0.498947, target accuracy 0.829545 target loss 0.548725 accuracy domain distinction 0.499645 loss domain distinction 1.058122,\n",
      "VALIDATION Loss: 0.40836518 Acc: 0.85352113\n",
      "New best validation loss:  0.4083651825785637\n",
      "Epoch 8 of 500 took 0.293s\n",
      "Accuracy total 0.813920, main loss classifier 0.704097, source accuracy 0.830966 source classification loss 0.572476, target accuracy 0.796875 target loss 0.623146 accuracy domain distinction 0.500355 loss domain distinction 1.062855,\n",
      "VALIDATION Loss: 0.38764069 Acc: 0.87042254\n",
      "New best validation loss:  0.38764068981011707\n",
      "Epoch 9 of 500 took 0.293s\n",
      "Accuracy total 0.818182, main loss classifier 0.672708, source accuracy 0.818182 source classification loss 0.574723, target accuracy 0.818182 target loss 0.559782 accuracy domain distinction 0.498935 loss domain distinction 1.054550,\n",
      "VALIDATION Loss: 0.31730628 Acc: 0.88732394\n",
      "New best validation loss:  0.3173062751690547\n",
      "Epoch 10 of 500 took 0.296s\n",
      "Accuracy total 0.827060, main loss classifier 0.630419, source accuracy 0.810369 source classification loss 0.559294, target accuracy 0.843750 target loss 0.491494 accuracy domain distinction 0.500000 loss domain distinction 1.050252,\n",
      "VALIDATION Loss: 0.40672409 Acc: 0.85915493\n",
      "Epoch 11 of 500 took 0.292s\n",
      "Accuracy total 0.832386, main loss classifier 0.612702, source accuracy 0.829545 source classification loss 0.519257, target accuracy 0.835227 target loss 0.495624 accuracy domain distinction 0.500355 loss domain distinction 1.052614,\n",
      "VALIDATION Loss: 0.36259292 Acc: 0.88169014\n",
      "Epoch 12 of 500 took 0.293s\n",
      "Accuracy total 0.827060, main loss classifier 0.632194, source accuracy 0.825284 source classification loss 0.518963, target accuracy 0.828835 target loss 0.534686 accuracy domain distinction 0.500000 loss domain distinction 1.053695,\n",
      "VALIDATION Loss: 0.40195494 Acc: 0.85915493\n",
      "Epoch 13 of 500 took 0.296s\n",
      "Accuracy total 0.835938, main loss classifier 0.590110, source accuracy 0.842330 source classification loss 0.482837, target accuracy 0.829545 target loss 0.486074 accuracy domain distinction 0.499645 loss domain distinction 1.056545,\n",
      "VALIDATION Loss: 0.34900332 Acc: 0.88169014\n",
      "Epoch 14 of 500 took 0.292s\n",
      "Accuracy total 0.830256, main loss classifier 0.624070, source accuracy 0.830966 source classification loss 0.524698, target accuracy 0.829545 target loss 0.513256 accuracy domain distinction 0.500355 loss domain distinction 1.050928,\n",
      "VALIDATION Loss: 0.40223489 Acc: 0.87042254\n",
      "Epoch 15 of 500 took 0.294s\n",
      "Accuracy total 0.840909, main loss classifier 0.590455, source accuracy 0.841619 source classification loss 0.499175, target accuracy 0.840199 target loss 0.472961 accuracy domain distinction 0.500000 loss domain distinction 1.043872,\n",
      "VALIDATION Loss: 0.31280137 Acc: 0.89014085\n",
      "New best validation loss:  0.31280137101809186\n",
      "Epoch 16 of 500 took 0.295s\n",
      "Accuracy total 0.846236, main loss classifier 0.564719, source accuracy 0.848722 source classification loss 0.451969, target accuracy 0.843750 target loss 0.468207 accuracy domain distinction 0.500000 loss domain distinction 1.046310,\n",
      "VALIDATION Loss: 0.37486326 Acc: 0.87042254\n",
      "Epoch 17 of 500 took 0.299s\n",
      "Accuracy total 0.851562, main loss classifier 0.566042, source accuracy 0.844460 source classification loss 0.498347, target accuracy 0.858665 target loss 0.424365 accuracy domain distinction 0.499645 loss domain distinction 1.046859,\n",
      "VALIDATION Loss: 0.36066487 Acc: 0.89014085\n",
      "Epoch 18 of 500 took 0.292s\n",
      "Accuracy total 0.839489, main loss classifier 0.598610, source accuracy 0.828125 source classification loss 0.516399, target accuracy 0.850852 target loss 0.472745 accuracy domain distinction 0.500000 loss domain distinction 1.040377,\n",
      "VALIDATION Loss: 0.31808605 Acc: 0.89577465\n",
      "Epoch 19 of 500 took 0.291s\n",
      "Accuracy total 0.832386, main loss classifier 0.600545, source accuracy 0.825284 source classification loss 0.541336, target accuracy 0.839489 target loss 0.451829 accuracy domain distinction 0.500000 loss domain distinction 1.039626,\n",
      "VALIDATION Loss: 0.35276171 Acc: 0.89014085\n",
      "Epoch 20 of 500 took 0.295s\n",
      "Accuracy total 0.835938, main loss classifier 0.574617, source accuracy 0.825284 source classification loss 0.503430, target accuracy 0.846591 target loss 0.437689 accuracy domain distinction 0.500000 loss domain distinction 1.040577,\n",
      "VALIDATION Loss: 0.32141362 Acc: 0.89295775\n",
      "Epoch 21 of 500 took 0.299s\n",
      "Accuracy total 0.847656, main loss classifier 0.580076, source accuracy 0.830256 source classification loss 0.530236, target accuracy 0.865057 target loss 0.422379 accuracy domain distinction 0.500000 loss domain distinction 1.037686,\n",
      "VALIDATION Loss: 0.45936095 Acc: 0.84507042\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0060e-04.\n",
      "Epoch 22 of 500 took 0.293s\n",
      "Accuracy total 0.853693, main loss classifier 0.518899, source accuracy 0.846591 source classification loss 0.428803, target accuracy 0.860795 target loss 0.402220 accuracy domain distinction 0.500000 loss domain distinction 1.033880,\n",
      "VALIDATION Loss: 0.35662622 Acc: 0.89295775\n",
      "Epoch 23 of 500 took 0.292s\n",
      "Accuracy total 0.834517, main loss classifier 0.586532, source accuracy 0.820312 source classification loss 0.537599, target accuracy 0.848722 target loss 0.428968 accuracy domain distinction 0.500000 loss domain distinction 1.032485,\n",
      "VALIDATION Loss: 0.34312123 Acc: 0.90140845\n",
      "Epoch 24 of 500 took 0.306s\n",
      "Accuracy total 0.854759, main loss classifier 0.541782, source accuracy 0.852273 source classification loss 0.459015, target accuracy 0.857244 target loss 0.418441 accuracy domain distinction 0.500000 loss domain distinction 1.030546,\n",
      "VALIDATION Loss: 0.33117542 Acc: 0.89295775\n",
      "Epoch 25 of 500 took 0.293s\n",
      "Accuracy total 0.860085, main loss classifier 0.524190, source accuracy 0.863636 source classification loss 0.410673, target accuracy 0.856534 target loss 0.431210 accuracy domain distinction 0.500000 loss domain distinction 1.032482,\n",
      "VALIDATION Loss: 0.36054528 Acc: 0.87605634\n",
      "Epoch 26 of 500 took 0.293s\n",
      "Accuracy total 0.861861, main loss classifier 0.505406, source accuracy 0.859375 source classification loss 0.428075, target accuracy 0.864347 target loss 0.375574 accuracy domain distinction 0.500000 loss domain distinction 1.035810,\n",
      "VALIDATION Loss: 0.31818290 Acc: 0.90140845\n",
      "Epoch 27 of 500 took 0.294s\n",
      "Training complete in 0m 8s\n",
      "['participant_0']\n"
     ]
    }
   ],
   "source": [
    "# percentage_same_gesture_stable = 0.75 \n",
    "# run_SCADANN_training_sessions(examples_datasets=examples_datasets_train, labels_datasets=labels_datasets_train,\n",
    "#                               num_kernels=num_kernels, feature_vector_input_length=feature_vector_input_length,\n",
    "#                               path_weights_to_save_to=path_SCADANN,\n",
    "#                               path_weights_Adversarial_training=path_DANN,\n",
    "#                               path_weights_Normal_training=path_TSD,\n",
    "#                               number_of_cycles_total = number_of_cycles_total, \n",
    "#                               number_of_cycle_for_first_training = number_of_cycle_for_first_training,\n",
    "#                               number_of_classes=number_of_classes,\n",
    "#                               learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET one participant_examples  (9,)\n",
      "   GET one training_index_examples  (8, 572, 252)  at  0\n",
      "   GOT one group XY  (4576, 252)    (4576,)\n",
      "       one group XY test  (1144, 252)    (1144, 252)\n",
      "       one group XY train (4118, 252)    (4118,)\n",
      "       one group XY valid (458, 252)    (458, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  1\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  2\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  3\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  4\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  5\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  6\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  7\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (4, 572, 252)  at  8\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "dataloaders: \n",
      "   train  (1, 9)\n",
      "   valid  (1, 9)\n",
      "   test  (1, 9)\n",
      "Participant:  0  Accuracy:  0.9527972027972028\n",
      "Participant:  0  Accuracy:  0.8811188811188811\n",
      "Participant:  0  Accuracy:  0.736013986013986\n",
      "Participant:  0  Accuracy:  0.8076923076923077\n",
      "Participant:  0  Accuracy:  0.6870629370629371\n",
      "Participant:  0  Accuracy:  0.722027972027972\n",
      "Participant:  0  Accuracy:  0.5524475524475524\n",
      "Participant:  0  Accuracy:  0.548951048951049\n",
      "Participant:  0  Accuracy:  0.506993006993007\n",
      "ACCURACY PARTICIPANT:  [0.9527972027972028, 0.8811188811188811, 0.736013986013986, 0.8076923076923077, 0.6870629370629371, 0.722027972027972, 0.5524475524475524, 0.548951048951049, 0.506993006993007]\n",
      "[[0.9527972  0.88111888 0.73601399 0.80769231 0.68706294 0.72202797\n",
      "  0.55244755 0.54895105 0.50699301]]\n",
      "[array([0.9527972 , 0.88111888, 0.73601399, 0.80769231, 0.68706294,\n",
      "       0.72202797, 0.55244755, 0.54895105, 0.50699301])]\n",
      "OVERALL ACCURACY: 0.7105672105672105\n"
     ]
    }
   ],
   "source": [
    "algo_name = \"SCADANN\"\n",
    "test_network_SCADANN(examples_datasets_train=examples_datasets_train, labels_datasets_train=labels_datasets_train,\n",
    "                     num_neurons=num_kernels, feature_vector_input_length=feature_vector_input_length,\n",
    "                     path_weights_SCADANN =path_SCADANN, path_weights_normal=path_TSD,\n",
    "                     algo_name=algo_name, cycle_test=3, number_of_cycles_total=number_of_cycles_total,\n",
    "                     number_of_cycle_for_first_training = number_of_cycle_for_first_training,\n",
    "                     number_of_classes=number_of_classes, save_path = save_SCADANN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Day_0~1</th>\n",
       "      <td>0.952797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_2</th>\n",
       "      <td>0.881119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_3</th>\n",
       "      <td>0.736014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_4</th>\n",
       "      <td>0.807692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_5</th>\n",
       "      <td>0.687063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_6</th>\n",
       "      <td>0.722028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_7</th>\n",
       "      <td>0.552448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_8</th>\n",
       "      <td>0.548951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_9</th>\n",
       "      <td>0.506993</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Participant_5\n",
       "Day_0~1      0.952797\n",
       "Day_2        0.881119\n",
       "Day_3        0.736014\n",
       "Day_4        0.807692\n",
       "Day_5        0.687063\n",
       "Day_6        0.722028\n",
       "Day_7        0.552448\n",
       "Day_8        0.548951\n",
       "Day_9        0.506993"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_filename = save_SCADANN + '/predictions_' + algo_name + \".npy\"\n",
    "results = np.load(results_filename, allow_pickle=True)\n",
    "SCADANN_acc = results[0]\n",
    "SCADANN_acc_overall = np.mean(SCADANN_acc)\n",
    "SCADANN_df = pd.DataFrame(SCADANN_acc.transpose(), \n",
    "                       index = [f'Day_{i}' for i in index_participant_list],\n",
    "                        columns = ['Participant_5'])\n",
    "SCADANN_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "SCADANN_df.transpose().plot.bar(rot=0, figsize=(10,5))\n",
    "plt.title(\"SCADANN Accuracies\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ground_truths  =  (1, 9)\n",
      "predictions =  (1, 9)\n",
      "index_participant_list  ['0~1', 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "accuracies_gestures =  (22, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Loc1_Sub5_Day0~1-&gt;0~1</th>\n",
       "      <th>Loc1_Sub5_Day0~1-&gt;2</th>\n",
       "      <th>Loc1_Sub5_Day0~1-&gt;3</th>\n",
       "      <th>Loc1_Sub5_Day0~1-&gt;4</th>\n",
       "      <th>Loc1_Sub5_Day0~1-&gt;5</th>\n",
       "      <th>Loc1_Sub5_Day0~1-&gt;6</th>\n",
       "      <th>Loc1_Sub5_Day0~1-&gt;7</th>\n",
       "      <th>Loc1_Sub5_Day0~1-&gt;8</th>\n",
       "      <th>Loc1_Sub5_Day0~1-&gt;9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M1</td>\n",
       "      <td>0.980769</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.807692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M2</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.961538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M4</td>\n",
       "      <td>0.788462</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>M5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.076923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>M6</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.346154</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>M7</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.692308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>M8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>M9</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>M10</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.192308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>M11</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.346154</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>M12</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.076923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>M13</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.615385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>M14</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>M15</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.230769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>M16</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>M17</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>M18</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.653846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>M19</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.346154</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.192308</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>M20</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.692308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>M21</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.807692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Mean</td>\n",
       "      <td>0.952797</td>\n",
       "      <td>0.881119</td>\n",
       "      <td>0.736014</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.687063</td>\n",
       "      <td>0.722028</td>\n",
       "      <td>0.552448</td>\n",
       "      <td>0.548951</td>\n",
       "      <td>0.506993</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Loc1_Sub5_Day0~1->0~1  Loc1_Sub5_Day0~1->2  \\\n",
       "0          M0               1.000000             1.000000   \n",
       "1          M1               0.980769             1.000000   \n",
       "2          M2               0.961538             0.807692   \n",
       "3          M3               1.000000             0.846154   \n",
       "4          M4               0.788462             1.000000   \n",
       "5          M5               1.000000             1.000000   \n",
       "6          M6               1.000000             1.000000   \n",
       "7          M7               1.000000             0.538462   \n",
       "8          M8               1.000000             0.769231   \n",
       "9          M9               1.000000             1.000000   \n",
       "10        M10               0.884615             0.961538   \n",
       "11        M11               1.000000             0.884615   \n",
       "12        M12               0.846154             0.692308   \n",
       "13        M13               0.846154             0.769231   \n",
       "14        M14               0.884615             0.807692   \n",
       "15        M15               0.846154             0.846154   \n",
       "16        M16               1.000000             1.000000   \n",
       "17        M17               1.000000             1.000000   \n",
       "18        M18               1.000000             1.000000   \n",
       "19        M19               1.000000             1.000000   \n",
       "20        M20               0.923077             0.807692   \n",
       "21        M21               1.000000             0.653846   \n",
       "22       Mean               0.952797             0.881119   \n",
       "\n",
       "    Loc1_Sub5_Day0~1->3  Loc1_Sub5_Day0~1->4  Loc1_Sub5_Day0~1->5  \\\n",
       "0              1.000000             1.000000             1.000000   \n",
       "1              0.692308             0.884615             1.000000   \n",
       "2              0.500000             0.961538             0.500000   \n",
       "3              0.000000             0.769231             0.692308   \n",
       "4              0.000000             1.000000             0.000000   \n",
       "5              1.000000             1.000000             0.961538   \n",
       "6              0.346154             1.000000             1.000000   \n",
       "7              1.000000             0.884615             0.884615   \n",
       "8              1.000000             1.000000             1.000000   \n",
       "9              0.692308             0.730769             0.692308   \n",
       "10             0.153846             0.615385             1.000000   \n",
       "11             0.923077             0.076923             0.000000   \n",
       "12             0.769231             0.846154             0.423077   \n",
       "13             1.000000             1.000000             1.000000   \n",
       "14             0.423077             0.000000             0.076923   \n",
       "15             0.846154             0.038462             0.307692   \n",
       "16             1.000000             1.000000             0.653846   \n",
       "17             1.000000             1.000000             1.000000   \n",
       "18             1.000000             1.000000             0.730769   \n",
       "19             0.961538             1.000000             0.346154   \n",
       "20             1.000000             0.961538             0.923077   \n",
       "21             0.884615             1.000000             0.923077   \n",
       "22             0.736014             0.807692             0.687063   \n",
       "\n",
       "    Loc1_Sub5_Day0~1->6  Loc1_Sub5_Day0~1->7  Loc1_Sub5_Day0~1->8  \\\n",
       "0              1.000000             1.000000             1.000000   \n",
       "1              0.846154             0.653846             0.576923   \n",
       "2              0.692308             0.269231             0.461538   \n",
       "3              0.115385             0.000000             0.000000   \n",
       "4              0.000000             0.000000             0.000000   \n",
       "5              1.000000             0.076923             0.692308   \n",
       "6              0.615385             0.576923             0.538462   \n",
       "7              1.000000             0.884615             1.000000   \n",
       "8              0.923077             1.000000             1.000000   \n",
       "9              0.884615             0.153846             0.769231   \n",
       "10             0.000000             0.615385             0.538462   \n",
       "11             0.961538             0.346154             0.269231   \n",
       "12             0.730769             0.653846             0.230769   \n",
       "13             1.000000             0.923077             1.000000   \n",
       "14             0.115385             0.000000             0.000000   \n",
       "15             0.115385             0.000000             0.000000   \n",
       "16             1.000000             0.923077             1.000000   \n",
       "17             1.000000             1.000000             1.000000   \n",
       "18             1.000000             0.846154             0.076923   \n",
       "19             1.000000             0.307692             0.192308   \n",
       "20             0.884615             0.923077             0.730769   \n",
       "21             1.000000             1.000000             1.000000   \n",
       "22             0.722028             0.552448             0.548951   \n",
       "\n",
       "    Loc1_Sub5_Day0~1->9  \n",
       "0              1.000000  \n",
       "1              0.807692  \n",
       "2              0.961538  \n",
       "3              0.000000  \n",
       "4              0.000000  \n",
       "5              0.076923  \n",
       "6              1.000000  \n",
       "7              0.692308  \n",
       "8              1.000000  \n",
       "9              0.500000  \n",
       "10             0.192308  \n",
       "11             0.000000  \n",
       "12             0.076923  \n",
       "13             0.615385  \n",
       "14             0.000000  \n",
       "15             0.230769  \n",
       "16             0.846154  \n",
       "17             1.000000  \n",
       "18             0.653846  \n",
       "19             0.000000  \n",
       "20             0.692308  \n",
       "21             0.807692  \n",
       "22             0.506993  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truths = results[1]\n",
    "predictions = results[2]\n",
    "print(\"ground_truths  = \", np.shape(ground_truths))\n",
    "print(\"predictions = \", np.shape(predictions))\n",
    "df = get_gesture_accuracies(ground_truths, predictions, number_of_classes=number_of_classes, \n",
    "                            m_name=m_name, n_name=n_name, path=save_TSD, algo_name=algo_name,\n",
    "                           index_participant_list_customized=index_participant_list,\n",
    "                           lump_day_at_participant=5)\n",
    "df = pd.read_csv(save_SCADANN+'/'+algo_name+'.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "Suppose there is a ndarray of NxM dataloaders, then N group of models will be trained, and each group will have M model. Each group is independent of the other, and each model within a group is dependent on its previous training weights.\n",
    "\n",
    "In general, overall accuracies of SCADANN are better than DANN, and DANN is better than TSD.\n",
    "Occasionally accuracies of SCADANN end up a little smaller than DANN, reasons may be lack of datasets put into training model (fixed) and non-optimal percentage_same_gesture_sable (fixed). Code should be reproducible if processed dataset sticks to the shape defined above.  \n",
    "\n",
    "The amount of increase in accuracies from DANN to SCADANN looks random. But if the base model is better at classifying one session, then its corresponding SCADANN is also better at classifying the same session. Given such result, to obtain the best performance from SCADANN, a good model trained with good data should be the starting point.\n",
    "\n",
    "* What to check if sth goes wrong:\n",
    "    * percentage_same_gesture_sable\n",
    "    * number of cycles or sessions\n",
    "    * shape of dataloaders (combination of train, test, valid should include all dataset)\n",
    "    * shape of procssed datasets\n",
    "    * directory paths of weights and results\n",
    "    * if weights are stored or loaded correcltyTSD_acc_overall_one = np.mean(TSD_acc, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TSD\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Day_0~1</th>\n",
       "      <td>0.952797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_2</th>\n",
       "      <td>0.825175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_3</th>\n",
       "      <td>0.68007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_4</th>\n",
       "      <td>0.699301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_5</th>\n",
       "      <td>0.573427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_6</th>\n",
       "      <td>0.604895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_7</th>\n",
       "      <td>0.40035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_8</th>\n",
       "      <td>0.445804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_9</th>\n",
       "      <td>0.407343</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Participant_5\n",
       "Day_0~1      0.952797\n",
       "Day_2        0.825175\n",
       "Day_3         0.68007\n",
       "Day_4        0.699301\n",
       "Day_5        0.573427\n",
       "Day_6        0.604895\n",
       "Day_7         0.40035\n",
       "Day_8        0.445804\n",
       "Day_9        0.407343"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DANN\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Day_0~1</th>\n",
       "      <td>0.952797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_2</th>\n",
       "      <td>0.826923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_3</th>\n",
       "      <td>0.72028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_4</th>\n",
       "      <td>0.755245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_5</th>\n",
       "      <td>0.627622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_6</th>\n",
       "      <td>0.692308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_7</th>\n",
       "      <td>0.45979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_8</th>\n",
       "      <td>0.491259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_9</th>\n",
       "      <td>0.433566</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Participant_5\n",
       "Day_0~1      0.952797\n",
       "Day_2        0.826923\n",
       "Day_3         0.72028\n",
       "Day_4        0.755245\n",
       "Day_5        0.627622\n",
       "Day_6        0.692308\n",
       "Day_7         0.45979\n",
       "Day_8        0.491259\n",
       "Day_9        0.433566"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCADANN\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Day_0~1</th>\n",
       "      <td>0.952797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_2</th>\n",
       "      <td>0.881119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_3</th>\n",
       "      <td>0.736014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_4</th>\n",
       "      <td>0.807692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_5</th>\n",
       "      <td>0.687063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_6</th>\n",
       "      <td>0.722028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_7</th>\n",
       "      <td>0.552448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_8</th>\n",
       "      <td>0.548951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_9</th>\n",
       "      <td>0.506993</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Participant_5\n",
       "Day_0~1      0.952797\n",
       "Day_2        0.881119\n",
       "Day_3        0.736014\n",
       "Day_4        0.807692\n",
       "Day_5        0.687063\n",
       "Day_6        0.722028\n",
       "Day_7        0.552448\n",
       "Day_8        0.548951\n",
       "Day_9        0.506993"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"TSD\")\n",
    "display(TSD_df)\n",
    "print(\"DANN\")\n",
    "display(DANN_df)\n",
    "print(\"SCADANN\")\n",
    "display(SCADANN_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Day_2</th>\n",
       "      <td>0.055944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_3</th>\n",
       "      <td>0.055944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_4</th>\n",
       "      <td>0.108392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_5</th>\n",
       "      <td>0.113636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_6</th>\n",
       "      <td>0.117133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_7</th>\n",
       "      <td>0.152098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_8</th>\n",
       "      <td>0.103147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_9</th>\n",
       "      <td>0.09965</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Participant_5\n",
       "Day_2      0.055944\n",
       "Day_3      0.055944\n",
       "Day_4      0.108392\n",
       "Day_5      0.113636\n",
       "Day_6      0.117133\n",
       "Day_7      0.152098\n",
       "Day_8      0.103147\n",
       "Day_9       0.09965"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "diff_df = SCADANN_df-TSD_df\n",
    "diff_df = diff_df.drop('Day_'+index_participant_list[0])\n",
    "display(diff_df)\n",
    "diff_df.to_csv(save_TSD+'/diff_results/across_day_loc1_lump2_diff.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Overall_Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TSD</th>\n",
       "      <td>0.621018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DANN</th>\n",
       "      <td>0.662199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SCADANN</th>\n",
       "      <td>0.710567</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Overall_Acc\n",
       "TSD         0.621018\n",
       "DANN        0.662199\n",
       "SCADANN     0.710567"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_acc_df = pd.DataFrame([TSD_acc_overall, DANN_acc_overall, SCADANN_acc_overall],\n",
    "                             index = [\"TSD\", \"DANN\", \"SCADANN\"],\n",
    "                             columns = [\"Overall_Acc\"])\n",
    "overall_acc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1800x1800 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(25,25))\n",
    "acc_list = [TSD_df, DANN_df, SCADANN_df, overall_acc_df]\n",
    "title_list = [\"TSD\", \"DANN\", \"SCADANN\", \"Overall\"]\n",
    "for idx, ax in enumerate(axes.reshape(-1)): \n",
    "    acc_list[idx].transpose().plot.bar(ax = ax, rot=0)\n",
    "    ax.set_title(title_list[idx])\n",
    "    ax.set_ylim([0, 1.0])\n",
    "    for p in ax.patches:\n",
    "        ax.annotate(str(np.round(p.get_height(),2)), (p.get_x()+p.get_width()/2., p.get_height()),\n",
    "                    ha='center', va='center', xytext=(0, 8),textcoords='offset points')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
