{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/home/laiy/gitrepos/msr_final/LongTermEMG_myo\")\n",
    "from PrepareAndLoadData.process_data import read_data_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/home/laiy/gitrepos/msr_final/Wearable_Sensor_Long-term_sEMG_Dataset/data\"\n",
    "processed_data_dir = \"/home/laiy/gitrepos/msr_final/LongTermEMG_myo/Processed_datasets\"\n",
    "code_dir = \"/home/laiy/gitrepos/msr_final/LongTermEMG_myo\"\n",
    "save_dir = \"/home/laiy/gitrepos/msr_final/LongTermEMG_myo/Results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_data_training(path=data_dir, store_path = processed_data_dir, num_participant=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. TSD_DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TrainingsAndEvaluations.ForTrainingSessions.train_tsd_dnn_standard import \\\n",
    "            test_TSD_DNN_on_training_sessions, train_fine_tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traning examples  (3, 3, 40, 572, 252)\n",
      "traning labels  (3, 3, 40, 572)\n"
     ]
    }
   ],
   "source": [
    "# check stored pickle \n",
    "with open(processed_data_dir + \"/training_session.pickle\", 'rb') as f:\n",
    "    dataset_training = pickle.load(file=f)\n",
    "\n",
    "examples_datasets_train = dataset_training['examples_training']\n",
    "print('traning examples ', np.shape(examples_datasets_train))\n",
    "labels_datasets_train = dataset_training['labels_training']\n",
    "print('traning labels ', np.shape(labels_datasets_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_kernels=[200, 200, 200]                        # model layer size \n",
    "number_of_cycle_for_first_training=40               # #session\n",
    "number_of_cycles_rest_of_training=40     \n",
    "path_to_save_to=\"/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/TSD\"\n",
    "number_of_classes=22\n",
    "batch_size=128          \n",
    "feature_vector_input_length=252                     # size of one example \n",
    "learning_rate=0.002515"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET one participant_examples  (3, 40, 572, 252)\n",
      "   GET one training_index_examples  (40, 572, 252)  at  0\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (40, 572, 252)  at  1\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (40, 572, 252)  at  2\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "dataloaders: \n",
      "   train  (1, 3)\n",
      "   valid  (1, 3)\n",
      "   test  (1, 0)\n",
      "GET one participant_examples  (3, 40, 572, 252)\n",
      "   GET one training_index_examples  (40, 572, 252)  at  0\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (40, 572, 252)  at  1\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (40, 572, 252)  at  2\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "dataloaders: \n",
      "   train  (2, 3)\n",
      "   valid  (2, 3)\n",
      "   test  (2, 0)\n",
      "GET one participant_examples  (3, 40, 572, 252)\n",
      "   GET one training_index_examples  (40, 572, 252)  at  0\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (40, 572, 252)  at  1\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (40, 572, 252)  at  2\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "dataloaders: \n",
      "   train  (3, 3)\n",
      "   valid  (3, 3)\n",
      "   test  (3, 0)\n",
      "START TRAINING\n",
      "Participant:  0\n",
      "Session:  0\n",
      "TSD_Network(\n",
      "  (_network): ModuleList(\n",
      "    (0): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=252, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=252, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (1): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (2): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (_output): Linear(in_features=200, out_features=22, bias=True)\n",
      "  (_output_discriminator): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "Number Parameters:  137624\n",
      "<generator object Module.parameters at 0x7f08eba0d3c0>\n",
      "Epoch 0/499\n",
      "----------\n",
      "train Loss: 0.01611891 Acc: 0.44873047\n",
      "val Loss: 0.00459397 Acc: 0.80349345\n",
      "New best validation loss: 0.004593971514805956\n",
      "Epoch 1 of 500 took 0.144s\n",
      "Epoch 1/499\n",
      "----------\n",
      "train Loss: 0.00730263 Acc: 0.78271484\n",
      "val Loss: 0.00224911 Acc: 0.89082969\n",
      "New best validation loss: 0.0022491073504285523\n",
      "Epoch 2 of 500 took 0.140s\n",
      "Epoch 2/499\n",
      "----------\n",
      "train Loss: 0.00446575 Acc: 0.85498047\n",
      "val Loss: 0.00143870 Acc: 0.95196507\n",
      "New best validation loss: 0.0014386960512685985\n",
      "Epoch 3 of 500 took 0.116s\n",
      "Epoch 3/499\n",
      "----------\n",
      "train Loss: 0.00346811 Acc: 0.88330078\n",
      "val Loss: 0.00133191 Acc: 0.91266376\n",
      "New best validation loss: 0.001331913679447757\n",
      "Epoch 4 of 500 took 0.110s\n",
      "Epoch 4/499\n",
      "----------\n",
      "train Loss: 0.00278408 Acc: 0.90527344\n",
      "val Loss: 0.00089462 Acc: 0.95633188\n",
      "New best validation loss: 0.0008946185419132616\n",
      "Epoch 5 of 500 took 0.120s\n",
      "Epoch 5/499\n",
      "----------\n",
      "train Loss: 0.00224029 Acc: 0.91992188\n",
      "val Loss: 0.00068019 Acc: 0.95633188\n",
      "New best validation loss: 0.000680188815146034\n",
      "Epoch 6 of 500 took 0.124s\n",
      "Epoch 6/499\n",
      "----------\n",
      "train Loss: 0.00198558 Acc: 0.92626953\n",
      "val Loss: 0.00070507 Acc: 0.95633188\n",
      "Epoch 7 of 500 took 0.117s\n",
      "Epoch 7/499\n",
      "----------\n",
      "train Loss: 0.00176745 Acc: 0.93505859\n",
      "val Loss: 0.00069905 Acc: 0.94323144\n",
      "Epoch 8 of 500 took 0.111s\n",
      "Epoch 8/499\n",
      "----------\n",
      "train Loss: 0.00163287 Acc: 0.94238281\n",
      "val Loss: 0.00052465 Acc: 0.95633188\n",
      "New best validation loss: 0.0005246486400933244\n",
      "Epoch 9 of 500 took 0.107s\n",
      "Epoch 9/499\n",
      "----------\n",
      "train Loss: 0.00152039 Acc: 0.93798828\n",
      "val Loss: 0.00053862 Acc: 0.96943231\n",
      "Epoch 10 of 500 took 0.133s\n",
      "Epoch 10/499\n",
      "----------\n",
      "train Loss: 0.00144496 Acc: 0.9453125\n",
      "val Loss: 0.00144229 Acc: 0.89082969\n",
      "Epoch 11 of 500 took 0.107s\n",
      "Epoch 11/499\n",
      "----------\n",
      "train Loss: 0.00147634 Acc: 0.94433594\n",
      "val Loss: 0.00046808 Acc: 0.96943231\n",
      "Epoch 12 of 500 took 0.111s\n",
      "Epoch 12/499\n",
      "----------\n",
      "train Loss: 0.00123377 Acc: 0.95361328\n",
      "val Loss: 0.00041256 Acc: 0.96943231\n",
      "New best validation loss: 0.0004125562492416415\n",
      "Epoch 13 of 500 took 0.124s\n",
      "Epoch 13/499\n",
      "----------\n",
      "train Loss: 0.00123315 Acc: 0.953125\n",
      "val Loss: 0.00061558 Acc: 0.95633188\n",
      "Epoch 14 of 500 took 0.119s\n",
      "Epoch 14/499\n",
      "----------\n",
      "train Loss: 0.00123005 Acc: 0.95117188\n",
      "val Loss: 0.00030455 Acc: 0.97379913\n",
      "New best validation loss: 0.0003045495745917075\n",
      "Epoch 15 of 500 took 0.106s\n",
      "Epoch 15/499\n",
      "----------\n",
      "train Loss: 0.00115623 Acc: 0.95361328\n",
      "val Loss: 0.00083518 Acc: 0.92576419\n",
      "Epoch 16 of 500 took 0.114s\n",
      "Epoch 16/499\n",
      "----------\n",
      "train Loss: 0.00091687 Acc: 0.96191406\n",
      "val Loss: 0.00060139 Acc: 0.96069869\n",
      "Epoch 17 of 500 took 0.117s\n",
      "Epoch 17/499\n",
      "----------\n",
      "train Loss: 0.00095384 Acc: 0.95605469\n",
      "val Loss: 0.00049510 Acc: 0.97379913\n",
      "Epoch 18 of 500 took 0.131s\n",
      "Epoch 18/499\n",
      "----------\n",
      "train Loss: 0.00102264 Acc: 0.95996094\n",
      "val Loss: 0.00048279 Acc: 0.95633188\n",
      "Epoch 19 of 500 took 0.135s\n",
      "Epoch 19/499\n",
      "----------\n",
      "train Loss: 0.00099108 Acc: 0.95996094\n",
      "val Loss: 0.00043186 Acc: 0.95633188\n",
      "Epoch 20 of 500 took 0.121s\n",
      "Epoch 20/499\n",
      "----------\n",
      "train Loss: 0.00079606 Acc: 0.96386719\n",
      "val Loss: 0.00040934 Acc: 0.97379913\n",
      "Epoch    21: reducing learning rate of group 0 to 5.0300e-04.\n",
      "Epoch 21 of 500 took 0.106s\n",
      "Epoch 21/499\n",
      "----------\n",
      "train Loss: 0.00072241 Acc: 0.96582031\n",
      "val Loss: 0.00048673 Acc: 0.9650655\n",
      "Epoch 22 of 500 took 0.108s\n",
      "Epoch 22/499\n",
      "----------\n",
      "train Loss: 0.00061219 Acc: 0.97412109\n",
      "val Loss: 0.00038066 Acc: 0.96943231\n",
      "Epoch 23 of 500 took 0.109s\n",
      "Epoch 23/499\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.00054783 Acc: 0.98095703\n",
      "val Loss: 0.00036010 Acc: 0.96943231\n",
      "Epoch 24 of 500 took 0.109s\n",
      "Epoch 24/499\n",
      "----------\n",
      "train Loss: 0.00046562 Acc: 0.984375\n",
      "val Loss: 0.00043158 Acc: 0.97379913\n",
      "Epoch 25 of 500 took 0.105s\n",
      "Epoch 25/499\n",
      "----------\n",
      "train Loss: 0.00048950 Acc: 0.98388672\n",
      "val Loss: 0.00033015 Acc: 0.97379913\n",
      "Epoch 26 of 500 took 0.113s\n",
      "\n",
      "Training complete in 0m 3s\n",
      "Best val loss: 0.000305\n",
      "Session:  1\n",
      "TSD_Network(\n",
      "  (_network): ModuleList(\n",
      "    (0): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=252, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=252, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (1): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (2): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (_output): Linear(in_features=200, out_features=22, bias=True)\n",
      "  (_output_discriminator): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "Number Parameters:  137624\n",
      "<generator object Module.parameters at 0x7f095b83d7b0>\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/TSD/participant_0/best_state_0.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/TSD/participant_0/best_state_0.pt' (epoch 15)\n",
      "Epoch 0/499\n",
      "----------\n",
      "train Loss: 0.00717094 Acc: 0.72607422\n",
      "val Loss: 0.00120767 Acc: 0.89956332\n",
      "New best validation loss: 0.0012076706605186629\n",
      "Epoch 1 of 500 took 0.133s\n",
      "Epoch 1/499\n",
      "----------\n",
      "train Loss: 0.00378298 Acc: 0.82861328\n",
      "val Loss: 0.00105503 Acc: 0.92139738\n",
      "New best validation loss: 0.0010550328607642494\n",
      "Epoch 2 of 500 took 0.140s\n",
      "Epoch 2/499\n",
      "----------\n",
      "train Loss: 0.00293802 Acc: 0.87109375\n",
      "val Loss: 0.00067113 Acc: 0.97379913\n",
      "New best validation loss: 0.0006711252001175193\n",
      "Epoch 3 of 500 took 0.143s\n",
      "Epoch 3/499\n",
      "----------\n",
      "train Loss: 0.00265866 Acc: 0.890625\n",
      "val Loss: 0.00054634 Acc: 0.96943231\n",
      "New best validation loss: 0.0005463369548581053\n",
      "Epoch 4 of 500 took 0.111s\n",
      "Epoch 4/499\n",
      "----------\n",
      "train Loss: 0.00238886 Acc: 0.89990234\n",
      "val Loss: 0.00055320 Acc: 0.98689956\n",
      "Epoch 5 of 500 took 0.119s\n",
      "Epoch 5/499\n",
      "----------\n",
      "train Loss: 0.00221832 Acc: 0.90771484\n",
      "val Loss: 0.00071225 Acc: 0.9650655\n",
      "Epoch 6 of 500 took 0.120s\n",
      "Epoch 6/499\n",
      "----------\n",
      "train Loss: 0.00196523 Acc: 0.91503906\n",
      "val Loss: 0.00054862 Acc: 0.96943231\n",
      "Epoch 7 of 500 took 0.109s\n",
      "Epoch 7/499\n",
      "----------\n",
      "train Loss: 0.00194888 Acc: 0.91357422\n",
      "val Loss: 0.00046053 Acc: 0.97816594\n",
      "Epoch 8 of 500 took 0.108s\n",
      "Epoch 8/499\n",
      "----------\n",
      "train Loss: 0.00185107 Acc: 0.91748047\n",
      "val Loss: 0.00041449 Acc: 0.97816594\n",
      "New best validation loss: 0.0004144912854032225\n",
      "Epoch 9 of 500 took 0.126s\n",
      "Epoch 9/499\n",
      "----------\n",
      "train Loss: 0.00182545 Acc: 0.92871094\n",
      "val Loss: 0.00058521 Acc: 0.95633188\n",
      "Epoch 10 of 500 took 0.113s\n",
      "Epoch 10/499\n",
      "----------\n",
      "train Loss: 0.00161647 Acc: 0.93017578\n",
      "val Loss: 0.00053310 Acc: 0.97379913\n",
      "Epoch 11 of 500 took 0.108s\n",
      "Epoch 11/499\n",
      "----------\n",
      "train Loss: 0.00170990 Acc: 0.92382812\n",
      "val Loss: 0.00056433 Acc: 0.96943231\n",
      "Epoch 12 of 500 took 0.112s\n",
      "Epoch 12/499\n",
      "----------\n",
      "train Loss: 0.00148596 Acc: 0.93945312\n",
      "val Loss: 0.00053161 Acc: 0.98253275\n",
      "Epoch 13 of 500 took 0.113s\n",
      "Epoch 13/499\n",
      "----------\n",
      "train Loss: 0.00140939 Acc: 0.9375\n",
      "val Loss: 0.00032940 Acc: 0.98689956\n",
      "Epoch 14 of 500 took 0.129s\n",
      "Epoch 14/499\n",
      "----------\n",
      "train Loss: 0.00150307 Acc: 0.93994141\n",
      "val Loss: 0.00036623 Acc: 0.98689956\n",
      "Epoch 15 of 500 took 0.132s\n",
      "Epoch 15/499\n",
      "----------\n",
      "train Loss: 0.00132223 Acc: 0.94775391\n",
      "val Loss: 0.00046070 Acc: 0.98253275\n",
      "Epoch 16 of 500 took 0.115s\n",
      "Epoch 16/499\n",
      "----------\n",
      "train Loss: 0.00140600 Acc: 0.93505859\n",
      "val Loss: 0.00043088 Acc: 0.97816594\n",
      "Epoch 17 of 500 took 0.108s\n",
      "Epoch 17/499\n",
      "----------\n",
      "train Loss: 0.00113860 Acc: 0.95898438\n",
      "val Loss: 0.00042295 Acc: 0.97379913\n",
      "Epoch 18 of 500 took 0.129s\n",
      "Epoch 18/499\n",
      "----------\n",
      "train Loss: 0.00103507 Acc: 0.95703125\n",
      "val Loss: 0.00032693 Acc: 0.98689956\n",
      "Epoch 19 of 500 took 0.109s\n",
      "Epoch 19/499\n",
      "----------\n",
      "train Loss: 0.00126008 Acc: 0.94189453\n",
      "val Loss: 0.00034772 Acc: 0.98253275\n",
      "Epoch 20 of 500 took 0.111s\n",
      "\n",
      "Training complete in 0m 2s\n",
      "Best val loss: 0.000414\n",
      "Session:  2\n",
      "TSD_Network(\n",
      "  (_network): ModuleList(\n",
      "    (0): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=252, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=252, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (1): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (2): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (_output): Linear(in_features=200, out_features=22, bias=True)\n",
      "  (_output_discriminator): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "Number Parameters:  137624\n",
      "<generator object Module.parameters at 0x7f08ea5d2ba0>\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/TSD/participant_0/best_state_1.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/TSD/participant_0/best_state_1.pt' (epoch 9)\n",
      "Epoch 0/499\n",
      "----------\n",
      "train Loss: 0.00834070 Acc: 0.69628906\n",
      "val Loss: 0.00107633 Acc: 0.89519651\n",
      "New best validation loss: 0.0010763315004032252\n",
      "Epoch 1 of 500 took 0.148s\n",
      "Epoch 1/499\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.00333842 Acc: 0.84277344\n",
      "val Loss: 0.00096802 Acc: 0.89956332\n",
      "New best validation loss: 0.0009680164563083232\n",
      "Epoch 2 of 500 took 0.131s\n",
      "Epoch 2/499\n",
      "----------\n",
      "train Loss: 0.00283507 Acc: 0.85888672\n",
      "val Loss: 0.00100796 Acc: 0.91703057\n",
      "Epoch 3 of 500 took 0.105s\n",
      "Epoch 3/499\n",
      "----------\n",
      "train Loss: 0.00240521 Acc: 0.89355469\n",
      "val Loss: 0.00070395 Acc: 0.93886463\n",
      "New best validation loss: 0.0007039501156869413\n",
      "Epoch 4 of 500 took 0.130s\n",
      "Epoch 4/499\n",
      "----------\n",
      "train Loss: 0.00202812 Acc: 0.90820312\n",
      "val Loss: 0.00050504 Acc: 0.9650655\n",
      "New best validation loss: 0.0005050448285960734\n",
      "Epoch 5 of 500 took 0.109s\n",
      "Epoch 5/499\n",
      "----------\n",
      "train Loss: 0.00210064 Acc: 0.90478516\n",
      "val Loss: 0.00044058 Acc: 0.97379913\n",
      "Epoch 6 of 500 took 0.116s\n",
      "Epoch 6/499\n",
      "----------\n",
      "train Loss: 0.00184191 Acc: 0.91748047\n",
      "val Loss: 0.00035263 Acc: 0.96943231\n",
      "New best validation loss: 0.0003526324333061818\n",
      "Epoch 7 of 500 took 0.110s\n",
      "Epoch 7/499\n",
      "----------\n",
      "train Loss: 0.00153126 Acc: 0.93164062\n",
      "val Loss: 0.00036767 Acc: 0.97379913\n",
      "Epoch 8 of 500 took 0.137s\n",
      "Epoch 8/499\n",
      "----------\n",
      "train Loss: 0.00156512 Acc: 0.92724609\n",
      "val Loss: 0.00032276 Acc: 0.9650655\n",
      "Epoch 9 of 500 took 0.106s\n",
      "Epoch 9/499\n",
      "----------\n",
      "train Loss: 0.00153528 Acc: 0.92529297\n",
      "val Loss: 0.00059987 Acc: 0.94759825\n",
      "Epoch 10 of 500 took 0.108s\n",
      "Epoch 10/499\n",
      "----------\n",
      "train Loss: 0.00148889 Acc: 0.92871094\n",
      "val Loss: 0.00041913 Acc: 0.97379913\n",
      "Epoch 11 of 500 took 0.113s\n",
      "Epoch 11/499\n",
      "----------\n",
      "train Loss: 0.00144071 Acc: 0.94042969\n",
      "val Loss: 0.00034231 Acc: 0.98253275\n",
      "Epoch 12 of 500 took 0.126s\n",
      "Epoch 12/499\n",
      "----------\n",
      "train Loss: 0.00127521 Acc: 0.93896484\n",
      "val Loss: 0.00032834 Acc: 0.9650655\n",
      "Epoch 13 of 500 took 0.106s\n",
      "Epoch 13/499\n",
      "----------\n",
      "train Loss: 0.00141100 Acc: 0.93554688\n",
      "val Loss: 0.00056418 Acc: 0.94323144\n",
      "Epoch 14 of 500 took 0.114s\n",
      "Epoch 14/499\n",
      "----------\n",
      "train Loss: 0.00136521 Acc: 0.93847656\n",
      "val Loss: 0.00034006 Acc: 0.9650655\n",
      "Epoch    15: reducing learning rate of group 0 to 5.0300e-04.\n",
      "Epoch 15 of 500 took 0.121s\n",
      "Epoch 15/499\n",
      "----------\n",
      "train Loss: 0.00105808 Acc: 0.95068359\n",
      "val Loss: 0.00021757 Acc: 0.98253275\n",
      "New best validation loss: 0.00021756549394286875\n",
      "Epoch 16 of 500 took 0.118s\n",
      "Epoch 16/499\n",
      "----------\n",
      "train Loss: 0.00095260 Acc: 0.96191406\n",
      "val Loss: 0.00030810 Acc: 0.97379913\n",
      "Epoch 17 of 500 took 0.110s\n",
      "Epoch 17/499\n",
      "----------\n",
      "train Loss: 0.00088652 Acc: 0.96386719\n",
      "val Loss: 0.00023183 Acc: 0.98689956\n",
      "Epoch 18 of 500 took 0.110s\n",
      "Epoch 18/499\n",
      "----------\n",
      "train Loss: 0.00079685 Acc: 0.96972656\n",
      "val Loss: 0.00024750 Acc: 0.98253275\n",
      "Epoch 19 of 500 took 0.132s\n",
      "Epoch 19/499\n",
      "----------\n",
      "train Loss: 0.00089476 Acc: 0.95947266\n",
      "val Loss: 0.00019746 Acc: 0.98689956\n",
      "Epoch 20 of 500 took 0.112s\n",
      "Epoch 20/499\n",
      "----------\n",
      "train Loss: 0.00078002 Acc: 0.96630859\n",
      "val Loss: 0.00016888 Acc: 0.98689956\n",
      "Epoch 21 of 500 took 0.105s\n",
      "Epoch 21/499\n",
      "----------\n",
      "train Loss: 0.00071972 Acc: 0.96826172\n",
      "val Loss: 0.00020896 Acc: 0.98253275\n",
      "Epoch 22 of 500 took 0.116s\n",
      "Epoch 22/499\n",
      "----------\n",
      "train Loss: 0.00070254 Acc: 0.96923828\n",
      "val Loss: 0.00023959 Acc: 0.98253275\n",
      "Epoch 23 of 500 took 0.133s\n",
      "Epoch 23/499\n",
      "----------\n",
      "train Loss: 0.00076550 Acc: 0.96826172\n",
      "val Loss: 0.00025717 Acc: 0.98253275\n",
      "Epoch 24 of 500 took 0.112s\n",
      "Epoch 24/499\n",
      "----------\n",
      "train Loss: 0.00077881 Acc: 0.96386719\n",
      "val Loss: 0.00022625 Acc: 0.98689956\n",
      "Epoch 25 of 500 took 0.105s\n",
      "Epoch 25/499\n",
      "----------\n",
      "train Loss: 0.00072994 Acc: 0.96875\n",
      "val Loss: 0.00016410 Acc: 0.99126638\n",
      "Epoch 26 of 500 took 0.112s\n",
      "Epoch 26/499\n",
      "----------\n",
      "train Loss: 0.00081770 Acc: 0.96679688\n",
      "val Loss: 0.00017760 Acc: 0.99126638\n",
      "Epoch 27 of 500 took 0.117s\n",
      "\n",
      "Training complete in 0m 3s\n",
      "Best val loss: 0.000218\n",
      "Participant:  1\n",
      "Session:  0\n",
      "TSD_Network(\n",
      "  (_network): ModuleList(\n",
      "    (0): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=252, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=252, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (1): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (2): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (_output): Linear(in_features=200, out_features=22, bias=True)\n",
      "  (_output_discriminator): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "Number Parameters:  137624\n",
      "<generator object Module.parameters at 0x7f08ea5d2ba0>\n",
      "Epoch 0/499\n",
      "----------\n",
      "train Loss: 0.01577542 Acc: 0.45117188\n",
      "val Loss: 0.00385594 Acc: 0.86026201\n",
      "New best validation loss: 0.0038559366521876974\n",
      "Epoch 1 of 500 took 0.125s\n",
      "Epoch 1/499\n",
      "----------\n",
      "train Loss: 0.00702670 Acc: 0.77636719\n",
      "val Loss: 0.00185520 Acc: 0.92139738\n",
      "New best validation loss: 0.0018552031579496559\n",
      "Epoch 2 of 500 took 0.106s\n",
      "Epoch 2/499\n",
      "----------\n",
      "train Loss: 0.00451169 Acc: 0.84472656\n",
      "val Loss: 0.00136863 Acc: 0.91266376\n",
      "New best validation loss: 0.0013686307913351268\n",
      "Epoch 3 of 500 took 0.110s\n",
      "Epoch 3/499\n",
      "----------\n",
      "train Loss: 0.00337009 Acc: 0.88037109\n",
      "val Loss: 0.00097330 Acc: 0.93886463\n",
      "New best validation loss: 0.0009732960223110482\n",
      "Epoch 4 of 500 took 0.110s\n",
      "Epoch 4/499\n",
      "----------\n",
      "train Loss: 0.00290425 Acc: 0.88378906\n",
      "val Loss: 0.00066418 Acc: 0.96069869\n",
      "New best validation loss: 0.0006641758581436357\n",
      "Epoch 5 of 500 took 0.114s\n",
      "Epoch 5/499\n",
      "----------\n",
      "train Loss: 0.00242104 Acc: 0.90869141\n",
      "val Loss: 0.00071639 Acc: 0.94759825\n",
      "Epoch 6 of 500 took 0.110s\n",
      "Epoch 6/499\n",
      "----------\n",
      "train Loss: 0.00204608 Acc: 0.91601562\n",
      "val Loss: 0.00070326 Acc: 0.92139738\n",
      "Epoch 7 of 500 took 0.114s\n",
      "Epoch 7/499\n",
      "----------\n",
      "train Loss: 0.00221288 Acc: 0.90380859\n",
      "val Loss: 0.00079379 Acc: 0.93449782\n",
      "Epoch 8 of 500 took 0.113s\n",
      "Epoch 8/499\n",
      "----------\n",
      "train Loss: 0.00195064 Acc: 0.91894531\n",
      "val Loss: 0.00051197 Acc: 0.96943231\n",
      "New best validation loss: 0.0005119676868467872\n",
      "Epoch 9 of 500 took 0.116s\n",
      "Epoch 9/499\n",
      "----------\n",
      "train Loss: 0.00175308 Acc: 0.91992188\n",
      "val Loss: 0.00078781 Acc: 0.93449782\n",
      "Epoch 10 of 500 took 0.112s\n",
      "Epoch 10/499\n",
      "----------\n",
      "train Loss: 0.00158904 Acc: 0.93115234\n",
      "val Loss: 0.00036640 Acc: 0.98253275\n",
      "New best validation loss: 0.0003663977876500792\n",
      "Epoch 11 of 500 took 0.128s\n",
      "Epoch 11/499\n",
      "----------\n",
      "train Loss: 0.00153577 Acc: 0.93603516\n",
      "val Loss: 0.00050830 Acc: 0.96069869\n",
      "Epoch 12 of 500 took 0.120s\n",
      "Epoch 12/499\n",
      "----------\n",
      "train Loss: 0.00163677 Acc: 0.92919922\n",
      "val Loss: 0.00039393 Acc: 0.97816594\n",
      "Epoch 13 of 500 took 0.146s\n",
      "Epoch 13/499\n",
      "----------\n",
      "train Loss: 0.00154226 Acc: 0.93164062\n",
      "val Loss: 0.00037082 Acc: 0.97816594\n",
      "Epoch 14 of 500 took 0.132s\n",
      "Epoch 14/499\n",
      "----------\n",
      "train Loss: 0.00122029 Acc: 0.95166016\n",
      "val Loss: 0.00042104 Acc: 0.97816594\n",
      "Epoch 15 of 500 took 0.141s\n",
      "Epoch 15/499\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.00110409 Acc: 0.95361328\n",
      "val Loss: 0.00064366 Acc: 0.93449782\n",
      "Epoch 16 of 500 took 0.204s\n",
      "Epoch 16/499\n",
      "----------\n",
      "train Loss: 0.00111769 Acc: 0.95068359\n",
      "val Loss: 0.00041353 Acc: 0.97379913\n",
      "Epoch    17: reducing learning rate of group 0 to 5.0300e-04.\n",
      "Epoch 17 of 500 took 0.134s\n",
      "Epoch 17/499\n",
      "----------\n",
      "train Loss: 0.00109525 Acc: 0.95019531\n",
      "val Loss: 0.00028006 Acc: 0.98689956\n",
      "Epoch 18 of 500 took 0.122s\n",
      "Epoch 18/499\n",
      "----------\n",
      "train Loss: 0.00093818 Acc: 0.96289062\n",
      "val Loss: 0.00033950 Acc: 0.98253275\n",
      "Epoch 19 of 500 took 0.112s\n",
      "Epoch 19/499\n",
      "----------\n",
      "train Loss: 0.00081474 Acc: 0.96386719\n",
      "val Loss: 0.00028660 Acc: 0.98253275\n",
      "Epoch 20 of 500 took 0.109s\n",
      "Epoch 20/499\n",
      "----------\n",
      "train Loss: 0.00084388 Acc: 0.96337891\n",
      "val Loss: 0.00025552 Acc: 0.98689956\n",
      "New best validation loss: 0.00025552317630255585\n",
      "Epoch 21 of 500 took 0.107s\n",
      "Epoch 21/499\n",
      "----------\n",
      "train Loss: 0.00076603 Acc: 0.96826172\n",
      "val Loss: 0.00030940 Acc: 0.98253275\n",
      "Epoch 22 of 500 took 0.127s\n",
      "Epoch 22/499\n",
      "----------\n",
      "train Loss: 0.00072918 Acc: 0.97119141\n",
      "val Loss: 0.00026568 Acc: 0.98253275\n",
      "Epoch 23 of 500 took 0.127s\n",
      "Epoch 23/499\n",
      "----------\n",
      "train Loss: 0.00074311 Acc: 0.96923828\n",
      "val Loss: 0.00030979 Acc: 0.98253275\n",
      "Epoch 24 of 500 took 0.112s\n",
      "Epoch 24/499\n",
      "----------\n",
      "train Loss: 0.00069202 Acc: 0.96826172\n",
      "val Loss: 0.00028883 Acc: 0.97816594\n",
      "Epoch 25 of 500 took 0.106s\n",
      "Epoch 25/499\n",
      "----------\n",
      "train Loss: 0.00069359 Acc: 0.96826172\n",
      "val Loss: 0.00026336 Acc: 0.98253275\n",
      "Epoch 26 of 500 took 0.124s\n",
      "Epoch 26/499\n",
      "----------\n",
      "train Loss: 0.00075195 Acc: 0.96875\n",
      "val Loss: 0.00026382 Acc: 0.98253275\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0060e-04.\n",
      "Epoch 27 of 500 took 0.117s\n",
      "Epoch 27/499\n",
      "----------\n",
      "train Loss: 0.00059768 Acc: 0.97900391\n",
      "val Loss: 0.00025489 Acc: 0.98253275\n",
      "Epoch 28 of 500 took 0.120s\n",
      "Epoch 28/499\n",
      "----------\n",
      "train Loss: 0.00062514 Acc: 0.97802734\n",
      "val Loss: 0.00019730 Acc: 0.98689956\n",
      "Epoch 29 of 500 took 0.111s\n",
      "Epoch 29/499\n",
      "----------\n",
      "train Loss: 0.00058090 Acc: 0.9765625\n",
      "val Loss: 0.00027565 Acc: 0.98689956\n",
      "Epoch 30 of 500 took 0.137s\n",
      "Epoch 30/499\n",
      "----------\n",
      "train Loss: 0.00054150 Acc: 0.98046875\n",
      "val Loss: 0.00027772 Acc: 0.98253275\n",
      "Epoch 31 of 500 took 0.118s\n",
      "Epoch 31/499\n",
      "----------\n",
      "train Loss: 0.00060799 Acc: 0.97607422\n",
      "val Loss: 0.00021176 Acc: 0.98253275\n",
      "Epoch 32 of 500 took 0.119s\n",
      "\n",
      "Training complete in 0m 4s\n",
      "Best val loss: 0.000256\n",
      "Session:  1\n",
      "TSD_Network(\n",
      "  (_network): ModuleList(\n",
      "    (0): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=252, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=252, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (1): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (2): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (_output): Linear(in_features=200, out_features=22, bias=True)\n",
      "  (_output_discriminator): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "Number Parameters:  137624\n",
      "<generator object Module.parameters at 0x7f08ea5d2ba0>\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/TSD/participant_1/best_state_0.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/TSD/participant_1/best_state_0.pt' (epoch 21)\n",
      "Epoch 0/499\n",
      "----------\n",
      "train Loss: 0.00226921 Acc: 0.90771484\n",
      "val Loss: 0.00036964 Acc: 0.97816594\n",
      "New best validation loss: 0.00036963800936286626\n",
      "Epoch 1 of 500 took 0.119s\n",
      "Epoch 1/499\n",
      "----------\n",
      "train Loss: 0.00129827 Acc: 0.94921875\n",
      "val Loss: 0.00020568 Acc: 0.99126638\n",
      "New best validation loss: 0.00020567950788543735\n",
      "Epoch 2 of 500 took 0.111s\n",
      "Epoch 2/499\n",
      "----------\n",
      "train Loss: 0.00111537 Acc: 0.94824219\n",
      "val Loss: 0.00019947 Acc: 0.99563319\n",
      "Epoch 3 of 500 took 0.105s\n",
      "Epoch 3/499\n",
      "----------\n",
      "train Loss: 0.00097925 Acc: 0.95751953\n",
      "val Loss: 0.00024139 Acc: 0.98689956\n",
      "Epoch 4 of 500 took 0.111s\n",
      "Epoch 4/499\n",
      "----------\n",
      "train Loss: 0.00081005 Acc: 0.97021484\n",
      "val Loss: 0.00017180 Acc: 0.98689956\n",
      "Epoch 5 of 500 took 0.111s\n",
      "Epoch 5/499\n",
      "----------\n",
      "train Loss: 0.00079550 Acc: 0.96679688\n",
      "val Loss: 0.00025453 Acc: 0.97379913\n",
      "Epoch 6 of 500 took 0.107s\n",
      "Epoch 6/499\n",
      "----------\n",
      "train Loss: 0.00078427 Acc: 0.97021484\n",
      "val Loss: 0.00011554 Acc: 0.99126638\n",
      "Epoch 7 of 500 took 0.106s\n",
      "Epoch 7/499\n",
      "----------\n",
      "train Loss: 0.00069534 Acc: 0.97167969\n",
      "val Loss: 0.00016142 Acc: 0.98689956\n",
      "Epoch 8 of 500 took 0.195s\n",
      "Epoch 8/499\n",
      "----------\n",
      "train Loss: 0.00060921 Acc: 0.97314453\n",
      "val Loss: 0.00009641 Acc: 0.99563319\n",
      "New best validation loss: 9.641196887826295e-05\n",
      "Epoch 9 of 500 took 0.107s\n",
      "Epoch 9/499\n",
      "----------\n",
      "train Loss: 0.00069509 Acc: 0.96679688\n",
      "val Loss: 0.00014597 Acc: 0.98689956\n",
      "Epoch 10 of 500 took 0.109s\n",
      "Epoch 10/499\n",
      "----------\n",
      "train Loss: 0.00062665 Acc: 0.97363281\n",
      "val Loss: 0.00011788 Acc: 0.99563319\n",
      "Epoch 11 of 500 took 0.106s\n",
      "Epoch 11/499\n",
      "----------\n",
      "train Loss: 0.00061153 Acc: 0.97412109\n",
      "val Loss: 0.00012838 Acc: 0.98689956\n",
      "Epoch 12 of 500 took 0.110s\n",
      "Epoch 12/499\n",
      "----------\n",
      "train Loss: 0.00039718 Acc: 0.98925781\n",
      "val Loss: 0.00010903 Acc: 0.98689956\n",
      "Epoch 13 of 500 took 0.106s\n",
      "Epoch 13/499\n",
      "----------\n",
      "train Loss: 0.00051971 Acc: 0.97607422\n",
      "val Loss: 0.00015437 Acc: 0.98689956\n",
      "Epoch 14 of 500 took 0.112s\n",
      "Epoch 14/499\n",
      "----------\n",
      "train Loss: 0.00040943 Acc: 0.98291016\n",
      "val Loss: 0.00017475 Acc: 0.98253275\n",
      "Epoch    15: reducing learning rate of group 0 to 5.0300e-04.\n",
      "Epoch 15 of 500 took 0.113s\n",
      "Epoch 15/499\n",
      "----------\n",
      "train Loss: 0.00040354 Acc: 0.984375\n",
      "val Loss: 0.00009470 Acc: 0.98689956\n",
      "Epoch 16 of 500 took 0.135s\n",
      "Epoch 16/499\n",
      "----------\n",
      "train Loss: 0.00035599 Acc: 0.98632812\n",
      "val Loss: 0.00011995 Acc: 0.98689956\n",
      "Epoch 17 of 500 took 0.115s\n",
      "Epoch 17/499\n",
      "----------\n",
      "train Loss: 0.00037986 Acc: 0.98486328\n",
      "val Loss: 0.00014542 Acc: 0.98253275\n",
      "Epoch 18 of 500 took 0.133s\n",
      "Epoch 18/499\n",
      "----------\n",
      "train Loss: 0.00032662 Acc: 0.98828125\n",
      "val Loss: 0.00010377 Acc: 0.99126638\n",
      "Epoch 19 of 500 took 0.108s\n",
      "Epoch 19/499\n",
      "----------\n",
      "train Loss: 0.00028049 Acc: 0.99169922\n",
      "val Loss: 0.00013215 Acc: 0.98253275\n",
      "Epoch 20 of 500 took 0.111s\n",
      "\n",
      "Training complete in 0m 2s\n",
      "Best val loss: 0.000096\n",
      "Session:  2\n",
      "TSD_Network(\n",
      "  (_network): ModuleList(\n",
      "    (0): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=252, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=252, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (1): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (2): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (_output): Linear(in_features=200, out_features=22, bias=True)\n",
      "  (_output_discriminator): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "Number Parameters:  137624\n",
      "<generator object Module.parameters at 0x7f08eba0d350>\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/TSD/participant_1/best_state_1.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/TSD/participant_1/best_state_1.pt' (epoch 9)\n",
      "Epoch 0/499\n",
      "----------\n",
      "train Loss: 0.00521420 Acc: 0.80224609\n",
      "val Loss: 0.00095595 Acc: 0.88209607\n",
      "New best validation loss: 0.0009559461253178692\n",
      "Epoch 1 of 500 took 0.113s\n",
      "Epoch 1/499\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.00267559 Acc: 0.87792969\n",
      "val Loss: 0.00068687 Acc: 0.93886463\n",
      "New best validation loss: 0.0006868690252304077\n",
      "Epoch 2 of 500 took 0.130s\n",
      "Epoch 2/499\n",
      "----------\n",
      "train Loss: 0.00204821 Acc: 0.90332031\n",
      "val Loss: 0.00051728 Acc: 0.94759825\n",
      "New best validation loss: 0.000517277347989478\n",
      "Epoch 3 of 500 took 0.130s\n",
      "Epoch 3/499\n",
      "----------\n",
      "train Loss: 0.00178105 Acc: 0.91796875\n",
      "val Loss: 0.00052111 Acc: 0.95633188\n",
      "Epoch 4 of 500 took 0.134s\n",
      "Epoch 4/499\n",
      "----------\n",
      "train Loss: 0.00164679 Acc: 0.92919922\n",
      "val Loss: 0.00044844 Acc: 0.9650655\n",
      "Epoch 5 of 500 took 0.132s\n",
      "Epoch 5/499\n",
      "----------\n",
      "train Loss: 0.00139599 Acc: 0.93652344\n",
      "val Loss: 0.00035753 Acc: 0.97379913\n",
      "New best validation loss: 0.0003575307833575786\n",
      "Epoch 6 of 500 took 0.136s\n",
      "Epoch 6/499\n",
      "----------\n",
      "train Loss: 0.00137159 Acc: 0.93603516\n",
      "val Loss: 0.00039772 Acc: 0.96943231\n",
      "Epoch 7 of 500 took 0.131s\n",
      "Epoch 7/499\n",
      "----------\n",
      "train Loss: 0.00125862 Acc: 0.94433594\n",
      "val Loss: 0.00086649 Acc: 0.92576419\n",
      "Epoch 8 of 500 took 0.132s\n",
      "Epoch 8/499\n",
      "----------\n",
      "train Loss: 0.00123790 Acc: 0.94482422\n",
      "val Loss: 0.00095707 Acc: 0.89519651\n",
      "Epoch 9 of 500 took 0.130s\n",
      "Epoch 9/499\n",
      "----------\n",
      "train Loss: 0.00111464 Acc: 0.94433594\n",
      "val Loss: 0.00026039 Acc: 0.97816594\n",
      "Epoch 10 of 500 took 0.115s\n",
      "Epoch 10/499\n",
      "----------\n",
      "train Loss: 0.00091800 Acc: 0.95751953\n",
      "val Loss: 0.00031746 Acc: 0.97816594\n",
      "Epoch 11 of 500 took 0.108s\n",
      "Epoch 11/499\n",
      "----------\n",
      "train Loss: 0.00098548 Acc: 0.95996094\n",
      "val Loss: 0.00072119 Acc: 0.94323144\n",
      "Epoch 12 of 500 took 0.115s\n",
      "Epoch 12/499\n",
      "----------\n",
      "train Loss: 0.00096255 Acc: 0.95800781\n",
      "val Loss: 0.00035171 Acc: 0.97379913\n",
      "Epoch 13 of 500 took 0.158s\n",
      "Epoch 13/499\n",
      "----------\n",
      "train Loss: 0.00088860 Acc: 0.96386719\n",
      "val Loss: 0.00024953 Acc: 0.97816594\n",
      "New best validation loss: 0.00024953466640809737\n",
      "Epoch 14 of 500 took 0.127s\n",
      "Epoch 14/499\n",
      "----------\n",
      "train Loss: 0.00081177 Acc: 0.96728516\n",
      "val Loss: 0.00021363 Acc: 0.99126638\n",
      "Epoch 15 of 500 took 0.143s\n",
      "Epoch 15/499\n",
      "----------\n",
      "train Loss: 0.00083383 Acc: 0.95947266\n",
      "val Loss: 0.00028351 Acc: 0.98253275\n",
      "Epoch 16 of 500 took 0.142s\n",
      "Epoch 16/499\n",
      "----------\n",
      "train Loss: 0.00080457 Acc: 0.96484375\n",
      "val Loss: 0.00038061 Acc: 0.9650655\n",
      "Epoch 17 of 500 took 0.148s\n",
      "Epoch 17/499\n",
      "----------\n",
      "train Loss: 0.00088492 Acc: 0.96582031\n",
      "val Loss: 0.00033841 Acc: 0.97379913\n",
      "Epoch 18 of 500 took 0.140s\n",
      "Epoch 18/499\n",
      "----------\n",
      "train Loss: 0.00074998 Acc: 0.96337891\n",
      "val Loss: 0.00031831 Acc: 0.96943231\n",
      "Epoch 19 of 500 took 0.105s\n",
      "Epoch 19/499\n",
      "----------\n",
      "train Loss: 0.00066377 Acc: 0.96972656\n",
      "val Loss: 0.00030528 Acc: 0.98253275\n",
      "Epoch 20 of 500 took 0.111s\n",
      "Epoch 20/499\n",
      "----------\n",
      "train Loss: 0.00062255 Acc: 0.97216797\n",
      "val Loss: 0.00017474 Acc: 0.98689956\n",
      "Epoch 21 of 500 took 0.110s\n",
      "Epoch 21/499\n",
      "----------\n",
      "train Loss: 0.00081510 Acc: 0.96142578\n",
      "val Loss: 0.00030050 Acc: 0.97379913\n",
      "Epoch 22 of 500 took 0.109s\n",
      "Epoch 22/499\n",
      "----------\n",
      "train Loss: 0.00068167 Acc: 0.97119141\n",
      "val Loss: 0.00021406 Acc: 0.97379913\n",
      "Epoch 23 of 500 took 0.108s\n",
      "Epoch 23/499\n",
      "----------\n",
      "train Loss: 0.00055547 Acc: 0.97558594\n",
      "val Loss: 0.00016256 Acc: 0.97816594\n",
      "Epoch 24 of 500 took 0.114s\n",
      "Epoch 24/499\n",
      "----------\n",
      "train Loss: 0.00064502 Acc: 0.97070312\n",
      "val Loss: 0.00025111 Acc: 0.98253275\n",
      "Epoch 25 of 500 took 0.122s\n",
      "\n",
      "Training complete in 0m 3s\n",
      "Best val loss: 0.000250\n",
      "Participant:  2\n",
      "Session:  0\n",
      "TSD_Network(\n",
      "  (_network): ModuleList(\n",
      "    (0): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=252, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=252, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (1): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (2): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (_output): Linear(in_features=200, out_features=22, bias=True)\n",
      "  (_output_discriminator): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "Number Parameters:  137624\n",
      "<generator object Module.parameters at 0x7f08eba0d350>\n",
      "Epoch 0/499\n",
      "----------\n",
      "train Loss: 0.01690971 Acc: 0.40185547\n",
      "val Loss: 0.00478098 Acc: 0.7860262\n",
      "New best validation loss: 0.004780977573977808\n",
      "Epoch 1 of 500 took 0.127s\n",
      "Epoch 1/499\n",
      "----------\n",
      "train Loss: 0.00853180 Acc: 0.70068359\n",
      "val Loss: 0.00277995 Acc: 0.80349345\n",
      "New best validation loss: 0.002779952823855471\n",
      "Epoch 2 of 500 took 0.120s\n",
      "Epoch 2/499\n",
      "----------\n",
      "train Loss: 0.00592508 Acc: 0.7734375\n",
      "val Loss: 0.00214960 Acc: 0.86899563\n",
      "New best validation loss: 0.0021496001289400993\n",
      "Epoch 3 of 500 took 0.121s\n",
      "Epoch 3/499\n",
      "----------\n",
      "train Loss: 0.00486849 Acc: 0.78662109\n",
      "val Loss: 0.00204328 Acc: 0.84279476\n",
      "New best validation loss: 0.0020432839227035055\n",
      "Epoch 4 of 500 took 0.127s\n",
      "Epoch 4/499\n",
      "----------\n",
      "train Loss: 0.00414385 Acc: 0.82324219\n",
      "val Loss: 0.00158636 Acc: 0.89082969\n",
      "New best validation loss: 0.0015863647367235876\n",
      "Epoch 5 of 500 took 0.132s\n",
      "Epoch 5/499\n",
      "----------\n",
      "train Loss: 0.00342582 Acc: 0.85351562\n",
      "val Loss: 0.00110709 Acc: 0.930131\n",
      "New best validation loss: 0.001107087301895608\n",
      "Epoch 6 of 500 took 0.138s\n",
      "Epoch 6/499\n",
      "----------\n",
      "train Loss: 0.00338541 Acc: 0.84277344\n",
      "val Loss: 0.00164401 Acc: 0.8558952\n",
      "Epoch 7 of 500 took 0.114s\n",
      "Epoch 7/499\n",
      "----------\n",
      "train Loss: 0.00309262 Acc: 0.86767578\n",
      "val Loss: 0.00119767 Acc: 0.92139738\n",
      "Epoch 8 of 500 took 0.106s\n",
      "Epoch 8/499\n",
      "----------\n",
      "train Loss: 0.00248976 Acc: 0.89453125\n",
      "val Loss: 0.00106118 Acc: 0.90829694\n",
      "Epoch 9 of 500 took 0.112s\n",
      "Epoch 9/499\n",
      "----------\n",
      "train Loss: 0.00268765 Acc: 0.88818359\n",
      "val Loss: 0.00123246 Acc: 0.87336245\n",
      "Epoch 10 of 500 took 0.105s\n",
      "Epoch 10/499\n",
      "----------\n",
      "train Loss: 0.00251270 Acc: 0.890625\n",
      "val Loss: 0.00102843 Acc: 0.91703057\n",
      "Epoch 11 of 500 took 0.109s\n",
      "Epoch 11/499\n",
      "----------\n",
      "train Loss: 0.00248822 Acc: 0.88623047\n",
      "val Loss: 0.00081427 Acc: 0.92576419\n",
      "New best validation loss: 0.0008142699190622854\n",
      "Epoch 12 of 500 took 0.109s\n",
      "Epoch 12/499\n",
      "----------\n",
      "train Loss: 0.00206656 Acc: 0.90966797\n",
      "val Loss: 0.00076797 Acc: 0.94323144\n",
      "Epoch 13 of 500 took 0.137s\n",
      "Epoch 13/499\n",
      "----------\n",
      "train Loss: 0.00200281 Acc: 0.90722656\n",
      "val Loss: 0.00063984 Acc: 0.93886463\n",
      "New best validation loss: 0.0006398436283961133\n",
      "Epoch 14 of 500 took 0.116s\n",
      "Epoch 14/499\n",
      "----------\n",
      "train Loss: 0.00220900 Acc: 0.89501953\n",
      "val Loss: 0.00068400 Acc: 0.96069869\n",
      "Epoch 15 of 500 took 0.121s\n",
      "Epoch 15/499\n",
      "----------\n",
      "train Loss: 0.00202161 Acc: 0.90820312\n",
      "val Loss: 0.00103077 Acc: 0.90393013\n",
      "Epoch 16 of 500 took 0.135s\n",
      "Epoch 16/499\n",
      "----------\n",
      "train Loss: 0.00168610 Acc: 0.92578125\n",
      "val Loss: 0.00056632 Acc: 0.96069869\n",
      "Epoch 17 of 500 took 0.199s\n",
      "Epoch 17/499\n",
      "----------\n",
      "train Loss: 0.00166857 Acc: 0.92089844\n",
      "val Loss: 0.00046371 Acc: 0.96943231\n",
      "New best validation loss: 0.0004637087471620485\n",
      "Epoch 18 of 500 took 0.138s\n",
      "Epoch 18/499\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.00149328 Acc: 0.93701172\n",
      "val Loss: 0.00042755 Acc: 0.97379913\n",
      "Epoch 19 of 500 took 0.145s\n",
      "Epoch 19/499\n",
      "----------\n",
      "train Loss: 0.00153171 Acc: 0.93505859\n",
      "val Loss: 0.00045890 Acc: 0.96943231\n",
      "Epoch 20 of 500 took 0.160s\n",
      "Epoch 20/499\n",
      "----------\n",
      "train Loss: 0.00157764 Acc: 0.93164062\n",
      "val Loss: 0.00065497 Acc: 0.94323144\n",
      "Epoch 21 of 500 took 0.143s\n",
      "Epoch 21/499\n",
      "----------\n",
      "train Loss: 0.00142436 Acc: 0.94335938\n",
      "val Loss: 0.00100738 Acc: 0.91266376\n",
      "Epoch 22 of 500 took 0.182s\n",
      "Epoch 22/499\n",
      "----------\n",
      "train Loss: 0.00163007 Acc: 0.93212891\n",
      "val Loss: 0.00048055 Acc: 0.95196507\n",
      "Epoch 23 of 500 took 0.124s\n",
      "Epoch 23/499\n",
      "----------\n",
      "train Loss: 0.00162260 Acc: 0.92773438\n",
      "val Loss: 0.00081787 Acc: 0.930131\n",
      "Epoch 24 of 500 took 0.179s\n",
      "Epoch 24/499\n",
      "----------\n",
      "train Loss: 0.00143067 Acc: 0.93847656\n",
      "val Loss: 0.00045568 Acc: 0.96069869\n",
      "Epoch    25: reducing learning rate of group 0 to 5.0300e-04.\n",
      "Epoch 25 of 500 took 0.135s\n",
      "Epoch 25/499\n",
      "----------\n",
      "train Loss: 0.00112000 Acc: 0.95166016\n",
      "val Loss: 0.00029483 Acc: 0.98689956\n",
      "New best validation loss: 0.00029483258594071503\n",
      "Epoch 26 of 500 took 0.207s\n",
      "Epoch 26/499\n",
      "----------\n",
      "train Loss: 0.00109119 Acc: 0.95458984\n",
      "val Loss: 0.00025027 Acc: 0.97816594\n",
      "Epoch 27 of 500 took 0.123s\n",
      "Epoch 27/499\n",
      "----------\n",
      "train Loss: 0.00094338 Acc: 0.96240234\n",
      "val Loss: 0.00023311 Acc: 0.98689956\n",
      "Epoch 28 of 500 took 0.127s\n",
      "Epoch 28/499\n",
      "----------\n",
      "train Loss: 0.00092801 Acc: 0.96289062\n",
      "val Loss: 0.00022599 Acc: 0.98689956\n",
      "Epoch 29 of 500 took 0.147s\n",
      "Epoch 29/499\n",
      "----------\n",
      "train Loss: 0.00097399 Acc: 0.96191406\n",
      "val Loss: 0.00022030 Acc: 0.98253275\n",
      "Epoch 30 of 500 took 0.128s\n",
      "Epoch 30/499\n",
      "----------\n",
      "train Loss: 0.00101241 Acc: 0.95849609\n",
      "val Loss: 0.00022213 Acc: 0.99126638\n",
      "Epoch 31 of 500 took 0.127s\n",
      "Epoch 31/499\n",
      "----------\n",
      "train Loss: 0.00095266 Acc: 0.95898438\n",
      "val Loss: 0.00022516 Acc: 0.98253275\n",
      "Epoch 32 of 500 took 0.158s\n",
      "Epoch 32/499\n",
      "----------\n",
      "train Loss: 0.00085955 Acc: 0.96191406\n",
      "val Loss: 0.00022203 Acc: 0.98689956\n",
      "Epoch 33 of 500 took 0.120s\n",
      "Epoch 33/499\n",
      "----------\n",
      "train Loss: 0.00085716 Acc: 0.96435547\n",
      "val Loss: 0.00021662 Acc: 0.98253275\n",
      "Epoch 34 of 500 took 0.135s\n",
      "Epoch 34/499\n",
      "----------\n",
      "train Loss: 0.00081083 Acc: 0.96972656\n",
      "val Loss: 0.00022829 Acc: 0.97816594\n",
      "Epoch 35 of 500 took 0.135s\n",
      "Epoch 35/499\n",
      "----------\n",
      "train Loss: 0.00091392 Acc: 0.96484375\n",
      "val Loss: 0.00019573 Acc: 0.98689956\n",
      "Epoch 36 of 500 took 0.128s\n",
      "Epoch 36/499\n",
      "----------\n",
      "train Loss: 0.00079441 Acc: 0.96582031\n",
      "val Loss: 0.00019024 Acc: 0.99563319\n",
      "New best validation loss: 0.00019024232784733503\n",
      "Epoch 37 of 500 took 0.129s\n",
      "Epoch 37/499\n",
      "----------\n",
      "train Loss: 0.00070504 Acc: 0.97167969\n",
      "val Loss: 0.00020261 Acc: 0.98689956\n",
      "Epoch 38 of 500 took 0.161s\n",
      "Epoch 38/499\n",
      "----------\n",
      "train Loss: 0.00077723 Acc: 0.97167969\n",
      "val Loss: 0.00021830 Acc: 0.98689956\n",
      "Epoch 39 of 500 took 0.120s\n",
      "Epoch 39/499\n",
      "----------\n",
      "train Loss: 0.00071292 Acc: 0.97851562\n",
      "val Loss: 0.00021766 Acc: 0.98253275\n",
      "Epoch 40 of 500 took 0.209s\n",
      "Epoch 40/499\n",
      "----------\n",
      "train Loss: 0.00074632 Acc: 0.96826172\n",
      "val Loss: 0.00024163 Acc: 0.97816594\n",
      "Epoch 41 of 500 took 0.128s\n",
      "Epoch 41/499\n",
      "----------\n",
      "train Loss: 0.00076883 Acc: 0.97167969\n",
      "val Loss: 0.00019899 Acc: 0.98689956\n",
      "Epoch 42 of 500 took 0.163s\n",
      "Epoch 42/499\n",
      "----------\n",
      "train Loss: 0.00076475 Acc: 0.97021484\n",
      "val Loss: 0.00022869 Acc: 0.98253275\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0060e-04.\n",
      "Epoch 43 of 500 took 0.141s\n",
      "Epoch 43/499\n",
      "----------\n",
      "train Loss: 0.00078584 Acc: 0.97021484\n",
      "val Loss: 0.00017666 Acc: 0.99126638\n",
      "Epoch 44 of 500 took 0.111s\n",
      "Epoch 44/499\n",
      "----------\n",
      "train Loss: 0.00075602 Acc: 0.96875\n",
      "val Loss: 0.00018247 Acc: 0.98253275\n",
      "Epoch 45 of 500 took 0.106s\n",
      "Epoch 45/499\n",
      "----------\n",
      "train Loss: 0.00060978 Acc: 0.9765625\n",
      "val Loss: 0.00019505 Acc: 0.98253275\n",
      "Epoch 46 of 500 took 0.113s\n",
      "Epoch 46/499\n",
      "----------\n",
      "train Loss: 0.00072169 Acc: 0.96582031\n",
      "val Loss: 0.00014265 Acc: 0.99126638\n",
      "Epoch 47 of 500 took 0.110s\n",
      "Epoch 47/499\n",
      "----------\n",
      "train Loss: 0.00066194 Acc: 0.97705078\n",
      "val Loss: 0.00015716 Acc: 0.99126638\n",
      "Epoch 48 of 500 took 0.115s\n",
      "\n",
      "Training complete in 0m 6s\n",
      "Best val loss: 0.000190\n",
      "Session:  1\n",
      "TSD_Network(\n",
      "  (_network): ModuleList(\n",
      "    (0): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=252, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=252, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (1): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (2): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (_output): Linear(in_features=200, out_features=22, bias=True)\n",
      "  (_output_discriminator): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "Number Parameters:  137624\n",
      "<generator object Module.parameters at 0x7f08ea5d2ba0>\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/TSD/participant_2/best_state_0.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/TSD/participant_2/best_state_0.pt' (epoch 37)\n",
      "Epoch 0/499\n",
      "----------\n",
      "train Loss: 0.00630569 Acc: 0.77441406\n",
      "val Loss: 0.00154854 Acc: 0.90393013\n",
      "New best validation loss: 0.001548542466226103\n",
      "Epoch 1 of 500 took 0.114s\n",
      "Epoch 1/499\n",
      "----------\n",
      "train Loss: 0.00394172 Acc: 0.83154297\n",
      "val Loss: 0.00138973 Acc: 0.90393013\n",
      "New best validation loss: 0.0013897252395163456\n",
      "Epoch 2 of 500 took 0.132s\n",
      "Epoch 2/499\n",
      "----------\n",
      "train Loss: 0.00337972 Acc: 0.85839844\n",
      "val Loss: 0.00152483 Acc: 0.89519651\n",
      "Epoch 3 of 500 took 0.130s\n",
      "Epoch 3/499\n",
      "----------\n",
      "train Loss: 0.00282470 Acc: 0.87402344\n",
      "val Loss: 0.00085707 Acc: 0.94759825\n",
      "New best validation loss: 0.0008570676157047655\n",
      "Epoch 4 of 500 took 0.155s\n",
      "Epoch 4/499\n",
      "----------\n",
      "train Loss: 0.00252988 Acc: 0.88867188\n",
      "val Loss: 0.00084324 Acc: 0.95633188\n",
      "Epoch 5 of 500 took 0.147s\n",
      "Epoch 5/499\n",
      "----------\n",
      "train Loss: 0.00249263 Acc: 0.88623047\n",
      "val Loss: 0.00084356 Acc: 0.930131\n",
      "Epoch 6 of 500 took 0.162s\n",
      "Epoch 6/499\n",
      "----------\n",
      "train Loss: 0.00258874 Acc: 0.88769531\n",
      "val Loss: 0.00066891 Acc: 0.95633188\n",
      "New best validation loss: 0.0006689108485217699\n",
      "Epoch 7 of 500 took 0.270s\n",
      "Epoch 7/499\n",
      "----------\n",
      "train Loss: 0.00240307 Acc: 0.89111328\n",
      "val Loss: 0.00055883 Acc: 0.96069869\n",
      "New best validation loss: 0.0005588280972434964\n",
      "Epoch 8 of 500 took 0.246s\n",
      "Epoch 8/499\n",
      "----------\n",
      "train Loss: 0.00196109 Acc: 0.91455078\n",
      "val Loss: 0.00051681 Acc: 0.96069869\n",
      "Epoch 9 of 500 took 0.232s\n",
      "Epoch 9/499\n",
      "----------\n",
      "train Loss: 0.00203541 Acc: 0.91259766\n",
      "val Loss: 0.00046070 Acc: 0.9650655\n",
      "Epoch 10 of 500 took 0.197s\n",
      "Epoch 10/499\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.00208847 Acc: 0.91455078\n",
      "val Loss: 0.00064201 Acc: 0.94759825\n",
      "Epoch 11 of 500 took 0.233s\n",
      "Epoch 11/499\n",
      "----------\n",
      "train Loss: 0.00182647 Acc: 0.91650391\n",
      "val Loss: 0.00068506 Acc: 0.94759825\n",
      "Epoch 12 of 500 took 0.171s\n",
      "Epoch 12/499\n",
      "----------\n",
      "train Loss: 0.00173598 Acc: 0.9296875\n",
      "val Loss: 0.00037512 Acc: 0.96943231\n",
      "New best validation loss: 0.0003751242069698317\n",
      "Epoch 13 of 500 took 0.149s\n",
      "Epoch 13/499\n",
      "----------\n",
      "train Loss: 0.00189191 Acc: 0.91845703\n",
      "val Loss: 0.00101163 Acc: 0.92576419\n",
      "Epoch 14 of 500 took 0.113s\n",
      "Epoch 14/499\n",
      "----------\n",
      "train Loss: 0.00177416 Acc: 0.92333984\n",
      "val Loss: 0.00047876 Acc: 0.95633188\n",
      "Epoch 15 of 500 took 0.118s\n",
      "Epoch 15/499\n",
      "----------\n",
      "train Loss: 0.00157206 Acc: 0.9296875\n",
      "val Loss: 0.00069403 Acc: 0.93886463\n",
      "Epoch 16 of 500 took 0.112s\n",
      "Epoch 16/499\n",
      "----------\n",
      "train Loss: 0.00167832 Acc: 0.92871094\n",
      "val Loss: 0.00029726 Acc: 0.97816594\n",
      "Epoch 17 of 500 took 0.112s\n",
      "Epoch 17/499\n",
      "----------\n",
      "train Loss: 0.00136877 Acc: 0.94189453\n",
      "val Loss: 0.00042917 Acc: 0.97379913\n",
      "Epoch 18 of 500 took 0.108s\n",
      "Epoch 18/499\n",
      "----------\n",
      "train Loss: 0.00151811 Acc: 0.93359375\n",
      "val Loss: 0.00054879 Acc: 0.9650655\n",
      "Epoch 19 of 500 took 0.110s\n",
      "Epoch 19/499\n",
      "----------\n",
      "train Loss: 0.00139610 Acc: 0.9375\n",
      "val Loss: 0.00045148 Acc: 0.96943231\n",
      "Epoch 20 of 500 took 0.110s\n",
      "Epoch 20/499\n",
      "----------\n",
      "train Loss: 0.00132183 Acc: 0.94482422\n",
      "val Loss: 0.00041520 Acc: 0.96069869\n",
      "Epoch 21 of 500 took 0.110s\n",
      "Epoch 21/499\n",
      "----------\n",
      "train Loss: 0.00128948 Acc: 0.94287109\n",
      "val Loss: 0.00043589 Acc: 0.95633188\n",
      "Epoch 22 of 500 took 0.106s\n",
      "Epoch 22/499\n",
      "----------\n",
      "train Loss: 0.00124485 Acc: 0.93945312\n",
      "val Loss: 0.00043164 Acc: 0.9650655\n",
      "Epoch    23: reducing learning rate of group 0 to 5.0300e-04.\n",
      "Epoch 23 of 500 took 0.117s\n",
      "Epoch 23/499\n",
      "----------\n",
      "train Loss: 0.00111741 Acc: 0.95068359\n",
      "val Loss: 0.00024231 Acc: 0.98689956\n",
      "New best validation loss: 0.00024230524442081367\n",
      "Epoch 24 of 500 took 0.263s\n",
      "Epoch 24/499\n",
      "----------\n",
      "train Loss: 0.00085687 Acc: 0.96630859\n",
      "val Loss: 0.00020777 Acc: 0.98689956\n",
      "Epoch 25 of 500 took 0.123s\n",
      "Epoch 25/499\n",
      "----------\n",
      "train Loss: 0.00091561 Acc: 0.95996094\n",
      "val Loss: 0.00020364 Acc: 0.98689956\n",
      "Epoch 26 of 500 took 0.179s\n",
      "Epoch 26/499\n",
      "----------\n",
      "train Loss: 0.00095637 Acc: 0.95751953\n",
      "val Loss: 0.00023938 Acc: 0.98253275\n",
      "Epoch 27 of 500 took 0.151s\n",
      "Epoch 27/499\n",
      "----------\n",
      "train Loss: 0.00088798 Acc: 0.95947266\n",
      "val Loss: 0.00020699 Acc: 0.98689956\n",
      "Epoch 28 of 500 took 0.240s\n",
      "Epoch 28/499\n",
      "----------\n",
      "train Loss: 0.00075089 Acc: 0.96777344\n",
      "val Loss: 0.00021447 Acc: 0.98253275\n",
      "Epoch 29 of 500 took 0.164s\n",
      "Epoch 29/499\n",
      "----------\n",
      "train Loss: 0.00089998 Acc: 0.96289062\n",
      "val Loss: 0.00021118 Acc: 0.97816594\n",
      "Epoch 30 of 500 took 0.253s\n",
      "Epoch 30/499\n",
      "----------\n",
      "train Loss: 0.00082289 Acc: 0.96337891\n",
      "val Loss: 0.00022025 Acc: 0.98253275\n",
      "Epoch 31 of 500 took 0.237s\n",
      "Epoch 31/499\n",
      "----------\n",
      "train Loss: 0.00076576 Acc: 0.96923828\n",
      "val Loss: 0.00022771 Acc: 0.98689956\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0060e-04.\n",
      "Epoch 32 of 500 took 0.229s\n",
      "Epoch 32/499\n",
      "----------\n",
      "train Loss: 0.00066565 Acc: 0.97460938\n",
      "val Loss: 0.00016996 Acc: 0.99126638\n",
      "Epoch 33 of 500 took 0.171s\n",
      "Epoch 33/499\n",
      "----------\n",
      "train Loss: 0.00068081 Acc: 0.97119141\n",
      "val Loss: 0.00019835 Acc: 0.98253275\n",
      "Epoch 34 of 500 took 0.294s\n",
      "Epoch 34/499\n",
      "----------\n",
      "train Loss: 0.00069126 Acc: 0.96777344\n",
      "val Loss: 0.00019824 Acc: 0.98689956\n",
      "Epoch 35 of 500 took 0.148s\n",
      "\n",
      "Training complete in 0m 6s\n",
      "Best val loss: 0.000242\n",
      "Session:  2\n",
      "TSD_Network(\n",
      "  (_network): ModuleList(\n",
      "    (0): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=252, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=252, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (1): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (2): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (_output): Linear(in_features=200, out_features=22, bias=True)\n",
      "  (_output_discriminator): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "Number Parameters:  137624\n",
      "<generator object Module.parameters at 0x7f08eba0d350>\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/TSD/participant_2/best_state_1.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/TSD/participant_2/best_state_1.pt' (epoch 24)\n",
      "Epoch 0/499\n",
      "----------\n",
      "train Loss: 0.01105338 Acc: 0.64208984\n",
      "val Loss: 0.00205873 Acc: 0.80786026\n",
      "New best validation loss: 0.002058725820358143\n",
      "Epoch 1 of 500 took 0.203s\n",
      "Epoch 1/499\n",
      "----------\n",
      "train Loss: 0.00551947 Acc: 0.75683594\n",
      "val Loss: 0.00158878 Acc: 0.83406114\n",
      "New best validation loss: 0.0015887750808849085\n",
      "Epoch 2 of 500 took 0.172s\n",
      "Epoch 2/499\n",
      "----------\n",
      "train Loss: 0.00448831 Acc: 0.796875\n",
      "val Loss: 0.00128422 Acc: 0.88209607\n",
      "New best validation loss: 0.0012842151275368236\n",
      "Epoch 3 of 500 took 0.236s\n",
      "Epoch 3/499\n",
      "----------\n",
      "train Loss: 0.00372896 Acc: 0.81982422\n",
      "val Loss: 0.00113425 Acc: 0.89956332\n",
      "New best validation loss: 0.001134254916786627\n",
      "Epoch 4 of 500 took 0.152s\n",
      "Epoch 4/499\n",
      "----------\n",
      "train Loss: 0.00341476 Acc: 0.83203125\n",
      "val Loss: 0.00125464 Acc: 0.86462882\n",
      "Epoch 5 of 500 took 0.120s\n",
      "Epoch 5/499\n",
      "----------\n",
      "train Loss: 0.00346051 Acc: 0.83105469\n",
      "val Loss: 0.00106128 Acc: 0.91266376\n",
      "Epoch 6 of 500 took 0.131s\n",
      "Epoch 6/499\n",
      "----------\n",
      "train Loss: 0.00323131 Acc: 0.84423828\n",
      "val Loss: 0.00133203 Acc: 0.87336245\n",
      "Epoch 7 of 500 took 0.130s\n",
      "Epoch 7/499\n",
      "----------\n",
      "train Loss: 0.00291754 Acc: 0.86181641\n",
      "val Loss: 0.00101172 Acc: 0.89956332\n",
      "New best validation loss: 0.0010117177879966502\n",
      "Epoch 8 of 500 took 0.164s\n",
      "Epoch 8/499\n",
      "----------\n",
      "train Loss: 0.00295588 Acc: 0.85888672\n",
      "val Loss: 0.00117494 Acc: 0.91703057\n",
      "Epoch 9 of 500 took 0.266s\n",
      "Epoch 9/499\n",
      "----------\n",
      "train Loss: 0.00271040 Acc: 0.85986328\n",
      "val Loss: 0.00094397 Acc: 0.91703057\n",
      "Epoch 10 of 500 took 0.169s\n",
      "Epoch 10/499\n",
      "----------\n",
      "train Loss: 0.00264206 Acc: 0.87304688\n",
      "val Loss: 0.00086863 Acc: 0.91703057\n",
      "New best validation loss: 0.0008686314373557745\n",
      "Epoch 11 of 500 took 0.193s\n",
      "Epoch 11/499\n",
      "----------\n",
      "train Loss: 0.00243290 Acc: 0.875\n",
      "val Loss: 0.00071280 Acc: 0.94323144\n",
      "New best validation loss: 0.0007127960696491092\n",
      "Epoch 12 of 500 took 0.196s\n",
      "Epoch 12/499\n",
      "----------\n",
      "train Loss: 0.00261230 Acc: 0.88427734\n",
      "val Loss: 0.00095725 Acc: 0.90829694\n",
      "Epoch 13 of 500 took 0.141s\n",
      "Epoch 13/499\n",
      "----------\n",
      "train Loss: 0.00229344 Acc: 0.88867188\n",
      "val Loss: 0.00079016 Acc: 0.93449782\n",
      "Epoch 14 of 500 took 0.127s\n",
      "Epoch 14/499\n",
      "----------\n",
      "train Loss: 0.00213711 Acc: 0.89697266\n",
      "val Loss: 0.00068066 Acc: 0.94323144\n",
      "Epoch 15 of 500 took 0.119s\n",
      "Epoch 15/499\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.00234901 Acc: 0.87988281\n",
      "val Loss: 0.00085479 Acc: 0.91266376\n",
      "Epoch 16 of 500 took 0.116s\n",
      "Epoch 16/499\n",
      "----------\n",
      "train Loss: 0.00224709 Acc: 0.89111328\n",
      "val Loss: 0.00070635 Acc: 0.94759825\n",
      "Epoch 17 of 500 took 0.126s\n",
      "Epoch 17/499\n",
      "----------\n",
      "train Loss: 0.00207005 Acc: 0.90283203\n",
      "val Loss: 0.00077290 Acc: 0.930131\n",
      "Epoch 18 of 500 took 0.120s\n",
      "Epoch 18/499\n",
      "----------\n",
      "train Loss: 0.00183066 Acc: 0.91162109\n",
      "val Loss: 0.00063830 Acc: 0.94759825\n",
      "Epoch 19 of 500 took 0.120s\n",
      "Epoch 19/499\n",
      "----------\n",
      "train Loss: 0.00186108 Acc: 0.90917969\n",
      "val Loss: 0.00066817 Acc: 0.94759825\n",
      "Epoch 20 of 500 took 0.115s\n",
      "Epoch 20/499\n",
      "----------\n",
      "train Loss: 0.00188589 Acc: 0.91601562\n",
      "val Loss: 0.00070592 Acc: 0.94759825\n",
      "Epoch 21 of 500 took 0.109s\n",
      "Epoch 21/499\n",
      "----------\n",
      "train Loss: 0.00196766 Acc: 0.91015625\n",
      "val Loss: 0.00089178 Acc: 0.90829694\n",
      "Epoch 22 of 500 took 0.129s\n",
      "Epoch 22/499\n",
      "----------\n",
      "train Loss: 0.00183956 Acc: 0.92138672\n",
      "val Loss: 0.00049202 Acc: 0.96943231\n",
      "New best validation loss: 0.000492021896953666\n",
      "Epoch 23 of 500 took 0.125s\n",
      "Epoch 23/499\n",
      "----------\n",
      "train Loss: 0.00192658 Acc: 0.91064453\n",
      "val Loss: 0.00059302 Acc: 0.93449782\n",
      "Epoch 24 of 500 took 0.120s\n",
      "Epoch 24/499\n",
      "----------\n",
      "train Loss: 0.00168018 Acc: 0.91503906\n",
      "val Loss: 0.00042739 Acc: 0.97379913\n",
      "Epoch 25 of 500 took 0.163s\n",
      "Epoch 25/499\n",
      "----------\n",
      "train Loss: 0.00164042 Acc: 0.92138672\n",
      "val Loss: 0.00070568 Acc: 0.94323144\n",
      "Epoch 26 of 500 took 0.168s\n",
      "Epoch 26/499\n",
      "----------\n",
      "train Loss: 0.00167626 Acc: 0.91894531\n",
      "val Loss: 0.00054825 Acc: 0.96069869\n",
      "Epoch 27 of 500 took 0.151s\n",
      "Epoch 27/499\n",
      "----------\n",
      "train Loss: 0.00155698 Acc: 0.92480469\n",
      "val Loss: 0.00055249 Acc: 0.95633188\n",
      "Epoch 28 of 500 took 0.166s\n",
      "Epoch 28/499\n",
      "----------\n",
      "train Loss: 0.00138179 Acc: 0.93066406\n",
      "val Loss: 0.00048628 Acc: 0.95633188\n",
      "Epoch 29 of 500 took 0.153s\n",
      "Epoch 29/499\n",
      "----------\n",
      "train Loss: 0.00146646 Acc: 0.93212891\n",
      "val Loss: 0.00083660 Acc: 0.93449782\n",
      "Epoch 30 of 500 took 0.140s\n",
      "Epoch 30/499\n",
      "----------\n",
      "train Loss: 0.00146763 Acc: 0.93359375\n",
      "val Loss: 0.00050067 Acc: 0.95633188\n",
      "Epoch    31: reducing learning rate of group 0 to 5.0300e-04.\n",
      "Epoch 31 of 500 took 0.115s\n",
      "Epoch 31/499\n",
      "----------\n",
      "train Loss: 0.00131640 Acc: 0.93652344\n",
      "val Loss: 0.00037086 Acc: 0.96943231\n",
      "New best validation loss: 0.00037086338585641185\n",
      "Epoch 32 of 500 took 0.110s\n",
      "Epoch 32/499\n",
      "----------\n",
      "train Loss: 0.00106539 Acc: 0.95410156\n",
      "val Loss: 0.00043687 Acc: 0.96943231\n",
      "Epoch 33 of 500 took 0.110s\n",
      "Epoch 33/499\n",
      "----------\n",
      "train Loss: 0.00103504 Acc: 0.95947266\n",
      "val Loss: 0.00043402 Acc: 0.96943231\n",
      "Epoch 34 of 500 took 0.109s\n",
      "Epoch 34/499\n",
      "----------\n",
      "train Loss: 0.00109162 Acc: 0.95019531\n",
      "val Loss: 0.00033730 Acc: 0.96943231\n",
      "Epoch 35 of 500 took 0.106s\n",
      "Epoch 35/499\n",
      "----------\n",
      "train Loss: 0.00095364 Acc: 0.96142578\n",
      "val Loss: 0.00029529 Acc: 0.96943231\n",
      "Epoch 36 of 500 took 0.109s\n",
      "Epoch 36/499\n",
      "----------\n",
      "train Loss: 0.00113265 Acc: 0.94970703\n",
      "val Loss: 0.00029024 Acc: 0.98253275\n",
      "Epoch 37 of 500 took 0.118s\n",
      "Epoch 37/499\n",
      "----------\n",
      "train Loss: 0.00107576 Acc: 0.94775391\n",
      "val Loss: 0.00041257 Acc: 0.96069869\n",
      "Epoch 38 of 500 took 0.113s\n",
      "Epoch 38/499\n",
      "----------\n",
      "train Loss: 0.00102933 Acc: 0.95117188\n",
      "val Loss: 0.00034218 Acc: 0.97379913\n",
      "Epoch 39 of 500 took 0.120s\n",
      "Epoch 39/499\n",
      "----------\n",
      "train Loss: 0.00102723 Acc: 0.94921875\n",
      "val Loss: 0.00033788 Acc: 0.96943231\n",
      "Epoch 40 of 500 took 0.132s\n",
      "Epoch 40/499\n",
      "----------\n",
      "train Loss: 0.00092589 Acc: 0.96289062\n",
      "val Loss: 0.00035850 Acc: 0.97379913\n",
      "Epoch 41 of 500 took 0.110s\n",
      "Epoch 41/499\n",
      "----------\n",
      "train Loss: 0.00094388 Acc: 0.95556641\n",
      "val Loss: 0.00031121 Acc: 0.97379913\n",
      "Epoch 42 of 500 took 0.115s\n",
      "Epoch 42/499\n",
      "----------\n",
      "train Loss: 0.00093511 Acc: 0.96240234\n",
      "val Loss: 0.00029622 Acc: 0.97816594\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0060e-04.\n",
      "Epoch 43 of 500 took 0.106s\n",
      "\n",
      "Training complete in 0m 6s\n",
      "Best val loss: 0.000371\n"
     ]
    }
   ],
   "source": [
    "train_fine_tuning(examples_datasets_train, labels_datasets_train,\n",
    "                  num_kernels=num_kernels, path_weight_to_save_to=path_to_save_to,\n",
    "                  number_of_classes=number_of_classes,\n",
    "                  batch_size=batch_size,\n",
    "                  feature_vector_input_length=feature_vector_input_length,\n",
    "                  learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET one participant_examples  (3, 40, 572, 252)\n",
      "   GET one training_index_examples  (40, 572, 252)  at  0\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (40, 572, 252)  at  1\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (40, 572, 252)  at  2\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "dataloaders: \n",
      "   train  (1, 3)\n",
      "   valid  (1, 3)\n",
      "   test  (1, 3)\n",
      "GET one participant_examples  (3, 40, 572, 252)\n",
      "   GET one training_index_examples  (40, 572, 252)  at  0\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (40, 572, 252)  at  1\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (40, 572, 252)  at  2\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "dataloaders: \n",
      "   train  (2, 3)\n",
      "   valid  (2, 3)\n",
      "   test  (2, 3)\n",
      "GET one participant_examples  (3, 40, 572, 252)\n",
      "   GET one training_index_examples  (40, 572, 252)  at  0\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (40, 572, 252)  at  1\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (40, 572, 252)  at  2\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "dataloaders: \n",
      "   train  (3, 3)\n",
      "   valid  (3, 3)\n",
      "   test  (3, 3)\n",
      "TSD_Network(\n",
      "  (_network): ModuleList(\n",
      "    (0): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=252, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=252, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (1): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (2): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (_output): Linear(in_features=200, out_features=22, bias=True)\n",
      "  (_output_discriminator): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "Number Parameters:  137624\n",
      "0  SESSION   data =  572\n",
      "Participant:  0  Accuracy:  0.9912587412587412\n",
      "1  SESSION   data =  572\n",
      "Participant:  0  Accuracy:  0.4388111888111888\n",
      "2  SESSION   data =  572\n",
      "Participant:  0  Accuracy:  0.49475524475524474\n",
      "ACCURACY PARTICIPANT  0 :  [0.9912587412587412, 0.4388111888111888, 0.49475524475524474]\n",
      "TSD_Network(\n",
      "  (_network): ModuleList(\n",
      "    (0): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=252, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=252, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (1): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (2): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (_output): Linear(in_features=200, out_features=22, bias=True)\n",
      "  (_output_discriminator): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "Number Parameters:  137624\n",
      "0  SESSION   data =  572\n",
      "Participant:  1  Accuracy:  0.9807692307692307\n",
      "1  SESSION   data =  572\n",
      "Participant:  1  Accuracy:  0.8461538461538461\n",
      "2  SESSION   data =  572\n",
      "Participant:  1  Accuracy:  0.6783216783216783\n",
      "ACCURACY PARTICIPANT  1 :  [0.9807692307692307, 0.8461538461538461, 0.6783216783216783]\n",
      "TSD_Network(\n",
      "  (_network): ModuleList(\n",
      "    (0): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=252, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=252, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (1): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (2): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (_output): Linear(in_features=200, out_features=22, bias=True)\n",
      "  (_output_discriminator): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "Number Parameters:  137624\n",
      "0  SESSION   data =  572\n",
      "Participant:  2  Accuracy:  0.986013986013986\n",
      "1  SESSION   data =  572\n",
      "Participant:  2  Accuracy:  0.6468531468531469\n",
      "2  SESSION   data =  572\n",
      "Participant:  2  Accuracy:  0.5227272727272727\n",
      "ACCURACY PARTICIPANT  2 :  [0.986013986013986, 0.6468531468531469, 0.5227272727272727]\n",
      "[array([0.99125874, 0.43881119, 0.49475524]), array([0.98076923, 0.84615385, 0.67832168]), array([0.98601399, 0.64685315, 0.52272727])]\n",
      "OVERALL ACCURACY: 0.7317404817404818\n"
     ]
    }
   ],
   "source": [
    "save_path = \"/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/results_tsd\"\n",
    "algo_name = \"standard_TSD\"\n",
    "test_TSD_DNN_on_training_sessions(examples_datasets_train, labels_datasets_train,\n",
    "                                  num_neurons=num_kernels, use_only_first_training=True,\n",
    "                                  path_weights=path_to_save_to,\n",
    "                                  feature_vector_input_length=feature_vector_input_length,\n",
    "                                  save_path = save_path, algo_name=algo_name,\n",
    "                                  number_of_classes=number_of_classes, cycle_for_test=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant_0</th>\n",
       "      <th>Participant_1</th>\n",
       "      <th>Participant_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Session_0</th>\n",
       "      <td>0.991259</td>\n",
       "      <td>0.980769</td>\n",
       "      <td>0.986014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Session_1</th>\n",
       "      <td>0.438811</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.646853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Session_2</th>\n",
       "      <td>0.494755</td>\n",
       "      <td>0.678322</td>\n",
       "      <td>0.522727</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Participant_0 Participant_1 Participant_2\n",
       "Session_0      0.991259      0.980769      0.986014\n",
       "Session_1      0.438811      0.846154      0.646853\n",
       "Session_2      0.494755      0.678322      0.522727"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_filename = save_path + '/predictions_' + algo_name + \"_no_retraining.npy\"\n",
    "results = np.load(results_filename, allow_pickle=True)\n",
    "TSD_acc = results[0]\n",
    "TSD_acc_overall = np.mean(TSD_acc)\n",
    "TSD_df = pd.DataFrame(TSD_acc.transpose(), \n",
    "                       index = [f'Session_{i}' for i in range(TSD_acc.shape[1])],\n",
    "                        columns = [f'Participant_{j}' for j in range(TSD_acc.shape[0])])\n",
    "TSD_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "TSD_df.transpose().plot.bar(rot=0, figsize=(10,5))\n",
    "plt.title(\"TSD Accuracies\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. DANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TrainingsAndEvaluations.ForTrainingSessions.train_tsd_dnn_DA import train_DANN, test_DANN_on_training_sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_save_to_DANN = \"Weights_TSD/DANN\"\n",
    "num_kernels=[200, 200, 200]                        # model layer size \n",
    "number_of_cycle_for_first_training=40               # #session\n",
    "number_of_cycles_rest_of_training=40     \n",
    "path_weights_standard_DNN =path_to_save_to\n",
    "path_weights_DANN =\"/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/DANN\"\n",
    "number_of_classes=22\n",
    "batch_size=128          \n",
    "feature_vector_input_length=252                     # size of one example \n",
    "learning_rate=0.002515"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET one participant_examples  (3, 40, 572, 252)\n",
      "   GET one training_index_examples  (40, 572, 252)  at  0\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (40, 572, 252)  at  1\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (40, 572, 252)  at  2\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "dataloaders: \n",
      "   train  (1, 3)\n",
      "   valid  (1, 3)\n",
      "   test  (1, 0)\n",
      "GET one participant_examples  (3, 40, 572, 252)\n",
      "   GET one training_index_examples  (40, 572, 252)  at  0\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (40, 572, 252)  at  1\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (40, 572, 252)  at  2\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "dataloaders: \n",
      "   train  (2, 3)\n",
      "   valid  (2, 3)\n",
      "   test  (2, 0)\n",
      "GET one participant_examples  (3, 40, 572, 252)\n",
      "   GET one training_index_examples  (40, 572, 252)  at  0\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (40, 572, 252)  at  1\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (40, 572, 252)  at  2\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "dataloaders: \n",
      "   train  (3, 3)\n",
      "   valid  (3, 3)\n",
      "   test  (3, 0)\n",
      "SHAPE SESSIONS:  (3,)\n",
      "()\n",
      "TSD_Network(\n",
      "  (_network): ModuleList(\n",
      "    (0): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=252, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=252, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (1): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (2): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (_output): Linear(in_features=200, out_features=22, bias=True)\n",
      "  (_output_discriminator): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "Number Parameters:  137624\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/TSD/participant_0/best_state_0.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/TSD/participant_0/best_state_0.pt' (epoch 15)\n",
      "STARTING TRAINING\n",
      "Accuracy source 0.944336, main loss classifier 0.169662, source classification loss 0.173603, loss domain distinction 0.311378, accuracy domain distinction 0.484131\n",
      "VALIDATION Loss: 0.14973506 Acc: 0.95196507\n",
      "New best validation loss:  0.14973506331443787\n",
      "Epoch 1 of 500 took 0.257s\n",
      "Accuracy source 0.954102, main loss classifier 0.152480, source classification loss 0.157051, loss domain distinction 0.190050, accuracy domain distinction 0.499512\n",
      "VALIDATION Loss: 0.06335960 Acc: 0.97816594\n",
      "New best validation loss:  0.06335959583520889\n",
      "Epoch 2 of 500 took 0.242s\n",
      "Accuracy source 0.951660, main loss classifier 0.142706, source classification loss 0.140448, loss domain distinction 0.187843, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.09682260 Acc: 0.9650655\n",
      "Epoch 3 of 500 took 0.228s\n",
      "Accuracy source 0.950684, main loss classifier 0.150384, source classification loss 0.156291, loss domain distinction 0.186196, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.16164181 Acc: 0.95196507\n",
      "Epoch 4 of 500 took 0.230s\n",
      "Accuracy source 0.953613, main loss classifier 0.143842, source classification loss 0.146237, loss domain distinction 0.182757, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.14790890 Acc: 0.94323144\n",
      "Epoch 5 of 500 took 0.229s\n",
      "Accuracy source 0.954102, main loss classifier 0.140626, source classification loss 0.139582, loss domain distinction 0.183892, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.11045232 Acc: 0.96069869\n",
      "Epoch    21: reducing learning rate of group 0 to 5.0300e-04.\n",
      "Epoch 6 of 500 took 0.229s\n",
      "Accuracy source 0.967773, main loss classifier 0.120872, source classification loss 0.104897, loss domain distinction 0.182783, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.06977585 Acc: 0.97379913\n",
      "Epoch 7 of 500 took 0.243s\n",
      "Accuracy source 0.974609, main loss classifier 0.111856, source classification loss 0.087433, loss domain distinction 0.181681, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.09747924 Acc: 0.95633188\n",
      "Epoch 8 of 500 took 0.228s\n",
      "Accuracy source 0.979980, main loss classifier 0.106750, source classification loss 0.076608, loss domain distinction 0.180824, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.07216839 Acc: 0.96943231\n",
      "Epoch 9 of 500 took 0.225s\n",
      "Accuracy source 0.980469, main loss classifier 0.105772, source classification loss 0.074560, loss domain distinction 0.182572, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.10104901 Acc: 0.96069869\n",
      "Epoch 10 of 500 took 0.228s\n",
      "Accuracy source 0.978027, main loss classifier 0.106657, source classification loss 0.076938, loss domain distinction 0.180638, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.06413803 Acc: 0.97816594\n",
      "Epoch 11 of 500 took 0.228s\n",
      "Accuracy source 0.981445, main loss classifier 0.104210, source classification loss 0.071583, loss domain distinction 0.181488, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.06975517 Acc: 0.97379913\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0060e-04.\n",
      "Epoch 12 of 500 took 0.226s\n",
      "Accuracy source 0.979004, main loss classifier 0.103422, source classification loss 0.071284, loss domain distinction 0.180287, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.09670057 Acc: 0.9650655\n",
      "Training complete in 0m 3s\n",
      "()\n",
      "TSD_Network(\n",
      "  (_network): ModuleList(\n",
      "    (0): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=252, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=252, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (1): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (2): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (_output): Linear(in_features=200, out_features=22, bias=True)\n",
      "  (_output_discriminator): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "Number Parameters:  137624\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/TSD/participant_0/best_state_0.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/TSD/participant_0/best_state_0.pt' (epoch 15)\n",
      "STARTING TRAINING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy source 0.944824, main loss classifier 0.157096, source classification loss 0.150953, loss domain distinction 0.304694, accuracy domain distinction 0.484375\n",
      "VALIDATION Loss: 0.12409252 Acc: 0.93886463\n",
      "New best validation loss:  0.12409251928329468\n",
      "Epoch 1 of 500 took 0.227s\n",
      "Accuracy source 0.943848, main loss classifier 0.161766, source classification loss 0.175100, loss domain distinction 0.189651, accuracy domain distinction 0.499512\n",
      "VALIDATION Loss: 0.08118036 Acc: 0.9650655\n",
      "New best validation loss:  0.08118036389350891\n",
      "Epoch 2 of 500 took 0.236s\n",
      "Accuracy source 0.943848, main loss classifier 0.154614, source classification loss 0.163991, loss domain distinction 0.186203, accuracy domain distinction 0.499756\n",
      "VALIDATION Loss: 0.11574779 Acc: 0.95633188\n",
      "Epoch 3 of 500 took 0.231s\n",
      "Accuracy source 0.941406, main loss classifier 0.164624, source classification loss 0.186273, loss domain distinction 0.186113, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.13562109 Acc: 0.94759825\n",
      "Epoch 4 of 500 took 0.231s\n",
      "Accuracy source 0.951660, main loss classifier 0.151123, source classification loss 0.160063, loss domain distinction 0.184559, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.10396226 Acc: 0.9650655\n",
      "Epoch 5 of 500 took 0.241s\n",
      "Accuracy source 0.968262, main loss classifier 0.124341, source classification loss 0.107158, loss domain distinction 0.183359, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.03586196 Acc: 0.98689956\n",
      "Epoch    21: reducing learning rate of group 0 to 5.0300e-04.\n",
      "New best validation loss:  0.03586195781826973\n",
      "Epoch 6 of 500 took 0.233s\n",
      "Accuracy source 0.970215, main loss classifier 0.116871, source classification loss 0.097213, loss domain distinction 0.179988, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.07736336 Acc: 0.96069869\n",
      "Epoch 7 of 500 took 0.230s\n",
      "Accuracy source 0.971680, main loss classifier 0.114842, source classification loss 0.093891, loss domain distinction 0.178989, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.06392714 Acc: 0.97379913\n",
      "Epoch 8 of 500 took 0.235s\n",
      "Accuracy source 0.976074, main loss classifier 0.105614, source classification loss 0.075312, loss domain distinction 0.179540, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.06580164 Acc: 0.97816594\n",
      "Epoch 9 of 500 took 0.226s\n",
      "Accuracy source 0.979492, main loss classifier 0.106919, source classification loss 0.077560, loss domain distinction 0.179183, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.08555407 Acc: 0.96943231\n",
      "Epoch 10 of 500 took 0.229s\n",
      "Accuracy source 0.980957, main loss classifier 0.105072, source classification loss 0.074413, loss domain distinction 0.179940, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.04755712 Acc: 0.97379913\n",
      "Epoch 11 of 500 took 0.229s\n",
      "Accuracy source 0.976074, main loss classifier 0.105003, source classification loss 0.074366, loss domain distinction 0.179317, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.04281605 Acc: 0.97816594\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0060e-04.\n",
      "Epoch 12 of 500 took 0.232s\n",
      "Accuracy source 0.979980, main loss classifier 0.102036, source classification loss 0.069598, loss domain distinction 0.177094, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.04478360 Acc: 0.97816594\n",
      "Epoch 13 of 500 took 0.231s\n",
      "Accuracy source 0.975586, main loss classifier 0.107335, source classification loss 0.079701, loss domain distinction 0.178895, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.02922603 Acc: 0.98689956\n",
      "New best validation loss:  0.029226025566458702\n",
      "Epoch 14 of 500 took 0.227s\n",
      "Accuracy source 0.982422, main loss classifier 0.101078, source classification loss 0.067397, loss domain distinction 0.177151, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.05199966 Acc: 0.97379913\n",
      "Epoch 15 of 500 took 0.226s\n",
      "Accuracy source 0.982422, main loss classifier 0.098486, source classification loss 0.062099, loss domain distinction 0.178924, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.03812821 Acc: 0.99126638\n",
      "Epoch 16 of 500 took 0.229s\n",
      "Accuracy source 0.987305, main loss classifier 0.094269, source classification loss 0.053754, loss domain distinction 0.178920, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.03398748 Acc: 0.97816594\n",
      "Epoch 17 of 500 took 0.228s\n",
      "Accuracy source 0.984863, main loss classifier 0.097075, source classification loss 0.059454, loss domain distinction 0.178435, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.03469402 Acc: 0.99126638\n",
      "Epoch 18 of 500 took 0.225s\n",
      "Accuracy source 0.978516, main loss classifier 0.101027, source classification loss 0.067041, loss domain distinction 0.178050, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.04189539 Acc: 0.97816594\n",
      "Epoch 19 of 500 took 0.228s\n",
      "Accuracy source 0.982422, main loss classifier 0.099593, source classification loss 0.064243, loss domain distinction 0.178916, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.03737088 Acc: 0.98253275\n",
      "Epoch 20 of 500 took 0.229s\n",
      "Accuracy source 0.979492, main loss classifier 0.097640, source classification loss 0.060546, loss domain distinction 0.178232, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.03246152 Acc: 0.99126638\n",
      "Epoch 21 of 500 took 0.228s\n",
      "Accuracy source 0.984375, main loss classifier 0.094332, source classification loss 0.054695, loss domain distinction 0.177490, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.03725056 Acc: 0.98253275\n",
      "Epoch 22 of 500 took 0.225s\n",
      "Accuracy source 0.981445, main loss classifier 0.102328, source classification loss 0.070375, loss domain distinction 0.178433, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.07468491 Acc: 0.97379913\n",
      "Epoch 23 of 500 took 0.226s\n",
      "Accuracy source 0.983398, main loss classifier 0.096532, source classification loss 0.057963, loss domain distinction 0.179039, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.02971953 Acc: 0.98689956\n",
      "Epoch 24 of 500 took 0.226s\n",
      "Accuracy source 0.984375, main loss classifier 0.094701, source classification loss 0.054909, loss domain distinction 0.178113, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.03553850 Acc: 0.97816594\n",
      "Training complete in 0m 6s\n",
      "SHAPE SESSIONS:  (3,)\n",
      "()\n",
      "TSD_Network(\n",
      "  (_network): ModuleList(\n",
      "    (0): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=252, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=252, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (1): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (2): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (_output): Linear(in_features=200, out_features=22, bias=True)\n",
      "  (_output_discriminator): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "Number Parameters:  137624\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/TSD/participant_1/best_state_0.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/TSD/participant_1/best_state_0.pt' (epoch 21)\n",
      "STARTING TRAINING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy source 0.967773, main loss classifier 0.129705, source classification loss 0.098891, loss domain distinction 0.464454, accuracy domain distinction 0.494629\n",
      "VALIDATION Loss: 0.08133432 Acc: 0.96943231\n",
      "New best validation loss:  0.08133431524038315\n",
      "Epoch 1 of 500 took 0.236s\n",
      "Accuracy source 0.969238, main loss classifier 0.119573, source classification loss 0.091813, loss domain distinction 0.222663, accuracy domain distinction 0.501953\n",
      "VALIDATION Loss: 0.05185409 Acc: 0.97379913\n",
      "New best validation loss:  0.05185409262776375\n",
      "Epoch 2 of 500 took 0.225s\n",
      "Accuracy source 0.971680, main loss classifier 0.121700, source classification loss 0.093574, loss domain distinction 0.197731, accuracy domain distinction 0.498779\n",
      "VALIDATION Loss: 0.11210596 Acc: 0.95633188\n",
      "Epoch 3 of 500 took 0.229s\n",
      "Accuracy source 0.973145, main loss classifier 0.119155, source classification loss 0.089280, loss domain distinction 0.192085, accuracy domain distinction 0.499512\n",
      "VALIDATION Loss: 0.10355277 Acc: 0.95633188\n",
      "Epoch 4 of 500 took 0.231s\n",
      "Accuracy source 0.972168, main loss classifier 0.119736, source classification loss 0.091350, loss domain distinction 0.191199, accuracy domain distinction 0.498779\n",
      "VALIDATION Loss: 0.13144208 Acc: 0.94323144\n",
      "Epoch 5 of 500 took 0.229s\n",
      "Accuracy source 0.977051, main loss classifier 0.116818, source classification loss 0.087239, loss domain distinction 0.188402, accuracy domain distinction 0.499512\n",
      "VALIDATION Loss: 0.10022186 Acc: 0.9650655\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0060e-04.\n",
      "Epoch 6 of 500 took 0.224s\n",
      "Accuracy source 0.973633, main loss classifier 0.115170, source classification loss 0.087038, loss domain distinction 0.183807, accuracy domain distinction 0.500244\n",
      "VALIDATION Loss: 0.11003087 Acc: 0.95633188\n",
      "Epoch 7 of 500 took 0.239s\n",
      "Accuracy source 0.978027, main loss classifier 0.111494, source classification loss 0.078563, loss domain distinction 0.187934, accuracy domain distinction 0.499756\n",
      "VALIDATION Loss: 0.11115076 Acc: 0.95633188\n",
      "Epoch 8 of 500 took 0.240s\n",
      "Accuracy source 0.976074, main loss classifier 0.112328, source classification loss 0.081017, loss domain distinction 0.186465, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.16043569 Acc: 0.94323144\n",
      "Epoch 9 of 500 took 0.234s\n",
      "Accuracy source 0.979980, main loss classifier 0.110116, source classification loss 0.076105, loss domain distinction 0.182845, accuracy domain distinction 0.500244\n",
      "VALIDATION Loss: 0.08881515 Acc: 0.9650655\n",
      "Epoch 10 of 500 took 0.224s\n",
      "Accuracy source 0.975586, main loss classifier 0.110180, source classification loss 0.076731, loss domain distinction 0.185400, accuracy domain distinction 0.500488\n",
      "VALIDATION Loss: 0.10130988 Acc: 0.96069869\n",
      "Epoch 11 of 500 took 0.250s\n",
      "Accuracy source 0.973145, main loss classifier 0.113008, source classification loss 0.082573, loss domain distinction 0.187471, accuracy domain distinction 0.500244\n",
      "VALIDATION Loss: 0.13852876 Acc: 0.94759825\n",
      "Epoch 12 of 500 took 0.261s\n",
      "Accuracy source 0.980957, main loss classifier 0.104735, source classification loss 0.067770, loss domain distinction 0.184715, accuracy domain distinction 0.500488\n",
      "VALIDATION Loss: 0.07776851 Acc: 0.9650655\n",
      "Training complete in 0m 3s\n",
      "()\n",
      "TSD_Network(\n",
      "  (_network): ModuleList(\n",
      "    (0): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=252, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=252, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (1): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (2): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (_output): Linear(in_features=200, out_features=22, bias=True)\n",
      "  (_output_discriminator): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "Number Parameters:  137624\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/TSD/participant_1/best_state_0.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/TSD/participant_1/best_state_0.pt' (epoch 21)\n",
      "STARTING TRAINING\n",
      "Accuracy source 0.971191, main loss classifier 0.129356, source classification loss 0.100493, loss domain distinction 0.461458, accuracy domain distinction 0.495117\n",
      "VALIDATION Loss: 0.05960940 Acc: 0.96943231\n",
      "New best validation loss:  0.05960940197110176\n",
      "Epoch 1 of 500 took 0.227s\n",
      "Accuracy source 0.975586, main loss classifier 0.119300, source classification loss 0.091364, loss domain distinction 0.225428, accuracy domain distinction 0.495850\n",
      "VALIDATION Loss: 0.04528327 Acc: 0.98253275\n",
      "New best validation loss:  0.04528327286243439\n",
      "Epoch 2 of 500 took 0.228s\n",
      "Accuracy source 0.972168, main loss classifier 0.121402, source classification loss 0.093401, loss domain distinction 0.199542, accuracy domain distinction 0.502441\n",
      "VALIDATION Loss: 0.04563939 Acc: 0.97379913\n",
      "Epoch 3 of 500 took 0.225s\n",
      "Accuracy source 0.969238, main loss classifier 0.122513, source classification loss 0.096494, loss domain distinction 0.189193, accuracy domain distinction 0.499756\n",
      "VALIDATION Loss: 0.04573277 Acc: 0.97816594\n",
      "Epoch 4 of 500 took 0.229s\n",
      "Accuracy source 0.965332, main loss classifier 0.121894, source classification loss 0.099284, loss domain distinction 0.184737, accuracy domain distinction 0.499756\n",
      "VALIDATION Loss: 0.06421783 Acc: 0.9650655\n",
      "Epoch 5 of 500 took 0.230s\n",
      "Accuracy source 0.972168, main loss classifier 0.117769, source classification loss 0.090273, loss domain distinction 0.187652, accuracy domain distinction 0.500244\n",
      "VALIDATION Loss: 0.06309174 Acc: 0.97379913\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0060e-04.\n",
      "Epoch 6 of 500 took 0.225s\n",
      "Accuracy source 0.972168, main loss classifier 0.117515, source classification loss 0.089623, loss domain distinction 0.188284, accuracy domain distinction 0.499268\n",
      "VALIDATION Loss: 0.07983147 Acc: 0.96069869\n",
      "Epoch 7 of 500 took 0.230s\n",
      "Accuracy source 0.977051, main loss classifier 0.115196, source classification loss 0.085259, loss domain distinction 0.187150, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.07485884 Acc: 0.9650655\n",
      "Epoch 8 of 500 took 0.229s\n",
      "Accuracy source 0.972168, main loss classifier 0.115970, source classification loss 0.088131, loss domain distinction 0.184699, accuracy domain distinction 0.499756\n",
      "VALIDATION Loss: 0.05978695 Acc: 0.9650655\n",
      "Epoch 9 of 500 took 0.228s\n",
      "Accuracy source 0.979004, main loss classifier 0.108309, source classification loss 0.073961, loss domain distinction 0.181760, accuracy domain distinction 0.499023\n",
      "VALIDATION Loss: 0.05051486 Acc: 0.97816594\n",
      "Epoch 10 of 500 took 0.225s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy source 0.981445, main loss classifier 0.106095, source classification loss 0.069880, loss domain distinction 0.184385, accuracy domain distinction 0.500488\n",
      "VALIDATION Loss: 0.10684148 Acc: 0.95633188\n",
      "Epoch 11 of 500 took 0.230s\n",
      "Accuracy source 0.975098, main loss classifier 0.111411, source classification loss 0.079265, loss domain distinction 0.187127, accuracy domain distinction 0.500244\n",
      "VALIDATION Loss: 0.05542408 Acc: 0.97379913\n",
      "Epoch 12 of 500 took 0.226s\n",
      "Accuracy source 0.974121, main loss classifier 0.111049, source classification loss 0.079388, loss domain distinction 0.183833, accuracy domain distinction 0.499512\n",
      "VALIDATION Loss: 0.04386269 Acc: 0.98253275\n",
      "New best validation loss:  0.04386268928647041\n",
      "Epoch 13 of 500 took 0.235s\n",
      "Accuracy source 0.977539, main loss classifier 0.109238, source classification loss 0.075304, loss domain distinction 0.186249, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.05926291 Acc: 0.97816594\n",
      "Epoch 14 of 500 took 0.225s\n",
      "Accuracy source 0.976074, main loss classifier 0.111300, source classification loss 0.080247, loss domain distinction 0.185132, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.08662173 Acc: 0.9650655\n",
      "Epoch 15 of 500 took 0.225s\n",
      "Accuracy source 0.978516, main loss classifier 0.108322, source classification loss 0.074980, loss domain distinction 0.179973, accuracy domain distinction 0.499756\n",
      "VALIDATION Loss: 0.05560002 Acc: 0.97379913\n",
      "Epoch 16 of 500 took 0.230s\n",
      "Accuracy source 0.976562, main loss classifier 0.108209, source classification loss 0.075360, loss domain distinction 0.182475, accuracy domain distinction 0.500732\n",
      "VALIDATION Loss: 0.05148174 Acc: 0.97816594\n",
      "Epoch 17 of 500 took 0.237s\n",
      "Accuracy source 0.975586, main loss classifier 0.109700, source classification loss 0.076723, loss domain distinction 0.187469, accuracy domain distinction 0.499756\n",
      "VALIDATION Loss: 0.07266779 Acc: 0.9650655\n",
      "Epoch 18 of 500 took 0.229s\n",
      "Accuracy source 0.976074, main loss classifier 0.112122, source classification loss 0.083601, loss domain distinction 0.180803, accuracy domain distinction 0.500244\n",
      "VALIDATION Loss: 0.06727490 Acc: 0.9650655\n",
      "Epoch 19 of 500 took 0.229s\n",
      "Accuracy source 0.976074, main loss classifier 0.108105, source classification loss 0.074643, loss domain distinction 0.184894, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.06860001 Acc: 0.96943231\n",
      "Epoch 20 of 500 took 0.230s\n",
      "Accuracy source 0.972656, main loss classifier 0.111592, source classification loss 0.081838, loss domain distinction 0.183313, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.02864244 Acc: 0.98689956\n",
      "New best validation loss:  0.02864244394004345\n",
      "Epoch 21 of 500 took 0.233s\n",
      "Accuracy source 0.982910, main loss classifier 0.103480, source classification loss 0.066614, loss domain distinction 0.182506, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.07629432 Acc: 0.97379913\n",
      "Epoch 22 of 500 took 0.237s\n",
      "Accuracy source 0.979004, main loss classifier 0.106286, source classification loss 0.071741, loss domain distinction 0.183976, accuracy domain distinction 0.499756\n",
      "VALIDATION Loss: 0.04389810 Acc: 0.97816594\n",
      "Epoch 23 of 500 took 0.252s\n",
      "Accuracy source 0.977539, main loss classifier 0.108043, source classification loss 0.075035, loss domain distinction 0.183841, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.08508387 Acc: 0.96069869\n",
      "Epoch 24 of 500 took 0.241s\n",
      "Accuracy source 0.975586, main loss classifier 0.107637, source classification loss 0.074048, loss domain distinction 0.184428, accuracy domain distinction 0.499512\n",
      "VALIDATION Loss: 0.05970993 Acc: 0.9650655\n",
      "Epoch 25 of 500 took 0.247s\n",
      "Accuracy source 0.980957, main loss classifier 0.106627, source classification loss 0.073169, loss domain distinction 0.180916, accuracy domain distinction 0.500244\n",
      "VALIDATION Loss: 0.03741437 Acc: 0.98689956\n",
      "Epoch 26 of 500 took 0.257s\n",
      "Accuracy source 0.982910, main loss classifier 0.101227, source classification loss 0.061164, loss domain distinction 0.182214, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.04966829 Acc: 0.97816594\n",
      "Epoch 27 of 500 took 0.234s\n",
      "Accuracy source 0.978027, main loss classifier 0.104804, source classification loss 0.069742, loss domain distinction 0.181606, accuracy domain distinction 0.499756\n",
      "VALIDATION Loss: 0.04135346 Acc: 0.98253275\n",
      "Epoch 28 of 500 took 0.245s\n",
      "Accuracy source 0.978027, main loss classifier 0.107804, source classification loss 0.075700, loss domain distinction 0.182539, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.05479646 Acc: 0.97816594\n",
      "Epoch 29 of 500 took 0.231s\n",
      "Accuracy source 0.979492, main loss classifier 0.103549, source classification loss 0.067768, loss domain distinction 0.183469, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.07071590 Acc: 0.9650655\n",
      "Epoch 30 of 500 took 0.251s\n",
      "Accuracy source 0.976074, main loss classifier 0.106801, source classification loss 0.072697, loss domain distinction 0.184976, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.06837278 Acc: 0.96943231\n",
      "Epoch 31 of 500 took 0.271s\n",
      "Accuracy source 0.986816, main loss classifier 0.099750, source classification loss 0.059985, loss domain distinction 0.178436, accuracy domain distinction 0.499756\n",
      "VALIDATION Loss: 0.11079412 Acc: 0.95633188\n",
      "Training complete in 0m 8s\n",
      "SHAPE SESSIONS:  (3,)\n",
      "()\n",
      "TSD_Network(\n",
      "  (_network): ModuleList(\n",
      "    (0): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=252, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=252, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (1): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (2): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (_output): Linear(in_features=200, out_features=22, bias=True)\n",
      "  (_output_discriminator): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "Number Parameters:  137624\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/TSD/participant_2/best_state_0.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/TSD/participant_2/best_state_0.pt' (epoch 37)\n",
      "STARTING TRAINING\n",
      "Accuracy source 0.966309, main loss classifier 0.137389, source classification loss 0.101256, loss domain distinction 0.558829, accuracy domain distinction 0.485840\n",
      "VALIDATION Loss: 0.07539635 Acc: 0.96943231\n",
      "New best validation loss:  0.0753963515162468\n",
      "Epoch 1 of 500 took 0.261s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy source 0.963379, main loss classifier 0.128653, source classification loss 0.108307, loss domain distinction 0.250543, accuracy domain distinction 0.494141\n",
      "VALIDATION Loss: 0.12713817 Acc: 0.95633188\n",
      "Epoch 2 of 500 took 0.256s\n",
      "Accuracy source 0.966309, main loss classifier 0.130952, source classification loss 0.112924, loss domain distinction 0.206607, accuracy domain distinction 0.498047\n",
      "VALIDATION Loss: 0.06980500 Acc: 0.98253275\n",
      "New best validation loss:  0.06980499625205994\n",
      "Epoch 3 of 500 took 0.258s\n",
      "Accuracy source 0.965332, main loss classifier 0.127527, source classification loss 0.106311, loss domain distinction 0.195708, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.05505801 Acc: 0.98689956\n",
      "New best validation loss:  0.055058013647794724\n",
      "Epoch 4 of 500 took 0.242s\n",
      "Accuracy source 0.963379, main loss classifier 0.123043, source classification loss 0.099920, loss domain distinction 0.191558, accuracy domain distinction 0.499756\n",
      "VALIDATION Loss: 0.13108055 Acc: 0.95196507\n",
      "Epoch 5 of 500 took 0.250s\n",
      "Accuracy source 0.973633, main loss classifier 0.118420, source classification loss 0.091980, loss domain distinction 0.190376, accuracy domain distinction 0.500488\n",
      "VALIDATION Loss: 0.05457710 Acc: 0.98253275\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0060e-04.\n",
      "New best validation loss:  0.05457710102200508\n",
      "Epoch 6 of 500 took 0.338s\n",
      "Accuracy source 0.968750, main loss classifier 0.123656, source classification loss 0.103126, loss domain distinction 0.189768, accuracy domain distinction 0.499756\n",
      "VALIDATION Loss: 0.14728698 Acc: 0.93886463\n",
      "Epoch 7 of 500 took 0.257s\n",
      "Accuracy source 0.972168, main loss classifier 0.117245, source classification loss 0.089534, loss domain distinction 0.188261, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.07378168 Acc: 0.96943231\n",
      "Epoch 8 of 500 took 0.237s\n",
      "Accuracy source 0.979004, main loss classifier 0.108667, source classification loss 0.073759, loss domain distinction 0.187300, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.08535568 Acc: 0.96069869\n",
      "Epoch 9 of 500 took 0.230s\n",
      "Accuracy source 0.977051, main loss classifier 0.118004, source classification loss 0.092971, loss domain distinction 0.186228, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.18157704 Acc: 0.92139738\n",
      "Epoch 10 of 500 took 0.230s\n",
      "Accuracy source 0.971680, main loss classifier 0.114589, source classification loss 0.086440, loss domain distinction 0.185808, accuracy domain distinction 0.500488\n",
      "VALIDATION Loss: 0.07928480 Acc: 0.97379913\n",
      "Epoch 11 of 500 took 0.228s\n",
      "Accuracy source 0.976562, main loss classifier 0.115507, source classification loss 0.087304, loss domain distinction 0.191885, accuracy domain distinction 0.499268\n",
      "VALIDATION Loss: 0.10703009 Acc: 0.96069869\n",
      "Epoch 12 of 500 took 0.229s\n",
      "Accuracy source 0.979980, main loss classifier 0.106949, source classification loss 0.072462, loss domain distinction 0.184988, accuracy domain distinction 0.499756\n",
      "VALIDATION Loss: 0.11575756 Acc: 0.94323144\n",
      "Epoch 13 of 500 took 0.227s\n",
      "Accuracy source 0.970703, main loss classifier 0.116811, source classification loss 0.090414, loss domain distinction 0.187296, accuracy domain distinction 0.500244\n",
      "VALIDATION Loss: 0.10420677 Acc: 0.9650655\n",
      "Epoch 14 of 500 took 0.233s\n",
      "Accuracy source 0.971680, main loss classifier 0.116092, source classification loss 0.089666, loss domain distinction 0.187161, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.06470049 Acc: 0.97379913\n",
      "Epoch 15 of 500 took 0.229s\n",
      "Accuracy source 0.976074, main loss classifier 0.112636, source classification loss 0.083870, loss domain distinction 0.184326, accuracy domain distinction 0.499512\n",
      "VALIDATION Loss: 0.07131688 Acc: 0.97816594\n",
      "Epoch 16 of 500 took 0.230s\n",
      "Accuracy source 0.975586, main loss classifier 0.109883, source classification loss 0.078840, loss domain distinction 0.184652, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.14152074 Acc: 0.93886463\n",
      "Training complete in 0m 4s\n",
      "()\n",
      "TSD_Network(\n",
      "  (_network): ModuleList(\n",
      "    (0): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=252, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=252, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (1): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (2): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (_output): Linear(in_features=200, out_features=22, bias=True)\n",
      "  (_output_discriminator): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "Number Parameters:  137624\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/TSD/participant_2/best_state_0.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/TSD/participant_2/best_state_0.pt' (epoch 37)\n",
      "STARTING TRAINING\n",
      "Accuracy source 0.967285, main loss classifier 0.138432, source classification loss 0.105369, loss domain distinction 0.545748, accuracy domain distinction 0.493896\n",
      "VALIDATION Loss: 0.17259020 Acc: 0.930131\n",
      "New best validation loss:  0.1725901961326599\n",
      "Epoch 1 of 500 took 0.282s\n",
      "Accuracy source 0.971191, main loss classifier 0.122170, source classification loss 0.093982, loss domain distinction 0.251866, accuracy domain distinction 0.498047\n",
      "VALIDATION Loss: 0.15056428 Acc: 0.94323144\n",
      "New best validation loss:  0.1505642831325531\n",
      "Epoch 2 of 500 took 0.280s\n",
      "Accuracy source 0.971680, main loss classifier 0.122563, source classification loss 0.093397, loss domain distinction 0.211337, accuracy domain distinction 0.496094\n",
      "VALIDATION Loss: 0.20919564 Acc: 0.91703057\n",
      "Epoch 3 of 500 took 0.276s\n",
      "Accuracy source 0.965332, main loss classifier 0.129938, source classification loss 0.110446, loss domain distinction 0.195598, accuracy domain distinction 0.498535\n",
      "VALIDATION Loss: 0.14978239 Acc: 0.95633188\n",
      "New best validation loss:  0.14978238940238953\n",
      "Epoch 4 of 500 took 0.280s\n",
      "Accuracy source 0.967773, main loss classifier 0.121013, source classification loss 0.095242, loss domain distinction 0.193084, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.09863889 Acc: 0.96069869\n",
      "New best validation loss:  0.09863889217376709\n",
      "Epoch 5 of 500 took 0.281s\n",
      "Accuracy source 0.968750, main loss classifier 0.121141, source classification loss 0.096239, loss domain distinction 0.193589, accuracy domain distinction 0.498535\n",
      "VALIDATION Loss: 0.29086286 Acc: 0.92139738\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0060e-04.\n",
      "Epoch 6 of 500 took 0.277s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy source 0.977051, main loss classifier 0.111561, source classification loss 0.080764, loss domain distinction 0.185880, accuracy domain distinction 0.500488\n",
      "VALIDATION Loss: 0.19848418 Acc: 0.92139738\n",
      "Epoch 7 of 500 took 0.276s\n",
      "Accuracy source 0.972656, main loss classifier 0.113694, source classification loss 0.084041, loss domain distinction 0.188889, accuracy domain distinction 0.500732\n",
      "VALIDATION Loss: 0.24461025 Acc: 0.91266376\n",
      "Epoch 8 of 500 took 0.236s\n",
      "Accuracy source 0.973633, main loss classifier 0.109136, source classification loss 0.075846, loss domain distinction 0.188110, accuracy domain distinction 0.500244\n",
      "VALIDATION Loss: 0.19218408 Acc: 0.930131\n",
      "Epoch 9 of 500 took 0.231s\n",
      "Accuracy source 0.968262, main loss classifier 0.121022, source classification loss 0.099235, loss domain distinction 0.188777, accuracy domain distinction 0.500244\n",
      "VALIDATION Loss: 0.23471788 Acc: 0.90829694\n",
      "Epoch 10 of 500 took 0.227s\n",
      "Accuracy source 0.976562, main loss classifier 0.110140, source classification loss 0.077121, loss domain distinction 0.190627, accuracy domain distinction 0.499756\n",
      "VALIDATION Loss: 0.13721824 Acc: 0.95196507\n",
      "Epoch 11 of 500 took 0.229s\n",
      "Accuracy source 0.976074, main loss classifier 0.110819, source classification loss 0.079553, loss domain distinction 0.184986, accuracy domain distinction 0.500244\n",
      "VALIDATION Loss: 0.21198346 Acc: 0.92139738\n",
      "Epoch 12 of 500 took 0.228s\n",
      "Accuracy source 0.979004, main loss classifier 0.109193, source classification loss 0.075092, loss domain distinction 0.188321, accuracy domain distinction 0.499268\n",
      "VALIDATION Loss: 0.21904910 Acc: 0.91266376\n",
      "Epoch 13 of 500 took 0.232s\n",
      "Accuracy source 0.974609, main loss classifier 0.112659, source classification loss 0.082839, loss domain distinction 0.188197, accuracy domain distinction 0.499756\n",
      "VALIDATION Loss: 0.19530742 Acc: 0.90829694\n",
      "Epoch 14 of 500 took 0.227s\n",
      "Accuracy source 0.975098, main loss classifier 0.113502, source classification loss 0.084044, loss domain distinction 0.187525, accuracy domain distinction 0.499756\n",
      "VALIDATION Loss: 0.15687609 Acc: 0.930131\n",
      "Epoch 15 of 500 took 0.229s\n",
      "Accuracy source 0.978516, main loss classifier 0.113665, source classification loss 0.083902, loss domain distinction 0.188674, accuracy domain distinction 0.499756\n",
      "VALIDATION Loss: 0.17046933 Acc: 0.92576419\n",
      "Training complete in 0m 4s\n"
     ]
    }
   ],
   "source": [
    "train_DANN(examples_datasets_train, labels_datasets_train, \n",
    "                      num_kernels=num_kernels,\n",
    "                      path_weights_fine_tuning=path_weights_standard_DNN,\n",
    "                      number_of_classes=number_of_classes,\n",
    "                      number_of_cycle_for_first_training=number_of_cycle_for_first_training,\n",
    "                      number_of_cycles_rest_of_training=number_of_cycles_rest_of_training,\n",
    "                      batch_size=batch_size,\n",
    "                      feature_vector_input_length=feature_vector_input_length,\n",
    "                      path_weights_to_save_to=path_weights_DANN, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET one participant_examples  (3, 40, 572, 252)\n",
      "   GET one training_index_examples  (40, 572, 252)  at  0\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (40, 572, 252)  at  1\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (40, 572, 252)  at  2\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "dataloaders: \n",
      "   train  (1, 3)\n",
      "   valid  (1, 3)\n",
      "   test  (1, 3)\n",
      "GET one participant_examples  (3, 40, 572, 252)\n",
      "   GET one training_index_examples  (40, 572, 252)  at  0\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (40, 572, 252)  at  1\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (40, 572, 252)  at  2\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "dataloaders: \n",
      "   train  (2, 3)\n",
      "   valid  (2, 3)\n",
      "   test  (2, 3)\n",
      "GET one participant_examples  (3, 40, 572, 252)\n",
      "   GET one training_index_examples  (40, 572, 252)  at  0\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (40, 572, 252)  at  1\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (40, 572, 252)  at  2\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "dataloaders: \n",
      "   train  (3, 3)\n",
      "   valid  (3, 3)\n",
      "   test  (3, 3)\n",
      "TSD_Network(\n",
      "  (_network): ModuleList(\n",
      "    (0): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=252, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=252, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (1): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (2): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (_output): Linear(in_features=200, out_features=22, bias=True)\n",
      "  (_output_discriminator): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "Number Parameters:  137624\n",
      "(3,)\n",
      "Participant ID:  0  Session ID:  0  Accuracy:  0.9912587412587412\n",
      "Participant ID:  0  Session ID:  1  Accuracy:  0.5961538461538461\n",
      "Participant ID:  0  Session ID:  2  Accuracy:  0.5209790209790209\n",
      "ACCURACY PARTICIPANT:  [0.9912587412587412, 0.5961538461538461, 0.5209790209790209]\n",
      "TSD_Network(\n",
      "  (_network): ModuleList(\n",
      "    (0): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=252, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=252, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (1): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (2): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (_output): Linear(in_features=200, out_features=22, bias=True)\n",
      "  (_output_discriminator): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "Number Parameters:  137624\n",
      "(3,)\n",
      "Participant ID:  1  Session ID:  0  Accuracy:  0.9807692307692307\n",
      "Participant ID:  1  Session ID:  1  Accuracy:  0.8706293706293706\n",
      "Participant ID:  1  Session ID:  2  Accuracy:  0.7482517482517482\n",
      "ACCURACY PARTICIPANT:  [0.9807692307692307, 0.8706293706293706, 0.7482517482517482]\n",
      "TSD_Network(\n",
      "  (_network): ModuleList(\n",
      "    (0): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=252, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=252, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (1): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (2): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (_output): Linear(in_features=200, out_features=22, bias=True)\n",
      "  (_output_discriminator): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "Number Parameters:  137624\n",
      "(3,)\n",
      "Participant ID:  2  Session ID:  0  Accuracy:  0.986013986013986\n",
      "Participant ID:  2  Session ID:  1  Accuracy:  0.7255244755244755\n",
      "Participant ID:  2  Session ID:  2  Accuracy:  0.5611888111888111\n",
      "ACCURACY PARTICIPANT:  [0.986013986013986, 0.7255244755244755, 0.5611888111888111]\n",
      "[[0.99125874 0.59615385 0.52097902]\n",
      " [0.98076923 0.87062937 0.74825175]\n",
      " [0.98601399 0.72552448 0.56118881]]\n",
      "[array([0.99125874, 0.59615385, 0.52097902]), array([0.98076923, 0.87062937, 0.74825175]), array([0.98601399, 0.72552448, 0.56118881])]\n",
      "OVERALL ACCURACY: 0.7756410256410257\n"
     ]
    }
   ],
   "source": [
    "save_path = \"/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/results_tsd\"\n",
    "algo_name = \"DANN\"\n",
    "test_DANN_on_training_sessions(examples_datasets_train, labels_datasets_train,\n",
    "                              feature_vector_input_length=feature_vector_input_length,\n",
    "                              num_neurons=num_kernels, path_weights_DA=path_weights_DANN,\n",
    "                              algo_name=algo_name, save_path = save_path,\n",
    "                              path_weights_normal=path_weights_standard_DNN, number_of_classes=number_of_classes,\n",
    "                              cycle_for_test=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant_0</th>\n",
       "      <th>Participant_1</th>\n",
       "      <th>Participant_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Session_0</th>\n",
       "      <td>0.991259</td>\n",
       "      <td>0.980769</td>\n",
       "      <td>0.986014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Session_1</th>\n",
       "      <td>0.596154</td>\n",
       "      <td>0.870629</td>\n",
       "      <td>0.725524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Session_2</th>\n",
       "      <td>0.520979</td>\n",
       "      <td>0.748252</td>\n",
       "      <td>0.561189</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Participant_0 Participant_1 Participant_2\n",
       "Session_0      0.991259      0.980769      0.986014\n",
       "Session_1      0.596154      0.870629      0.725524\n",
       "Session_2      0.520979      0.748252      0.561189"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_filename = save_path + '/predictions_' + algo_name + \".npy\"\n",
    "results = np.load(results_filename, allow_pickle=True)\n",
    "DANN_acc = results[0]\n",
    "DANN_acc_overall = np.mean(DANN_acc)\n",
    "DANN_df = pd.DataFrame(DANN_acc.transpose(), \n",
    "                       index = [f'Session_{i}' for i in range(DANN_acc.shape[1])],\n",
    "                        columns = [f'Participant_{j}' for j in range(DANN_acc.shape[0])])\n",
    "DANN_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "DANN_df.transpose().plot.bar(rot=0, figsize=(10,5))\n",
    "plt.title(\"DANN Accuracies\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. SCADANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TrainingsAndEvaluations.ForTrainingSessions.train_tsd_dnn_SCADANN import run_SCADANN_training_sessions, test_network_SLADANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_kernels=[200, 200, 200]                        # model layer size \n",
    "number_of_cycle_for_first_training=40               # #session\n",
    "number_of_cycles_rest_of_training=40     \n",
    "number_of_classes=22\n",
    "batch_size=128          \n",
    "feature_vector_input_length=252                     # size of one example \n",
    "learning_rate=0.002515\n",
    "percentage_same_gesture_stable = 0.75 \n",
    "path_weights_SCADANN =\"/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/SCADANN\"\n",
    "path_weight_to_save_to = path_weights_SCADANN\n",
    "path_weights_DANN =\"/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/DANN\"\n",
    "path_weights_start_with = path_weights_DANN\n",
    "path_weights_TSD =\"/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/TSD\"\n",
    "path_weights_Normal_training = path_weights_TSD\n",
    "algo_name = \"SCADANN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET one participant_examples  (3, 40, 572, 252)\n",
      "   GET one training_index_examples  (40, 572, 252)  at  0\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2288, 252)    (2288,)\n",
      "   GET one training_index_examples  (40, 572, 252)  at  1\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2288, 252)    (2288,)\n",
      "   GET one training_index_examples  (40, 572, 252)  at  2\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2288, 252)    (2288,)\n",
      "dataloaders: \n",
      "   train  (1, 3)\n",
      "   valid  (0,)\n",
      "   test  (1, 0)\n",
      "GET one participant_examples  (3, 40, 572, 252)\n",
      "   GET one training_index_examples  (40, 572, 252)  at  0\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2288, 252)    (2288,)\n",
      "   GET one training_index_examples  (40, 572, 252)  at  1\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2288, 252)    (2288,)\n",
      "   GET one training_index_examples  (40, 572, 252)  at  2\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2288, 252)    (2288,)\n",
      "dataloaders: \n",
      "   train  (2, 3)\n",
      "   valid  (0,)\n",
      "   test  (2, 0)\n",
      "GET one participant_examples  (3, 40, 572, 252)\n",
      "   GET one training_index_examples  (40, 572, 252)  at  0\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2288, 252)    (2288,)\n",
      "   GET one training_index_examples  (40, 572, 252)  at  1\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2288, 252)    (2288,)\n",
      "   GET one training_index_examples  (40, 572, 252)  at  2\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2288, 252)    (2288,)\n",
      "dataloaders: \n",
      "   train  (3, 3)\n",
      "   valid  (0,)\n",
      "   test  (3, 0)\n",
      "participants_train =  3\n",
      "TSD_Network(\n",
      "  (_network): ModuleList(\n",
      "    (0): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=252, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=252, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (1): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (2): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (_output): Linear(in_features=200, out_features=22, bias=True)\n",
      "  (_output_discriminator): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "Number Parameters:  137624\n",
      "Optimizer =  <generator object Module.parameters at 0x7f0850069e40>\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/DANN/participant_0/best_state_1.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/DANN/participant_0/best_state_1.pt' (epoch 2)\n",
      "TSD_Network(\n",
      "  (_network): ModuleList(\n",
      "    (0): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=252, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=252, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (1): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (2): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (_output): Linear(in_features=200, out_features=22, bias=True)\n",
      "  (_output_discriminator): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "Number Parameters:  137624\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/TSD/participant_0/best_state_0.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/TSD/participant_0/best_state_0.pt' (epoch 15)\n",
      "TSD_Network(\n",
      "  (_network): ModuleList(\n",
      "    (0): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=252, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=252, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (1): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (2): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (_output): Linear(in_features=200, out_features=22, bias=True)\n",
      "  (_output_discriminator): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "Number Parameters:  137624\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/DANN/participant_0/best_state_1.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/DANN/participant_0/best_state_1.pt' (epoch 2)\n",
      "==== models_array =  (2,)  @ session  1\n",
      "HANDLING NEW SESSION  0\n",
      "HANDLING NEW SESSION  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.0  len before:  26   len after:  6\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.6666666666666666  len before:  26   len after:  15\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.4230769230769231   AFTER:  1.0  len before:  26   len after:  11\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  2\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  1.0  len before:  26   len after:  8\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.0  len before:  26   len after:  4\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.6111111111111112  len before:  26   len after:  18\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.4230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.3076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  3\n",
      "BEFORE:  0.11538461538461539   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.19230769230769232  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  1.0  len before:  26   len after:  6\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.11538461538461539   AFTER:  0.0  len before:  26   len after:  1\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  1.0  len before:  26   len after:  2\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  6\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.0  len before:  26   len after:  17\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  4\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  1.0  len before:  26   len after:  4\n",
      "BEFORE:  0.5769230769230769   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.8333333333333334  len before:  26   len after:  6\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  4\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.6538461538461539  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  8\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.1111111111111111  len before:  26   len after:  9\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.07692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  9\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  1.0  len before:  26   len after:  13\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  17\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.5721153846153846   Accuracy pseudo: 0.6509433962264151  len pseudo:  1802    len predictions 2288\n",
      "STARTING TRAINING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laiy/.local/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/laiy/.local/lib/python3.8/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy total 0.779474, main loss classifier 0.821074, source accuracy 0.854403 source classification loss 0.452090, target accuracy 0.704545 target loss 0.955126 accuracy domain distinction 0.498935 loss domain distinction 1.174656,\n",
      "VALIDATION Loss: 0.41208940 Acc: 0.84764543\n",
      "New best validation loss:  0.41208940496047336\n",
      "Epoch 2 of 500 took 0.320s\n",
      "Accuracy total 0.827415, main loss classifier 0.623176, source accuracy 0.884233 source classification loss 0.345634, target accuracy 0.770597 target loss 0.674407 accuracy domain distinction 0.499645 loss domain distinction 1.131549,\n",
      "VALIDATION Loss: 0.28781486 Acc: 0.88088643\n",
      "New best validation loss:  0.2878148630261421\n",
      "Epoch 3 of 500 took 0.289s\n",
      "Accuracy total 0.860795, main loss classifier 0.536824, source accuracy 0.915483 source classification loss 0.258854, target accuracy 0.806108 target loss 0.591453 accuracy domain distinction 0.499645 loss domain distinction 1.116705,\n",
      "VALIDATION Loss: 0.25924731 Acc: 0.92520776\n",
      "New best validation loss:  0.2592473092178504\n",
      "Epoch 4 of 500 took 0.291s\n",
      "Accuracy total 0.875710, main loss classifier 0.492306, source accuracy 0.906960 source classification loss 0.288222, target accuracy 0.844460 target loss 0.474169 accuracy domain distinction 0.500000 loss domain distinction 1.111099,\n",
      "VALIDATION Loss: 0.26874989 Acc: 0.89750693\n",
      "Epoch 5 of 500 took 0.296s\n",
      "Accuracy total 0.881392, main loss classifier 0.448255, source accuracy 0.913352 source classification loss 0.236944, target accuracy 0.849432 target loss 0.436963 accuracy domain distinction 0.500000 loss domain distinction 1.113011,\n",
      "VALIDATION Loss: 0.18987854 Acc: 0.93905817\n",
      "New best validation loss:  0.1898785444597403\n",
      "Epoch 6 of 500 took 0.296s\n",
      "Accuracy total 0.894886, main loss classifier 0.421609, source accuracy 0.927557 source classification loss 0.211953, target accuracy 0.862216 target loss 0.409756 accuracy domain distinction 0.500000 loss domain distinction 1.107550,\n",
      "VALIDATION Loss: 0.27255097 Acc: 0.90027701\n",
      "Epoch 7 of 500 took 0.289s\n",
      "Accuracy total 0.893466, main loss classifier 0.423240, source accuracy 0.921875 source classification loss 0.211865, target accuracy 0.865057 target loss 0.414305 accuracy domain distinction 0.500000 loss domain distinction 1.101554,\n",
      "VALIDATION Loss: 0.24678958 Acc: 0.89473684\n",
      "Epoch 8 of 500 took 0.287s\n",
      "Accuracy total 0.905895, main loss classifier 0.394756, source accuracy 0.938210 source classification loss 0.183665, target accuracy 0.873580 target loss 0.385949 accuracy domain distinction 0.500000 loss domain distinction 1.099495,\n",
      "VALIDATION Loss: 0.20392532 Acc: 0.93628809\n",
      "Epoch 9 of 500 took 0.295s\n",
      "Accuracy total 0.909091, main loss classifier 0.380914, source accuracy 0.948864 source classification loss 0.159426, target accuracy 0.869318 target loss 0.381638 accuracy domain distinction 0.500000 loss domain distinction 1.103822,\n",
      "VALIDATION Loss: 0.21280445 Acc: 0.92797784\n",
      "Epoch 10 of 500 took 0.287s\n",
      "Accuracy total 0.910866, main loss classifier 0.386104, source accuracy 0.947443 source classification loss 0.176293, target accuracy 0.874290 target loss 0.374762 accuracy domain distinction 0.500000 loss domain distinction 1.105765,\n",
      "VALIDATION Loss: 0.19346010 Acc: 0.93351801\n",
      "Epoch 11 of 500 took 0.287s\n",
      "Accuracy total 0.897727, main loss classifier 0.416549, source accuracy 0.934659 source classification loss 0.195395, target accuracy 0.860795 target loss 0.416395 accuracy domain distinction 0.500000 loss domain distinction 1.106537,\n",
      "VALIDATION Loss: 0.20611715 Acc: 0.91412742\n",
      "Epoch    11: reducing learning rate of group 0 to 5.0300e-04.\n",
      "Epoch 12 of 500 took 0.288s\n",
      "Accuracy total 0.914418, main loss classifier 0.335834, source accuracy 0.952415 source classification loss 0.129198, target accuracy 0.876420 target loss 0.334153 accuracy domain distinction 0.500000 loss domain distinction 1.041587,\n",
      "VALIDATION Loss: 0.17837010 Acc: 0.93628809\n",
      "New best validation loss:  0.17837009951472282\n",
      "Epoch 13 of 500 took 0.292s\n",
      "Accuracy total 0.930398, main loss classifier 0.307404, source accuracy 0.958807 source classification loss 0.124720, target accuracy 0.901989 target loss 0.281871 accuracy domain distinction 0.500000 loss domain distinction 1.041091,\n",
      "VALIDATION Loss: 0.13303093 Acc: 0.94736842\n",
      "New best validation loss:  0.13303092618783316\n",
      "Epoch 14 of 500 took 0.291s\n",
      "Accuracy total 0.942827, main loss classifier 0.295882, source accuracy 0.965909 source classification loss 0.107010, target accuracy 0.919744 target loss 0.276095 accuracy domain distinction 0.500000 loss domain distinction 1.043296,\n",
      "VALIDATION Loss: 0.14360455 Acc: 0.94736842\n",
      "Epoch 15 of 500 took 0.288s\n",
      "Accuracy total 0.929688, main loss classifier 0.301032, source accuracy 0.956676 source classification loss 0.126573, target accuracy 0.902699 target loss 0.266887 accuracy domain distinction 0.500000 loss domain distinction 1.043027,\n",
      "VALIDATION Loss: 0.12342546 Acc: 0.95844875\n",
      "New best validation loss:  0.12342546321451664\n",
      "Epoch 16 of 500 took 0.290s\n",
      "Accuracy total 0.933239, main loss classifier 0.286059, source accuracy 0.963068 source classification loss 0.105212, target accuracy 0.903409 target loss 0.258384 accuracy domain distinction 0.500000 loss domain distinction 1.042604,\n",
      "VALIDATION Loss: 0.17329047 Acc: 0.93074792\n",
      "Epoch 17 of 500 took 0.285s\n",
      "Accuracy total 0.943182, main loss classifier 0.275283, source accuracy 0.975142 source classification loss 0.091205, target accuracy 0.911222 target loss 0.251935 accuracy domain distinction 0.500000 loss domain distinction 1.037129,\n",
      "VALIDATION Loss: 0.11424226 Acc: 0.95844875\n",
      "New best validation loss:  0.11424225692947705\n",
      "Epoch 18 of 500 took 0.289s\n",
      "Accuracy total 0.947088, main loss classifier 0.265635, source accuracy 0.978693 source classification loss 0.074823, target accuracy 0.915483 target loss 0.247819 accuracy domain distinction 0.500000 loss domain distinction 1.043139,\n",
      "VALIDATION Loss: 0.12255437 Acc: 0.96121884\n",
      "Epoch 19 of 500 took 0.286s\n",
      "Accuracy total 0.943182, main loss classifier 0.269719, source accuracy 0.969460 source classification loss 0.100654, target accuracy 0.916903 target loss 0.230848 accuracy domain distinction 0.500000 loss domain distinction 1.039673,\n",
      "VALIDATION Loss: 0.11445200 Acc: 0.96398892\n",
      "Epoch 20 of 500 took 0.292s\n",
      "Accuracy total 0.939986, main loss classifier 0.272178, source accuracy 0.969460 source classification loss 0.097781, target accuracy 0.910511 target loss 0.239551 accuracy domain distinction 0.500000 loss domain distinction 1.035118,\n",
      "VALIDATION Loss: 0.16867869 Acc: 0.93351801\n",
      "Epoch 21 of 500 took 0.287s\n",
      "Accuracy total 0.947088, main loss classifier 0.267395, source accuracy 0.973722 source classification loss 0.088956, target accuracy 0.920455 target loss 0.238160 accuracy domain distinction 0.500000 loss domain distinction 1.038372,\n",
      "VALIDATION Loss: 0.17423664 Acc: 0.91966759\n",
      "Epoch 22 of 500 took 0.288s\n",
      "Accuracy total 0.948509, main loss classifier 0.265856, source accuracy 0.975142 source classification loss 0.081400, target accuracy 0.921875 target loss 0.242391 accuracy domain distinction 0.500000 loss domain distinction 1.039604,\n",
      "VALIDATION Loss: 0.11554998 Acc: 0.96952909\n",
      "Epoch 23 of 500 took 0.295s\n",
      "Accuracy total 0.951705, main loss classifier 0.249431, source accuracy 0.977273 source classification loss 0.075577, target accuracy 0.926136 target loss 0.214513 accuracy domain distinction 0.500000 loss domain distinction 1.043860,\n",
      "VALIDATION Loss: 0.11200055 Acc: 0.96398892\n",
      "New best validation loss:  0.11200055293738842\n",
      "Epoch 24 of 500 took 0.288s\n",
      "Accuracy total 0.949574, main loss classifier 0.251519, source accuracy 0.975142 source classification loss 0.073698, target accuracy 0.924006 target loss 0.222590 accuracy domain distinction 0.500000 loss domain distinction 1.033746,\n",
      "VALIDATION Loss: 0.13528774 Acc: 0.9501385\n",
      "Epoch 25 of 500 took 0.287s\n",
      "Accuracy total 0.949219, main loss classifier 0.265983, source accuracy 0.973011 source classification loss 0.086087, target accuracy 0.925426 target loss 0.237344 accuracy domain distinction 0.500000 loss domain distinction 1.042677,\n",
      "VALIDATION Loss: 0.12239857 Acc: 0.95290859\n",
      "Epoch 26 of 500 took 0.292s\n",
      "Accuracy total 0.953480, main loss classifier 0.242759, source accuracy 0.981534 source classification loss 0.059885, target accuracy 0.925426 target loss 0.217084 accuracy domain distinction 0.500000 loss domain distinction 1.042744,\n",
      "VALIDATION Loss: 0.12644349 Acc: 0.95290859\n",
      "Epoch 27 of 500 took 0.292s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy total 0.959162, main loss classifier 0.248485, source accuracy 0.985795 source classification loss 0.070089, target accuracy 0.932528 target loss 0.218450 accuracy domain distinction 0.500000 loss domain distinction 1.042155,\n",
      "VALIDATION Loss: 0.11220662 Acc: 0.96121884\n",
      "Epoch 28 of 500 took 0.289s\n",
      "Accuracy total 0.950284, main loss classifier 0.244478, source accuracy 0.976562 source classification loss 0.066202, target accuracy 0.924006 target loss 0.216219 accuracy domain distinction 0.500000 loss domain distinction 1.032672,\n",
      "VALIDATION Loss: 0.11152857 Acc: 0.96398892\n",
      "New best validation loss:  0.11152856796979904\n",
      "Epoch 29 of 500 took 0.288s\n",
      "Accuracy total 0.953125, main loss classifier 0.246119, source accuracy 0.978693 source classification loss 0.072853, target accuracy 0.927557 target loss 0.211581 accuracy domain distinction 0.500000 loss domain distinction 1.039020,\n",
      "VALIDATION Loss: 0.12497542 Acc: 0.96121884\n",
      "Epoch 30 of 500 took 0.289s\n",
      "Accuracy total 0.951349, main loss classifier 0.235346, source accuracy 0.970881 source classification loss 0.084273, target accuracy 0.931818 target loss 0.179011 accuracy domain distinction 0.500000 loss domain distinction 1.037039,\n",
      "VALIDATION Loss: 0.09321828 Acc: 0.97229917\n",
      "New best validation loss:  0.09321827565630277\n",
      "Epoch 31 of 500 took 0.287s\n",
      "Accuracy total 0.952415, main loss classifier 0.239645, source accuracy 0.975142 source classification loss 0.080141, target accuracy 0.929688 target loss 0.190714 accuracy domain distinction 0.500000 loss domain distinction 1.042171,\n",
      "VALIDATION Loss: 0.15351593 Acc: 0.94736842\n",
      "Epoch 32 of 500 took 0.287s\n",
      "Accuracy total 0.954545, main loss classifier 0.250537, source accuracy 0.978693 source classification loss 0.075118, target accuracy 0.930398 target loss 0.218522 accuracy domain distinction 0.500000 loss domain distinction 1.037169,\n",
      "VALIDATION Loss: 0.11327921 Acc: 0.96121884\n",
      "Epoch 33 of 500 took 0.289s\n",
      "Accuracy total 0.961648, main loss classifier 0.219969, source accuracy 0.983665 source classification loss 0.054537, target accuracy 0.939631 target loss 0.177418 accuracy domain distinction 0.500000 loss domain distinction 1.039920,\n",
      "VALIDATION Loss: 0.10071339 Acc: 0.966759\n",
      "Epoch 34 of 500 took 0.290s\n",
      "Accuracy total 0.957386, main loss classifier 0.230827, source accuracy 0.978693 source classification loss 0.067493, target accuracy 0.936080 target loss 0.187110 accuracy domain distinction 0.500000 loss domain distinction 1.035251,\n",
      "VALIDATION Loss: 0.12433506 Acc: 0.9501385\n",
      "Epoch 35 of 500 took 0.288s\n",
      "Accuracy total 0.958807, main loss classifier 0.232319, source accuracy 0.980114 source classification loss 0.070275, target accuracy 0.937500 target loss 0.186378 accuracy domain distinction 0.500000 loss domain distinction 1.039925,\n",
      "VALIDATION Loss: 0.08876288 Acc: 0.97783934\n",
      "New best validation loss:  0.08876287875076135\n",
      "Epoch 36 of 500 took 0.290s\n",
      "Accuracy total 0.956321, main loss classifier 0.230509, source accuracy 0.977273 source classification loss 0.068082, target accuracy 0.935369 target loss 0.184951 accuracy domain distinction 0.500000 loss domain distinction 1.039928,\n",
      "VALIDATION Loss: 0.11420239 Acc: 0.96398892\n",
      "Epoch 37 of 500 took 0.316s\n",
      "Accuracy total 0.964489, main loss classifier 0.219112, source accuracy 0.982955 source classification loss 0.061920, target accuracy 0.946023 target loss 0.169432 accuracy domain distinction 0.500000 loss domain distinction 1.034361,\n",
      "VALIDATION Loss: 0.10552641 Acc: 0.96398892\n",
      "Epoch 38 of 500 took 0.288s\n",
      "Accuracy total 0.958807, main loss classifier 0.234353, source accuracy 0.979403 source classification loss 0.066909, target accuracy 0.938210 target loss 0.194151 accuracy domain distinction 0.500000 loss domain distinction 1.038232,\n",
      "VALIDATION Loss: 0.10922530 Acc: 0.96952909\n",
      "Epoch 39 of 500 took 0.287s\n",
      "Accuracy total 0.960227, main loss classifier 0.228974, source accuracy 0.980114 source classification loss 0.069421, target accuracy 0.940341 target loss 0.180404 accuracy domain distinction 0.500000 loss domain distinction 1.040612,\n",
      "VALIDATION Loss: 0.11607329 Acc: 0.95844875\n",
      "Epoch 40 of 500 took 0.294s\n",
      "Accuracy total 0.959517, main loss classifier 0.220328, source accuracy 0.980114 source classification loss 0.059734, target accuracy 0.938920 target loss 0.172794 accuracy domain distinction 0.500000 loss domain distinction 1.040635,\n",
      "VALIDATION Loss: 0.14174597 Acc: 0.95290859\n",
      "Epoch 41 of 500 took 0.293s\n",
      "Accuracy total 0.958452, main loss classifier 0.227728, source accuracy 0.980114 source classification loss 0.064606, target accuracy 0.936790 target loss 0.184204 accuracy domain distinction 0.500000 loss domain distinction 1.033226,\n",
      "VALIDATION Loss: 0.10419384 Acc: 0.96398892\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0060e-04.\n",
      "Epoch 42 of 500 took 0.288s\n",
      "Accuracy total 0.958807, main loss classifier 0.238227, source accuracy 0.979403 source classification loss 0.064590, target accuracy 0.938210 target loss 0.204503 accuracy domain distinction 0.500000 loss domain distinction 1.036807,\n",
      "VALIDATION Loss: 0.09542073 Acc: 0.966759\n",
      "Epoch 43 of 500 took 0.292s\n",
      "Accuracy total 0.963423, main loss classifier 0.216100, source accuracy 0.986506 source classification loss 0.047369, target accuracy 0.940341 target loss 0.179188 accuracy domain distinction 0.500000 loss domain distinction 1.028216,\n",
      "VALIDATION Loss: 0.13086264 Acc: 0.96398892\n",
      "Epoch 44 of 500 took 0.289s\n",
      "Accuracy total 0.959162, main loss classifier 0.217551, source accuracy 0.981534 source classification loss 0.059667, target accuracy 0.936790 target loss 0.169241 accuracy domain distinction 0.500000 loss domain distinction 1.030967,\n",
      "VALIDATION Loss: 0.12634712 Acc: 0.95844875\n",
      "Epoch 45 of 500 took 0.286s\n",
      "Accuracy total 0.966619, main loss classifier 0.209011, source accuracy 0.991477 source classification loss 0.047016, target accuracy 0.941761 target loss 0.165858 accuracy domain distinction 0.500000 loss domain distinction 1.025735,\n",
      "VALIDATION Loss: 0.09735543 Acc: 0.97229917\n",
      "Epoch 46 of 500 took 0.287s\n",
      "Accuracy total 0.959162, main loss classifier 0.224711, source accuracy 0.981534 source classification loss 0.060223, target accuracy 0.936790 target loss 0.183627 accuracy domain distinction 0.500000 loss domain distinction 1.027858,\n",
      "VALIDATION Loss: 0.08741176 Acc: 0.97506925\n",
      "New best validation loss:  0.08741175755858421\n",
      "Epoch 47 of 500 took 0.290s\n",
      "Accuracy total 0.963423, main loss classifier 0.209889, source accuracy 0.980824 source classification loss 0.054650, target accuracy 0.946023 target loss 0.158894 accuracy domain distinction 0.500000 loss domain distinction 1.031167,\n",
      "VALIDATION Loss: 0.10571043 Acc: 0.96952909\n",
      "Epoch 48 of 500 took 0.290s\n",
      "Accuracy total 0.958452, main loss classifier 0.221413, source accuracy 0.977983 source classification loss 0.066496, target accuracy 0.938920 target loss 0.169999 accuracy domain distinction 0.500000 loss domain distinction 1.031651,\n",
      "VALIDATION Loss: 0.10383029 Acc: 0.96952909\n",
      "Epoch 49 of 500 took 0.288s\n",
      "Accuracy total 0.970881, main loss classifier 0.213408, source accuracy 0.988636 source classification loss 0.055914, target accuracy 0.953125 target loss 0.164280 accuracy domain distinction 0.500000 loss domain distinction 1.033115,\n",
      "VALIDATION Loss: 0.09757526 Acc: 0.97506925\n",
      "Epoch 50 of 500 took 0.290s\n",
      "Accuracy total 0.961648, main loss classifier 0.216424, source accuracy 0.987216 source classification loss 0.051936, target accuracy 0.936080 target loss 0.174984 accuracy domain distinction 0.500000 loss domain distinction 1.029645,\n",
      "VALIDATION Loss: 0.08174733 Acc: 0.97783934\n",
      "New best validation loss:  0.08174732979387045\n",
      "Epoch 51 of 500 took 0.293s\n",
      "Accuracy total 0.962713, main loss classifier 0.213847, source accuracy 0.989347 source classification loss 0.043487, target accuracy 0.936080 target loss 0.178133 accuracy domain distinction 0.500000 loss domain distinction 1.030365,\n",
      "VALIDATION Loss: 0.11315951 Acc: 0.96398892\n",
      "Epoch 52 of 500 took 0.286s\n",
      "Accuracy total 0.966619, main loss classifier 0.204296, source accuracy 0.987926 source classification loss 0.047064, target accuracy 0.945312 target loss 0.155805 accuracy domain distinction 0.500000 loss domain distinction 1.028613,\n",
      "VALIDATION Loss: 0.09442638 Acc: 0.97229917\n",
      "Epoch 53 of 500 took 0.288s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy total 0.966264, main loss classifier 0.208093, source accuracy 0.987926 source classification loss 0.049813, target accuracy 0.944602 target loss 0.159780 accuracy domain distinction 0.500000 loss domain distinction 1.032967,\n",
      "VALIDATION Loss: 0.09925724 Acc: 0.96952909\n",
      "Epoch 54 of 500 took 0.288s\n",
      "Accuracy total 0.963068, main loss classifier 0.211643, source accuracy 0.983665 source classification loss 0.053053, target accuracy 0.942472 target loss 0.163192 accuracy domain distinction 0.500000 loss domain distinction 1.035203,\n",
      "VALIDATION Loss: 0.11859609 Acc: 0.96121884\n",
      "Epoch 55 of 500 took 0.290s\n",
      "Accuracy total 0.965199, main loss classifier 0.210685, source accuracy 0.984375 source classification loss 0.054947, target accuracy 0.946023 target loss 0.161024 accuracy domain distinction 0.500000 loss domain distinction 1.026997,\n",
      "VALIDATION Loss: 0.11571856 Acc: 0.96398892\n",
      "Epoch 56 of 500 took 0.287s\n",
      "Accuracy total 0.966619, main loss classifier 0.193480, source accuracy 0.983665 source classification loss 0.046225, target accuracy 0.949574 target loss 0.135737 accuracy domain distinction 0.500000 loss domain distinction 1.024989,\n",
      "VALIDATION Loss: 0.11637602 Acc: 0.95567867\n",
      "Epoch    56: reducing learning rate of group 0 to 2.0120e-05.\n",
      "Epoch 57 of 500 took 0.303s\n",
      "Accuracy total 0.965199, main loss classifier 0.211140, source accuracy 0.982244 source classification loss 0.058780, target accuracy 0.948153 target loss 0.157038 accuracy domain distinction 0.500000 loss domain distinction 1.032312,\n",
      "VALIDATION Loss: 0.10010034 Acc: 0.966759\n",
      "Epoch 58 of 500 took 0.292s\n",
      "Accuracy total 0.964489, main loss classifier 0.211180, source accuracy 0.982955 source classification loss 0.056492, target accuracy 0.946023 target loss 0.160181 accuracy domain distinction 0.500000 loss domain distinction 1.028437,\n",
      "VALIDATION Loss: 0.09979449 Acc: 0.96952909\n",
      "Epoch 59 of 500 took 0.286s\n",
      "Accuracy total 0.961293, main loss classifier 0.212851, source accuracy 0.982955 source classification loss 0.052927, target accuracy 0.939631 target loss 0.167046 accuracy domain distinction 0.500000 loss domain distinction 1.028643,\n",
      "VALIDATION Loss: 0.09332547 Acc: 0.96952909\n",
      "Epoch 60 of 500 took 0.286s\n",
      "Accuracy total 0.969105, main loss classifier 0.200666, source accuracy 0.986506 source classification loss 0.050462, target accuracy 0.951705 target loss 0.144881 accuracy domain distinction 0.500000 loss domain distinction 1.029941,\n",
      "VALIDATION Loss: 0.10313965 Acc: 0.966759\n",
      "Epoch 61 of 500 took 0.291s\n",
      "Accuracy total 0.963778, main loss classifier 0.218217, source accuracy 0.982244 source classification loss 0.055813, target accuracy 0.945312 target loss 0.176113 accuracy domain distinction 0.500000 loss domain distinction 1.022543,\n",
      "VALIDATION Loss: 0.10904269 Acc: 0.94736842\n",
      "Epoch 62 of 500 took 0.292s\n",
      "Training complete in 0m 18s\n",
      "['participant_1', 'participant_2', 'participant_0']\n",
      "TSD_Network(\n",
      "  (_network): ModuleList(\n",
      "    (0): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=252, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=252, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (1): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (2): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (_output): Linear(in_features=200, out_features=22, bias=True)\n",
      "  (_output_discriminator): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "Number Parameters:  137624\n",
      "Optimizer =  <generator object Module.parameters at 0x7f08511c4ac0>\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/DANN/participant_0/best_state_2.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/DANN/participant_0/best_state_2.pt' (epoch 14)\n",
      "TSD_Network(\n",
      "  (_network): ModuleList(\n",
      "    (0): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=252, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=252, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (1): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (2): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (_output): Linear(in_features=200, out_features=22, bias=True)\n",
      "  (_output_discriminator): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "Number Parameters:  137624\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/TSD/participant_0/best_state_0.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/TSD/participant_0/best_state_0.pt' (epoch 15)\n",
      "TSD_Network(\n",
      "  (_network): ModuleList(\n",
      "    (0): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=252, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=252, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (1): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (2): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (_output): Linear(in_features=200, out_features=22, bias=True)\n",
      "  (_output_discriminator): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "Number Parameters:  137624\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/DANN/participant_0/best_state_1.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/DANN/participant_0/best_state_1.pt' (epoch 2)\n",
      "TSD_Network(\n",
      "  (_network): ModuleList(\n",
      "    (0): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=252, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=252, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (1): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (2): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (_output): Linear(in_features=200, out_features=22, bias=True)\n",
      "  (_output_discriminator): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "Number Parameters:  137624\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/DANN/participant_0/best_state_2.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/DANN/participant_0/best_state_2.pt' (epoch 14)\n",
      "==== models_array =  (3,)  @ session  2\n",
      "HANDLING NEW SESSION  0\n",
      "HANDLING NEW SESSION  1\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.0  len before:  26   len after:  6\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.6666666666666666  len before:  26   len after:  15\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.4230769230769231   AFTER:  1.0  len before:  26   len after:  11\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  2\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  1.0  len before:  26   len after:  8\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.0  len before:  26   len after:  4\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.6111111111111112  len before:  26   len after:  18\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.4230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.3076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  3\n",
      "BEFORE:  0.11538461538461539   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.19230769230769232  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  1.0  len before:  26   len after:  6\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.11538461538461539   AFTER:  0.0  len before:  26   len after:  1\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  1.0  len before:  26   len after:  2\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  6\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.0  len before:  26   len after:  17\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  4\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  1.0  len before:  26   len after:  4\n",
      "BEFORE:  0.5769230769230769   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.8333333333333334  len before:  26   len after:  6\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  4\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.6538461538461539  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  8\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.1111111111111111  len before:  26   len after:  9\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.07692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  9\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  1.0  len before:  26   len after:  13\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  17\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.5721153846153846   Accuracy pseudo: 0.6509433962264151  len pseudo:  1802    len predictions 2288\n",
      "HANDLING NEW SESSION  2\n",
      "Finish segment dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  1\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  18\n",
      "BEFORE:  0.3076923076923077   AFTER:  1.0  len before:  26   len after:  7\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  4\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.11538461538461539   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.4230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  1.0  len before:  26   len after:  3\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.5384615384615384  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.3076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.058823529411764705  len before:  26   len after:  17\n",
      "BEFORE:  0.23076923076923078   AFTER:  1.0  len before:  26   len after:  6\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  10\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  17\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  1\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.5384615384615384  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.23076923076923078  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.11538461538461539   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.11538461538461539   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  10\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.35294117647058826  len before:  26   len after:  17\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  18\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.5769230769230769  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  15\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  6\n",
      "BEFORE:  0.23076923076923078   AFTER:  1.0  len before:  26   len after:  11\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  1.0  len before:  26   len after:  13\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  9\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  15\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.5144230769230769   Accuracy pseudo: 0.5889830508474576  len pseudo:  1888    len predictions 2288\n",
      "STARTING TRAINING\n",
      "Accuracy total 0.804348, main loss classifier 0.788499, source accuracy 0.861413 source classification loss 0.463888, target accuracy 0.747283 target loss 0.885573 accuracy domain distinction 0.500000 loss domain distinction 1.137686,\n",
      "VALIDATION Loss: 0.35387969 Acc: 0.86772487\n",
      "New best validation loss:  0.3538796852032344\n",
      "Epoch 2 of 500 took 0.383s\n",
      "Accuracy total 0.843410, main loss classifier 0.649932, source accuracy 0.896739 source classification loss 0.333309, target accuracy 0.790082 target loss 0.755157 accuracy domain distinction 0.500000 loss domain distinction 1.056988,\n",
      "VALIDATION Loss: 0.31779353 Acc: 0.89417989\n",
      "New best validation loss:  0.3177935282389323\n",
      "Epoch 3 of 500 took 0.319s\n",
      "Accuracy total 0.841372, main loss classifier 0.598413, source accuracy 0.879755 source classification loss 0.364814, target accuracy 0.802989 target loss 0.624618 accuracy domain distinction 0.500000 loss domain distinction 1.036976,\n",
      "VALIDATION Loss: 0.28484944 Acc: 0.8968254\n",
      "New best validation loss:  0.2848494400580724\n",
      "Epoch 4 of 500 took 0.306s\n",
      "Accuracy total 0.851902, main loss classifier 0.583650, source accuracy 0.885870 source classification loss 0.351767, target accuracy 0.817935 target loss 0.608628 accuracy domain distinction 0.500000 loss domain distinction 1.034533,\n",
      "VALIDATION Loss: 0.29634150 Acc: 0.91534392\n",
      "Epoch 5 of 500 took 0.303s\n",
      "Accuracy total 0.852582, main loss classifier 0.572607, source accuracy 0.901495 source classification loss 0.304894, target accuracy 0.803668 target loss 0.635138 accuracy domain distinction 0.500000 loss domain distinction 1.025912,\n",
      "VALIDATION Loss: 0.30536986 Acc: 0.89153439\n",
      "Epoch 6 of 500 took 0.302s\n",
      "Accuracy total 0.858696, main loss classifier 0.555274, source accuracy 0.899457 source classification loss 0.303357, target accuracy 0.817935 target loss 0.601018 accuracy domain distinction 0.500000 loss domain distinction 1.030863,\n",
      "VALIDATION Loss: 0.26703141 Acc: 0.9021164\n",
      "New best validation loss:  0.26703141381343204\n",
      "Epoch 7 of 500 took 0.303s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy total 0.861753, main loss classifier 0.551136, source accuracy 0.889946 source classification loss 0.331121, target accuracy 0.833560 target loss 0.563140 accuracy domain distinction 0.500000 loss domain distinction 1.040052,\n",
      "VALIDATION Loss: 0.31313784 Acc: 0.9047619\n",
      "Epoch 8 of 500 took 0.304s\n",
      "Accuracy total 0.868886, main loss classifier 0.507955, source accuracy 0.908288 source classification loss 0.285284, target accuracy 0.829484 target loss 0.525021 accuracy domain distinction 0.500000 loss domain distinction 1.028022,\n",
      "VALIDATION Loss: 0.31368973 Acc: 0.88888889\n",
      "Epoch 9 of 500 took 0.303s\n",
      "Accuracy total 0.866508, main loss classifier 0.518680, source accuracy 0.904212 source classification loss 0.282142, target accuracy 0.828804 target loss 0.548371 accuracy domain distinction 0.500000 loss domain distinction 1.034233,\n",
      "VALIDATION Loss: 0.25916544 Acc: 0.9047619\n",
      "New best validation loss:  0.2591654360294342\n",
      "Epoch 10 of 500 took 0.305s\n",
      "Accuracy total 0.870584, main loss classifier 0.512240, source accuracy 0.891984 source classification loss 0.344319, target accuracy 0.849185 target loss 0.473440 accuracy domain distinction 0.500000 loss domain distinction 1.033603,\n",
      "VALIDATION Loss: 0.32693930 Acc: 0.88888889\n",
      "Epoch 11 of 500 took 0.302s\n",
      "Accuracy total 0.877717, main loss classifier 0.496114, source accuracy 0.921196 source classification loss 0.285428, target accuracy 0.834239 target loss 0.499309 accuracy domain distinction 0.500000 loss domain distinction 1.037454,\n",
      "VALIDATION Loss: 0.27368707 Acc: 0.8994709\n",
      "Epoch 12 of 500 took 0.306s\n",
      "Accuracy total 0.887568, main loss classifier 0.458277, source accuracy 0.908288 source classification loss 0.281250, target accuracy 0.866848 target loss 0.428470 accuracy domain distinction 0.500000 loss domain distinction 1.034166,\n",
      "VALIDATION Loss: 0.34000500 Acc: 0.87830688\n",
      "Epoch 13 of 500 took 0.304s\n",
      "Accuracy total 0.875000, main loss classifier 0.496503, source accuracy 0.896060 source classification loss 0.304841, target accuracy 0.853940 target loss 0.482701 accuracy domain distinction 0.500000 loss domain distinction 1.027317,\n",
      "VALIDATION Loss: 0.22129522 Acc: 0.91798942\n",
      "New best validation loss:  0.22129521518945694\n",
      "Epoch 14 of 500 took 0.304s\n",
      "Accuracy total 0.875679, main loss classifier 0.498332, source accuracy 0.904891 source classification loss 0.319551, target accuracy 0.846467 target loss 0.470690 accuracy domain distinction 0.500000 loss domain distinction 1.032117,\n",
      "VALIDATION Loss: 0.27051215 Acc: 0.89153439\n",
      "Epoch 15 of 500 took 0.309s\n",
      "Accuracy total 0.884851, main loss classifier 0.469714, source accuracy 0.915761 source classification loss 0.255882, target accuracy 0.853940 target loss 0.477445 accuracy domain distinction 0.500000 loss domain distinction 1.030508,\n",
      "VALIDATION Loss: 0.23188728 Acc: 0.9047619\n",
      "Epoch 16 of 500 took 0.304s\n",
      "Accuracy total 0.887228, main loss classifier 0.447634, source accuracy 0.915082 source classification loss 0.248592, target accuracy 0.859375 target loss 0.441331 accuracy domain distinction 0.500000 loss domain distinction 1.026730,\n",
      "VALIDATION Loss: 0.23269559 Acc: 0.92063492\n",
      "Epoch 17 of 500 took 0.302s\n",
      "Accuracy total 0.882473, main loss classifier 0.468599, source accuracy 0.904212 source classification loss 0.286586, target accuracy 0.860734 target loss 0.444419 accuracy domain distinction 0.500000 loss domain distinction 1.030973,\n",
      "VALIDATION Loss: 0.25204588 Acc: 0.91534392\n",
      "Epoch 18 of 500 took 0.302s\n",
      "Accuracy total 0.883492, main loss classifier 0.463826, source accuracy 0.923913 source classification loss 0.235871, target accuracy 0.843071 target loss 0.485297 accuracy domain distinction 0.500000 loss domain distinction 1.032417,\n",
      "VALIDATION Loss: 0.25138363 Acc: 0.91798942\n",
      "Epoch 19 of 500 took 0.302s\n",
      "Accuracy total 0.894361, main loss classifier 0.430895, source accuracy 0.911005 source classification loss 0.260863, target accuracy 0.877717 target loss 0.394823 accuracy domain distinction 0.500000 loss domain distinction 1.030526,\n",
      "VALIDATION Loss: 0.20403502 Acc: 0.92328042\n",
      "New best validation loss:  0.20403502136468887\n",
      "Epoch 20 of 500 took 0.306s\n",
      "Accuracy total 0.889606, main loss classifier 0.452541, source accuracy 0.923234 source classification loss 0.237017, target accuracy 0.855978 target loss 0.462045 accuracy domain distinction 0.500000 loss domain distinction 1.030095,\n",
      "VALIDATION Loss: 0.21704444 Acc: 0.92328042\n",
      "Epoch 21 of 500 took 0.300s\n",
      "Accuracy total 0.886889, main loss classifier 0.458145, source accuracy 0.917120 source classification loss 0.252489, target accuracy 0.856658 target loss 0.457991 accuracy domain distinction 0.500000 loss domain distinction 1.029052,\n",
      "VALIDATION Loss: 0.25160883 Acc: 0.91798942\n",
      "Epoch 22 of 500 took 0.303s\n",
      "Accuracy total 0.889266, main loss classifier 0.442006, source accuracy 0.913043 source classification loss 0.270733, target accuracy 0.865489 target loss 0.407805 accuracy domain distinction 0.500000 loss domain distinction 1.027374,\n",
      "VALIDATION Loss: 0.23337637 Acc: 0.91534392\n",
      "Epoch 23 of 500 took 0.303s\n",
      "Accuracy total 0.901495, main loss classifier 0.418568, source accuracy 0.918478 source classification loss 0.246328, target accuracy 0.884511 target loss 0.384013 accuracy domain distinction 0.500000 loss domain distinction 1.033973,\n",
      "VALIDATION Loss: 0.23584488 Acc: 0.91005291\n",
      "Epoch 24 of 500 took 0.303s\n",
      "Accuracy total 0.895041, main loss classifier 0.438491, source accuracy 0.923234 source classification loss 0.242929, target accuracy 0.866848 target loss 0.428058 accuracy domain distinction 0.500000 loss domain distinction 1.029973,\n",
      "VALIDATION Loss: 0.20716285 Acc: 0.93915344\n",
      "Epoch 25 of 500 took 0.305s\n",
      "Accuracy total 0.895720, main loss classifier 0.420709, source accuracy 0.914402 source classification loss 0.263378, target accuracy 0.877038 target loss 0.372996 accuracy domain distinction 0.500000 loss domain distinction 1.025217,\n",
      "VALIDATION Loss: 0.22038702 Acc: 0.91534392\n",
      "Epoch    25: reducing learning rate of group 0 to 2.0120e-05.\n",
      "Epoch 26 of 500 took 0.304s\n",
      "Accuracy total 0.907609, main loss classifier 0.402827, source accuracy 0.933424 source classification loss 0.210733, target accuracy 0.881793 target loss 0.388699 accuracy domain distinction 0.500000 loss domain distinction 1.031110,\n",
      "VALIDATION Loss: 0.21350108 Acc: 0.92328042\n",
      "Epoch 27 of 500 took 0.301s\n",
      "Accuracy total 0.901155, main loss classifier 0.395701, source accuracy 0.917120 source classification loss 0.232030, target accuracy 0.885190 target loss 0.353408 accuracy domain distinction 0.500000 loss domain distinction 1.029818,\n",
      "VALIDATION Loss: 0.18177559 Acc: 0.92857143\n",
      "New best validation loss:  0.18177559102574983\n",
      "Epoch 28 of 500 took 0.308s\n",
      "Accuracy total 0.898098, main loss classifier 0.407363, source accuracy 0.933424 source classification loss 0.205549, target accuracy 0.862772 target loss 0.404177 accuracy domain distinction 0.500000 loss domain distinction 1.025001,\n",
      "VALIDATION Loss: 0.22290890 Acc: 0.92857143\n",
      "Epoch 29 of 500 took 0.302s\n",
      "Accuracy total 0.888927, main loss classifier 0.425573, source accuracy 0.906929 source classification loss 0.260777, target accuracy 0.870924 target loss 0.384575 accuracy domain distinction 0.500000 loss domain distinction 1.028969,\n",
      "VALIDATION Loss: 0.21390697 Acc: 0.91798942\n",
      "Epoch 30 of 500 took 0.305s\n",
      "Accuracy total 0.897758, main loss classifier 0.419659, source accuracy 0.923913 source classification loss 0.224946, target accuracy 0.871603 target loss 0.408859 accuracy domain distinction 0.500000 loss domain distinction 1.027564,\n",
      "VALIDATION Loss: 0.18472924 Acc: 0.93650794\n",
      "Epoch 31 of 500 took 0.306s\n",
      "Accuracy total 0.888927, main loss classifier 0.437167, source accuracy 0.913723 source classification loss 0.246376, target accuracy 0.864130 target loss 0.422916 accuracy domain distinction 0.500000 loss domain distinction 1.025208,\n",
      "VALIDATION Loss: 0.20007911 Acc: 0.92328042\n",
      "Epoch 32 of 500 took 0.301s\n",
      "Accuracy total 0.908967, main loss classifier 0.381570, source accuracy 0.938179 source classification loss 0.183060, target accuracy 0.879755 target loss 0.374252 accuracy domain distinction 0.500000 loss domain distinction 1.029138,\n",
      "VALIDATION Loss: 0.20512145 Acc: 0.93915344\n",
      "Epoch 33 of 500 took 0.303s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy total 0.903193, main loss classifier 0.405298, source accuracy 0.928668 source classification loss 0.225025, target accuracy 0.877717 target loss 0.379600 accuracy domain distinction 0.500000 loss domain distinction 1.029860,\n",
      "VALIDATION Loss: 0.19796951 Acc: 0.92592593\n",
      "Epoch    33: reducing learning rate of group 0 to 4.0240e-06.\n",
      "Epoch 34 of 500 took 0.302s\n",
      "Accuracy total 0.900815, main loss classifier 0.417394, source accuracy 0.925951 source classification loss 0.221505, target accuracy 0.875679 target loss 0.407658 accuracy domain distinction 0.500000 loss domain distinction 1.028122,\n",
      "VALIDATION Loss: 0.19390251 Acc: 0.92857143\n",
      "Epoch 35 of 500 took 0.305s\n",
      "Accuracy total 0.892663, main loss classifier 0.429473, source accuracy 0.916440 source classification loss 0.255856, target accuracy 0.868886 target loss 0.397451 accuracy domain distinction 0.500000 loss domain distinction 1.028199,\n",
      "VALIDATION Loss: 0.20592755 Acc: 0.94179894\n",
      "Epoch 36 of 500 took 0.302s\n",
      "Accuracy total 0.906250, main loss classifier 0.384299, source accuracy 0.932745 source classification loss 0.209567, target accuracy 0.879755 target loss 0.353520 accuracy domain distinction 0.500000 loss domain distinction 1.027549,\n",
      "VALIDATION Loss: 0.20064724 Acc: 0.92857143\n",
      "Epoch 37 of 500 took 0.301s\n",
      "Accuracy total 0.890625, main loss classifier 0.428860, source accuracy 0.922554 source classification loss 0.231902, target accuracy 0.858696 target loss 0.419933 accuracy domain distinction 0.500000 loss domain distinction 1.029424,\n",
      "VALIDATION Loss: 0.22252158 Acc: 0.91798942\n",
      "Epoch 38 of 500 took 0.301s\n",
      "Accuracy total 0.898098, main loss classifier 0.405179, source accuracy 0.915761 source classification loss 0.229966, target accuracy 0.880435 target loss 0.374837 accuracy domain distinction 0.500000 loss domain distinction 1.027782,\n",
      "VALIDATION Loss: 0.21580243 Acc: 0.92328042\n",
      "Epoch 39 of 500 took 0.301s\n",
      "Training complete in 0m 12s\n",
      "['participant_1', 'participant_2', 'participant_0']\n",
      "TSD_Network(\n",
      "  (_network): ModuleList(\n",
      "    (0): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=252, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=252, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (1): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (2): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (_output): Linear(in_features=200, out_features=22, bias=True)\n",
      "  (_output_discriminator): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "Number Parameters:  137624\n",
      "Optimizer =  <generator object Module.parameters at 0x7f08511c4820>\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/DANN/participant_1/best_state_1.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/DANN/participant_1/best_state_1.pt' (epoch 2)\n",
      "TSD_Network(\n",
      "  (_network): ModuleList(\n",
      "    (0): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=252, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=252, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (1): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (2): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (_output): Linear(in_features=200, out_features=22, bias=True)\n",
      "  (_output_discriminator): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "Number Parameters:  137624\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/TSD/participant_1/best_state_0.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/TSD/participant_1/best_state_0.pt' (epoch 21)\n",
      "TSD_Network(\n",
      "  (_network): ModuleList(\n",
      "    (0): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=252, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=252, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (1): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (2): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (_output): Linear(in_features=200, out_features=22, bias=True)\n",
      "  (_output_discriminator): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "Number Parameters:  137624\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/DANN/participant_1/best_state_1.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/DANN/participant_1/best_state_1.pt' (epoch 2)\n",
      "==== models_array =  (2,)  @ session  1\n",
      "HANDLING NEW SESSION  0\n",
      "HANDLING NEW SESSION  1\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  9\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.0  len before:  26   len after:  17\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  10\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.5769230769230769  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  9\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  7\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.0  len before:  26   len after:  10\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.5769230769230769  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.8509615384615384   Accuracy pseudo: 0.9029261495587553  len pseudo:  2153    len predictions 2288\n",
      "STARTING TRAINING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy total 0.920373, main loss classifier 0.358233, source accuracy 0.953726 source classification loss 0.142784, target accuracy 0.887019 target loss 0.336347 accuracy domain distinction 0.499399 loss domain distinction 1.186683,\n",
      "VALIDATION Loss: 0.20694298 Acc: 0.92343387\n",
      "New best validation loss:  0.2069429819072996\n",
      "Epoch 2 of 500 took 0.340s\n",
      "Accuracy total 0.931490, main loss classifier 0.323008, source accuracy 0.950120 source classification loss 0.152678, target accuracy 0.912861 target loss 0.265069 accuracy domain distinction 0.497897 loss domain distinction 1.141341,\n",
      "VALIDATION Loss: 0.12131898 Acc: 0.95823666\n",
      "New best validation loss:  0.12131898424455098\n",
      "Epoch 3 of 500 took 0.342s\n",
      "Accuracy total 0.938401, main loss classifier 0.303023, source accuracy 0.948317 source classification loss 0.141811, target accuracy 0.928486 target loss 0.244887 accuracy domain distinction 0.499099 loss domain distinction 1.096736,\n",
      "VALIDATION Loss: 0.06477715 Acc: 0.97911833\n",
      "New best validation loss:  0.0647771456944091\n",
      "Epoch 4 of 500 took 0.343s\n",
      "Accuracy total 0.947115, main loss classifier 0.281186, source accuracy 0.953125 source classification loss 0.137141, target accuracy 0.941106 target loss 0.206426 accuracy domain distinction 0.500300 loss domain distinction 1.094030,\n",
      "VALIDATION Loss: 0.08158978 Acc: 0.97447796\n",
      "Epoch 5 of 500 took 0.344s\n",
      "Accuracy total 0.942608, main loss classifier 0.275182, source accuracy 0.957332 source classification loss 0.121988, target accuracy 0.927885 target loss 0.212202 accuracy domain distinction 0.500601 loss domain distinction 1.080871,\n",
      "VALIDATION Loss: 0.09165712 Acc: 0.9675174\n",
      "Epoch 6 of 500 took 0.341s\n",
      "Accuracy total 0.947115, main loss classifier 0.269157, source accuracy 0.954327 source classification loss 0.138496, target accuracy 0.939904 target loss 0.183170 accuracy domain distinction 0.500300 loss domain distinction 1.083236,\n",
      "VALIDATION Loss: 0.07635590 Acc: 0.97215777\n",
      "Epoch 7 of 500 took 0.342s\n",
      "Accuracy total 0.947716, main loss classifier 0.265260, source accuracy 0.958534 source classification loss 0.116124, target accuracy 0.936899 target loss 0.200671 accuracy domain distinction 0.499700 loss domain distinction 1.068621,\n",
      "VALIDATION Loss: 0.08541520 Acc: 0.96983759\n",
      "Epoch 8 of 500 took 0.344s\n",
      "Accuracy total 0.953726, main loss classifier 0.254002, source accuracy 0.965144 source classification loss 0.113280, target accuracy 0.942308 target loss 0.182029 accuracy domain distinction 0.499399 loss domain distinction 1.063473,\n",
      "VALIDATION Loss: 0.07084609 Acc: 0.97679814\n",
      "Epoch 9 of 500 took 0.339s\n",
      "Accuracy total 0.951022, main loss classifier 0.255085, source accuracy 0.961538 source classification loss 0.109677, target accuracy 0.940505 target loss 0.189824 accuracy domain distinction 0.500000 loss domain distinction 1.053345,\n",
      "VALIDATION Loss: 0.04691213 Acc: 0.98607889\n",
      "New best validation loss:  0.04691213369369507\n",
      "Epoch 10 of 500 took 0.342s\n",
      "Accuracy total 0.957031, main loss classifier 0.247867, source accuracy 0.965745 source classification loss 0.109327, target accuracy 0.948317 target loss 0.176987 accuracy domain distinction 0.500000 loss domain distinction 1.047097,\n",
      "VALIDATION Loss: 0.07467575 Acc: 0.97679814\n",
      "Epoch 11 of 500 took 0.340s\n",
      "Accuracy total 0.955529, main loss classifier 0.243341, source accuracy 0.964543 source classification loss 0.107178, target accuracy 0.946514 target loss 0.170004 accuracy domain distinction 0.500000 loss domain distinction 1.047501,\n",
      "VALIDATION Loss: 0.06886840 Acc: 0.97679814\n",
      "Epoch 12 of 500 took 0.339s\n",
      "Accuracy total 0.955829, main loss classifier 0.234389, source accuracy 0.959736 source classification loss 0.118447, target accuracy 0.951923 target loss 0.139737 accuracy domain distinction 0.500000 loss domain distinction 1.052971,\n",
      "VALIDATION Loss: 0.04708165 Acc: 0.98143852\n",
      "Epoch 13 of 500 took 0.342s\n",
      "Accuracy total 0.956430, main loss classifier 0.232696, source accuracy 0.963341 source classification loss 0.097407, target accuracy 0.949519 target loss 0.158324 accuracy domain distinction 0.500000 loss domain distinction 1.048306,\n",
      "VALIDATION Loss: 0.05922686 Acc: 0.9837587\n",
      "Epoch 14 of 500 took 0.341s\n",
      "Accuracy total 0.959736, main loss classifier 0.230332, source accuracy 0.966947 source classification loss 0.100771, target accuracy 0.952524 target loss 0.149891 accuracy domain distinction 0.500000 loss domain distinction 1.050011,\n",
      "VALIDATION Loss: 0.05115150 Acc: 0.98607889\n",
      "Epoch 15 of 500 took 0.339s\n",
      "Accuracy total 0.957031, main loss classifier 0.230436, source accuracy 0.959736 source classification loss 0.111566, target accuracy 0.954327 target loss 0.140032 accuracy domain distinction 0.500000 loss domain distinction 1.046374,\n",
      "VALIDATION Loss: 0.07360626 Acc: 0.97679814\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0060e-04.\n",
      "Epoch 16 of 500 took 0.342s\n",
      "Accuracy total 0.962440, main loss classifier 0.213559, source accuracy 0.965745 source classification loss 0.091291, target accuracy 0.959135 target loss 0.130553 accuracy domain distinction 0.500000 loss domain distinction 1.026370,\n",
      "VALIDATION Loss: 0.07315722 Acc: 0.97679814\n",
      "Epoch 17 of 500 took 0.340s\n",
      "Accuracy total 0.960938, main loss classifier 0.216203, source accuracy 0.966346 source classification loss 0.088666, target accuracy 0.955529 target loss 0.136505 accuracy domain distinction 0.500000 loss domain distinction 1.036179,\n",
      "VALIDATION Loss: 0.05760244 Acc: 0.9837587\n",
      "Epoch 18 of 500 took 0.339s\n",
      "Accuracy total 0.965745, main loss classifier 0.215315, source accuracy 0.968750 source classification loss 0.092571, target accuracy 0.962740 target loss 0.130191 accuracy domain distinction 0.500000 loss domain distinction 1.039342,\n",
      "VALIDATION Loss: 0.04972824 Acc: 0.98839907\n",
      "Epoch 19 of 500 took 0.345s\n",
      "Accuracy total 0.964543, main loss classifier 0.214204, source accuracy 0.971154 source classification loss 0.081002, target accuracy 0.957933 target loss 0.140991 accuracy domain distinction 0.500000 loss domain distinction 1.032072,\n",
      "VALIDATION Loss: 0.03739436 Acc: 0.99071926\n",
      "New best validation loss:  0.03739436323355351\n",
      "Epoch 20 of 500 took 0.346s\n",
      "Accuracy total 0.971755, main loss classifier 0.191456, source accuracy 0.979567 source classification loss 0.068231, target accuracy 0.963942 target loss 0.108145 accuracy domain distinction 0.500000 loss domain distinction 1.032680,\n",
      "VALIDATION Loss: 0.05351400 Acc: 0.98143852\n",
      "Epoch 21 of 500 took 0.338s\n",
      "Accuracy total 0.969952, main loss classifier 0.200883, source accuracy 0.977163 source classification loss 0.073544, target accuracy 0.962740 target loss 0.121783 accuracy domain distinction 0.500000 loss domain distinction 1.032195,\n",
      "VALIDATION Loss: 0.05366841 Acc: 0.97911833\n",
      "Epoch 22 of 500 took 0.343s\n",
      "Accuracy total 0.975962, main loss classifier 0.181637, source accuracy 0.980769 source classification loss 0.065872, target accuracy 0.971154 target loss 0.091007 accuracy domain distinction 0.500000 loss domain distinction 1.031968,\n",
      "VALIDATION Loss: 0.06614220 Acc: 0.98143852\n",
      "Epoch 23 of 500 took 0.346s\n",
      "Accuracy total 0.966947, main loss classifier 0.200318, source accuracy 0.971755 source classification loss 0.081277, target accuracy 0.962139 target loss 0.113059 accuracy domain distinction 0.500000 loss domain distinction 1.031498,\n",
      "VALIDATION Loss: 0.05938670 Acc: 0.98143852\n",
      "Epoch 24 of 500 took 0.338s\n",
      "Accuracy total 0.970553, main loss classifier 0.203333, source accuracy 0.973558 source classification loss 0.079764, target accuracy 0.967548 target loss 0.120018 accuracy domain distinction 0.500000 loss domain distinction 1.034422,\n",
      "VALIDATION Loss: 0.08670445 Acc: 0.97215777\n",
      "Epoch 25 of 500 took 0.342s\n",
      "Accuracy total 0.969050, main loss classifier 0.195974, source accuracy 0.979567 source classification loss 0.059202, target accuracy 0.958534 target loss 0.126103 accuracy domain distinction 0.500000 loss domain distinction 1.033211,\n",
      "VALIDATION Loss: 0.05380399 Acc: 0.98143852\n",
      "Epoch    25: reducing learning rate of group 0 to 2.0120e-05.\n",
      "Epoch 26 of 500 took 0.340s\n",
      "Accuracy total 0.965144, main loss classifier 0.204734, source accuracy 0.972356 source classification loss 0.076679, target accuracy 0.957933 target loss 0.126741 accuracy domain distinction 0.500000 loss domain distinction 1.030245,\n",
      "VALIDATION Loss: 0.04315190 Acc: 0.9837587\n",
      "Epoch 27 of 500 took 0.339s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy total 0.970252, main loss classifier 0.192462, source accuracy 0.974760 source classification loss 0.079522, target accuracy 0.965745 target loss 0.099299 accuracy domain distinction 0.500000 loss domain distinction 1.030515,\n",
      "VALIDATION Loss: 0.06217261 Acc: 0.97911833\n",
      "Epoch 28 of 500 took 0.342s\n",
      "Accuracy total 0.972356, main loss classifier 0.193411, source accuracy 0.980769 source classification loss 0.073090, target accuracy 0.963942 target loss 0.107903 accuracy domain distinction 0.500000 loss domain distinction 1.029147,\n",
      "VALIDATION Loss: 0.07046738 Acc: 0.97679814\n",
      "Epoch 29 of 500 took 0.338s\n",
      "Accuracy total 0.974760, main loss classifier 0.192617, source accuracy 0.979567 source classification loss 0.065193, target accuracy 0.969952 target loss 0.113380 accuracy domain distinction 0.500000 loss domain distinction 1.033305,\n",
      "VALIDATION Loss: 0.05430312 Acc: 0.98143852\n",
      "Epoch 30 of 500 took 0.349s\n",
      "Accuracy total 0.970553, main loss classifier 0.196538, source accuracy 0.979567 source classification loss 0.067222, target accuracy 0.961538 target loss 0.119794 accuracy domain distinction 0.500000 loss domain distinction 1.030306,\n",
      "VALIDATION Loss: 0.04419158 Acc: 0.9837587\n",
      "Epoch 31 of 500 took 0.368s\n",
      "Training complete in 0m 10s\n",
      "['participant_1', 'participant_2', 'participant_0']\n",
      "TSD_Network(\n",
      "  (_network): ModuleList(\n",
      "    (0): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=252, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=252, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (1): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (2): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (_output): Linear(in_features=200, out_features=22, bias=True)\n",
      "  (_output_discriminator): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "Number Parameters:  137624\n",
      "Optimizer =  <generator object Module.parameters at 0x7f08511c4ac0>\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/DANN/participant_1/best_state_2.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/DANN/participant_1/best_state_2.pt' (epoch 21)\n",
      "TSD_Network(\n",
      "  (_network): ModuleList(\n",
      "    (0): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=252, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=252, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (1): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (2): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (_output): Linear(in_features=200, out_features=22, bias=True)\n",
      "  (_output_discriminator): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "Number Parameters:  137624\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/TSD/participant_1/best_state_0.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/TSD/participant_1/best_state_0.pt' (epoch 21)\n",
      "TSD_Network(\n",
      "  (_network): ModuleList(\n",
      "    (0): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=252, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=252, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (1): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (2): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (_output): Linear(in_features=200, out_features=22, bias=True)\n",
      "  (_output_discriminator): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "Number Parameters:  137624\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/DANN/participant_1/best_state_1.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/DANN/participant_1/best_state_1.pt' (epoch 2)\n",
      "TSD_Network(\n",
      "  (_network): ModuleList(\n",
      "    (0): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=252, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=252, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (1): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (2): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (_output): Linear(in_features=200, out_features=22, bias=True)\n",
      "  (_output_discriminator): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "Number Parameters:  137624\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/DANN/participant_1/best_state_2.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/DANN/participant_1/best_state_2.pt' (epoch 21)\n",
      "==== models_array =  (3,)  @ session  2\n",
      "HANDLING NEW SESSION  0\n",
      "HANDLING NEW SESSION  1\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  1.0  len before:  26   len after:  9\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.0  len before:  26   len after:  17\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  10\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.5769230769230769  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  9\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  7\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.0  len before:  26   len after:  10\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.5769230769230769  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.8509615384615384   Accuracy pseudo: 0.9029261495587553  len pseudo:  2153    len predictions 2288\n",
      "HANDLING NEW SESSION  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  10\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.5  len before:  26   len after:  2\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.7  len before:  26   len after:  10\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.19230769230769232  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.11538461538461539   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.5769230769230769  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  6\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  15\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  15\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.11538461538461539   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.9  len before:  26   len after:  10\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  12\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.0  len before:  26   len after:  12\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  1.0  len before:  26   len after:  16\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  3\n",
      "BEFORE:  0.5   AFTER:  1.0  len before:  26   len after:  12\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.7513111888111889   Accuracy pseudo: 0.80634310427679  len pseudo:  2081    len predictions 2288\n",
      "STARTING TRAINING\n",
      "Accuracy total 0.868690, main loss classifier 0.505007, source accuracy 0.905048 source classification loss 0.297803, target accuracy 0.832332 target loss 0.473538 accuracy domain distinction 0.499399 loss domain distinction 1.193362,\n",
      "VALIDATION Loss: 0.12756293 Acc: 0.9616307\n",
      "New best validation loss:  0.12756293320230075\n",
      "Epoch 2 of 500 took 0.436s\n",
      "Accuracy total 0.891827, main loss classifier 0.428253, source accuracy 0.921274 source classification loss 0.215590, target accuracy 0.862380 target loss 0.419330 accuracy domain distinction 0.500300 loss domain distinction 1.107934,\n",
      "VALIDATION Loss: 0.21388713 Acc: 0.92326139\n",
      "Epoch 3 of 500 took 0.378s\n",
      "Accuracy total 0.911058, main loss classifier 0.388376, source accuracy 0.931490 source classification loss 0.204967, target accuracy 0.890625 target loss 0.356396 accuracy domain distinction 0.499700 loss domain distinction 1.076941,\n",
      "VALIDATION Loss: 0.14869472 Acc: 0.95923261\n",
      "Epoch 4 of 500 took 0.357s\n",
      "Accuracy total 0.915565, main loss classifier 0.362671, source accuracy 0.932091 source classification loss 0.200538, target accuracy 0.899038 target loss 0.310297 accuracy domain distinction 0.499099 loss domain distinction 1.072540,\n",
      "VALIDATION Loss: 0.09965660 Acc: 0.97601918\n",
      "New best validation loss:  0.09965659997292928\n",
      "Epoch 5 of 500 took 0.387s\n",
      "Accuracy total 0.924880, main loss classifier 0.338927, source accuracy 0.942308 source classification loss 0.183130, target accuracy 0.907452 target loss 0.279897 accuracy domain distinction 0.501202 loss domain distinction 1.074127,\n",
      "VALIDATION Loss: 0.09139173 Acc: 0.97601918\n",
      "New best validation loss:  0.09139173211795944\n",
      "Epoch 6 of 500 took 0.388s\n",
      "Accuracy total 0.916466, main loss classifier 0.360108, source accuracy 0.930288 source classification loss 0.211811, target accuracy 0.902644 target loss 0.294788 accuracy domain distinction 0.500000 loss domain distinction 1.068083,\n",
      "VALIDATION Loss: 0.09741934 Acc: 0.97122302\n",
      "Epoch 7 of 500 took 0.458s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy total 0.914363, main loss classifier 0.358542, source accuracy 0.927284 source classification loss 0.201752, target accuracy 0.901442 target loss 0.301729 accuracy domain distinction 0.499399 loss domain distinction 1.068017,\n",
      "VALIDATION Loss: 0.08886771 Acc: 0.9736211\n",
      "New best validation loss:  0.08886770930673395\n",
      "Epoch 8 of 500 took 0.362s\n",
      "Accuracy total 0.923077, main loss classifier 0.334834, source accuracy 0.935096 source classification loss 0.185062, target accuracy 0.911058 target loss 0.270852 accuracy domain distinction 0.500601 loss domain distinction 1.068769,\n",
      "VALIDATION Loss: 0.08606160 Acc: 0.97122302\n",
      "New best validation loss:  0.08606160112789699\n",
      "Epoch 9 of 500 took 0.367s\n",
      "Accuracy total 0.930288, main loss classifier 0.315346, source accuracy 0.941106 source classification loss 0.185704, target accuracy 0.919471 target loss 0.231991 accuracy domain distinction 0.499399 loss domain distinction 1.064984,\n",
      "VALIDATION Loss: 0.12750430 Acc: 0.95443645\n",
      "Epoch 10 of 500 took 0.400s\n",
      "Accuracy total 0.930589, main loss classifier 0.337678, source accuracy 0.954327 source classification loss 0.170328, target accuracy 0.906851 target loss 0.291271 accuracy domain distinction 0.500000 loss domain distinction 1.068778,\n",
      "VALIDATION Loss: 0.17570446 Acc: 0.94004796\n",
      "Epoch 11 of 500 took 0.437s\n",
      "Accuracy total 0.926983, main loss classifier 0.328784, source accuracy 0.942909 source classification loss 0.183284, target accuracy 0.911058 target loss 0.261424 accuracy domain distinction 0.500000 loss domain distinction 1.064302,\n",
      "VALIDATION Loss: 0.20304863 Acc: 0.92805755\n",
      "Epoch 12 of 500 took 0.432s\n",
      "Accuracy total 0.927885, main loss classifier 0.316102, source accuracy 0.938101 source classification loss 0.185011, target accuracy 0.917668 target loss 0.234121 accuracy domain distinction 0.500300 loss domain distinction 1.065357,\n",
      "VALIDATION Loss: 0.09878989 Acc: 0.96882494\n",
      "Epoch 13 of 500 took 0.401s\n",
      "Accuracy total 0.919471, main loss classifier 0.332109, source accuracy 0.936298 source classification loss 0.175658, target accuracy 0.902644 target loss 0.278854 accuracy domain distinction 0.499700 loss domain distinction 1.048528,\n",
      "VALIDATION Loss: 0.09805405 Acc: 0.97841727\n",
      "Epoch 14 of 500 took 0.341s\n",
      "Accuracy total 0.931190, main loss classifier 0.305660, source accuracy 0.947115 source classification loss 0.158603, target accuracy 0.915264 target loss 0.240726 accuracy domain distinction 0.501202 loss domain distinction 1.059955,\n",
      "VALIDATION Loss: 0.08031748 Acc: 0.98081535\n",
      "New best validation loss:  0.08031747756259781\n",
      "Epoch 15 of 500 took 0.388s\n",
      "Accuracy total 0.939904, main loss classifier 0.300532, source accuracy 0.953726 source classification loss 0.148213, target accuracy 0.926082 target loss 0.241724 accuracy domain distinction 0.500000 loss domain distinction 1.055634,\n",
      "VALIDATION Loss: 0.13526000 Acc: 0.9736211\n",
      "Epoch 16 of 500 took 0.370s\n",
      "Accuracy total 0.932692, main loss classifier 0.311235, source accuracy 0.950721 source classification loss 0.167557, target accuracy 0.914663 target loss 0.243344 accuracy domain distinction 0.500300 loss domain distinction 1.057845,\n",
      "VALIDATION Loss: 0.09419146 Acc: 0.97841727\n",
      "Epoch 17 of 500 took 0.380s\n",
      "Accuracy total 0.934796, main loss classifier 0.308124, source accuracy 0.944712 source classification loss 0.168980, target accuracy 0.924880 target loss 0.236635 accuracy domain distinction 0.500000 loss domain distinction 1.053163,\n",
      "VALIDATION Loss: 0.10868533 Acc: 0.96882494\n",
      "Epoch 18 of 500 took 0.435s\n",
      "Accuracy total 0.930889, main loss classifier 0.318269, source accuracy 0.941106 source classification loss 0.177183, target accuracy 0.920673 target loss 0.248500 accuracy domain distinction 0.499399 loss domain distinction 1.054272,\n",
      "VALIDATION Loss: 0.08201367 Acc: 0.98081535\n",
      "Epoch 19 of 500 took 0.390s\n",
      "Accuracy total 0.933894, main loss classifier 0.294312, source accuracy 0.945913 source classification loss 0.149557, target accuracy 0.921875 target loss 0.227443 accuracy domain distinction 0.499099 loss domain distinction 1.058119,\n",
      "VALIDATION Loss: 0.13362374 Acc: 0.94964029\n",
      "Epoch 20 of 500 took 0.388s\n",
      "Accuracy total 0.939303, main loss classifier 0.287080, source accuracy 0.951923 source classification loss 0.147566, target accuracy 0.926683 target loss 0.217136 accuracy domain distinction 0.500300 loss domain distinction 1.047294,\n",
      "VALIDATION Loss: 0.10438099 Acc: 0.96642686\n",
      "Epoch    20: reducing learning rate of group 0 to 2.0120e-05.\n",
      "Epoch 21 of 500 took 0.342s\n",
      "Accuracy total 0.944712, main loss classifier 0.280471, source accuracy 0.951923 source classification loss 0.150368, target accuracy 0.937500 target loss 0.201859 accuracy domain distinction 0.500000 loss domain distinction 1.043578,\n",
      "VALIDATION Loss: 0.11652338 Acc: 0.97601918\n",
      "Epoch 22 of 500 took 0.391s\n",
      "Accuracy total 0.939603, main loss classifier 0.279119, source accuracy 0.953125 source classification loss 0.143010, target accuracy 0.926082 target loss 0.205847 accuracy domain distinction 0.500000 loss domain distinction 1.046899,\n",
      "VALIDATION Loss: 0.15248122 Acc: 0.94724221\n",
      "Epoch 23 of 500 took 0.345s\n",
      "Accuracy total 0.942608, main loss classifier 0.282906, source accuracy 0.956130 source classification loss 0.151683, target accuracy 0.929087 target loss 0.203689 accuracy domain distinction 0.500000 loss domain distinction 1.052202,\n",
      "VALIDATION Loss: 0.14074502 Acc: 0.94724221\n",
      "Epoch 24 of 500 took 0.343s\n",
      "Accuracy total 0.939603, main loss classifier 0.307760, source accuracy 0.950721 source classification loss 0.176878, target accuracy 0.928486 target loss 0.229645 accuracy domain distinction 0.500300 loss domain distinction 1.044991,\n",
      "VALIDATION Loss: 0.10321316 Acc: 0.9616307\n",
      "Epoch 25 of 500 took 0.343s\n",
      "Accuracy total 0.946514, main loss classifier 0.276906, source accuracy 0.955529 source classification loss 0.154510, target accuracy 0.937500 target loss 0.189842 accuracy domain distinction 0.500000 loss domain distinction 1.047305,\n",
      "VALIDATION Loss: 0.10287488 Acc: 0.96642686\n",
      "Epoch 26 of 500 took 0.345s\n",
      "Training complete in 0m 10s\n",
      "['participant_1', 'participant_2', 'participant_0']\n",
      "TSD_Network(\n",
      "  (_network): ModuleList(\n",
      "    (0): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=252, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=252, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (1): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (2): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (_output): Linear(in_features=200, out_features=22, bias=True)\n",
      "  (_output_discriminator): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "Number Parameters:  137624\n",
      "Optimizer =  <generator object Module.parameters at 0x7f08511c4ba0>\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/DANN/participant_2/best_state_1.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/DANN/participant_2/best_state_1.pt' (epoch 6)\n",
      "TSD_Network(\n",
      "  (_network): ModuleList(\n",
      "    (0): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=252, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=252, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (1): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (2): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (_output): Linear(in_features=200, out_features=22, bias=True)\n",
      "  (_output_discriminator): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "Number Parameters:  137624\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/TSD/participant_2/best_state_0.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/TSD/participant_2/best_state_0.pt' (epoch 37)\n",
      "TSD_Network(\n",
      "  (_network): ModuleList(\n",
      "    (0): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=252, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=252, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (1): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (2): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (_output): Linear(in_features=200, out_features=22, bias=True)\n",
      "  (_output_discriminator): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "Number Parameters:  137624\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/DANN/participant_2/best_state_1.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/DANN/participant_2/best_state_1.pt' (epoch 6)\n",
      "==== models_array =  (2,)  @ session  1\n",
      "HANDLING NEW SESSION  0\n",
      "HANDLING NEW SESSION  1\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  16\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  1.0  len before:  26   len after:  7\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  10\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  15\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.5769230769230769  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  2\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  14\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  18\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.5769230769230769  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.1875  len before:  26   len after:  16\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.11538461538461539   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.4230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.7106643356643356   Accuracy pseudo: 0.785347643490434  len pseudo:  2143    len predictions 2288\n",
      "STARTING TRAINING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy total 0.884615, main loss classifier 0.521964, source accuracy 0.957332 source classification loss 0.120853, target accuracy 0.811899 target loss 0.679475 accuracy domain distinction 0.500601 loss domain distinction 1.218003,\n",
      "VALIDATION Loss: 0.43621386 Acc: 0.87878788\n",
      "New best validation loss:  0.43621385523251127\n",
      "Epoch 2 of 500 took 0.342s\n",
      "Accuracy total 0.894531, main loss classifier 0.468816, source accuracy 0.963942 source classification loss 0.124629, target accuracy 0.825120 target loss 0.584113 accuracy domain distinction 0.496394 loss domain distinction 1.144443,\n",
      "VALIDATION Loss: 0.33192976 Acc: 0.91142191\n",
      "New best validation loss:  0.33192976457732065\n",
      "Epoch 3 of 500 took 0.395s\n",
      "Accuracy total 0.903546, main loss classifier 0.415926, source accuracy 0.952524 source classification loss 0.127848, target accuracy 0.854567 target loss 0.481885 accuracy domain distinction 0.497897 loss domain distinction 1.110592,\n",
      "VALIDATION Loss: 0.20676004 Acc: 0.93240093\n",
      "New best validation loss:  0.20676004460879735\n",
      "Epoch 4 of 500 took 0.349s\n",
      "Accuracy total 0.905950, main loss classifier 0.411203, source accuracy 0.962740 source classification loss 0.120540, target accuracy 0.849159 target loss 0.480988 accuracy domain distinction 0.499399 loss domain distinction 1.104391,\n",
      "VALIDATION Loss: 0.22470741 Acc: 0.91375291\n",
      "Epoch 5 of 500 took 0.341s\n",
      "Accuracy total 0.909555, main loss classifier 0.406007, source accuracy 0.964543 source classification loss 0.122110, target accuracy 0.854567 target loss 0.470078 accuracy domain distinction 0.498197 loss domain distinction 1.099136,\n",
      "VALIDATION Loss: 0.27079682 Acc: 0.90675991\n",
      "Epoch 6 of 500 took 0.345s\n",
      "Accuracy total 0.909555, main loss classifier 0.398864, source accuracy 0.959135 source classification loss 0.124022, target accuracy 0.859976 target loss 0.454075 accuracy domain distinction 0.500000 loss domain distinction 1.098160,\n",
      "VALIDATION Loss: 0.27124719 Acc: 0.90909091\n",
      "Epoch 7 of 500 took 0.348s\n",
      "Accuracy total 0.913161, main loss classifier 0.392797, source accuracy 0.960938 source classification loss 0.132186, target accuracy 0.865385 target loss 0.435874 accuracy domain distinction 0.499399 loss domain distinction 1.087670,\n",
      "VALIDATION Loss: 0.25212952 Acc: 0.92773893\n",
      "Epoch 8 of 500 took 0.341s\n",
      "Accuracy total 0.908954, main loss classifier 0.385414, source accuracy 0.959135 source classification loss 0.121589, target accuracy 0.858774 target loss 0.432027 accuracy domain distinction 0.502103 loss domain distinction 1.086066,\n",
      "VALIDATION Loss: 0.18326757 Acc: 0.93006993\n",
      "New best validation loss:  0.18326756996767862\n",
      "Epoch 9 of 500 took 0.346s\n",
      "Accuracy total 0.917067, main loss classifier 0.367284, source accuracy 0.962740 source classification loss 0.111375, target accuracy 0.871394 target loss 0.405932 accuracy domain distinction 0.498498 loss domain distinction 1.086311,\n",
      "VALIDATION Loss: 0.21231348 Acc: 0.92540793\n",
      "Epoch 10 of 500 took 0.341s\n",
      "Accuracy total 0.915865, main loss classifier 0.383372, source accuracy 0.960337 source classification loss 0.125934, target accuracy 0.871394 target loss 0.425850 accuracy domain distinction 0.497296 loss domain distinction 1.074798,\n",
      "VALIDATION Loss: 0.28238376 Acc: 0.91142191\n",
      "Epoch 11 of 500 took 0.340s\n",
      "Accuracy total 0.917368, main loss classifier 0.367736, source accuracy 0.963942 source classification loss 0.119089, target accuracy 0.870793 target loss 0.402223 accuracy domain distinction 0.501502 loss domain distinction 1.070801,\n",
      "VALIDATION Loss: 0.19706956 Acc: 0.93473193\n",
      "Epoch 12 of 500 took 0.345s\n",
      "Accuracy total 0.913161, main loss classifier 0.382315, source accuracy 0.955529 source classification loss 0.137815, target accuracy 0.870793 target loss 0.411934 accuracy domain distinction 0.500601 loss domain distinction 1.074405,\n",
      "VALIDATION Loss: 0.24903126 Acc: 0.91608392\n",
      "Epoch 13 of 500 took 0.341s\n",
      "Accuracy total 0.914363, main loss classifier 0.364890, source accuracy 0.963341 source classification loss 0.112168, target accuracy 0.865385 target loss 0.402217 accuracy domain distinction 0.500601 loss domain distinction 1.076975,\n",
      "VALIDATION Loss: 0.18923635 Acc: 0.92540793\n",
      "Epoch 14 of 500 took 0.342s\n",
      "Accuracy total 0.915565, main loss classifier 0.369550, source accuracy 0.953726 source classification loss 0.132022, target accuracy 0.877404 target loss 0.392118 accuracy domain distinction 0.500300 loss domain distinction 1.074801,\n",
      "VALIDATION Loss: 0.16058380 Acc: 0.95337995\n",
      "New best validation loss:  0.16058380263192312\n",
      "Epoch 15 of 500 took 0.347s\n",
      "Accuracy total 0.928185, main loss classifier 0.347086, source accuracy 0.966947 source classification loss 0.105757, target accuracy 0.889423 target loss 0.372987 accuracy domain distinction 0.500300 loss domain distinction 1.077142,\n",
      "VALIDATION Loss: 0.27669481 Acc: 0.92074592\n",
      "Epoch 16 of 500 took 0.344s\n",
      "Accuracy total 0.926082, main loss classifier 0.334546, source accuracy 0.967548 source classification loss 0.114930, target accuracy 0.884615 target loss 0.340749 accuracy domain distinction 0.500300 loss domain distinction 1.067069,\n",
      "VALIDATION Loss: 0.22340549 Acc: 0.92074592\n",
      "Epoch 17 of 500 took 0.340s\n",
      "Accuracy total 0.920974, main loss classifier 0.350692, source accuracy 0.957332 source classification loss 0.125346, target accuracy 0.884615 target loss 0.363021 accuracy domain distinction 0.500300 loss domain distinction 1.065083,\n",
      "VALIDATION Loss: 0.17208820 Acc: 0.92773893\n",
      "Epoch 18 of 500 took 0.389s\n",
      "Accuracy total 0.928185, main loss classifier 0.336784, source accuracy 0.960337 source classification loss 0.116982, target accuracy 0.896034 target loss 0.341546 accuracy domain distinction 0.499700 loss domain distinction 1.075205,\n",
      "VALIDATION Loss: 0.16272002 Acc: 0.94638695\n",
      "Epoch 19 of 500 took 0.379s\n",
      "Accuracy total 0.924279, main loss classifier 0.351346, source accuracy 0.966346 source classification loss 0.116025, target accuracy 0.882212 target loss 0.374355 accuracy domain distinction 0.500300 loss domain distinction 1.061564,\n",
      "VALIDATION Loss: 0.18892441 Acc: 0.93240093\n",
      "Epoch 20 of 500 took 0.379s\n",
      "Accuracy total 0.926082, main loss classifier 0.337200, source accuracy 0.964543 source classification loss 0.104014, target accuracy 0.887620 target loss 0.357871 accuracy domain distinction 0.500000 loss domain distinction 1.062575,\n",
      "VALIDATION Loss: 0.17131539 Acc: 0.93706294\n",
      "Epoch    20: reducing learning rate of group 0 to 2.0120e-05.\n",
      "Epoch 21 of 500 took 0.405s\n",
      "Accuracy total 0.927284, main loss classifier 0.347629, source accuracy 0.963942 source classification loss 0.112659, target accuracy 0.890625 target loss 0.370308 accuracy domain distinction 0.499399 loss domain distinction 1.061463,\n",
      "VALIDATION Loss: 0.27837141 Acc: 0.9020979\n",
      "Epoch 22 of 500 took 0.386s\n",
      "Accuracy total 0.925180, main loss classifier 0.333286, source accuracy 0.963942 source classification loss 0.108084, target accuracy 0.886418 target loss 0.346643 accuracy domain distinction 0.500000 loss domain distinction 1.059223,\n",
      "VALIDATION Loss: 0.14906102 Acc: 0.94871795\n",
      "New best validation loss:  0.14906102418899536\n",
      "Epoch 23 of 500 took 0.414s\n",
      "Accuracy total 0.927584, main loss classifier 0.341252, source accuracy 0.966346 source classification loss 0.123214, target accuracy 0.888822 target loss 0.346567 accuracy domain distinction 0.500300 loss domain distinction 1.063615,\n",
      "VALIDATION Loss: 0.17896816 Acc: 0.94172494\n",
      "Epoch 24 of 500 took 0.426s\n",
      "Accuracy total 0.925781, main loss classifier 0.334755, source accuracy 0.962740 source classification loss 0.113491, target accuracy 0.888822 target loss 0.344368 accuracy domain distinction 0.499700 loss domain distinction 1.058252,\n",
      "VALIDATION Loss: 0.20849649 Acc: 0.93006993\n",
      "Epoch 25 of 500 took 0.386s\n",
      "Accuracy total 0.932692, main loss classifier 0.339748, source accuracy 0.966346 source classification loss 0.113887, target accuracy 0.899038 target loss 0.352865 accuracy domain distinction 0.499700 loss domain distinction 1.063724,\n",
      "VALIDATION Loss: 0.22634460 Acc: 0.93240093\n",
      "Epoch 26 of 500 took 0.403s\n",
      "Accuracy total 0.924880, main loss classifier 0.336224, source accuracy 0.965745 source classification loss 0.108981, target accuracy 0.884014 target loss 0.352278 accuracy domain distinction 0.500000 loss domain distinction 1.055937,\n",
      "VALIDATION Loss: 0.17103130 Acc: 0.94871795\n",
      "Epoch 27 of 500 took 0.410s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy total 0.926382, main loss classifier 0.330832, source accuracy 0.965745 source classification loss 0.108747, target accuracy 0.887019 target loss 0.341594 accuracy domain distinction 0.500000 loss domain distinction 1.056610,\n",
      "VALIDATION Loss: 0.17362978 Acc: 0.93473193\n",
      "Epoch 28 of 500 took 0.406s\n",
      "Accuracy total 0.930288, main loss classifier 0.333538, source accuracy 0.970553 source classification loss 0.098024, target accuracy 0.890024 target loss 0.357295 accuracy domain distinction 0.499700 loss domain distinction 1.058785,\n",
      "VALIDATION Loss: 0.28041397 Acc: 0.88811189\n",
      "Epoch    28: reducing learning rate of group 0 to 4.0240e-06.\n",
      "Epoch 29 of 500 took 0.389s\n",
      "Accuracy total 0.928786, main loss classifier 0.320322, source accuracy 0.968149 source classification loss 0.103528, target accuracy 0.889423 target loss 0.327039 accuracy domain distinction 0.500000 loss domain distinction 1.050390,\n",
      "VALIDATION Loss: 0.17554300 Acc: 0.94405594\n",
      "Epoch 30 of 500 took 0.386s\n",
      "Accuracy total 0.922476, main loss classifier 0.340835, source accuracy 0.958534 source classification loss 0.132261, target accuracy 0.886418 target loss 0.336009 accuracy domain distinction 0.499700 loss domain distinction 1.067007,\n",
      "VALIDATION Loss: 0.26699972 Acc: 0.91375291\n",
      "Epoch 31 of 500 took 0.392s\n",
      "Accuracy total 0.931490, main loss classifier 0.331196, source accuracy 0.965144 source classification loss 0.102401, target accuracy 0.897837 target loss 0.347043 accuracy domain distinction 0.500000 loss domain distinction 1.064742,\n",
      "VALIDATION Loss: 0.16718473 Acc: 0.93240093\n",
      "Epoch 32 of 500 took 0.384s\n",
      "Accuracy total 0.933293, main loss classifier 0.318243, source accuracy 0.971755 source classification loss 0.105254, target accuracy 0.894832 target loss 0.319706 accuracy domain distinction 0.499700 loss domain distinction 1.057629,\n",
      "VALIDATION Loss: 0.22488215 Acc: 0.91608392\n",
      "Epoch 33 of 500 took 0.391s\n",
      "Accuracy total 0.923678, main loss classifier 0.344434, source accuracy 0.965144 source classification loss 0.103276, target accuracy 0.882212 target loss 0.373163 accuracy domain distinction 0.498798 loss domain distinction 1.062139,\n",
      "VALIDATION Loss: 0.14778221 Acc: 0.95804196\n",
      "New best validation loss:  0.1477822054709707\n",
      "Epoch 34 of 500 took 0.427s\n",
      "Accuracy total 0.924579, main loss classifier 0.333617, source accuracy 0.962740 source classification loss 0.113983, target accuracy 0.886418 target loss 0.340229 accuracy domain distinction 0.500300 loss domain distinction 1.065106,\n",
      "VALIDATION Loss: 0.19848705 Acc: 0.93473193\n",
      "Epoch 35 of 500 took 0.394s\n",
      "Accuracy total 0.931791, main loss classifier 0.324575, source accuracy 0.963942 source classification loss 0.111051, target accuracy 0.899639 target loss 0.325473 accuracy domain distinction 0.500000 loss domain distinction 1.063131,\n",
      "VALIDATION Loss: 0.14509057 Acc: 0.94638695\n",
      "New best validation loss:  0.14509057360036032\n",
      "Epoch 36 of 500 took 0.374s\n",
      "Accuracy total 0.929688, main loss classifier 0.323231, source accuracy 0.971755 source classification loss 0.099773, target accuracy 0.887620 target loss 0.334822 accuracy domain distinction 0.500000 loss domain distinction 1.059332,\n",
      "VALIDATION Loss: 0.24108411 Acc: 0.92307692\n",
      "Epoch 37 of 500 took 0.385s\n",
      "Accuracy total 0.927584, main loss classifier 0.322454, source accuracy 0.961538 source classification loss 0.120760, target accuracy 0.893630 target loss 0.313499 accuracy domain distinction 0.500300 loss domain distinction 1.053243,\n",
      "VALIDATION Loss: 0.16518861 Acc: 0.93939394\n",
      "Epoch 38 of 500 took 0.384s\n",
      "Accuracy total 0.926382, main loss classifier 0.340531, source accuracy 0.962139 source classification loss 0.121996, target accuracy 0.890625 target loss 0.346567 accuracy domain distinction 0.499700 loss domain distinction 1.062500,\n",
      "VALIDATION Loss: 0.16725014 Acc: 0.94405594\n",
      "Epoch 39 of 500 took 0.388s\n",
      "Accuracy total 0.925180, main loss classifier 0.332824, source accuracy 0.960337 source classification loss 0.108435, target accuracy 0.890024 target loss 0.345361 accuracy domain distinction 0.499399 loss domain distinction 1.059266,\n",
      "VALIDATION Loss: 0.17423555 Acc: 0.92773893\n",
      "Epoch 40 of 500 took 0.385s\n",
      "Accuracy total 0.924880, main loss classifier 0.338328, source accuracy 0.965745 source classification loss 0.102927, target accuracy 0.884014 target loss 0.361940 accuracy domain distinction 0.500300 loss domain distinction 1.058951,\n",
      "VALIDATION Loss: 0.16478093 Acc: 0.95104895\n",
      "Epoch 41 of 500 took 0.394s\n",
      "Accuracy total 0.921875, main loss classifier 0.344165, source accuracy 0.956130 source classification loss 0.124800, target accuracy 0.887620 target loss 0.351657 accuracy domain distinction 0.500000 loss domain distinction 1.059361,\n",
      "VALIDATION Loss: 0.19448948 Acc: 0.93939394\n",
      "Epoch    41: reducing learning rate of group 0 to 8.0480e-07.\n",
      "Epoch 42 of 500 took 0.396s\n",
      "Accuracy total 0.926382, main loss classifier 0.333368, source accuracy 0.960337 source classification loss 0.114511, target accuracy 0.892428 target loss 0.339305 accuracy domain distinction 0.500000 loss domain distinction 1.064594,\n",
      "VALIDATION Loss: 0.19273967 Acc: 0.93006993\n",
      "Epoch 43 of 500 took 0.401s\n",
      "Accuracy total 0.935998, main loss classifier 0.328922, source accuracy 0.975361 source classification loss 0.104646, target accuracy 0.896635 target loss 0.341463 accuracy domain distinction 0.500000 loss domain distinction 1.058674,\n",
      "VALIDATION Loss: 0.17301388 Acc: 0.93939394\n",
      "Epoch 44 of 500 took 0.403s\n",
      "Accuracy total 0.934195, main loss classifier 0.314846, source accuracy 0.964543 source classification loss 0.110831, target accuracy 0.903846 target loss 0.306336 accuracy domain distinction 0.500300 loss domain distinction 1.062622,\n",
      "VALIDATION Loss: 0.20683435 Acc: 0.92773893\n",
      "Epoch 45 of 500 took 0.392s\n",
      "Accuracy total 0.926983, main loss classifier 0.328227, source accuracy 0.960337 source classification loss 0.121545, target accuracy 0.893630 target loss 0.323590 accuracy domain distinction 0.499700 loss domain distinction 1.056593,\n",
      "VALIDATION Loss: 0.17455610 Acc: 0.94871795\n",
      "Epoch 46 of 500 took 0.406s\n",
      "Accuracy total 0.931791, main loss classifier 0.312759, source accuracy 0.965144 source classification loss 0.100262, target accuracy 0.898438 target loss 0.313403 accuracy domain distinction 0.500000 loss domain distinction 1.059263,\n",
      "VALIDATION Loss: 0.19672930 Acc: 0.93473193\n",
      "Epoch 47 of 500 took 0.431s\n",
      "Training complete in 0m 17s\n",
      "['participant_1', 'participant_2', 'participant_0']\n",
      "TSD_Network(\n",
      "  (_network): ModuleList(\n",
      "    (0): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=252, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=252, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (1): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (2): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (_output): Linear(in_features=200, out_features=22, bias=True)\n",
      "  (_output_discriminator): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "Number Parameters:  137624\n",
      "Optimizer =  <generator object Module.parameters at 0x7f08511c4ac0>\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/DANN/participant_2/best_state_2.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/DANN/participant_2/best_state_2.pt' (epoch 5)\n",
      "TSD_Network(\n",
      "  (_network): ModuleList(\n",
      "    (0): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=252, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=252, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (1): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (2): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (_output): Linear(in_features=200, out_features=22, bias=True)\n",
      "  (_output_discriminator): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "Number Parameters:  137624\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/TSD/participant_2/best_state_0.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/TSD/participant_2/best_state_0.pt' (epoch 37)\n",
      "TSD_Network(\n",
      "  (_network): ModuleList(\n",
      "    (0): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=252, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=252, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (1): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (2): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (_output): Linear(in_features=200, out_features=22, bias=True)\n",
      "  (_output_discriminator): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "Number Parameters:  137624\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/DANN/participant_2/best_state_1.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/DANN/participant_2/best_state_1.pt' (epoch 6)\n",
      "TSD_Network(\n",
      "  (_network): ModuleList(\n",
      "    (0): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=252, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=252, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (1): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (2): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (_output): Linear(in_features=200, out_features=22, bias=True)\n",
      "  (_output_discriminator): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "Number Parameters:  137624\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/DANN/participant_2/best_state_2.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/DANN/participant_2/best_state_2.pt' (epoch 5)\n",
      "==== models_array =  (3,)  @ session  2\n",
      "HANDLING NEW SESSION  0\n",
      "HANDLING NEW SESSION  1\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  16\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  1.0  len before:  26   len after:  7\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  10\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5384615384615384   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  15\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  0.5769230769230769  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  2\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  14\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  18\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.5769230769230769  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  17\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.1875  len before:  26   len after:  16\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.11538461538461539   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.2692307692307692   AFTER:  0.4230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.7106643356643356   Accuracy pseudo: 0.785347643490434  len pseudo:  2143    len predictions 2288\n",
      "HANDLING NEW SESSION  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  6\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.46153846153846156   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  1.0  len before:  26   len after:  12\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.038461538461538464   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6923076923076923   AFTER:  1.0  len before:  26   len after:  8\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  6\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  16\n",
      "BEFORE:  0.4230769230769231   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  0.7692307692307693   AFTER:  0.8076923076923077  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5   AFTER:  0.5  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.4230769230769231   AFTER:  1.0  len before:  26   len after:  10\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  14\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.9615384615384616   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.0  len before:  26   len after:  1\n",
      "BEFORE:  0.07692307692307693   AFTER:  0.0  len before:  26   len after:  9\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.5769230769230769  len before:  26   len after:  26\n",
      "BEFORE:  0.38461538461538464   AFTER:  0.5769230769230769  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  1.0  len before:  26   len after:  13\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.3076923076923077   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.0  len before:  26   len after:  3\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.19230769230769232   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.07692307692307693   AFTER:  nan  len before:  26   len after:  0\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.8461538461538461   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  0.0  len before:  26   len after:  14\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7692307692307693   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.9230769230769231  len before:  26   len after:  26\n",
      "BEFORE:  0.7307692307692307   AFTER:  0.7307692307692307  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  26\n",
      "BEFORE:  0.5769230769230769   AFTER:  0.7692307692307693  len before:  26   len after:  26\n",
      "BEFORE:  0.9230769230769231   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.34615384615384615   AFTER:  1.0  len before:  26   len after:  9\n",
      "BEFORE:  0.23076923076923078   AFTER:  0.38461538461538464  len before:  26   len after:  26\n",
      "BEFORE:  0.6153846153846154   AFTER:  0.6153846153846154  len before:  26   len after:  26\n",
      "BEFORE:  0.8846153846153846   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.6538461538461539   AFTER:  0.5769230769230769  len before:  26   len after:  26\n",
      "BEFORE:  0.8076923076923077   AFTER:  1.0  len before:  26   len after:  26\n",
      "BEFORE:  0.15384615384615385   AFTER:  0.0  len before:  26   len after:  14\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  26   len after:  26\n",
      "ACCURACY MODEL:  0.6232517482517482   Accuracy pseudo: 0.7169062964799207  len pseudo:  2017    len predictions 2288\n",
      "STARTING TRAINING\n",
      "Accuracy total 0.821875, main loss classifier 0.725730, source accuracy 0.879375 source classification loss 0.423688, target accuracy 0.764375 target loss 0.797154 accuracy domain distinction 0.501875 loss domain distinction 1.153083,\n",
      "VALIDATION Loss: 0.34253127 Acc: 0.87128713\n",
      "New best validation loss:  0.34253126808575224\n",
      "Epoch 2 of 500 took 0.450s\n",
      "Accuracy total 0.840313, main loss classifier 0.619694, source accuracy 0.871875 source classification loss 0.386557, target accuracy 0.808750 target loss 0.633523 accuracy domain distinction 0.501250 loss domain distinction 1.096544,\n",
      "VALIDATION Loss: 0.31578914 Acc: 0.8960396\n",
      "New best validation loss:  0.31578914182526724\n",
      "Epoch 3 of 500 took 0.432s\n",
      "Accuracy total 0.838125, main loss classifier 0.595956, source accuracy 0.865000 source classification loss 0.387493, target accuracy 0.811250 target loss 0.585845 accuracy domain distinction 0.499063 loss domain distinction 1.092874,\n",
      "VALIDATION Loss: 0.29947218 Acc: 0.88118812\n",
      "New best validation loss:  0.2994721813925675\n",
      "Epoch 4 of 500 took 0.379s\n",
      "Accuracy total 0.860625, main loss classifier 0.539773, source accuracy 0.895000 source classification loss 0.329976, target accuracy 0.826250 target loss 0.535206 accuracy domain distinction 0.500625 loss domain distinction 1.071818,\n",
      "VALIDATION Loss: 0.28155691 Acc: 0.90594059\n",
      "New best validation loss:  0.28155691283089773\n",
      "Epoch 5 of 500 took 0.378s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy total 0.869375, main loss classifier 0.513832, source accuracy 0.900000 source classification loss 0.327084, target accuracy 0.838750 target loss 0.487276 accuracy domain distinction 0.500000 loss domain distinction 1.066525,\n",
      "VALIDATION Loss: 0.33509152 Acc: 0.8960396\n",
      "Epoch 6 of 500 took 0.380s\n",
      "Accuracy total 0.863750, main loss classifier 0.508481, source accuracy 0.894375 source classification loss 0.316964, target accuracy 0.833125 target loss 0.487151 accuracy domain distinction 0.500000 loss domain distinction 1.064233,\n",
      "VALIDATION Loss: 0.22589902 Acc: 0.90346535\n",
      "New best validation loss:  0.2258990215403693\n",
      "Epoch 7 of 500 took 0.405s\n",
      "Accuracy total 0.876250, main loss classifier 0.485413, source accuracy 0.903750 source classification loss 0.292255, target accuracy 0.848750 target loss 0.465587 accuracy domain distinction 0.500000 loss domain distinction 1.064928,\n",
      "VALIDATION Loss: 0.21823173 Acc: 0.92821782\n",
      "New best validation loss:  0.21823173335620336\n",
      "Epoch 8 of 500 took 0.351s\n",
      "Accuracy total 0.870313, main loss classifier 0.498154, source accuracy 0.899375 source classification loss 0.297297, target accuracy 0.841250 target loss 0.486520 accuracy domain distinction 0.500000 loss domain distinction 1.062458,\n",
      "VALIDATION Loss: 0.20515920 Acc: 0.92574257\n",
      "New best validation loss:  0.2051591979605811\n",
      "Epoch 9 of 500 took 0.339s\n",
      "Accuracy total 0.883125, main loss classifier 0.473957, source accuracy 0.907500 source classification loss 0.288870, target accuracy 0.858750 target loss 0.447741 accuracy domain distinction 0.500000 loss domain distinction 1.056512,\n",
      "VALIDATION Loss: 0.24990176 Acc: 0.91831683\n",
      "Epoch 10 of 500 took 0.374s\n",
      "Accuracy total 0.884375, main loss classifier 0.465584, source accuracy 0.906250 source classification loss 0.285522, target accuracy 0.862500 target loss 0.436111 accuracy domain distinction 0.500000 loss domain distinction 1.047676,\n",
      "VALIDATION Loss: 0.22570114 Acc: 0.92326733\n",
      "Epoch 11 of 500 took 0.378s\n",
      "Accuracy total 0.885312, main loss classifier 0.458392, source accuracy 0.910625 source classification loss 0.266110, target accuracy 0.860000 target loss 0.441556 accuracy domain distinction 0.500000 loss domain distinction 1.045593,\n",
      "VALIDATION Loss: 0.27464660 Acc: 0.9009901\n",
      "Epoch 12 of 500 took 0.376s\n",
      "Accuracy total 0.878437, main loss classifier 0.458675, source accuracy 0.899375 source classification loss 0.295739, target accuracy 0.857500 target loss 0.411112 accuracy domain distinction 0.500000 loss domain distinction 1.052492,\n",
      "VALIDATION Loss: 0.28329803 Acc: 0.88366337\n",
      "Epoch 13 of 500 took 0.349s\n",
      "Accuracy total 0.890938, main loss classifier 0.429661, source accuracy 0.911250 source classification loss 0.263683, target accuracy 0.870625 target loss 0.385199 accuracy domain distinction 0.500000 loss domain distinction 1.052202,\n",
      "VALIDATION Loss: 0.21316840 Acc: 0.91831683\n",
      "Epoch 14 of 500 took 0.358s\n",
      "Accuracy total 0.889687, main loss classifier 0.425969, source accuracy 0.918750 source classification loss 0.252060, target accuracy 0.860625 target loss 0.392057 accuracy domain distinction 0.500000 loss domain distinction 1.039107,\n",
      "VALIDATION Loss: 0.23335672 Acc: 0.91831683\n",
      "Epoch    14: reducing learning rate of group 0 to 1.0060e-04.\n",
      "Epoch 15 of 500 took 0.375s\n",
      "Accuracy total 0.883437, main loss classifier 0.449626, source accuracy 0.898750 source classification loss 0.299238, target accuracy 0.868125 target loss 0.393341 accuracy domain distinction 0.500000 loss domain distinction 1.033366,\n",
      "VALIDATION Loss: 0.20460374 Acc: 0.93564356\n",
      "New best validation loss:  0.2046037412115506\n",
      "Epoch 16 of 500 took 0.373s\n",
      "Accuracy total 0.888125, main loss classifier 0.420589, source accuracy 0.910625 source classification loss 0.249191, target accuracy 0.865625 target loss 0.384555 accuracy domain distinction 0.500000 loss domain distinction 1.037154,\n",
      "VALIDATION Loss: 0.22320773 Acc: 0.92574257\n",
      "Epoch 17 of 500 took 0.377s\n",
      "Accuracy total 0.894062, main loss classifier 0.414808, source accuracy 0.915625 source classification loss 0.257810, target accuracy 0.872500 target loss 0.364726 accuracy domain distinction 0.500000 loss domain distinction 1.035400,\n",
      "VALIDATION Loss: 0.26321267 Acc: 0.91336634\n",
      "Epoch 18 of 500 took 0.355s\n",
      "Accuracy total 0.894375, main loss classifier 0.424738, source accuracy 0.919375 source classification loss 0.254269, target accuracy 0.869375 target loss 0.388020 accuracy domain distinction 0.500000 loss domain distinction 1.035936,\n",
      "VALIDATION Loss: 0.21793621 Acc: 0.92079208\n",
      "Epoch 19 of 500 took 0.330s\n",
      "Accuracy total 0.905000, main loss classifier 0.397030, source accuracy 0.919375 source classification loss 0.238220, target accuracy 0.890625 target loss 0.349184 accuracy domain distinction 0.500000 loss domain distinction 1.033279,\n",
      "VALIDATION Loss: 0.19714302 Acc: 0.94306931\n",
      "New best validation loss:  0.19714301505259105\n",
      "Epoch 20 of 500 took 0.330s\n",
      "Accuracy total 0.891875, main loss classifier 0.403892, source accuracy 0.914375 source classification loss 0.235635, target accuracy 0.869375 target loss 0.365004 accuracy domain distinction 0.500000 loss domain distinction 1.035731,\n",
      "VALIDATION Loss: 0.21374901 Acc: 0.92326733\n",
      "Epoch 21 of 500 took 0.330s\n",
      "Accuracy total 0.904375, main loss classifier 0.387768, source accuracy 0.927500 source classification loss 0.224784, target accuracy 0.881250 target loss 0.344065 accuracy domain distinction 0.500000 loss domain distinction 1.033435,\n",
      "VALIDATION Loss: 0.18529087 Acc: 0.95544554\n",
      "New best validation loss:  0.18529086879321507\n",
      "Epoch 22 of 500 took 0.329s\n",
      "Accuracy total 0.906250, main loss classifier 0.390526, source accuracy 0.933125 source classification loss 0.219190, target accuracy 0.879375 target loss 0.354574 accuracy domain distinction 0.500000 loss domain distinction 1.036445,\n",
      "VALIDATION Loss: 0.15368270 Acc: 0.9480198\n",
      "New best validation loss:  0.15368270022528513\n",
      "Epoch 23 of 500 took 0.332s\n",
      "Accuracy total 0.899687, main loss classifier 0.401477, source accuracy 0.916250 source classification loss 0.257534, target accuracy 0.883125 target loss 0.338926 accuracy domain distinction 0.500000 loss domain distinction 1.032476,\n",
      "VALIDATION Loss: 0.18182739 Acc: 0.94554455\n",
      "Epoch 24 of 500 took 0.382s\n",
      "Accuracy total 0.904687, main loss classifier 0.381410, source accuracy 0.923750 source classification loss 0.231281, target accuracy 0.885625 target loss 0.323795 accuracy domain distinction 0.500000 loss domain distinction 1.038721,\n",
      "VALIDATION Loss: 0.19765415 Acc: 0.93564356\n",
      "Epoch 25 of 500 took 0.353s\n",
      "Accuracy total 0.906563, main loss classifier 0.400868, source accuracy 0.923125 source classification loss 0.255242, target accuracy 0.890000 target loss 0.337630 accuracy domain distinction 0.500000 loss domain distinction 1.044318,\n",
      "VALIDATION Loss: 0.19250883 Acc: 0.93564356\n",
      "Epoch 26 of 500 took 0.335s\n",
      "Accuracy total 0.908750, main loss classifier 0.376986, source accuracy 0.926875 source classification loss 0.215240, target accuracy 0.890625 target loss 0.333298 accuracy domain distinction 0.500000 loss domain distinction 1.027168,\n",
      "VALIDATION Loss: 0.20442409 Acc: 0.93316832\n",
      "Epoch 27 of 500 took 0.335s\n",
      "Accuracy total 0.905000, main loss classifier 0.385218, source accuracy 0.921250 source classification loss 0.241358, target accuracy 0.888750 target loss 0.322174 accuracy domain distinction 0.500000 loss domain distinction 1.034522,\n",
      "VALIDATION Loss: 0.21976117 Acc: 0.92326733\n",
      "Epoch 28 of 500 took 0.328s\n",
      "Accuracy total 0.905000, main loss classifier 0.384413, source accuracy 0.925000 source classification loss 0.225097, target accuracy 0.885000 target loss 0.336794 accuracy domain distinction 0.500000 loss domain distinction 1.034669,\n",
      "VALIDATION Loss: 0.16910605 Acc: 0.94306931\n",
      "Epoch    28: reducing learning rate of group 0 to 2.0120e-05.\n",
      "Epoch 29 of 500 took 0.329s\n",
      "Accuracy total 0.912813, main loss classifier 0.369802, source accuracy 0.932500 source classification loss 0.214247, target accuracy 0.893125 target loss 0.318901 accuracy domain distinction 0.500000 loss domain distinction 1.032280,\n",
      "VALIDATION Loss: 0.17316765 Acc: 0.93811881\n",
      "Epoch 30 of 500 took 0.330s\n",
      "Accuracy total 0.903125, main loss classifier 0.377769, source accuracy 0.927500 source classification loss 0.218044, target accuracy 0.878750 target loss 0.330943 accuracy domain distinction 0.500000 loss domain distinction 1.032754,\n",
      "VALIDATION Loss: 0.19515038 Acc: 0.92079208\n",
      "Epoch 31 of 500 took 0.391s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy total 0.910000, main loss classifier 0.357932, source accuracy 0.924375 source classification loss 0.218885, target accuracy 0.895625 target loss 0.290972 accuracy domain distinction 0.500000 loss domain distinction 1.030034,\n",
      "VALIDATION Loss: 0.20786103 Acc: 0.92821782\n",
      "Epoch 32 of 500 took 0.380s\n",
      "Accuracy total 0.911875, main loss classifier 0.361148, source accuracy 0.935000 source classification loss 0.211921, target accuracy 0.888750 target loss 0.303547 accuracy domain distinction 0.500000 loss domain distinction 1.034134,\n",
      "VALIDATION Loss: 0.19786704 Acc: 0.92821782\n",
      "Epoch 33 of 500 took 0.330s\n",
      "Accuracy total 0.912500, main loss classifier 0.359376, source accuracy 0.933125 source classification loss 0.199815, target accuracy 0.891875 target loss 0.312727 accuracy domain distinction 0.500000 loss domain distinction 1.031050,\n",
      "VALIDATION Loss: 0.21087738 Acc: 0.93316832\n",
      "Epoch 34 of 500 took 0.329s\n",
      "Training complete in 0m 12s\n",
      "['participant_1', 'participant_2', 'participant_0']\n"
     ]
    }
   ],
   "source": [
    "run_SCADANN_training_sessions(examples_datasets=examples_datasets_train, labels_datasets=labels_datasets_train,\n",
    "                              num_kernels=num_kernels, feature_vector_input_length=feature_vector_input_length,\n",
    "                              path_weights_to_save_to=path_weight_to_save_to,\n",
    "                              path_weights_Adversarial_training=path_weights_start_with,\n",
    "                              path_weights_Normal_training=path_weights_Normal_training,\n",
    "                              number_of_cycle_for_first_training=40, number_of_cycles_rest_of_training=40,\n",
    "                              number_of_classes=number_of_classes,\n",
    "                              learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET one participant_examples  (3, 40, 572, 252)\n",
      "   GET one training_index_examples  (40, 572, 252)  at  0\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (40, 572, 252)  at  1\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (40, 572, 252)  at  2\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "dataloaders: \n",
      "   train  (1, 3)\n",
      "   valid  (1, 3)\n",
      "   test  (1, 3)\n",
      "GET one participant_examples  (3, 40, 572, 252)\n",
      "   GET one training_index_examples  (40, 572, 252)  at  0\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (40, 572, 252)  at  1\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (40, 572, 252)  at  2\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "dataloaders: \n",
      "   train  (2, 3)\n",
      "   valid  (2, 3)\n",
      "   test  (2, 3)\n",
      "GET one participant_examples  (3, 40, 572, 252)\n",
      "   GET one training_index_examples  (40, 572, 252)  at  0\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (40, 572, 252)  at  1\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "   GET one training_index_examples  (40, 572, 252)  at  2\n",
      "   GOT one group XY  (2288, 252)    (2288,)\n",
      "       one group XY test  (572, 252)    (572, 252)\n",
      "       one group XY train (2059, 252)    (2059,)\n",
      "       one group XY valid (229, 252)    (229, 252)\n",
      "dataloaders: \n",
      "   train  (3, 3)\n",
      "   valid  (3, 3)\n",
      "   test  (3, 3)\n",
      "TSD_Network(\n",
      "  (_network): ModuleList(\n",
      "    (0): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=252, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=252, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (1): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (2): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (_output): Linear(in_features=200, out_features=22, bias=True)\n",
      "  (_output_discriminator): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "Number Parameters:  137624\n",
      "Participant:  0  Accuracy:  0.9912587412587412\n",
      "Participant:  0  Accuracy:  0.6083916083916084\n",
      "Participant:  0  Accuracy:  0.5262237762237763\n",
      "ACCURACY PARTICIPANT:  [0.9912587412587412, 0.6083916083916084, 0.5262237762237763]\n",
      "TSD_Network(\n",
      "  (_network): ModuleList(\n",
      "    (0): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=252, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=252, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (1): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (2): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (_output): Linear(in_features=200, out_features=22, bias=True)\n",
      "  (_output_discriminator): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "Number Parameters:  137624\n",
      "Participant:  1  Accuracy:  0.9807692307692307\n",
      "Participant:  1  Accuracy:  0.8671328671328671\n",
      "Participant:  1  Accuracy:  0.7972027972027972\n",
      "ACCURACY PARTICIPANT:  [0.9807692307692307, 0.8671328671328671, 0.7972027972027972]\n",
      "TSD_Network(\n",
      "  (_network): ModuleList(\n",
      "    (0): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=252, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=252, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (1): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (2): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (_output): Linear(in_features=200, out_features=22, bias=True)\n",
      "  (_output_discriminator): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "Number Parameters:  137624\n",
      "Participant:  2  Accuracy:  0.986013986013986\n",
      "Participant:  2  Accuracy:  0.736013986013986\n",
      "Participant:  2  Accuracy:  0.6171328671328671\n",
      "ACCURACY PARTICIPANT:  [0.986013986013986, 0.736013986013986, 0.6171328671328671]\n",
      "[[0.99125874 0.60839161 0.52622378]\n",
      " [0.98076923 0.86713287 0.7972028 ]\n",
      " [0.98601399 0.73601399 0.61713287]]\n",
      "[array([0.99125874, 0.60839161, 0.52622378]), array([0.98076923, 0.86713287, 0.7972028 ]), array([0.98601399, 0.73601399, 0.61713287])]\n",
      "OVERALL ACCURACY: 0.79001554001554\n"
     ]
    }
   ],
   "source": [
    "save_path = \"/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/results_tsd\"\n",
    "test_network_SLADANN(examples_datasets_train=examples_datasets_train, labels_datasets_train=labels_datasets_train,\n",
    "                     num_neurons=num_kernels, feature_vector_input_length=feature_vector_input_length,\n",
    "                     path_weights_SCADANN =path_weight_to_save_to, path_weights_normal=path_weights_Normal_training,\n",
    "                     algo_name=algo_name, cycle_test=3, \n",
    "                     number_of_classes=number_of_classes, save_path = save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant_0</th>\n",
       "      <th>Participant_1</th>\n",
       "      <th>Participant_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Session_0</th>\n",
       "      <td>0.991259</td>\n",
       "      <td>0.980769</td>\n",
       "      <td>0.986014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Session_1</th>\n",
       "      <td>0.608392</td>\n",
       "      <td>0.867133</td>\n",
       "      <td>0.736014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Session_2</th>\n",
       "      <td>0.526224</td>\n",
       "      <td>0.797203</td>\n",
       "      <td>0.617133</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Participant_0 Participant_1 Participant_2\n",
       "Session_0      0.991259      0.980769      0.986014\n",
       "Session_1      0.608392      0.867133      0.736014\n",
       "Session_2      0.526224      0.797203      0.617133"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_filename = save_path + '/predictions_' + algo_name + \".npy\"\n",
    "results = np.load(results_filename, allow_pickle=True)\n",
    "SCADANN_acc = results[0]\n",
    "SCADANN_acc_overall = np.mean(SCADANN_acc)\n",
    "SCADANN_df = pd.DataFrame(SCADANN_acc.transpose(), \n",
    "                       index = [f'Session_{i}' for i in range(SCADANN_acc.shape[1])],\n",
    "                        columns = [f'Participant_{j}' for j in range(SCADANN_acc.shape[0])])\n",
    "SCADANN_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "SCADANN_df.transpose().plot.bar(rot=0, figsize=(10,5))\n",
    "plt.title(\"SCADANN Accuracies\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TSD\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant_0</th>\n",
       "      <th>Participant_1</th>\n",
       "      <th>Participant_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Session_0</th>\n",
       "      <td>0.991259</td>\n",
       "      <td>0.980769</td>\n",
       "      <td>0.986014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Session_1</th>\n",
       "      <td>0.438811</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.646853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Session_2</th>\n",
       "      <td>0.494755</td>\n",
       "      <td>0.678322</td>\n",
       "      <td>0.522727</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Participant_0 Participant_1 Participant_2\n",
       "Session_0      0.991259      0.980769      0.986014\n",
       "Session_1      0.438811      0.846154      0.646853\n",
       "Session_2      0.494755      0.678322      0.522727"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DANN\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant_0</th>\n",
       "      <th>Participant_1</th>\n",
       "      <th>Participant_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Session_0</th>\n",
       "      <td>0.991259</td>\n",
       "      <td>0.980769</td>\n",
       "      <td>0.986014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Session_1</th>\n",
       "      <td>0.596154</td>\n",
       "      <td>0.870629</td>\n",
       "      <td>0.725524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Session_2</th>\n",
       "      <td>0.520979</td>\n",
       "      <td>0.748252</td>\n",
       "      <td>0.561189</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Participant_0 Participant_1 Participant_2\n",
       "Session_0      0.991259      0.980769      0.986014\n",
       "Session_1      0.596154      0.870629      0.725524\n",
       "Session_2      0.520979      0.748252      0.561189"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCADANN\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant_0</th>\n",
       "      <th>Participant_1</th>\n",
       "      <th>Participant_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Session_0</th>\n",
       "      <td>0.991259</td>\n",
       "      <td>0.980769</td>\n",
       "      <td>0.986014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Session_1</th>\n",
       "      <td>0.608392</td>\n",
       "      <td>0.867133</td>\n",
       "      <td>0.736014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Session_2</th>\n",
       "      <td>0.526224</td>\n",
       "      <td>0.797203</td>\n",
       "      <td>0.617133</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Participant_0 Participant_1 Participant_2\n",
       "Session_0      0.991259      0.980769      0.986014\n",
       "Session_1      0.608392      0.867133      0.736014\n",
       "Session_2      0.526224      0.797203      0.617133"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"TSD\")\n",
    "display(TSD_df)\n",
    "print(\"DANN\")\n",
    "display(DANN_df)\n",
    "print(\"SCADANN\")\n",
    "display(SCADANN_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Overall_Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TSD</th>\n",
       "      <td>0.731740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DANN</th>\n",
       "      <td>0.775641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SCADANN</th>\n",
       "      <td>0.790016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Overall_Acc\n",
       "TSD         0.731740\n",
       "DANN        0.775641\n",
       "SCADANN     0.790016"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_acc_df = pd.DataFrame([TSD_acc_overall, DANN_acc_overall, SCADANN_acc_overall],\n",
    "                             index = [\"TSD\", \"DANN\", \"SCADANN\"],\n",
    "                             columns = [\"Overall_Acc\"])\n",
    "overall_acc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x1008 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(14,14))\n",
    "acc_list = [TSD_df, DANN_df, SCADANN_df, overall_acc_df]\n",
    "title_list = [\"TSD\", \"DANN\", \"SCADANN\", \"Overall\"]\n",
    "for idx, ax in enumerate(axes.reshape(-1)): \n",
    "    acc_list[idx].transpose().plot.bar(ax = ax, rot=0)\n",
    "    ax.set_title(title_list[idx])\n",
    "    ax.set_ylim([0, 1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
