{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import pickle\n",
    "import numpy as np  \n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import copy\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "sys.path.insert(0,'../../LongTermEMG-master')\n",
    "os.chdir(\"/home/laiy/gitrepos/msr_final/LongTermEMG-master/LongTermClassificationMain/TrainingsAndEvaluations/ForTrainingSessions/TSD_DNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traning examples  (1, 4, 4)\n",
      "one group example  (1009, 385)\n",
      "traning labels  (1, 4, 4)\n",
      "one group label  (1009,)\n"
     ]
    }
   ],
   "source": [
    "with open(\"../../../Processed_datasets/TSD_features_set_training_session.pickle\", 'rb') as f:\n",
    "    dataset_training = pickle.load(file=f)\n",
    "\n",
    "examples_datasets_train = dataset_training['examples_training']\n",
    "print('traning examples ', np.shape(examples_datasets_train))\n",
    "print(\"one group example \", np.shape(examples_datasets_train[0][0][0]))\n",
    "labels_datasets_train = dataset_training['labels_training']\n",
    "print('traning labels ', np.shape(labels_datasets_train))\n",
    "print(\"one group label \", np.shape(labels_datasets_train[0][0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/home/laiy/gitrepos/msr_final/LongTermEMG_myo\")\n",
    "\n",
    "from Models.TSD_neural_network import TSD_Network\n",
    "from PrepareAndLoadData.load_dataset_in_dataloader import load_dataloaders_training_sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DANN_BN_Training(gesture_classifier, crossEntropyLoss, optimizer_classifier, train_dataset_source, scheduler,\n",
    "                     train_dataset_target, validation_dataset_source, patience_increment=10, max_epochs=500,\n",
    "                     domain_loss_weight=1e-1):\n",
    "    \"\"\"\n",
    "    gesture_classification: model\n",
    "    crossEntropyLoss\n",
    "    optimizer_classifier\n",
    "    scheduler\n",
    "    \n",
    "    target: unlabeled; source: labeled\n",
    "    train_dataset_source: the first session of a participant's training set\n",
    "    validation_dataset_source:  the first session of a participant's validation set\n",
    "    train_dataset_target: one seesion (except for the first) of a participant's traning set\n",
    "\n",
    "    patience_increment: number of epchos to wait after no best loss is found and before existing training\n",
    "    max_epochs\n",
    "    \n",
    "    domain_loss_weight: coefficient of doman loss percantage to account in calculating loss (loss_main_source and loss_doman_target)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    since = time.time()\n",
    "    patience = 0 + patience_increment\n",
    "\n",
    "    # Create a list of dictionaries that will hold the weights of the batch normalisation layers for each dataset\n",
    "    #  (i.e. each participants)\n",
    "    list_dictionaries_BN_weights = []\n",
    "    for index_BN_weights in range(2):\n",
    "        state_dict = gesture_classifier.state_dict()\n",
    "        batch_norm_dict = {}\n",
    "        for key in state_dict:\n",
    "            if \"batchNorm\" in key:\n",
    "                batch_norm_dict.update({key: state_dict[key]})\n",
    "        list_dictionaries_BN_weights.append(copy.deepcopy(batch_norm_dict))\n",
    "\n",
    "    best_loss = float(\"inf\")\n",
    "    best_state = {'epoch': 0, 'state_dict': copy.deepcopy(gesture_classifier.state_dict()),\n",
    "                  'optimizer': optimizer_classifier.state_dict(), 'scheduler': scheduler.state_dict()}\n",
    "\n",
    "    print(\"STARTING TRAINING\")\n",
    "    for epoch in range(1, max_epochs):\n",
    "        epoch_start = time.time()\n",
    "\n",
    "        loss_main_sum, n_total = 0, 0\n",
    "        loss_domain_sum, loss_src_class_sum, loss_src_vat_sum, loss_trg_cent_sum, loss_trg_vat_sum = 0, 0, 0, 0, 0\n",
    "        running_corrects, running_correct_domain, total_for_accuracy, total_for_domain_accuracy = 0, 0, 0, 0\n",
    "\n",
    "        'TRAINING'\n",
    "        gesture_classifier.train()\n",
    "        for source_batch, target_batch in zip(train_dataset_source, train_dataset_target):\n",
    "\n",
    "            input_source, labels_source = source_batch\n",
    "            input_source, labels_source = input_source, labels_source\n",
    "            input_target, _ = target_batch\n",
    "            input_target = input_target\n",
    "\n",
    "            # Feed the inputs to the classifier network\n",
    "            # Retrieves the BN weights calculated so far for the source dataset\n",
    "            BN_weights = list_dictionaries_BN_weights[0]\n",
    "            gesture_classifier.load_state_dict(BN_weights, strict=False)\n",
    "            pred_gesture_source, pred_domain_source = gesture_classifier(input_source, get_all_tasks_output=True)\n",
    "\n",
    "            'Classifier losses setup.'\n",
    "            # Supervised/self-supervised gesture classification\n",
    "            loss_source_class = crossEntropyLoss(pred_gesture_source, labels_source)\n",
    "\n",
    "            # Try to be bad at the domain discrimination for the full network\n",
    "\n",
    "            label_source_domain = torch.zeros(len(pred_domain_source), device='cpu', dtype=torch.long)\n",
    "            loss_domain_source = crossEntropyLoss(pred_domain_source, label_source_domain)\n",
    "            # Combine all the loss of the classifier\n",
    "            loss_main_source = (0.5 * loss_source_class + domain_loss_weight * loss_domain_source)\n",
    "\n",
    "            ' Update networks '\n",
    "            # Update classifiers.\n",
    "            # Zero the gradients\n",
    "            optimizer_classifier.zero_grad()\n",
    "            # loss_main_source.backward(retain_graph=True)\n",
    "            loss_main_source.backward()\n",
    "            optimizer_classifier.step()\n",
    "            # Save the BN stats for the source\n",
    "            state_dict = gesture_classifier.state_dict()\n",
    "            batch_norm_dict = {}\n",
    "            for key in state_dict:\n",
    "                if \"batchNorm\" in key:\n",
    "                    batch_norm_dict.update({key: state_dict[key]})\n",
    "            list_dictionaries_BN_weights[0] = copy.deepcopy(batch_norm_dict)\n",
    "\n",
    "            _, pred_domain_target = gesture_classifier(input_target, get_all_tasks_output=True)\n",
    "            label_target_domain = torch.ones(len(pred_domain_target), device='cpu', dtype=torch.long)\n",
    "            loss_domain_target = 0.5 * (crossEntropyLoss(pred_domain_target, label_target_domain))\n",
    "            # Combine all the loss of the classifier\n",
    "            loss_domain_target = 0.5 * domain_loss_weight * loss_domain_target\n",
    "            # Update classifiers.\n",
    "            # Zero the gradients\n",
    "            loss_domain_target.backward()\n",
    "            optimizer_classifier.step()\n",
    "\n",
    "            # Save the BN stats for the target\n",
    "            state_dict = gesture_classifier.state_dict()\n",
    "            batch_norm_dict = {}\n",
    "            for key in state_dict:\n",
    "                if \"batchNorm\" in key:\n",
    "                    batch_norm_dict.update({key: state_dict[key]})\n",
    "            list_dictionaries_BN_weights[1] = copy.deepcopy(batch_norm_dict)\n",
    "\n",
    "            loss_main = loss_main_source + loss_domain_target\n",
    "            loss_domain = loss_domain_source + loss_domain_target\n",
    "\n",
    "            loss_domain_sum += loss_domain.item()\n",
    "            loss_src_class_sum += loss_source_class.item()\n",
    "            loss_main_sum += loss_main.item()\n",
    "            n_total += 1\n",
    "\n",
    "            _, gestures_predictions_source = torch.max(pred_gesture_source.data, 1)\n",
    "            running_corrects += torch.sum(gestures_predictions_source == labels_source.data)\n",
    "            total_for_accuracy += labels_source.size(0)\n",
    "\n",
    "            _, gestures_predictions_domain_source = torch.max(pred_domain_source.data, 1)\n",
    "            _, gestures_predictions_domain_target = torch.max(pred_domain_target.data, 1)\n",
    "            running_correct_domain += torch.sum(gestures_predictions_domain_source == label_source_domain.data)\n",
    "            running_correct_domain += torch.sum(gestures_predictions_domain_target == label_target_domain.data)\n",
    "            total_for_domain_accuracy += label_source_domain.size(0)\n",
    "            total_for_domain_accuracy += label_target_domain.size(0)\n",
    "\n",
    "        print('Accuracy source %4f,'\n",
    "              ' main loss classifier %4f,'\n",
    "              ' source classification loss %4f,'\n",
    "              ' loss domain distinction %4f,'\n",
    "              ' accuracy domain distinction %4f'\n",
    "              %\n",
    "              (running_corrects.item() / total_for_accuracy,\n",
    "               loss_main_sum / n_total,\n",
    "               loss_src_class_sum / n_total,\n",
    "               loss_domain_sum / n_total,\n",
    "               running_correct_domain.item() / total_for_domain_accuracy\n",
    "               ))\n",
    "\n",
    "        'VALIDATION STEP'\n",
    "        running_loss_validation = 0.\n",
    "        running_corrects_validation = 0\n",
    "        total_validation = 0\n",
    "        n_total_val = 0\n",
    "\n",
    "        # BN_weights = copy.deepcopy(list_dictionaries_BN_weights[0])\n",
    "        # gesture_classifier.load_state_dict(BN_weights, strict=False)\n",
    "        gesture_classifier.eval()\n",
    "        for validation_batch in validation_dataset_source:\n",
    "            # get the inputs\n",
    "            inputs, labels = validation_batch\n",
    "\n",
    "            inputs, labels = inputs, labels\n",
    "            # zero the parameter gradients\n",
    "            optimizer_classifier.zero_grad()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # forward\n",
    "                outputs = gesture_classifier(inputs)\n",
    "                _, predictions = torch.max(outputs.data, 1)\n",
    "\n",
    "                loss = crossEntropyLoss(outputs, labels)\n",
    "                loss = loss.item()\n",
    "\n",
    "                # statistics\n",
    "                running_loss_validation += loss\n",
    "                running_corrects_validation += torch.sum(predictions == labels.data)\n",
    "                total_validation += labels.size(0)\n",
    "                n_total_val += 1\n",
    "\n",
    "        epoch_loss = running_loss_validation / n_total_val\n",
    "        epoch_acc = running_corrects_validation.item() / total_validation\n",
    "        print('{} Loss: {:.8f} Acc: {:.8}'.format(\"VALIDATION\", epoch_loss, epoch_acc))\n",
    "\n",
    "        scheduler.step(running_loss_validation / n_total_val)\n",
    "        if running_loss_validation / n_total_val < best_loss:\n",
    "            print(\"New best validation loss: \", running_loss_validation / n_total_val)\n",
    "            best_loss = running_loss_validation / n_total_val\n",
    "            BN_weights = copy.deepcopy(list_dictionaries_BN_weights[1])\n",
    "            gesture_classifier.load_state_dict(BN_weights, strict=False)\n",
    "            best_state = {'epoch': epoch, 'state_dict': copy.deepcopy(gesture_classifier.state_dict()),\n",
    "                          'optimizer': optimizer_classifier.state_dict(), 'scheduler': scheduler.state_dict()}\n",
    "            patience = epoch + patience_increment\n",
    "\n",
    "        if patience < epoch:\n",
    "            break\n",
    "\n",
    "        print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "            epoch, max_epochs, time.time() - epoch_start))\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "    return best_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_DA_spectrograms(examples_datasets_train, labels_datasets_train, num_kernels, filter_size=(4, 10),\n",
    "                          algo_name=\"DANN\",\n",
    "                          path_weights_to_save_to=\"../Weights/weights_\", batch_size=512, patience_increment=10,\n",
    "                          path_weights_fine_tuning=\"../weights_TWO_CYCLES_normal_training_fine_tuning\",\n",
    "                          number_of_cycle_for_first_training=3, number_of_cycles_rest_of_training=3,\n",
    "                          number_of_classes=11, \n",
    "                          feature_vector_input_length=None, learning_rate=0.001316):\n",
    "    \"\"\"\n",
    "    examples_datasets_train\n",
    "    labels_datasets_train\n",
    "    num_kernels\n",
    "    filter_size\n",
    "    algo_name\n",
    "    path_weights_to_save_to: path to save DANN weights\n",
    "    batch_size\n",
    "    patience_increment\n",
    "    path_weights_fine_tuning: path to load normal TSD_DNN weights \n",
    "    number_of_cycle_for_first_training\n",
    "    number_of_cycles_rest_of_training\n",
    "    number_of_classes\n",
    "    spectrogram_model\n",
    "    \"\"\"\n",
    "    participants_train, participants_validation, participants_test = load_dataloaders_training_sessions(\n",
    "        examples_datasets_train, labels_datasets_train, batch_size=batch_size,\n",
    "        number_of_cycle_for_first_training=number_of_cycle_for_first_training, get_validation_set=True,\n",
    "        number_of_cycles_rest_of_training=number_of_cycles_rest_of_training)\n",
    "\n",
    "    for participant_i in range(len(participants_train)):\n",
    "        print(\"SHAPE SESSIONS: \", np.shape(participants_train[participant_i]))\n",
    "\n",
    "        # Skip the first session as it will be identical to normal training\n",
    "        for session_j in range(1, len(participants_train[participant_i])):\n",
    "            print(np.shape(participants_train[participant_i][session_j]))\n",
    "\n",
    "            gesture_classification = TSD_Network(number_of_class=number_of_classes, num_neurons=num_kernels,\n",
    "                                                 feature_vector_input_length=feature_vector_input_length)\n",
    "\n",
    "            # loss functions\n",
    "            crossEntropyLoss = nn.CrossEntropyLoss()\n",
    "            # optimizer\n",
    "            precision = 1e-8\n",
    "            optimizer_classifier = optim.Adam(gesture_classification.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
    "            scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer_classifier, mode='min', factor=.2,\n",
    "                                                             patience=5, verbose=True, eps=precision)\n",
    "            # Fine-tune from the previous training\n",
    "            gesture_classification, optimizer_classifier, scheduler, start_epoch = load_checkpoint( \n",
    "                model=gesture_classification, optimizer=optimizer_classifier, scheduler=scheduler,  \n",
    "                filename=path_weights_fine_tuning + \"/participant_%d/best_state_%d.pt\" % (participant_i, 0))  \n",
    "\n",
    "            best_weights = DANN_BN_Training(gesture_classifier=gesture_classification, scheduler=scheduler, \n",
    "                                            optimizer_classifier=optimizer_classifier,  \n",
    "                                            train_dataset_source=participants_train[participant_i][0],  \n",
    "                                            train_dataset_target=participants_train[participant_i][session_j],  \n",
    "                                            validation_dataset_source=participants_validation[participant_i][0],  \n",
    "                                            crossEntropyLoss=crossEntropyLoss,  \n",
    "                                            patience_increment=patience_increment,  \n",
    "                                            domain_loss_weight=1e-1)\n",
    "\n",
    "            if not os.path.exists(path_weights_to_save_to + algo_name + \"/participant_%d\" % participant_i):\n",
    "                os.makedirs(path_weights_to_save_to + algo_name + \"/participant_%d\" % participant_i)\n",
    "            torch.save(best_weights, f=path_weights_to_save_to + algo_name + \"/participant_%d/best_state_%d.pt\" % (participant_i, session_j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/home/laiy/gitrepos/msr_final/LongTermEMG-master\")\n",
    "from LongTermClassificationMain.Models.TSD_neural_network import TSD_Network\n",
    "from LongTermClassificationMain.TrainingsAndEvaluations.training_loops_preparations import load_checkpoint\n",
    "from LongTermClassificationMain.Models.model_training import train_model_standard\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "os.chdir(\"/home/laiy/gitrepos/msr_final/LongTermEMG-master/LongTermClassificationMain/TrainingsAndEvaluations/ForTrainingSessions/TSD_DNN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_neurons = [200, 200, 200]\n",
    "feature_vector_input_length = 385\n",
    "gestures_to_remove = [5, 6, 9, 10]\n",
    "gestures_to_remove = None\n",
    "number_of_class = 11\n",
    "number_of_cycle_for_first_training = 4\n",
    "number_of_cycles_rest_of_training = 4\n",
    "learning_rate = 0.002515\n",
    "\n",
    "path_weights_fine_tuning = \"Weights_TSD/weights_THREE_CYCLES_TSD_ELEVEN_Gestures\"\n",
    "algo_name = \"DANN_THREE_CYCLES_11Gestures_TSD\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET one participant_examples  (4, 4)\n",
      "   GET one training_index_examples  (4,)  at  0\n",
      "   GOT one group XY  (4024, 385)    (4024,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (3621, 385)    (3621,)\n",
      "       one group XY valid (403, 385)    (403, 385)\n",
      "   GET one training_index_examples  (4,)  at  1\n",
      "   GOT one group XY  (4190, 385)    (4190,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (3771, 385)    (3771,)\n",
      "       one group XY valid (419, 385)    (419, 385)\n",
      "   GET one training_index_examples  (4,)  at  2\n",
      "   GOT one group XY  (4289, 385)    (4289,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (3860, 385)    (3860,)\n",
      "       one group XY valid (429, 385)    (429, 385)\n",
      "   GET one training_index_examples  (4,)  at  3\n",
      "   GOT one group XY  (4099, 385)    (4099,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (3689, 385)    (3689,)\n",
      "       one group XY valid (410, 385)    (410, 385)\n",
      "dataloaders: \n",
      "   train  (1, 4)\n",
      "   valid  (1, 4)\n",
      "   test  (1, 0)\n",
      "SHAPE SESSIONS:  (4,)\n",
      "()\n",
      "TSD_Network(\n",
      "  (_network): ModuleList(\n",
      "    (0): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=385, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=385, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (1): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (2): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (_output): Linear(in_features=200, out_features=11, bias=True)\n",
      "  (_output_discriminator): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "Number Parameters:  162013\n",
      "=> loading checkpoint 'Weights_TSD/weights_THREE_CYCLES_TSD_ELEVEN_Gestures/participant_0/best_state_0.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint 'Weights_TSD/weights_THREE_CYCLES_TSD_ELEVEN_Gestures/participant_0/best_state_0.pt' (epoch 50)\n",
      "STARTING TRAINING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laiy/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:130: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  Variable._execution_engine.run_backward(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy source 0.891462, main loss classifier 0.286335, source classification loss 0.400808, loss domain distinction 0.641986, accuracy domain distinction 0.501953\n",
      "VALIDATION Loss: 0.49979112 Acc: 0.84863524\n",
      "New best validation loss:  0.49979111552238464\n",
      "Epoch 1 of 500 took 0.594s\n",
      "Accuracy source 0.903739, main loss classifier 0.250760, source classification loss 0.337574, loss domain distinction 0.578003, accuracy domain distinction 0.511858\n",
      "VALIDATION Loss: 0.51330012 Acc: 0.83870968\n",
      "Epoch 2 of 500 took 0.633s\n",
      "Accuracy source 0.906808, main loss classifier 0.230268, source classification loss 0.302816, loss domain distinction 0.526905, accuracy domain distinction 0.510324\n",
      "VALIDATION Loss: 0.48187330 Acc: 0.8560794\n",
      "New best validation loss:  0.4818733036518097\n",
      "Epoch 3 of 500 took 0.532s\n",
      "Accuracy source 0.919643, main loss classifier 0.208102, source classification loss 0.262303, loss domain distinction 0.486857, accuracy domain distinction 0.501814\n",
      "VALIDATION Loss: 0.55613410 Acc: 0.83126551\n",
      "Epoch 4 of 500 took 0.585s\n",
      "Accuracy source 0.916016, main loss classifier 0.208571, source classification loss 0.268441, loss domain distinction 0.441441, accuracy domain distinction 0.499721\n",
      "VALIDATION Loss: 0.58539969 Acc: 0.82630273\n",
      "Epoch 5 of 500 took 0.640s\n",
      "Accuracy source 0.916016, main loss classifier 0.208607, source classification loss 0.270808, loss domain distinction 0.411766, accuracy domain distinction 0.504325\n",
      "VALIDATION Loss: 0.45724308 Acc: 0.86104218\n",
      "Epoch    56: reducing learning rate of group 0 to 4.0240e-06.\n",
      "New best validation loss:  0.45724308490753174\n",
      "Epoch 6 of 500 took 0.552s\n",
      "Accuracy source 0.920201, main loss classifier 0.204249, source classification loss 0.261389, loss domain distinction 0.399893, accuracy domain distinction 0.493164\n",
      "VALIDATION Loss: 0.59260333 Acc: 0.82630273\n",
      "Epoch 7 of 500 took 0.625s\n",
      "Accuracy source 0.921317, main loss classifier 0.197125, source classification loss 0.249675, loss domain distinction 0.386095, accuracy domain distinction 0.500558\n",
      "VALIDATION Loss: 0.57032335 Acc: 0.83126551\n",
      "Epoch 8 of 500 took 0.620s\n",
      "Accuracy source 0.917690, main loss classifier 0.203332, source classification loss 0.262705, loss domain distinction 0.376349, accuracy domain distinction 0.501116\n",
      "VALIDATION Loss: 0.47237912 Acc: 0.8560794\n",
      "Epoch 9 of 500 took 0.591s\n",
      "Accuracy source 0.919922, main loss classifier 0.197126, source classification loss 0.250895, loss domain distinction 0.367860, accuracy domain distinction 0.502372\n",
      "VALIDATION Loss: 0.50361419 Acc: 0.84367246\n",
      "Epoch 10 of 500 took 0.543s\n",
      "Accuracy source 0.921596, main loss classifier 0.195447, source classification loss 0.245718, loss domain distinction 0.374225, accuracy domain distinction 0.501953\n",
      "VALIDATION Loss: 0.43444535 Acc: 0.87096774\n",
      "New best validation loss:  0.4344453513622284\n",
      "Epoch 11 of 500 took 0.589s\n",
      "Accuracy source 0.915737, main loss classifier 0.200818, source classification loss 0.259477, loss domain distinction 0.362090, accuracy domain distinction 0.499442\n",
      "VALIDATION Loss: 0.42424935 Acc: 0.87344913\n",
      "Epoch    62: reducing learning rate of group 0 to 8.0480e-07.\n",
      "New best validation loss:  0.4242493510246277\n",
      "Epoch 12 of 500 took 0.528s\n",
      "Accuracy source 0.922712, main loss classifier 0.193096, source classification loss 0.242552, loss domain distinction 0.362339, accuracy domain distinction 0.500140\n",
      "VALIDATION Loss: 0.61298299 Acc: 0.82382134\n",
      "Epoch 13 of 500 took 0.522s\n",
      "Accuracy source 0.917969, main loss classifier 0.205837, source classification loss 0.269106, loss domain distinction 0.356110, accuracy domain distinction 0.503488\n",
      "VALIDATION Loss: 0.47623101 Acc: 0.85856079\n",
      "Epoch 14 of 500 took 0.527s\n",
      "Accuracy source 0.922991, main loss classifier 0.193450, source classification loss 0.242393, loss domain distinction 0.364029, accuracy domain distinction 0.496931\n",
      "VALIDATION Loss: 0.38407785 Acc: 0.86848635\n",
      "New best validation loss:  0.38407784700393677\n",
      "Epoch 15 of 500 took 0.519s\n",
      "Accuracy source 0.919364, main loss classifier 0.193002, source classification loss 0.243425, loss domain distinction 0.356578, accuracy domain distinction 0.501814\n",
      "VALIDATION Loss: 0.42417604 Acc: 0.86600496\n",
      "Epoch 16 of 500 took 0.590s\n",
      "Accuracy source 0.916853, main loss classifier 0.197614, source classification loss 0.250853, loss domain distinction 0.364505, accuracy domain distinction 0.498465\n",
      "VALIDATION Loss: 0.38968828 Acc: 0.88337469\n",
      "Epoch 17 of 500 took 0.648s\n",
      "Accuracy source 0.919085, main loss classifier 0.195539, source classification loss 0.246182, loss domain distinction 0.364690, accuracy domain distinction 0.500140\n",
      "VALIDATION Loss: 0.43891165 Acc: 0.87593052\n",
      "Epoch    68: reducing learning rate of group 0 to 1.6096e-07.\n",
      "Epoch 18 of 500 took 0.529s\n",
      "Accuracy source 0.921596, main loss classifier 0.200632, source classification loss 0.259242, loss domain distinction 0.351067, accuracy domain distinction 0.503348\n",
      "VALIDATION Loss: 0.47010958 Acc: 0.8560794\n",
      "Epoch 19 of 500 took 0.593s\n",
      "Accuracy source 0.919922, main loss classifier 0.197888, source classification loss 0.252736, loss domain distinction 0.353548, accuracy domain distinction 0.499721\n",
      "VALIDATION Loss: 0.68986672 Acc: 0.80645161\n",
      "Epoch 20 of 500 took 0.536s\n",
      "Accuracy source 0.917411, main loss classifier 0.195626, source classification loss 0.249260, loss domain distinction 0.353433, accuracy domain distinction 0.502930\n",
      "VALIDATION Loss: 0.48775557 Acc: 0.85111663\n",
      "Epoch 21 of 500 took 0.537s\n",
      "Accuracy source 0.919364, main loss classifier 0.193390, source classification loss 0.243816, loss domain distinction 0.357002, accuracy domain distinction 0.496791\n",
      "VALIDATION Loss: 0.45235091 Acc: 0.86600496\n",
      "Epoch 22 of 500 took 0.554s\n",
      "Accuracy source 0.920201, main loss classifier 0.198370, source classification loss 0.253434, loss domain distinction 0.356765, accuracy domain distinction 0.496791\n",
      "VALIDATION Loss: 0.53730029 Acc: 0.84367246\n",
      "Epoch 23 of 500 took 0.527s\n",
      "Accuracy source 0.921875, main loss classifier 0.193851, source classification loss 0.244937, loss domain distinction 0.353066, accuracy domain distinction 0.501116\n",
      "VALIDATION Loss: 0.53692943 Acc: 0.84615385\n",
      "Epoch    74: reducing learning rate of group 0 to 3.2192e-08.\n",
      "Epoch 24 of 500 took 0.524s\n",
      "Accuracy source 0.921317, main loss classifier 0.192427, source classification loss 0.242337, loss domain distinction 0.353140, accuracy domain distinction 0.496512\n",
      "VALIDATION Loss: 0.50099009 Acc: 0.84367246\n",
      "Epoch 25 of 500 took 0.533s\n",
      "Accuracy source 0.919922, main loss classifier 0.198229, source classification loss 0.253754, loss domain distinction 0.351522, accuracy domain distinction 0.500279\n",
      "VALIDATION Loss: 0.45097801 Acc: 0.86600496\n",
      "Training complete in 0m 15s\n",
      "()\n",
      "TSD_Network(\n",
      "  (_network): ModuleList(\n",
      "    (0): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=385, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=385, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (1): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (2): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (_output): Linear(in_features=200, out_features=11, bias=True)\n",
      "  (_output_discriminator): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "Number Parameters:  162013\n",
      "=> loading checkpoint 'Weights_TSD/weights_THREE_CYCLES_TSD_ELEVEN_Gestures/participant_0/best_state_0.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint 'Weights_TSD/weights_THREE_CYCLES_TSD_ELEVEN_Gestures/participant_0/best_state_0.pt' (epoch 50)\n",
      "STARTING TRAINING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy source 0.885324, main loss classifier 0.301480, source classification loss 0.430092, loss domain distinction 0.643969, accuracy domain distinction 0.497210\n",
      "VALIDATION Loss: 0.63950646 Acc: 0.81141439\n",
      "New best validation loss:  0.639506459236145\n",
      "Epoch 1 of 500 took 0.532s\n",
      "Accuracy source 0.899275, main loss classifier 0.250791, source classification loss 0.336432, loss domain distinction 0.586922, accuracy domain distinction 0.499442\n",
      "VALIDATION Loss: 0.55062532 Acc: 0.83126551\n",
      "New best validation loss:  0.5506253242492676\n",
      "Epoch 2 of 500 took 0.526s\n",
      "Accuracy source 0.907924, main loss classifier 0.233763, source classification loss 0.308926, loss domain distinction 0.532995, accuracy domain distinction 0.492327\n",
      "VALIDATION Loss: 0.55077571 Acc: 0.82878412\n",
      "Epoch 3 of 500 took 0.520s\n",
      "Accuracy source 0.912109, main loss classifier 0.217942, source classification loss 0.281916, loss domain distinction 0.484622, accuracy domain distinction 0.496094\n",
      "VALIDATION Loss: 0.46626574 Acc: 0.8337469\n",
      "New best validation loss:  0.4662657380104065\n",
      "Epoch 4 of 500 took 0.528s\n",
      "Accuracy source 0.919085, main loss classifier 0.208732, source classification loss 0.270623, loss domain distinction 0.431153, accuracy domain distinction 0.505162\n",
      "VALIDATION Loss: 0.55863565 Acc: 0.81637717\n",
      "Epoch 5 of 500 took 0.521s\n",
      "Accuracy source 0.924107, main loss classifier 0.195876, source classification loss 0.244944, loss domain distinction 0.413712, accuracy domain distinction 0.502232\n",
      "VALIDATION Loss: 0.37837663 Acc: 0.88337469\n",
      "Epoch    56: reducing learning rate of group 0 to 4.0240e-06.\n",
      "New best validation loss:  0.37837663292884827\n",
      "Epoch 6 of 500 took 0.527s\n",
      "Accuracy source 0.918527, main loss classifier 0.194787, source classification loss 0.243518, loss domain distinction 0.391998, accuracy domain distinction 0.499023\n",
      "VALIDATION Loss: 0.42883319 Acc: 0.84119107\n",
      "Epoch 7 of 500 took 0.523s\n",
      "Accuracy source 0.917411, main loss classifier 0.204327, source classification loss 0.263697, loss domain distinction 0.387766, accuracy domain distinction 0.496931\n",
      "VALIDATION Loss: 0.44647032 Acc: 0.86104218\n",
      "Epoch 8 of 500 took 0.528s\n",
      "Accuracy source 0.916574, main loss classifier 0.197623, source classification loss 0.249749, loss domain distinction 0.384539, accuracy domain distinction 0.502372\n",
      "VALIDATION Loss: 0.49717885 Acc: 0.84119107\n",
      "Epoch 9 of 500 took 0.519s\n",
      "Accuracy source 0.916016, main loss classifier 0.202008, source classification loss 0.258152, loss domain distinction 0.384317, accuracy domain distinction 0.494420\n",
      "VALIDATION Loss: 0.68658006 Acc: 0.79652605\n",
      "Epoch 10 of 500 took 0.520s\n",
      "Accuracy source 0.915458, main loss classifier 0.195645, source classification loss 0.247487, loss domain distinction 0.369793, accuracy domain distinction 0.499721\n",
      "VALIDATION Loss: 0.51750463 Acc: 0.83126551\n",
      "Epoch 11 of 500 took 0.520s\n",
      "Accuracy source 0.921875, main loss classifier 0.195649, source classification loss 0.246895, loss domain distinction 0.366877, accuracy domain distinction 0.497907\n",
      "VALIDATION Loss: 0.61313790 Acc: 0.80397022\n",
      "Epoch    62: reducing learning rate of group 0 to 8.0480e-07.\n",
      "Epoch 12 of 500 took 0.518s\n",
      "Accuracy source 0.918527, main loss classifier 0.194597, source classification loss 0.245838, loss domain distinction 0.361921, accuracy domain distinction 0.501674\n",
      "VALIDATION Loss: 0.56947756 Acc: 0.83622829\n",
      "Epoch 13 of 500 took 0.522s\n",
      "Accuracy source 0.922154, main loss classifier 0.197149, source classification loss 0.250003, loss domain distinction 0.363581, accuracy domain distinction 0.494001\n",
      "VALIDATION Loss: 0.55497205 Acc: 0.80397022\n",
      "Epoch 14 of 500 took 0.526s\n",
      "Accuracy source 0.915458, main loss classifier 0.202185, source classification loss 0.261026, loss domain distinction 0.360146, accuracy domain distinction 0.495954\n",
      "VALIDATION Loss: 0.40977299 Acc: 0.85856079\n",
      "Epoch 15 of 500 took 0.533s\n",
      "Accuracy source 0.921875, main loss classifier 0.198004, source classification loss 0.251744, loss domain distinction 0.361985, accuracy domain distinction 0.498326\n",
      "VALIDATION Loss: 0.54265624 Acc: 0.82878412\n",
      "Epoch 16 of 500 took 0.541s\n",
      "Accuracy source 0.921875, main loss classifier 0.188725, source classification loss 0.235157, loss domain distinction 0.350719, accuracy domain distinction 0.497768\n",
      "VALIDATION Loss: 0.56054670 Acc: 0.82630273\n",
      "Training complete in 0m 9s\n",
      "()\n",
      "TSD_Network(\n",
      "  (_network): ModuleList(\n",
      "    (0): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=385, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=385, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (1): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (2): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (_output): Linear(in_features=200, out_features=11, bias=True)\n",
      "  (_output_discriminator): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "Number Parameters:  162013\n",
      "=> loading checkpoint 'Weights_TSD/weights_THREE_CYCLES_TSD_ELEVEN_Gestures/participant_0/best_state_0.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint 'Weights_TSD/weights_THREE_CYCLES_TSD_ELEVEN_Gestures/participant_0/best_state_0.pt' (epoch 50)\n",
      "STARTING TRAINING\n",
      "Accuracy source 0.887556, main loss classifier 0.302173, source classification loss 0.429697, loss domain distinction 0.655773, accuracy domain distinction 0.502511\n",
      "VALIDATION Loss: 0.52116990 Acc: 0.8337469\n",
      "New best validation loss:  0.521169900894165\n",
      "Epoch 1 of 500 took 0.559s\n",
      "Accuracy source 0.901507, main loss classifier 0.251035, source classification loss 0.334932, loss domain distinction 0.594138, accuracy domain distinction 0.497349\n",
      "VALIDATION Loss: 0.58242440 Acc: 0.81885856\n",
      "Epoch 2 of 500 took 0.516s\n",
      "Accuracy source 0.907087, main loss classifier 0.233949, source classification loss 0.307938, loss domain distinction 0.538058, accuracy domain distinction 0.509068\n",
      "VALIDATION Loss: 0.40648788 Acc: 0.87593052\n",
      "New best validation loss:  0.4064878821372986\n",
      "Epoch 3 of 500 took 0.527s\n",
      "Accuracy source 0.911272, main loss classifier 0.217072, source classification loss 0.281177, loss domain distinction 0.481893, accuracy domain distinction 0.509487\n",
      "VALIDATION Loss: 0.47880930 Acc: 0.83870968\n",
      "Epoch 4 of 500 took 0.524s\n",
      "Accuracy source 0.909319, main loss classifier 0.208512, source classification loss 0.268483, loss domain distinction 0.440118, accuracy domain distinction 0.505301\n",
      "VALIDATION Loss: 0.35732794 Acc: 0.88833747\n",
      "New best validation loss:  0.357327938079834\n",
      "Epoch 5 of 500 took 0.527s\n",
      "Accuracy source 0.920480, main loss classifier 0.195164, source classification loss 0.242645, loss domain distinction 0.415638, accuracy domain distinction 0.502372\n",
      "VALIDATION Loss: 0.49123195 Acc: 0.84615385\n",
      "Epoch    56: reducing learning rate of group 0 to 4.0240e-06.\n",
      "Epoch 6 of 500 took 0.518s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy source 0.910993, main loss classifier 0.205884, source classification loss 0.267212, loss domain distinction 0.390884, accuracy domain distinction 0.504046\n",
      "VALIDATION Loss: 0.42521551 Acc: 0.85111663\n",
      "Epoch 7 of 500 took 0.519s\n",
      "Accuracy source 0.914900, main loss classifier 0.198051, source classification loss 0.251089, loss domain distinction 0.387187, accuracy domain distinction 0.501535\n",
      "VALIDATION Loss: 0.41860244 Acc: 0.86848635\n",
      "Epoch 8 of 500 took 0.526s\n",
      "Accuracy source 0.921038, main loss classifier 0.195871, source classification loss 0.246116, loss domain distinction 0.384408, accuracy domain distinction 0.494420\n",
      "VALIDATION Loss: 0.54058003 Acc: 0.82630273\n",
      "Epoch 9 of 500 took 0.518s\n",
      "Accuracy source 0.916853, main loss classifier 0.199931, source classification loss 0.256751, loss domain distinction 0.372859, accuracy domain distinction 0.502790\n",
      "VALIDATION Loss: 0.56878448 Acc: 0.82133995\n",
      "Epoch 10 of 500 took 0.521s\n",
      "Accuracy source 0.915458, main loss classifier 0.201753, source classification loss 0.258893, loss domain distinction 0.375083, accuracy domain distinction 0.501674\n",
      "VALIDATION Loss: 0.39642227 Acc: 0.87841191\n",
      "Epoch 11 of 500 took 0.521s\n",
      "Accuracy source 0.926897, main loss classifier 0.187763, source classification loss 0.231855, loss domain distinction 0.364249, accuracy domain distinction 0.499860\n",
      "VALIDATION Loss: 0.44742656 Acc: 0.8560794\n",
      "Epoch    62: reducing learning rate of group 0 to 8.0480e-07.\n",
      "Epoch 12 of 500 took 0.518s\n",
      "Accuracy source 0.921317, main loss classifier 0.189637, source classification loss 0.237325, loss domain distinction 0.356174, accuracy domain distinction 0.496931\n",
      "VALIDATION Loss: 0.59534019 Acc: 0.81885856\n",
      "Epoch 13 of 500 took 0.519s\n",
      "Accuracy source 0.917690, main loss classifier 0.193858, source classification loss 0.244509, loss domain distinction 0.362046, accuracy domain distinction 0.494141\n",
      "VALIDATION Loss: 0.41289985 Acc: 0.85111663\n",
      "Epoch 14 of 500 took 0.521s\n",
      "Accuracy source 0.925223, main loss classifier 0.190783, source classification loss 0.239630, loss domain distinction 0.354155, accuracy domain distinction 0.507115\n",
      "VALIDATION Loss: 0.48406762 Acc: 0.83622829\n",
      "Epoch 15 of 500 took 0.521s\n",
      "Accuracy source 0.921875, main loss classifier 0.192599, source classification loss 0.243613, loss domain distinction 0.354232, accuracy domain distinction 0.504046\n",
      "VALIDATION Loss: 0.53471464 Acc: 0.83622829\n",
      "Training complete in 0m 8s\n"
     ]
    }
   ],
   "source": [
    "train_DA_spectrograms(examples_datasets_train, labels_datasets_train, filter_size=None,\n",
    "                      num_kernels=num_neurons, algo_name=algo_name,\n",
    "                      path_weights_fine_tuning=path_weights_fine_tuning,\n",
    "                      number_of_classes=number_of_class,\n",
    "                      number_of_cycle_for_first_training=number_of_cycle_for_first_training,\n",
    "                      number_of_cycles_rest_of_training=number_of_cycles_rest_of_training,\n",
    "                      batch_size=128,\n",
    "                      feature_vector_input_length=feature_vector_input_length,\n",
    "                      path_weights_to_save_to=\"Weights_TSD/weights_\", learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
