{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import pickle \n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/home/laiy/gitrepos/msr_final/LongTermEMG_myo\")\n",
    "# from PrepareAndLoadData.process_data import read_data_training\n",
    "from PrepareAndLoadData.process_data import format_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/home/laiy/gitrepos/msr_final/Wearable_Sensor_Long-term_sEMG_Dataset/data\"\n",
    "processed_data_dir = \"/home/laiy/gitrepos/msr_final/LongTermEMG_myo/Processed_datasets\"\n",
    "code_dir = \"/home/laiy/gitrepos/msr_final/LongTermEMG_myo\"\n",
    "save_dir = \"/home/laiy/gitrepos/msr_final/LongTermEMG_myo/Results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read_data_training(path=data_dir, store_path = processed_data_dir, num_participant=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pos = ['N', 'I', 'O', 'N', 'I', 'I', 'O', 'O', 'N', 'N',      # 1-10th day \n",
    "      'O', 'N', 'N', 'O', 'O', 'I', 'I', 'I', 'N', 'O',       # 11-20th day\n",
    "      'O', 'I', 'O', 'I', 'I', 'N', 'N', 'I', 'N', 'O']       # 21-30th day\n",
    "\n",
    "pos_label = [1, 2, 3, 1, 2, 2, 3, 3, 1, 1,\n",
    "            3, 1, 1, 3, 3, 2, 2, 2, 1, 3,\n",
    "            3, 2, 3, 2, 2, 1, 1, 2, 1, 3] # N: 1, I: 2, O: 3\n",
    "# days correspond to N, I, and O positions\n",
    "sessions_idx = [[],[],[]]\n",
    "for idx, pl in enumerate(pos_label):\n",
    "    sessions_idx[pl-1].append(idx+1)\n",
    "\n",
    "day_num = 30\n",
    "sub_num = 5\n",
    "mov_num = 22\n",
    "fs = 200\n",
    "ch_num = 8\n",
    "trial_num = 4\n",
    "\n",
    "win_size = 30            # 300ms window\n",
    "win_inc = 5             # 50ms overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files_to_format_training_session(path_folder_examples, day_num,\n",
    "                                          number_of_cycles, number_of_gestures, window_size,\n",
    "                                          size_non_overlap):\n",
    "    \"\"\"\n",
    "    path_folder_examples: path to load training data\n",
    "    feature_set_function\n",
    "    number_of_cycles: number of trials recorded for each motion\n",
    "    number_of_gestures\n",
    "    window_size: analysis window size\n",
    "    size_non_overlap: length of non-overlap portion between each analysis window\n",
    "    \n",
    "    shape(formated_example) = (26, 50, 8)\n",
    "    \"\"\"\n",
    "    examples_training, labels_training = [], []\n",
    "    \n",
    "    for cycle in range(1, number_of_cycles+1):\n",
    "        examples, labels = [], []\n",
    "        for gesture_index in range(1, number_of_gestures+1):\n",
    "            read_file = path_folder_examples + \"/D\" + str(day_num) + \"M\" + str(gesture_index) + \"T\" + str(cycle) + \".csv\"\n",
    "            # print(\"      READ \", read_file)\n",
    "            # skipping odd number rows to have freq = 100Hz\n",
    "            examples_to_format = pd.read_csv(read_file, header=None, skiprows=lambda x: x%2 == 1).to_numpy()\n",
    "            # each file contains 15s (300 rows) of 8 channel signals \n",
    "            # print(\"            data = \", np.shape(examples_to_format))\n",
    "            \n",
    "            examples_formatted = format_examples(examples_to_format,\n",
    "                                     window_size=window_size,\n",
    "                                     size_non_overlap=size_non_overlap)\n",
    "            # print(\"            formated = \", np.shape(examples_formatted))\n",
    "\n",
    "            examples.extend(examples_formatted)\n",
    "            labels.extend(np.ones(len(examples_formatted)) * (gesture_index-1))\n",
    "            \n",
    "        # print(\"   SHAPE SESSION \", cycle, \" EXAMPLES: \", np.shape(examples))\n",
    "        examples_training.append(examples)\n",
    "        labels_training.append(labels)\n",
    "        # print(\"   SHAPE ALL SESSION EXAMPLES: \", np.shape(examples_training))  \n",
    "\n",
    "    return examples_training, labels_training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_and_process_it_from_file(path, number_of_gestures=22, number_of_cycles=4, window_size=50, \n",
    "                                        size_non_overlap=10, num_participant=5):\n",
    "\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        path: path to load training data\n",
    "        number_of_gestures\n",
    "        number_of_cycles: number of trials recorded for each motion\n",
    "        window_size: analysis window size\n",
    "        size_non_overlap: length of non-overlap portion between each analysis window\n",
    "\n",
    "    Returns:\n",
    "        loaded data dictionary containing `examples_training` and `labels_training`\n",
    "    \"\"\"\n",
    "    examples_training_sessions_datasets, labels_training_sessions_datasets = [], []\n",
    "\n",
    "    # load one participant for now\n",
    "    for index_participant in range(1,1+num_participant):\n",
    "        # load one participant data \n",
    "        folder_participant = \"sub\" + str(index_participant)\n",
    "        examples_participant_training_sessions, labels_participant_training_sessions = [], []\n",
    "        for days_of_current_session in sessions_idx:\n",
    "            print(\"process data in days \", days_of_current_session)\n",
    "            examples_per_session, labels_per_session = [], []\n",
    "            for day_num in days_of_current_session:\n",
    "                path_folder_examples = path + \"/\" + folder_participant + \"/day\" + str(day_num)\n",
    "                # print(\"current dr = \", day_num)\n",
    "                \n",
    "                examples_training, labels_training  = \\\n",
    "                    read_files_to_format_training_session(path_folder_examples=path_folder_examples,\n",
    "                                                        day_num = day_num,\n",
    "                                                        number_of_cycles=number_of_cycles,\n",
    "                                                        number_of_gestures=number_of_gestures,\n",
    "                                                        window_size=window_size,\n",
    "                                                        size_non_overlap=size_non_overlap)\n",
    "                examples_per_session.extend(examples_training)\n",
    "                labels_per_session.extend(labels_training)\n",
    "            examples_participant_training_sessions.append(examples_per_session)\n",
    "            labels_participant_training_sessions.append(labels_per_session)\n",
    "            print(\"@ traning sessions = \", np.shape(examples_participant_training_sessions))\n",
    "\n",
    "\n",
    "        # participants_num x sessions_num(3) x days_per_session(10)*trail_per_day(4) x #examples_window*#mov(26*22=572) x window_size x channel_num\n",
    "        print('traning examples ', np.shape(examples_participant_training_sessions))\n",
    "        examples_training_sessions_datasets.append(examples_participant_training_sessions)\n",
    "        print('all traning examples ', np.shape(examples_training_sessions_datasets))\n",
    "\n",
    "        # participants_num x sessions_num(3) x days_per_session(10)*trail_per_day(4) x #examples_window*#mov(26*22=572)\n",
    "        print('traning labels ', np.shape(labels_participant_training_sessions))\n",
    "        labels_training_sessions_datasets.append(labels_participant_training_sessions)\n",
    "        print('all traning labels ', np.shape(labels_training_sessions_datasets))\n",
    "    \n",
    "    # store processed data to dictionary\n",
    "    dataset_dictionnary = {\"examples_training\": np.array(examples_training_sessions_datasets, dtype=object),\n",
    "                        \"labels_training\": np.array(labels_training_sessions_datasets, dtype=object)}\n",
    "    return dataset_dictionnary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_training(path, store_path, number_of_gestures=22, number_of_cycles=4, window_size=50, \n",
    "                        size_non_overlap=10, num_participant=5):\n",
    "    \"\"\"\n",
    "    path: path to load training data\n",
    "    store_path: path to stored loaded data dictionary\n",
    "        contains `examples_training` and `labels_training`\n",
    "    number_of_gestures\n",
    "    number_of_cycles: number of trials recorded for each motion\n",
    "    window_size: analysis window size\n",
    "    size_non_overlap: length of non-overlap portion between each analysis window\n",
    "    num_participant: number of participant dataset to load\n",
    "    \"\"\"\n",
    "    print(\"Loading and preparing Training datasets...\")\n",
    "    dataset_dictionnary = get_data_and_process_it_from_file(path=path, number_of_gestures=number_of_gestures,\n",
    "                                                            number_of_cycles=number_of_cycles, window_size=window_size,\n",
    "                                                            size_non_overlap=size_non_overlap, num_participant=num_participant)\n",
    "\n",
    "    # store dictionary to pickle\n",
    "    training_session_dataset_dictionnary = {}\n",
    "    training_session_dataset_dictionnary[\"examples_training\"] = dataset_dictionnary[\"examples_training\"]\n",
    "    training_session_dataset_dictionnary[\"labels_training\"] = dataset_dictionnary[\"labels_training\"]\n",
    "\n",
    "    with open(store_path + \"/training_session.pickle\", 'wb') as f:\n",
    "        pickle.dump(training_session_dataset_dictionnary, f, pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read_data_training(path=data_dir, store_path = processed_data_dir, num_participant=3,\n",
    "#                   window_size = 30, size_non_overlap = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. TSD_DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TrainingsAndEvaluations.ForTrainingSessions.train_tsd_dnn_standard import \\\n",
    "            test_TSD_DNN_on_training_sessions, train_fine_tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traning examples  (3, 3, 40, 550, 252)\n",
      "traning labels  (3, 3, 40, 550)\n"
     ]
    }
   ],
   "source": [
    "# check stored pickle \n",
    "with open(processed_data_dir + \"/training_session.pickle\", 'rb') as f:\n",
    "    dataset_training = pickle.load(file=f)\n",
    "\n",
    "examples_datasets_train = dataset_training['examples_training']\n",
    "print('traning examples ', np.shape(examples_datasets_train))\n",
    "labels_datasets_train = dataset_training['labels_training']\n",
    "print('traning labels ', np.shape(labels_datasets_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_kernels=[200, 200, 200]                        # model layer size \n",
    "number_of_cycle_for_first_training=40               # #session\n",
    "number_of_cycles_rest_of_training=40     \n",
    "path_to_save_to=\"/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/TSD\"\n",
    "number_of_classes=22\n",
    "batch_size=128          \n",
    "feature_vector_input_length=252                     # size of one example \n",
    "learning_rate=0.002515"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET one participant_examples  (3, 40, 550, 252)\n",
      "   GET one training_index_examples  (40, 550, 252)  at  0\n",
      "   GOT one group XY  (2200, 252)    (2200,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (1980, 252)    (1980,)\n",
      "       one group XY valid (220, 252)    (220, 252)\n",
      "   GET one training_index_examples  (40, 550, 252)  at  1\n",
      "   GOT one group XY  (2200, 252)    (2200,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (1980, 252)    (1980,)\n",
      "       one group XY valid (220, 252)    (220, 252)\n",
      "   GET one training_index_examples  (40, 550, 252)  at  2\n",
      "   GOT one group XY  (2200, 252)    (2200,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (1980, 252)    (1980,)\n",
      "       one group XY valid (220, 252)    (220, 252)\n",
      "dataloaders: \n",
      "   train  (1, 3)\n",
      "   valid  (1, 3)\n",
      "   test  (1, 0)\n",
      "GET one participant_examples  (3, 40, 550, 252)\n",
      "   GET one training_index_examples  (40, 550, 252)  at  0\n",
      "   GOT one group XY  (2200, 252)    (2200,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (1980, 252)    (1980,)\n",
      "       one group XY valid (220, 252)    (220, 252)\n",
      "   GET one training_index_examples  (40, 550, 252)  at  1\n",
      "   GOT one group XY  (2200, 252)    (2200,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (1980, 252)    (1980,)\n",
      "       one group XY valid (220, 252)    (220, 252)\n",
      "   GET one training_index_examples  (40, 550, 252)  at  2\n",
      "   GOT one group XY  (2200, 252)    (2200,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (1980, 252)    (1980,)\n",
      "       one group XY valid (220, 252)    (220, 252)\n",
      "dataloaders: \n",
      "   train  (2, 3)\n",
      "   valid  (2, 3)\n",
      "   test  (2, 0)\n",
      "GET one participant_examples  (3, 40, 550, 252)\n",
      "   GET one training_index_examples  (40, 550, 252)  at  0\n",
      "   GOT one group XY  (2200, 252)    (2200,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (1980, 252)    (1980,)\n",
      "       one group XY valid (220, 252)    (220, 252)\n",
      "   GET one training_index_examples  (40, 550, 252)  at  1\n",
      "   GOT one group XY  (2200, 252)    (2200,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (1980, 252)    (1980,)\n",
      "       one group XY valid (220, 252)    (220, 252)\n",
      "   GET one training_index_examples  (40, 550, 252)  at  2\n",
      "   GOT one group XY  (2200, 252)    (2200,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (1980, 252)    (1980,)\n",
      "       one group XY valid (220, 252)    (220, 252)\n",
      "dataloaders: \n",
      "   train  (3, 3)\n",
      "   valid  (3, 3)\n",
      "   test  (3, 0)\n",
      "START TRAINING\n",
      "Participant:  0\n",
      "Session:  0\n",
      "TSD_Network(\n",
      "  (_network): ModuleList(\n",
      "    (0): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=252, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=252, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (1): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (2): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (_output): Linear(in_features=200, out_features=22, bias=True)\n",
      "  (_output_discriminator): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "Number Parameters:  137624\n",
      "<generator object Module.parameters at 0x7fb108d9ceb0>\n",
      "Epoch 0/499\n",
      "----------\n",
      "train Loss: 0.01657957 Acc: 0.45208333\n",
      "val Loss: 0.00468287 Acc: 0.87727273\n",
      "New best validation loss: 0.0046828714284029875\n",
      "Epoch 1 of 500 took 0.124s\n",
      "Epoch 1/499\n",
      "----------\n",
      "train Loss: 0.00746641 Acc: 0.78541667\n",
      "val Loss: 0.00194610 Acc: 0.93181818\n",
      "New best validation loss: 0.0019461001862179149\n",
      "Epoch 2 of 500 took 0.104s\n",
      "Epoch 2/499\n",
      "----------\n",
      "train Loss: 0.00497356 Acc: 0.8390625\n",
      "val Loss: 0.00157622 Acc: 0.90909091\n",
      "New best validation loss: 0.001576217602599751\n",
      "Epoch 3 of 500 took 0.100s\n",
      "Epoch 3/499\n",
      "----------\n",
      "train Loss: 0.00373044 Acc: 0.88020833\n",
      "val Loss: 0.00087748 Acc: 0.95\n",
      "New best validation loss: 0.0008774787187576294\n",
      "Epoch 4 of 500 took 0.099s\n",
      "Epoch 4/499\n",
      "----------\n",
      "train Loss: 0.00300420 Acc: 0.88489583\n",
      "val Loss: 0.00085425 Acc: 0.95909091\n",
      "Epoch 5 of 500 took 0.104s\n",
      "Epoch 5/499\n",
      "----------\n",
      "train Loss: 0.00245154 Acc: 0.90260417\n",
      "val Loss: 0.00070822 Acc: 0.95\n",
      "New best validation loss: 0.0007082153450358998\n",
      "Epoch 6 of 500 took 0.100s\n",
      "Epoch 6/499\n",
      "----------\n",
      "train Loss: 0.00223800 Acc: 0.9203125\n",
      "val Loss: 0.00043995 Acc: 0.98181818\n",
      "New best validation loss: 0.0004399520429697904\n",
      "Epoch 7 of 500 took 0.102s\n",
      "Epoch 7/499\n",
      "----------\n",
      "train Loss: 0.00198037 Acc: 0.92239583\n",
      "val Loss: 0.00044956 Acc: 0.97727273\n",
      "Epoch 8 of 500 took 0.100s\n",
      "Epoch 8/499\n",
      "----------\n",
      "train Loss: 0.00189314 Acc: 0.92864583\n",
      "val Loss: 0.00042321 Acc: 0.97272727\n",
      "Epoch 9 of 500 took 0.101s\n",
      "Epoch 9/499\n",
      "----------\n",
      "train Loss: 0.00170614 Acc: 0.93385417\n",
      "val Loss: 0.00034650 Acc: 0.98181818\n",
      "Epoch 10 of 500 took 0.103s\n",
      "Epoch 10/499\n",
      "----------\n",
      "train Loss: 0.00159156 Acc: 0.93802083\n",
      "val Loss: 0.00075607 Acc: 0.93636364\n",
      "Epoch 11 of 500 took 0.102s\n",
      "Epoch 11/499\n",
      "----------\n",
      "train Loss: 0.00158233 Acc: 0.94010417\n",
      "val Loss: 0.00036963 Acc: 0.97272727\n",
      "Epoch 12 of 500 took 0.098s\n",
      "Epoch 12/499\n",
      "----------\n",
      "train Loss: 0.00157059 Acc: 0.9375\n",
      "val Loss: 0.00041683 Acc: 0.98181818\n",
      "Epoch 13 of 500 took 0.102s\n",
      "Epoch 13/499\n",
      "----------\n",
      "train Loss: 0.00130830 Acc: 0.946875\n",
      "val Loss: 0.00032587 Acc: 0.97727273\n",
      "New best validation loss: 0.0003258727490901947\n",
      "Epoch 14 of 500 took 0.100s\n",
      "Epoch 14/499\n",
      "----------\n",
      "train Loss: 0.00122276 Acc: 0.95416667\n",
      "val Loss: 0.00031051 Acc: 0.97727273\n",
      "Epoch 15 of 500 took 0.105s\n",
      "Epoch 15/499\n",
      "----------\n",
      "train Loss: 0.00120209 Acc: 0.95104167\n",
      "val Loss: 0.00021297 Acc: 0.98181818\n",
      "New best validation loss: 0.0002129722555929964\n",
      "Epoch 16 of 500 took 0.100s\n",
      "Epoch 16/499\n",
      "----------\n",
      "train Loss: 0.00098579 Acc: 0.96145833\n",
      "val Loss: 0.00024452 Acc: 0.98636364\n",
      "Epoch 17 of 500 took 0.101s\n",
      "Epoch 17/499\n",
      "----------\n",
      "train Loss: 0.00101225 Acc: 0.95885417\n",
      "val Loss: 0.00029933 Acc: 0.99090909\n",
      "Epoch 18 of 500 took 0.099s\n",
      "Epoch 18/499\n",
      "----------\n",
      "train Loss: 0.00098748 Acc: 0.96614583\n",
      "val Loss: 0.00023124 Acc: 0.97727273\n",
      "Epoch 19 of 500 took 0.102s\n",
      "Epoch 19/499\n",
      "----------\n",
      "train Loss: 0.00095623 Acc: 0.95729167\n",
      "val Loss: 0.00031673 Acc: 0.97727273\n",
      "Epoch 20 of 500 took 0.099s\n",
      "Epoch 20/499\n",
      "----------\n",
      "train Loss: 0.00084202 Acc: 0.9671875\n",
      "val Loss: 0.00016494 Acc: 0.98636364\n",
      "Epoch 21 of 500 took 0.102s\n",
      "Epoch 21/499\n",
      "----------\n",
      "train Loss: 0.00099070 Acc: 0.95989583\n",
      "val Loss: 0.00020976 Acc: 0.98636364\n",
      "Epoch 22 of 500 took 0.097s\n",
      "Epoch 22/499\n",
      "----------\n",
      "train Loss: 0.00089987 Acc: 0.9625\n",
      "val Loss: 0.00017605 Acc: 0.98636364\n",
      "Epoch 23 of 500 took 0.103s\n",
      "Epoch 23/499\n",
      "----------\n",
      "train Loss: 0.00082961 Acc: 0.96927083\n",
      "val Loss: 0.00014252 Acc: 0.99545455\n",
      "Epoch 24 of 500 took 0.098s\n",
      "Epoch 24/499\n",
      "----------\n",
      "train Loss: 0.00085278 Acc: 0.9640625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.00021362 Acc: 0.98636364\n",
      "Epoch 25 of 500 took 0.101s\n",
      "Epoch 25/499\n",
      "----------\n",
      "train Loss: 0.00073431 Acc: 0.9703125\n",
      "val Loss: 0.00018839 Acc: 0.99090909\n",
      "Epoch 26 of 500 took 0.101s\n",
      "Epoch 26/499\n",
      "----------\n",
      "train Loss: 0.00064744 Acc: 0.9734375\n",
      "val Loss: 0.00013684 Acc: 0.99090909\n",
      "Epoch 27 of 500 took 0.098s\n",
      "\n",
      "Training complete in 0m 3s\n",
      "Best val loss: 0.000213\n",
      "Session:  1\n",
      "TSD_Network(\n",
      "  (_network): ModuleList(\n",
      "    (0): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=252, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=252, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (1): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (2): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (_output): Linear(in_features=200, out_features=22, bias=True)\n",
      "  (_output_discriminator): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "Number Parameters:  137624\n",
      "<generator object Module.parameters at 0x7fb108d9cdd0>\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/TSD/participant_0/best_state_0.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/TSD/participant_0/best_state_0.pt' (epoch 16)\n",
      "Epoch 0/499\n",
      "----------\n",
      "train Loss: 0.00773419 Acc: 0.7140625\n",
      "val Loss: 0.00135155 Acc: 0.88181818\n",
      "New best validation loss: 0.0013515518470243973\n",
      "Epoch 1 of 500 took 0.127s\n",
      "Epoch 1/499\n",
      "----------\n",
      "train Loss: 0.00415940 Acc: 0.809375\n",
      "val Loss: 0.00106400 Acc: 0.92272727\n",
      "New best validation loss: 0.0010640023784203962\n",
      "Epoch 2 of 500 took 0.128s\n",
      "Epoch 2/499\n",
      "----------\n",
      "train Loss: 0.00330559 Acc: 0.85104167\n",
      "val Loss: 0.00076317 Acc: 0.96363636\n",
      "New best validation loss: 0.0007631652057170868\n",
      "Epoch 3 of 500 took 0.108s\n",
      "Epoch 3/499\n",
      "----------\n",
      "train Loss: 0.00287612 Acc: 0.87864583\n",
      "val Loss: 0.00083421 Acc: 0.94545455\n",
      "Epoch 4 of 500 took 0.111s\n",
      "Epoch 4/499\n",
      "----------\n",
      "train Loss: 0.00250758 Acc: 0.89270833\n",
      "val Loss: 0.00049750 Acc: 0.97727273\n",
      "New best validation loss: 0.0004975043575872074\n",
      "Epoch 5 of 500 took 0.101s\n",
      "Epoch 5/499\n",
      "----------\n",
      "train Loss: 0.00225778 Acc: 0.89270833\n",
      "val Loss: 0.00065589 Acc: 0.95454545\n",
      "Epoch 6 of 500 took 0.099s\n",
      "Epoch 6/499\n",
      "----------\n",
      "train Loss: 0.00215158 Acc: 0.9046875\n",
      "val Loss: 0.00044247 Acc: 0.97727273\n",
      "Epoch 7 of 500 took 0.104s\n",
      "Epoch 7/499\n",
      "----------\n",
      "train Loss: 0.00200612 Acc: 0.9109375\n",
      "val Loss: 0.00055497 Acc: 0.95909091\n",
      "Epoch 8 of 500 took 0.100s\n",
      "Epoch 8/499\n",
      "----------\n",
      "train Loss: 0.00204137 Acc: 0.91041667\n",
      "val Loss: 0.00040273 Acc: 0.97272727\n",
      "Epoch 9 of 500 took 0.102s\n",
      "Epoch 9/499\n",
      "----------\n",
      "train Loss: 0.00184901 Acc: 0.92447917\n",
      "val Loss: 0.00029449 Acc: 0.97727273\n",
      "New best validation loss: 0.0002944864671338688\n",
      "Epoch 10 of 500 took 0.102s\n",
      "Epoch 10/499\n",
      "----------\n",
      "train Loss: 0.00172036 Acc: 0.9234375\n",
      "val Loss: 0.00037790 Acc: 0.95454545\n",
      "Epoch 11 of 500 took 0.102s\n",
      "Epoch 11/499\n",
      "----------\n",
      "train Loss: 0.00159201 Acc: 0.934375\n",
      "val Loss: 0.00065159 Acc: 0.94545455\n",
      "Epoch 12 of 500 took 0.102s\n",
      "Epoch 12/499\n",
      "----------\n",
      "train Loss: 0.00150476 Acc: 0.93645833\n",
      "val Loss: 0.00029405 Acc: 0.97727273\n",
      "Epoch 13 of 500 took 0.103s\n",
      "Epoch 13/499\n",
      "----------\n",
      "train Loss: 0.00134512 Acc: 0.940625\n",
      "val Loss: 0.00025490 Acc: 0.98181818\n",
      "Epoch 14 of 500 took 0.101s\n",
      "Epoch 14/499\n",
      "----------\n",
      "train Loss: 0.00135653 Acc: 0.94166667\n",
      "val Loss: 0.00045756 Acc: 0.96818182\n",
      "Epoch 15 of 500 took 0.108s\n",
      "Epoch 15/499\n",
      "----------\n",
      "train Loss: 0.00146671 Acc: 0.93645833\n",
      "val Loss: 0.00063979 Acc: 0.94090909\n",
      "Epoch 16 of 500 took 0.101s\n",
      "Epoch 16/499\n",
      "----------\n",
      "train Loss: 0.00124147 Acc: 0.946875\n",
      "val Loss: 0.00019826 Acc: 0.99090909\n",
      "Epoch 17 of 500 took 0.101s\n",
      "Epoch 17/499\n",
      "----------\n",
      "train Loss: 0.00127796 Acc: 0.94895833\n",
      "val Loss: 0.00039468 Acc: 0.96818182\n",
      "Epoch 18 of 500 took 0.099s\n",
      "Epoch 18/499\n",
      "----------\n",
      "train Loss: 0.00124475 Acc: 0.94375\n",
      "val Loss: 0.00039458 Acc: 0.97272727\n",
      "Epoch 19 of 500 took 0.102s\n",
      "Epoch 19/499\n",
      "----------\n",
      "train Loss: 0.00133317 Acc: 0.94166667\n",
      "val Loss: 0.00019668 Acc: 0.99090909\n",
      "Epoch 20 of 500 took 0.100s\n",
      "Epoch 20/499\n",
      "----------\n",
      "train Loss: 0.00113702 Acc: 0.9484375\n",
      "val Loss: 0.00024515 Acc: 0.98636364\n",
      "Epoch 21 of 500 took 0.102s\n",
      "\n",
      "Training complete in 0m 2s\n",
      "Best val loss: 0.000294\n",
      "Session:  2\n",
      "TSD_Network(\n",
      "  (_network): ModuleList(\n",
      "    (0): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=252, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=252, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (1): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (2): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (_output): Linear(in_features=200, out_features=22, bias=True)\n",
      "  (_output_discriminator): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "Number Parameters:  137624\n",
      "<generator object Module.parameters at 0x7fb10a6ad510>\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/TSD/participant_0/best_state_1.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/TSD/participant_0/best_state_1.pt' (epoch 10)\n",
      "Epoch 0/499\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.00879533 Acc: 0.70208333\n",
      "val Loss: 0.00096966 Acc: 0.94090909\n",
      "New best validation loss: 0.0009696642106229609\n",
      "Epoch 1 of 500 took 0.123s\n",
      "Epoch 1/499\n",
      "----------\n",
      "train Loss: 0.00332787 Acc: 0.85052083\n",
      "val Loss: 0.00077164 Acc: 0.96363636\n",
      "New best validation loss: 0.000771642340855165\n",
      "Epoch 2 of 500 took 0.100s\n",
      "Epoch 2/499\n",
      "----------\n",
      "train Loss: 0.00312712 Acc: 0.85989583\n",
      "val Loss: 0.00043992 Acc: 0.98636364\n",
      "New best validation loss: 0.00043991509486328474\n",
      "Epoch 3 of 500 took 0.103s\n",
      "Epoch 3/499\n",
      "----------\n",
      "train Loss: 0.00258813 Acc: 0.88385417\n",
      "val Loss: 0.00051087 Acc: 0.97272727\n",
      "Epoch 4 of 500 took 0.100s\n",
      "Epoch 4/499\n",
      "----------\n",
      "train Loss: 0.00236105 Acc: 0.89427083\n",
      "val Loss: 0.00034428 Acc: 0.98181818\n",
      "Epoch 5 of 500 took 0.100s\n",
      "Epoch 5/499\n",
      "----------\n",
      "train Loss: 0.00208343 Acc: 0.9125\n",
      "val Loss: 0.00036418 Acc: 0.97727273\n",
      "Epoch 6 of 500 took 0.109s\n",
      "Epoch 6/499\n",
      "----------\n",
      "train Loss: 0.00203166 Acc: 0.91041667\n",
      "val Loss: 0.00049069 Acc: 0.96818182\n",
      "Epoch 7 of 500 took 0.102s\n",
      "Epoch 7/499\n",
      "----------\n",
      "train Loss: 0.00205735 Acc: 0.89895833\n",
      "val Loss: 0.00029524 Acc: 0.99090909\n",
      "New best validation loss: 0.00029524100775068455\n",
      "Epoch 8 of 500 took 0.101s\n",
      "Epoch 8/499\n",
      "----------\n",
      "train Loss: 0.00172634 Acc: 0.91979167\n",
      "val Loss: 0.00053292 Acc: 0.95454545\n",
      "Epoch 9 of 500 took 0.101s\n",
      "Epoch 9/499\n",
      "----------\n",
      "train Loss: 0.00168899 Acc: 0.92291667\n",
      "val Loss: 0.00025762 Acc: 0.99090909\n",
      "Epoch 10 of 500 took 0.101s\n",
      "Epoch 10/499\n",
      "----------\n",
      "train Loss: 0.00172209 Acc: 0.92395833\n",
      "val Loss: 0.00020809 Acc: 0.99545455\n",
      "Epoch 11 of 500 took 0.101s\n",
      "Epoch 11/499\n",
      "----------\n",
      "train Loss: 0.00166841 Acc: 0.9265625\n",
      "val Loss: 0.00022969 Acc: 0.98181818\n",
      "Epoch 12 of 500 took 0.101s\n",
      "Epoch 12/499\n",
      "----------\n",
      "train Loss: 0.00138442 Acc: 0.93645833\n",
      "val Loss: 0.00018570 Acc: 0.99090909\n",
      "New best validation loss: 0.0001857022839513692\n",
      "Epoch 13 of 500 took 0.101s\n",
      "Epoch 13/499\n",
      "----------\n",
      "train Loss: 0.00142334 Acc: 0.93645833\n",
      "val Loss: 0.00017742 Acc: 0.99090909\n",
      "Epoch 14 of 500 took 0.101s\n",
      "Epoch 14/499\n",
      "----------\n",
      "train Loss: 0.00137967 Acc: 0.940625\n",
      "val Loss: 0.00019337 Acc: 0.98636364\n",
      "Epoch 15 of 500 took 0.099s\n",
      "Epoch 15/499\n",
      "----------\n",
      "train Loss: 0.00109991 Acc: 0.95208333\n",
      "val Loss: 0.00013213 Acc: 0.99545455\n",
      "Epoch 16 of 500 took 0.102s\n",
      "Epoch 16/499\n",
      "----------\n",
      "train Loss: 0.00107650 Acc: 0.95208333\n",
      "val Loss: 0.00022886 Acc: 0.98181818\n",
      "Epoch 17 of 500 took 0.100s\n",
      "Epoch 17/499\n",
      "----------\n",
      "train Loss: 0.00123753 Acc: 0.94895833\n",
      "val Loss: 0.00019443 Acc: 0.98181818\n",
      "Epoch 18 of 500 took 0.104s\n",
      "Epoch 18/499\n",
      "----------\n",
      "train Loss: 0.00117589 Acc: 0.9453125\n",
      "val Loss: 0.00013027 Acc: 0.99545455\n",
      "Epoch 19 of 500 took 0.099s\n",
      "Epoch 19/499\n",
      "----------\n",
      "train Loss: 0.00101688 Acc: 0.95677083\n",
      "val Loss: 0.00016785 Acc: 0.97727273\n",
      "Epoch 20 of 500 took 0.102s\n",
      "Epoch 20/499\n",
      "----------\n",
      "train Loss: 0.00108355 Acc: 0.9515625\n",
      "val Loss: 0.00016868 Acc: 0.99090909\n",
      "Epoch 21 of 500 took 0.099s\n",
      "Epoch 21/499\n",
      "----------\n",
      "train Loss: 0.00100655 Acc: 0.95520833\n",
      "val Loss: 0.00025622 Acc: 0.97272727\n",
      "Epoch 22 of 500 took 0.103s\n",
      "Epoch 22/499\n",
      "----------\n",
      "train Loss: 0.00127551 Acc: 0.940625\n",
      "val Loss: 0.00022095 Acc: 0.98181818\n",
      "Epoch 23 of 500 took 0.099s\n",
      "Epoch 23/499\n",
      "----------\n",
      "train Loss: 0.00104004 Acc: 0.95260417\n",
      "val Loss: 0.00013170 Acc: 0.98636364\n",
      "Epoch 24 of 500 took 0.098s\n",
      "\n",
      "Training complete in 0m 2s\n",
      "Best val loss: 0.000186\n",
      "Participant:  1\n",
      "Session:  0\n",
      "TSD_Network(\n",
      "  (_network): ModuleList(\n",
      "    (0): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=252, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=252, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (1): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (2): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (_output): Linear(in_features=200, out_features=22, bias=True)\n",
      "  (_output_discriminator): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "Number Parameters:  137624\n",
      "<generator object Module.parameters at 0x7fb108db3cf0>\n",
      "Epoch 0/499\n",
      "----------\n",
      "train Loss: 0.01627193 Acc: 0.43645833\n",
      "val Loss: 0.00467976 Acc: 0.87727273\n",
      "New best validation loss: 0.004679759524085305\n",
      "Epoch 1 of 500 took 0.126s\n",
      "Epoch 1/499\n",
      "----------\n",
      "train Loss: 0.00755124 Acc: 0.76927083\n",
      "val Loss: 0.00230443 Acc: 0.90454545\n",
      "New best validation loss: 0.0023044334216551347\n",
      "Epoch 2 of 500 took 0.124s\n",
      "Epoch 2/499\n",
      "----------\n",
      "train Loss: 0.00487434 Acc: 0.8375\n",
      "val Loss: 0.00155143 Acc: 0.91363636\n",
      "New best validation loss: 0.0015514250506054271\n",
      "Epoch 3 of 500 took 0.100s\n",
      "Epoch 3/499\n",
      "----------\n",
      "train Loss: 0.00358658 Acc: 0.87291667\n",
      "val Loss: 0.00099999 Acc: 0.94090909\n",
      "New best validation loss: 0.0009999899701638654\n",
      "Epoch 4 of 500 took 0.100s\n",
      "Epoch 4/499\n",
      "----------\n",
      "train Loss: 0.00283413 Acc: 0.90052083\n",
      "val Loss: 0.00082798 Acc: 0.98636364\n",
      "New best validation loss: 0.0008279828185384924\n",
      "Epoch 5 of 500 took 0.103s\n",
      "Epoch 5/499\n",
      "----------\n",
      "train Loss: 0.00247566 Acc: 0.9015625\n",
      "val Loss: 0.00070831 Acc: 0.93636364\n",
      "New best validation loss: 0.0007083050229332664\n",
      "Epoch 6 of 500 took 0.102s\n",
      "Epoch 6/499\n",
      "----------\n",
      "train Loss: 0.00218268 Acc: 0.9109375\n",
      "val Loss: 0.00060594 Acc: 0.96818182\n",
      "New best validation loss: 0.0006059415638446808\n",
      "Epoch 7 of 500 took 0.103s\n",
      "Epoch 7/499\n",
      "----------\n",
      "train Loss: 0.00195703 Acc: 0.925\n",
      "val Loss: 0.00080421 Acc: 0.94545455\n",
      "Epoch 8 of 500 took 0.099s\n",
      "Epoch 8/499\n",
      "----------\n",
      "train Loss: 0.00188938 Acc: 0.92552083\n",
      "val Loss: 0.00060037 Acc: 0.94545455\n",
      "Epoch 9 of 500 took 0.101s\n",
      "Epoch 9/499\n",
      "----------\n",
      "train Loss: 0.00168162 Acc: 0.93125\n",
      "val Loss: 0.00068832 Acc: 0.90909091\n",
      "Epoch 10 of 500 took 0.099s\n",
      "Epoch 10/499\n",
      "----------\n",
      "train Loss: 0.00153725 Acc: 0.9421875\n",
      "val Loss: 0.00042529 Acc: 0.97727273\n",
      "New best validation loss: 0.0004252864555879073\n",
      "Epoch 11 of 500 took 0.105s\n",
      "Epoch 11/499\n",
      "----------\n",
      "train Loss: 0.00144151 Acc: 0.940625\n",
      "val Loss: 0.00052760 Acc: 0.94090909\n",
      "Epoch 12 of 500 took 0.100s\n",
      "Epoch 12/499\n",
      "----------\n",
      "train Loss: 0.00144820 Acc: 0.93645833\n",
      "val Loss: 0.00049856 Acc: 0.95\n",
      "Epoch 13 of 500 took 0.125s\n",
      "Epoch 13/499\n",
      "----------\n",
      "train Loss: 0.00130737 Acc: 0.95052083\n",
      "val Loss: 0.00044167 Acc: 0.95909091\n",
      "Epoch 14 of 500 took 0.105s\n",
      "Epoch 14/499\n",
      "----------\n",
      "train Loss: 0.00120929 Acc: 0.95260417\n",
      "val Loss: 0.00114792 Acc: 0.91363636\n",
      "Epoch 15 of 500 took 0.100s\n",
      "Epoch 15/499\n",
      "----------\n",
      "train Loss: 0.00112268 Acc: 0.95364583\n",
      "val Loss: 0.00086365 Acc: 0.92727273\n",
      "Epoch 16 of 500 took 0.104s\n",
      "Epoch 16/499\n",
      "----------\n",
      "train Loss: 0.00107942 Acc: 0.95833333\n",
      "val Loss: 0.00061032 Acc: 0.92272727\n",
      "Epoch    17: reducing learning rate of group 0 to 5.0300e-04.\n",
      "Epoch 17 of 500 took 0.100s\n",
      "Epoch 17/499\n",
      "----------\n",
      "train Loss: 0.00100631 Acc: 0.95677083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.00026000 Acc: 0.98181818\n",
      "New best validation loss: 0.00026000470600344916\n",
      "Epoch 18 of 500 took 0.101s\n",
      "Epoch 18/499\n",
      "----------\n",
      "train Loss: 0.00085618 Acc: 0.96458333\n",
      "val Loss: 0.00019863 Acc: 0.99090909\n",
      "Epoch 19 of 500 took 0.102s\n",
      "Epoch 19/499\n",
      "----------\n",
      "train Loss: 0.00074469 Acc: 0.97291667\n",
      "val Loss: 0.00019733 Acc: 0.99090909\n",
      "Epoch 20 of 500 took 0.099s\n",
      "Epoch 20/499\n",
      "----------\n",
      "train Loss: 0.00077810 Acc: 0.97083333\n",
      "val Loss: 0.00020818 Acc: 0.99090909\n",
      "Epoch 21 of 500 took 0.104s\n",
      "Epoch 21/499\n",
      "----------\n",
      "train Loss: 0.00067376 Acc: 0.9765625\n",
      "val Loss: 0.00018361 Acc: 0.99090909\n",
      "Epoch 22 of 500 took 0.100s\n",
      "Epoch 22/499\n",
      "----------\n",
      "train Loss: 0.00064919 Acc: 0.9765625\n",
      "val Loss: 0.00019227 Acc: 0.98636364\n",
      "Epoch 23 of 500 took 0.102s\n",
      "Epoch 23/499\n",
      "----------\n",
      "train Loss: 0.00057657 Acc: 0.98177083\n",
      "val Loss: 0.00020466 Acc: 0.99090909\n",
      "Epoch 24 of 500 took 0.098s\n",
      "Epoch 24/499\n",
      "----------\n",
      "train Loss: 0.00063349 Acc: 0.97708333\n",
      "val Loss: 0.00018758 Acc: 0.99090909\n",
      "Epoch 25 of 500 took 0.108s\n",
      "Epoch 25/499\n",
      "----------\n",
      "train Loss: 0.00057530 Acc: 0.9796875\n",
      "val Loss: 0.00018334 Acc: 0.99090909\n",
      "Epoch 26 of 500 took 0.099s\n",
      "Epoch 26/499\n",
      "----------\n",
      "train Loss: 0.00059789 Acc: 0.98020833\n",
      "val Loss: 0.00017961 Acc: 0.99090909\n",
      "Epoch 27 of 500 took 0.099s\n",
      "Epoch 27/499\n",
      "----------\n",
      "train Loss: 0.00062439 Acc: 0.97760417\n",
      "val Loss: 0.00017246 Acc: 0.99090909\n",
      "Epoch 28 of 500 took 0.125s\n",
      "Epoch 28/499\n",
      "----------\n",
      "train Loss: 0.00056582 Acc: 0.9796875\n",
      "val Loss: 0.00018248 Acc: 0.98636364\n",
      "Epoch 29 of 500 took 0.117s\n",
      "\n",
      "Training complete in 0m 3s\n",
      "Best val loss: 0.000260\n",
      "Session:  1\n",
      "TSD_Network(\n",
      "  (_network): ModuleList(\n",
      "    (0): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=252, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=252, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (1): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (2): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (_output): Linear(in_features=200, out_features=22, bias=True)\n",
      "  (_output_discriminator): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "Number Parameters:  137624\n",
      "<generator object Module.parameters at 0x7fb10a6ad510>\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/TSD/participant_1/best_state_0.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/TSD/participant_1/best_state_0.pt' (epoch 18)\n",
      "Epoch 0/499\n",
      "----------\n",
      "train Loss: 0.00270426 Acc: 0.89010417\n",
      "val Loss: 0.00039868 Acc: 0.97727273\n",
      "New best validation loss: 0.00039867867122996936\n",
      "Epoch 1 of 500 took 0.101s\n",
      "Epoch 1/499\n",
      "----------\n",
      "train Loss: 0.00159768 Acc: 0.93177083\n",
      "val Loss: 0.00054337 Acc: 0.94545455\n",
      "Epoch 2 of 500 took 0.104s\n",
      "Epoch 2/499\n",
      "----------\n",
      "train Loss: 0.00118929 Acc: 0.9453125\n",
      "val Loss: 0.00029527 Acc: 0.98636364\n",
      "New best validation loss: 0.00029526620425961235\n",
      "Epoch 3 of 500 took 0.125s\n",
      "Epoch 3/499\n",
      "----------\n",
      "train Loss: 0.00108440 Acc: 0.95572917\n",
      "val Loss: 0.00047862 Acc: 0.96818182\n",
      "Epoch 4 of 500 took 0.109s\n",
      "Epoch 4/499\n",
      "----------\n",
      "train Loss: 0.00097390 Acc: 0.95677083\n",
      "val Loss: 0.00040241 Acc: 0.95909091\n",
      "Epoch 5 of 500 took 0.099s\n",
      "Epoch 5/499\n",
      "----------\n",
      "train Loss: 0.00102277 Acc: 0.95833333\n",
      "val Loss: 0.00020394 Acc: 0.97727273\n",
      "Epoch 6 of 500 took 0.098s\n",
      "Epoch 6/499\n",
      "----------\n",
      "train Loss: 0.00088065 Acc: 0.9609375\n",
      "val Loss: 0.00016163 Acc: 0.98636364\n",
      "New best validation loss: 0.0001616328785365278\n",
      "Epoch 7 of 500 took 0.104s\n",
      "Epoch 7/499\n",
      "----------\n",
      "train Loss: 0.00084471 Acc: 0.96510417\n",
      "val Loss: 0.00027637 Acc: 0.99090909\n",
      "Epoch 8 of 500 took 0.099s\n",
      "Epoch 8/499\n",
      "----------\n",
      "train Loss: 0.00073327 Acc: 0.96875\n",
      "val Loss: 0.00009467 Acc: 0.99090909\n",
      "Epoch 9 of 500 took 0.102s\n",
      "Epoch 9/499\n",
      "----------\n",
      "train Loss: 0.00067063 Acc: 0.975\n",
      "val Loss: 0.00021600 Acc: 0.97727273\n",
      "Epoch 10 of 500 took 0.098s\n",
      "Epoch 10/499\n",
      "----------\n",
      "train Loss: 0.00060614 Acc: 0.97447917\n",
      "val Loss: 0.00037536 Acc: 0.96363636\n",
      "Epoch 11 of 500 took 0.105s\n",
      "Epoch 11/499\n",
      "----------\n",
      "train Loss: 0.00081238 Acc: 0.96927083\n",
      "val Loss: 0.00027562 Acc: 0.97727273\n",
      "Epoch 12 of 500 took 0.099s\n",
      "Epoch 12/499\n",
      "----------\n",
      "train Loss: 0.00064492 Acc: 0.97552083\n",
      "val Loss: 0.00012475 Acc: 0.99090909\n",
      "Epoch 13 of 500 took 0.099s\n",
      "Epoch 13/499\n",
      "----------\n",
      "train Loss: 0.00051781 Acc: 0.9796875\n",
      "val Loss: 0.00015019 Acc: 0.99090909\n",
      "Epoch 14 of 500 took 0.126s\n",
      "Epoch 14/499\n",
      "----------\n",
      "train Loss: 0.00052259 Acc: 0.97864583\n",
      "val Loss: 0.00017672 Acc: 0.98636364\n",
      "Epoch    15: reducing learning rate of group 0 to 5.0300e-04.\n",
      "Epoch 15 of 500 took 0.125s\n",
      "Epoch 15/499\n",
      "----------\n",
      "train Loss: 0.00048969 Acc: 0.98333333\n",
      "val Loss: 0.00016384 Acc: 0.98636364\n",
      "Epoch 16 of 500 took 0.099s\n",
      "Epoch 16/499\n",
      "----------\n",
      "train Loss: 0.00036376 Acc: 0.98645833\n",
      "val Loss: 0.00012543 Acc: 0.98636364\n",
      "Epoch 17 of 500 took 0.098s\n",
      "Epoch 17/499\n",
      "----------\n",
      "train Loss: 0.00035883 Acc: 0.98854167\n",
      "val Loss: 0.00007823 Acc: 0.99090909\n",
      "Epoch 18 of 500 took 0.104s\n",
      "\n",
      "Training complete in 0m 2s\n",
      "Best val loss: 0.000162\n",
      "Session:  2\n",
      "TSD_Network(\n",
      "  (_network): ModuleList(\n",
      "    (0): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=252, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=252, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (1): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (2): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (_output): Linear(in_features=200, out_features=22, bias=True)\n",
      "  (_output_discriminator): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "Number Parameters:  137624\n",
      "<generator object Module.parameters at 0x7fb10a6ad510>\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/TSD/participant_1/best_state_1.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/TSD/participant_1/best_state_1.pt' (epoch 7)\n",
      "Epoch 0/499\n",
      "----------\n",
      "train Loss: 0.00588101 Acc: 0.7703125\n",
      "val Loss: 0.00106189 Acc: 0.89545455\n",
      "New best validation loss: 0.001061889326030558\n",
      "Epoch 1 of 500 took 0.100s\n",
      "Epoch 1/499\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.00261685 Acc: 0.8734375\n",
      "val Loss: 0.00050196 Acc: 0.96818182\n",
      "New best validation loss: 0.000501961430365389\n",
      "Epoch 2 of 500 took 0.106s\n",
      "Epoch 2/499\n",
      "----------\n",
      "train Loss: 0.00206882 Acc: 0.89375\n",
      "val Loss: 0.00048371 Acc: 0.97272727\n",
      "Epoch 3 of 500 took 0.115s\n",
      "Epoch 3/499\n",
      "----------\n",
      "train Loss: 0.00205998 Acc: 0.8953125\n",
      "val Loss: 0.00044639 Acc: 0.95909091\n",
      "Epoch 4 of 500 took 0.104s\n",
      "Epoch 4/499\n",
      "----------\n",
      "train Loss: 0.00190653 Acc: 0.90572917\n",
      "val Loss: 0.00043852 Acc: 0.96818182\n",
      "Epoch 5 of 500 took 0.099s\n",
      "Epoch 5/499\n",
      "----------\n",
      "train Loss: 0.00159553 Acc: 0.92291667\n",
      "val Loss: 0.00046110 Acc: 0.94090909\n",
      "Epoch 6 of 500 took 0.098s\n",
      "Epoch 6/499\n",
      "----------\n",
      "train Loss: 0.00154531 Acc: 0.93072917\n",
      "val Loss: 0.00023927 Acc: 0.98181818\n",
      "New best validation loss: 0.00023927387188781392\n",
      "Epoch 7 of 500 took 0.104s\n",
      "Epoch 7/499\n",
      "----------\n",
      "train Loss: 0.00131461 Acc: 0.94322917\n",
      "val Loss: 0.00024509 Acc: 0.98636364\n",
      "Epoch 8 of 500 took 0.099s\n",
      "Epoch 8/499\n",
      "----------\n",
      "train Loss: 0.00121315 Acc: 0.94270833\n",
      "val Loss: 0.00024062 Acc: 0.98636364\n",
      "Epoch 9 of 500 took 0.107s\n",
      "Epoch 9/499\n",
      "----------\n",
      "train Loss: 0.00118273 Acc: 0.94427083\n",
      "val Loss: 0.00025168 Acc: 0.98181818\n",
      "Epoch 10 of 500 took 0.100s\n",
      "Epoch 10/499\n",
      "----------\n",
      "train Loss: 0.00102911 Acc: 0.953125\n",
      "val Loss: 0.00021344 Acc: 0.98181818\n",
      "Epoch 11 of 500 took 0.101s\n",
      "Epoch 11/499\n",
      "----------\n",
      "train Loss: 0.00111701 Acc: 0.953125\n",
      "val Loss: 0.00017198 Acc: 0.99090909\n",
      "Epoch 12 of 500 took 0.100s\n",
      "Epoch 12/499\n",
      "----------\n",
      "train Loss: 0.00097373 Acc: 0.9546875\n",
      "val Loss: 0.00015397 Acc: 0.99090909\n",
      "Epoch 13 of 500 took 0.103s\n",
      "Epoch 13/499\n",
      "----------\n",
      "train Loss: 0.00088497 Acc: 0.95989583\n",
      "val Loss: 0.00020306 Acc: 0.98636364\n",
      "Epoch 14 of 500 took 0.100s\n",
      "Epoch 14/499\n",
      "----------\n",
      "train Loss: 0.00091864 Acc: 0.95729167\n",
      "val Loss: 0.00012478 Acc: 0.99545455\n",
      "New best validation loss: 0.00012478420341556723\n",
      "Epoch 15 of 500 took 0.102s\n",
      "Epoch 15/499\n",
      "----------\n",
      "train Loss: 0.00091380 Acc: 0.9625\n",
      "val Loss: 0.00012652 Acc: 0.98636364\n",
      "Epoch 16 of 500 took 0.107s\n",
      "Epoch 16/499\n",
      "----------\n",
      "train Loss: 0.00087778 Acc: 0.96614583\n",
      "val Loss: 0.00017394 Acc: 0.99090909\n",
      "Epoch 17 of 500 took 0.105s\n",
      "Epoch 17/499\n",
      "----------\n",
      "train Loss: 0.00085216 Acc: 0.96770833\n",
      "val Loss: 0.00014447 Acc: 0.99090909\n",
      "Epoch 18 of 500 took 0.102s\n",
      "Epoch 18/499\n",
      "----------\n",
      "train Loss: 0.00077928 Acc: 0.96354167\n",
      "val Loss: 0.00010392 Acc: 0.99545455\n",
      "Epoch 19 of 500 took 0.100s\n",
      "Epoch 19/499\n",
      "----------\n",
      "train Loss: 0.00061360 Acc: 0.97239583\n",
      "val Loss: 0.00011026 Acc: 0.99090909\n",
      "Epoch 20 of 500 took 0.100s\n",
      "Epoch 20/499\n",
      "----------\n",
      "train Loss: 0.00070449 Acc: 0.9703125\n",
      "val Loss: 0.00019149 Acc: 0.97727273\n",
      "Epoch 21 of 500 took 0.124s\n",
      "Epoch 21/499\n",
      "----------\n",
      "train Loss: 0.00068430 Acc: 0.97291667\n",
      "val Loss: 0.00036969 Acc: 0.97272727\n",
      "Epoch 22 of 500 took 0.128s\n",
      "Epoch 22/499\n",
      "----------\n",
      "train Loss: 0.00066644 Acc: 0.96927083\n",
      "val Loss: 0.00028323 Acc: 0.97272727\n",
      "Epoch 23 of 500 took 0.121s\n",
      "Epoch 23/499\n",
      "----------\n",
      "train Loss: 0.00073677 Acc: 0.96770833\n",
      "val Loss: 0.00013687 Acc: 0.99090909\n",
      "Epoch 24 of 500 took 0.125s\n",
      "Epoch 24/499\n",
      "----------\n",
      "train Loss: 0.00067853 Acc: 0.96875\n",
      "val Loss: 0.00012449 Acc: 0.99090909\n",
      "Epoch    25: reducing learning rate of group 0 to 5.0300e-04.\n",
      "Epoch 25 of 500 took 0.125s\n",
      "Epoch 25/499\n",
      "----------\n",
      "train Loss: 0.00060546 Acc: 0.97135417\n",
      "val Loss: 0.00003971 Acc: 1.0\n",
      "Epoch 26 of 500 took 0.102s\n",
      "\n",
      "Training complete in 0m 3s\n",
      "Best val loss: 0.000125\n",
      "Participant:  2\n",
      "Session:  0\n",
      "TSD_Network(\n",
      "  (_network): ModuleList(\n",
      "    (0): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=252, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=252, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (1): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (2): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (_output): Linear(in_features=200, out_features=22, bias=True)\n",
      "  (_output_discriminator): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "Number Parameters:  137624\n",
      "<generator object Module.parameters at 0x7fb10a6ad510>\n",
      "Epoch 0/499\n",
      "----------\n",
      "train Loss: 0.01702000 Acc: 0.38541667\n",
      "val Loss: 0.00571058 Acc: 0.71818182\n",
      "New best validation loss: 0.005710582841526379\n",
      "Epoch 1 of 500 took 0.101s\n",
      "Epoch 1/499\n",
      "----------\n",
      "train Loss: 0.00901311 Acc: 0.67760417\n",
      "val Loss: 0.00361574 Acc: 0.77272727\n",
      "New best validation loss: 0.0036157356067137284\n",
      "Epoch 2 of 500 took 0.103s\n",
      "Epoch 2/499\n",
      "----------\n",
      "train Loss: 0.00602402 Acc: 0.778125\n",
      "val Loss: 0.00273190 Acc: 0.79090909\n",
      "New best validation loss: 0.0027319024909626356\n",
      "Epoch 3 of 500 took 0.101s\n",
      "Epoch 3/499\n",
      "----------\n",
      "train Loss: 0.00488587 Acc: 0.7953125\n",
      "val Loss: 0.00231290 Acc: 0.82727273\n",
      "New best validation loss: 0.0023128967393528333\n",
      "Epoch 4 of 500 took 0.101s\n",
      "Epoch 4/499\n",
      "----------\n",
      "train Loss: 0.00409673 Acc: 0.825\n",
      "val Loss: 0.00202271 Acc: 0.82727273\n",
      "New best validation loss: 0.0020227081396362997\n",
      "Epoch 5 of 500 took 0.104s\n",
      "Epoch 5/499\n",
      "----------\n",
      "train Loss: 0.00370987 Acc: 0.83854167\n",
      "val Loss: 0.00142685 Acc: 0.89090909\n",
      "New best validation loss: 0.0014268502593040467\n",
      "Epoch 6 of 500 took 0.103s\n",
      "Epoch 6/499\n",
      "----------\n",
      "train Loss: 0.00321620 Acc: 0.86197917\n",
      "val Loss: 0.00124881 Acc: 0.91363636\n",
      "New best validation loss: 0.0012488149783828042\n",
      "Epoch 7 of 500 took 0.103s\n",
      "Epoch 7/499\n",
      "----------\n",
      "train Loss: 0.00285050 Acc: 0.8875\n",
      "val Loss: 0.00133855 Acc: 0.9\n",
      "Epoch 8 of 500 took 0.099s\n",
      "Epoch 8/499\n",
      "----------\n",
      "train Loss: 0.00323083 Acc: 0.8578125\n",
      "val Loss: 0.00159261 Acc: 0.88636364\n",
      "Epoch 9 of 500 took 0.102s\n",
      "Epoch 9/499\n",
      "----------\n",
      "train Loss: 0.00273982 Acc: 0.878125\n",
      "val Loss: 0.00111368 Acc: 0.91818182\n",
      "New best validation loss: 0.0011136810210618105\n",
      "Epoch 10 of 500 took 0.101s\n",
      "Epoch 10/499\n",
      "----------\n",
      "train Loss: 0.00256531 Acc: 0.88333333\n",
      "val Loss: 0.00183259 Acc: 0.85454545\n",
      "Epoch 11 of 500 took 0.102s\n",
      "Epoch 11/499\n",
      "----------\n",
      "train Loss: 0.00247840 Acc: 0.8921875\n",
      "val Loss: 0.00085799 Acc: 0.94090909\n",
      "New best validation loss: 0.0008579932153224945\n",
      "Epoch 12 of 500 took 0.101s\n",
      "Epoch 12/499\n",
      "----------\n",
      "train Loss: 0.00211424 Acc: 0.90729167\n",
      "val Loss: 0.00097080 Acc: 0.91818182\n",
      "Epoch 13 of 500 took 0.112s\n",
      "Epoch 13/499\n",
      "----------\n",
      "train Loss: 0.00211669 Acc: 0.90625\n",
      "val Loss: 0.00094425 Acc: 0.91818182\n",
      "Epoch 14 of 500 took 0.102s\n",
      "Epoch 14/499\n",
      "----------\n",
      "train Loss: 0.00196538 Acc: 0.909375\n",
      "val Loss: 0.00115248 Acc: 0.91818182\n",
      "Epoch 15 of 500 took 0.102s\n",
      "Epoch 15/499\n",
      "----------\n",
      "train Loss: 0.00200763 Acc: 0.90885417\n",
      "val Loss: 0.00108662 Acc: 0.90909091\n",
      "Epoch 16 of 500 took 0.101s\n",
      "Epoch 16/499\n",
      "----------\n",
      "train Loss: 0.00181557 Acc: 0.93072917\n",
      "val Loss: 0.00052921 Acc: 0.97727273\n",
      "New best validation loss: 0.000529205020178448\n",
      "Epoch 17 of 500 took 0.104s\n",
      "Epoch 17/499\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.00157826 Acc: 0.934375\n",
      "val Loss: 0.00048903 Acc: 0.95909091\n",
      "Epoch 18 of 500 took 0.099s\n",
      "Epoch 18/499\n",
      "----------\n",
      "train Loss: 0.00169040 Acc: 0.93125\n",
      "val Loss: 0.00104824 Acc: 0.9\n",
      "Epoch 19 of 500 took 0.101s\n",
      "Epoch 19/499\n",
      "----------\n",
      "train Loss: 0.00172730 Acc: 0.92708333\n",
      "val Loss: 0.00115759 Acc: 0.89545455\n",
      "Epoch 20 of 500 took 0.099s\n",
      "Epoch 20/499\n",
      "----------\n",
      "train Loss: 0.00158172 Acc: 0.93385417\n",
      "val Loss: 0.00059537 Acc: 0.95\n",
      "Epoch 21 of 500 took 0.101s\n",
      "Epoch 21/499\n",
      "----------\n",
      "train Loss: 0.00133400 Acc: 0.9390625\n",
      "val Loss: 0.00068852 Acc: 0.93636364\n",
      "Epoch 22 of 500 took 0.099s\n",
      "Epoch 22/499\n",
      "----------\n",
      "train Loss: 0.00141311 Acc: 0.94322917\n",
      "val Loss: 0.00054809 Acc: 0.95909091\n",
      "Epoch 23 of 500 took 0.104s\n",
      "Epoch 23/499\n",
      "----------\n",
      "train Loss: 0.00130620 Acc: 0.94479167\n",
      "val Loss: 0.00052187 Acc: 0.95909091\n",
      "Epoch    24: reducing learning rate of group 0 to 5.0300e-04.\n",
      "Epoch 24 of 500 took 0.098s\n",
      "Epoch 24/499\n",
      "----------\n",
      "train Loss: 0.00126594 Acc: 0.95052083\n",
      "val Loss: 0.00033463 Acc: 0.98181818\n",
      "New best validation loss: 0.0003346298905936154\n",
      "Epoch 25 of 500 took 0.103s\n",
      "Epoch 25/499\n",
      "----------\n",
      "train Loss: 0.00110509 Acc: 0.95729167\n",
      "val Loss: 0.00030101 Acc: 0.97727273\n",
      "Epoch 26 of 500 took 0.101s\n",
      "Epoch 26/499\n",
      "----------\n",
      "train Loss: 0.00094605 Acc: 0.9625\n",
      "val Loss: 0.00028155 Acc: 0.98181818\n",
      "Epoch 27 of 500 took 0.098s\n",
      "Epoch 27/499\n",
      "----------\n",
      "train Loss: 0.00103814 Acc: 0.95729167\n",
      "val Loss: 0.00034543 Acc: 0.96818182\n",
      "Epoch 28 of 500 took 0.104s\n",
      "Epoch 28/499\n",
      "----------\n",
      "train Loss: 0.00090640 Acc: 0.96770833\n",
      "val Loss: 0.00029518 Acc: 0.97727273\n",
      "Epoch 29 of 500 took 0.100s\n",
      "Epoch 29/499\n",
      "----------\n",
      "train Loss: 0.00085158 Acc: 0.96354167\n",
      "val Loss: 0.00046266 Acc: 0.95454545\n",
      "Epoch 30 of 500 took 0.099s\n",
      "Epoch 30/499\n",
      "----------\n",
      "train Loss: 0.00074738 Acc: 0.97395833\n",
      "val Loss: 0.00046910 Acc: 0.96818182\n",
      "Epoch 31 of 500 took 0.102s\n",
      "Epoch 31/499\n",
      "----------\n",
      "train Loss: 0.00082835 Acc: 0.96770833\n",
      "val Loss: 0.00035628 Acc: 0.96818182\n",
      "Epoch 32 of 500 took 0.098s\n",
      "Epoch 32/499\n",
      "----------\n",
      "train Loss: 0.00085956 Acc: 0.9671875\n",
      "val Loss: 0.00024362 Acc: 0.98636364\n",
      "Epoch 33 of 500 took 0.103s\n",
      "Epoch 33/499\n",
      "----------\n",
      "train Loss: 0.00071588 Acc: 0.9734375\n",
      "val Loss: 0.00027903 Acc: 0.98181818\n",
      "Epoch 34 of 500 took 0.098s\n",
      "Epoch 34/499\n",
      "----------\n",
      "train Loss: 0.00081146 Acc: 0.96822917\n",
      "val Loss: 0.00024232 Acc: 0.98181818\n",
      "Epoch 35 of 500 took 0.102s\n",
      "Epoch 35/499\n",
      "----------\n",
      "train Loss: 0.00070351 Acc: 0.97395833\n",
      "val Loss: 0.00029220 Acc: 0.98181818\n",
      "Epoch 36 of 500 took 0.103s\n",
      "\n",
      "Training complete in 0m 4s\n",
      "Best val loss: 0.000335\n",
      "Session:  1\n",
      "TSD_Network(\n",
      "  (_network): ModuleList(\n",
      "    (0): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=252, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=252, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (1): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (2): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (_output): Linear(in_features=200, out_features=22, bias=True)\n",
      "  (_output_discriminator): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "Number Parameters:  137624\n",
      "<generator object Module.parameters at 0x7fb108db34a0>\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/TSD/participant_2/best_state_0.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/TSD/participant_2/best_state_0.pt' (epoch 25)\n",
      "Epoch 0/499\n",
      "----------\n",
      "train Loss: 0.00714850 Acc: 0.7515625\n",
      "val Loss: 0.00203148 Acc: 0.85\n",
      "New best validation loss: 0.0020314790985801003\n",
      "Epoch 1 of 500 took 0.104s\n",
      "Epoch 1/499\n",
      "----------\n",
      "train Loss: 0.00461960 Acc: 0.80625\n",
      "val Loss: 0.00155939 Acc: 0.87727273\n",
      "New best validation loss: 0.001559388366612521\n",
      "Epoch 2 of 500 took 0.107s\n",
      "Epoch 2/499\n",
      "----------\n",
      "train Loss: 0.00387908 Acc: 0.83177083\n",
      "val Loss: 0.00120637 Acc: 0.90454545\n",
      "New best validation loss: 0.0012063664468851957\n",
      "Epoch 3 of 500 took 0.105s\n",
      "Epoch 3/499\n",
      "----------\n",
      "train Loss: 0.00369795 Acc: 0.84427083\n",
      "val Loss: 0.00132310 Acc: 0.90454545\n",
      "Epoch 4 of 500 took 0.125s\n",
      "Epoch 4/499\n",
      "----------\n",
      "train Loss: 0.00291959 Acc: 0.87135417\n",
      "val Loss: 0.00097509 Acc: 0.93636364\n",
      "New best validation loss: 0.0009750936519015919\n",
      "Epoch 5 of 500 took 0.127s\n",
      "Epoch 5/499\n",
      "----------\n",
      "train Loss: 0.00295380 Acc: 0.87760417\n",
      "val Loss: 0.00083693 Acc: 0.92272727\n",
      "New best validation loss: 0.0008369281210682609\n",
      "Epoch 6 of 500 took 0.125s\n",
      "Epoch 6/499\n",
      "----------\n",
      "train Loss: 0.00254715 Acc: 0.88541667\n",
      "val Loss: 0.00090864 Acc: 0.94090909\n",
      "Epoch 7 of 500 took 0.126s\n",
      "Epoch 7/499\n",
      "----------\n",
      "train Loss: 0.00247931 Acc: 0.88802083\n",
      "val Loss: 0.00106075 Acc: 0.90909091\n",
      "Epoch 8 of 500 took 0.124s\n",
      "Epoch 8/499\n",
      "----------\n",
      "train Loss: 0.00221660 Acc: 0.9\n",
      "val Loss: 0.00101571 Acc: 0.90454545\n",
      "Epoch 9 of 500 took 0.126s\n",
      "Epoch 9/499\n",
      "----------\n",
      "train Loss: 0.00188594 Acc: 0.9140625\n",
      "val Loss: 0.00092787 Acc: 0.91818182\n",
      "Epoch 10 of 500 took 0.125s\n",
      "Epoch 10/499\n",
      "----------\n",
      "train Loss: 0.00199803 Acc: 0.91770833\n",
      "val Loss: 0.00075553 Acc: 0.94090909\n",
      "Epoch 11 of 500 took 0.126s\n",
      "Epoch 11/499\n",
      "----------\n",
      "train Loss: 0.00199741 Acc: 0.9140625\n",
      "val Loss: 0.00060911 Acc: 0.95909091\n",
      "New best validation loss: 0.0006091109053655104\n",
      "Epoch 12 of 500 took 0.125s\n",
      "Epoch 12/499\n",
      "----------\n",
      "train Loss: 0.00190908 Acc: 0.915625\n",
      "val Loss: 0.00068473 Acc: 0.93636364\n",
      "Epoch 13 of 500 took 0.127s\n",
      "Epoch 13/499\n",
      "----------\n",
      "train Loss: 0.00163718 Acc: 0.93125\n",
      "val Loss: 0.00061877 Acc: 0.94090909\n",
      "Epoch 14 of 500 took 0.122s\n",
      "Epoch 14/499\n",
      "----------\n",
      "train Loss: 0.00180759 Acc: 0.91927083\n",
      "val Loss: 0.00116895 Acc: 0.9\n",
      "Epoch 15 of 500 took 0.126s\n",
      "Epoch 15/499\n",
      "----------\n",
      "train Loss: 0.00166097 Acc: 0.9203125\n",
      "val Loss: 0.00057740 Acc: 0.96363636\n",
      "Epoch 16 of 500 took 0.124s\n",
      "Epoch 16/499\n",
      "----------\n",
      "train Loss: 0.00169234 Acc: 0.92864583\n",
      "val Loss: 0.00074818 Acc: 0.93636364\n",
      "Epoch 17 of 500 took 0.119s\n",
      "Epoch 17/499\n",
      "----------\n",
      "train Loss: 0.00168548 Acc: 0.92604167\n",
      "val Loss: 0.00056548 Acc: 0.96363636\n",
      "Epoch 18 of 500 took 0.098s\n",
      "Epoch 18/499\n",
      "----------\n",
      "train Loss: 0.00145626 Acc: 0.93333333\n",
      "val Loss: 0.00060315 Acc: 0.95\n",
      "Epoch 19 of 500 took 0.100s\n",
      "Epoch 19/499\n",
      "----------\n",
      "train Loss: 0.00132683 Acc: 0.94322917\n",
      "val Loss: 0.00043545 Acc: 0.96818182\n",
      "New best validation loss: 0.00043544843792915345\n",
      "Epoch 20 of 500 took 0.129s\n",
      "Epoch 20/499\n",
      "----------\n",
      "train Loss: 0.00132243 Acc: 0.94114583\n",
      "val Loss: 0.00046885 Acc: 0.95909091\n",
      "Epoch 21 of 500 took 0.118s\n",
      "Epoch 21/499\n",
      "----------\n",
      "train Loss: 0.00129773 Acc: 0.94635417\n",
      "val Loss: 0.00064415 Acc: 0.95454545\n",
      "Epoch 22 of 500 took 0.098s\n",
      "Epoch 22/499\n",
      "----------\n",
      "train Loss: 0.00136145 Acc: 0.94010417\n",
      "val Loss: 0.00039184 Acc: 0.96818182\n",
      "Epoch 23 of 500 took 0.099s\n",
      "Epoch 23/499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "train Loss: 0.00130568 Acc: 0.94583333\n",
      "val Loss: 0.00057378 Acc: 0.95909091\n",
      "Epoch 24 of 500 took 0.104s\n",
      "Epoch 24/499\n",
      "----------\n",
      "train Loss: 0.00144225 Acc: 0.93697917\n",
      "val Loss: 0.00067339 Acc: 0.93636364\n",
      "Epoch 25 of 500 took 0.102s\n",
      "Epoch 25/499\n",
      "----------\n",
      "train Loss: 0.00135226 Acc: 0.93541667\n",
      "val Loss: 0.00047531 Acc: 0.96818182\n",
      "Epoch 26 of 500 took 0.101s\n",
      "Epoch 26/499\n",
      "----------\n",
      "train Loss: 0.00123106 Acc: 0.9484375\n",
      "val Loss: 0.00049208 Acc: 0.95\n",
      "Epoch 27 of 500 took 0.099s\n",
      "Epoch 27/499\n",
      "----------\n",
      "train Loss: 0.00113153 Acc: 0.9546875\n",
      "val Loss: 0.00050887 Acc: 0.95909091\n",
      "Epoch 28 of 500 took 0.102s\n",
      "Epoch 28/499\n",
      "----------\n",
      "train Loss: 0.00129412 Acc: 0.94114583\n",
      "val Loss: 0.00038153 Acc: 0.97727273\n",
      "Epoch 29 of 500 took 0.098s\n",
      "Epoch 29/499\n",
      "----------\n",
      "train Loss: 0.00115484 Acc: 0.95677083\n",
      "val Loss: 0.00038766 Acc: 0.96818182\n",
      "Epoch 30 of 500 took 0.103s\n",
      "Epoch 30/499\n",
      "----------\n",
      "train Loss: 0.00107792 Acc: 0.94739583\n",
      "val Loss: 0.00036556 Acc: 0.97272727\n",
      "Epoch 31 of 500 took 0.098s\n",
      "\n",
      "Training complete in 0m 4s\n",
      "Best val loss: 0.000435\n",
      "Session:  2\n",
      "TSD_Network(\n",
      "  (_network): ModuleList(\n",
      "    (0): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=252, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=252, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (1): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (2): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (_output): Linear(in_features=200, out_features=22, bias=True)\n",
      "  (_output_discriminator): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "Number Parameters:  137624\n",
      "<generator object Module.parameters at 0x7fb10a6ad510>\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/TSD/participant_2/best_state_1.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/TSD/participant_2/best_state_1.pt' (epoch 20)\n",
      "Epoch 0/499\n",
      "----------\n",
      "train Loss: 0.01225457 Acc: 0.615625\n",
      "val Loss: 0.00283309 Acc: 0.77272727\n",
      "New best validation loss: 0.002833091128956188\n",
      "Epoch 1 of 500 took 0.105s\n",
      "Epoch 1/499\n",
      "----------\n",
      "train Loss: 0.00593624 Acc: 0.73333333\n",
      "val Loss: 0.00270848 Acc: 0.74090909\n",
      "New best validation loss: 0.002708480574867942\n",
      "Epoch 2 of 500 took 0.126s\n",
      "Epoch 2/499\n",
      "----------\n",
      "train Loss: 0.00492883 Acc: 0.76927083\n",
      "val Loss: 0.00206801 Acc: 0.78636364\n",
      "New best validation loss: 0.0020680106498978355\n",
      "Epoch 3 of 500 took 0.109s\n",
      "Epoch 3/499\n",
      "----------\n",
      "train Loss: 0.00425773 Acc: 0.790625\n",
      "val Loss: 0.00214695 Acc: 0.8\n",
      "Epoch 4 of 500 took 0.100s\n",
      "Epoch 4/499\n",
      "----------\n",
      "train Loss: 0.00380605 Acc: 0.81822917\n",
      "val Loss: 0.00170944 Acc: 0.83181818\n",
      "New best validation loss: 0.0017094415697184477\n",
      "Epoch 5 of 500 took 0.101s\n",
      "Epoch 5/499\n",
      "----------\n",
      "train Loss: 0.00380597 Acc: 0.82395833\n",
      "val Loss: 0.00193834 Acc: 0.8\n",
      "Epoch 6 of 500 took 0.126s\n",
      "Epoch 6/499\n",
      "----------\n",
      "train Loss: 0.00368944 Acc: 0.81979167\n",
      "val Loss: 0.00176533 Acc: 0.83181818\n",
      "Epoch 7 of 500 took 0.104s\n",
      "Epoch 7/499\n",
      "----------\n",
      "train Loss: 0.00303748 Acc: 0.84947917\n",
      "val Loss: 0.00148309 Acc: 0.85454545\n",
      "New best validation loss: 0.0014830895445563576\n",
      "Epoch 8 of 500 took 0.100s\n",
      "Epoch 8/499\n",
      "----------\n",
      "train Loss: 0.00304978 Acc: 0.840625\n",
      "val Loss: 0.00174895 Acc: 0.85909091\n",
      "Epoch 9 of 500 took 0.098s\n",
      "Epoch 9/499\n",
      "----------\n",
      "train Loss: 0.00290289 Acc: 0.86458333\n",
      "val Loss: 0.00137140 Acc: 0.87272727\n",
      "New best validation loss: 0.0013713992454788902\n",
      "Epoch 10 of 500 took 0.103s\n",
      "Epoch 10/499\n",
      "----------\n",
      "train Loss: 0.00277085 Acc: 0.86875\n",
      "val Loss: 0.00136711 Acc: 0.85454545\n",
      "Epoch 11 of 500 took 0.098s\n",
      "Epoch 11/499\n",
      "----------\n",
      "train Loss: 0.00258166 Acc: 0.87447917\n",
      "val Loss: 0.00148657 Acc: 0.88181818\n",
      "Epoch 12 of 500 took 0.102s\n",
      "Epoch 12/499\n",
      "----------\n",
      "train Loss: 0.00246407 Acc: 0.8859375\n",
      "val Loss: 0.00134751 Acc: 0.88636364\n",
      "Epoch 13 of 500 took 0.098s\n",
      "Epoch 13/499\n",
      "----------\n",
      "train Loss: 0.00257589 Acc: 0.86458333\n",
      "val Loss: 0.00155074 Acc: 0.86818182\n",
      "Epoch 14 of 500 took 0.108s\n",
      "Epoch 14/499\n",
      "----------\n",
      "train Loss: 0.00235218 Acc: 0.8921875\n",
      "val Loss: 0.00127245 Acc: 0.86818182\n",
      "Epoch 15 of 500 took 0.098s\n",
      "Epoch 15/499\n",
      "----------\n",
      "train Loss: 0.00229996 Acc: 0.88072917\n",
      "val Loss: 0.00119764 Acc: 0.90454545\n",
      "New best validation loss: 0.001197635992006822\n",
      "Epoch 16 of 500 took 0.107s\n",
      "Epoch 16/499\n",
      "----------\n",
      "train Loss: 0.00229367 Acc: 0.8890625\n",
      "val Loss: 0.00134396 Acc: 0.90909091\n",
      "Epoch 17 of 500 took 0.103s\n",
      "Epoch 17/499\n",
      "----------\n",
      "train Loss: 0.00196383 Acc: 0.90989583\n",
      "val Loss: 0.00114794 Acc: 0.89090909\n",
      "Epoch 18 of 500 took 0.102s\n",
      "Epoch 18/499\n",
      "----------\n",
      "train Loss: 0.00198433 Acc: 0.90885417\n",
      "val Loss: 0.00098770 Acc: 0.92727273\n",
      "New best validation loss: 0.0009876973249695518\n",
      "Epoch 19 of 500 took 0.102s\n",
      "Epoch 19/499\n",
      "----------\n",
      "train Loss: 0.00207761 Acc: 0.90729167\n",
      "val Loss: 0.00130066 Acc: 0.88636364\n",
      "Epoch 20 of 500 took 0.099s\n",
      "Epoch 20/499\n",
      "----------\n",
      "train Loss: 0.00212675 Acc: 0.90104167\n",
      "val Loss: 0.00104286 Acc: 0.90454545\n",
      "Epoch 21 of 500 took 0.101s\n",
      "Epoch 21/499\n",
      "----------\n",
      "train Loss: 0.00206621 Acc: 0.90416667\n",
      "val Loss: 0.00095135 Acc: 0.92272727\n",
      "Epoch 22 of 500 took 0.099s\n",
      "Epoch 22/499\n",
      "----------\n",
      "train Loss: 0.00175848 Acc: 0.92135417\n",
      "val Loss: 0.00088339 Acc: 0.92727273\n",
      "New best validation loss: 0.0008833869614384391\n",
      "Epoch 23 of 500 took 0.104s\n",
      "Epoch 23/499\n",
      "----------\n",
      "train Loss: 0.00181537 Acc: 0.925\n",
      "val Loss: 0.00087461 Acc: 0.94090909\n",
      "Epoch 24 of 500 took 0.101s\n",
      "Epoch 24/499\n",
      "----------\n",
      "train Loss: 0.00153377 Acc: 0.9265625\n",
      "val Loss: 0.00142250 Acc: 0.86818182\n",
      "Epoch 25 of 500 took 0.101s\n",
      "Epoch 25/499\n",
      "----------\n",
      "train Loss: 0.00163367 Acc: 0.928125\n",
      "val Loss: 0.00085870 Acc: 0.95\n",
      "Epoch 26 of 500 took 0.107s\n",
      "Epoch 26/499\n",
      "----------\n",
      "train Loss: 0.00153291 Acc: 0.9265625\n",
      "val Loss: 0.00081597 Acc: 0.93636364\n",
      "Epoch 27 of 500 took 0.100s\n",
      "Epoch 27/499\n",
      "----------\n",
      "train Loss: 0.00168327 Acc: 0.92552083\n",
      "val Loss: 0.00091710 Acc: 0.91818182\n",
      "Epoch 28 of 500 took 0.098s\n",
      "Epoch 28/499\n",
      "----------\n",
      "train Loss: 0.00165310 Acc: 0.92291667\n",
      "val Loss: 0.00081367 Acc: 0.92727273\n",
      "Epoch 29 of 500 took 0.103s\n",
      "Epoch 29/499\n",
      "----------\n",
      "train Loss: 0.00163309 Acc: 0.92395833\n",
      "val Loss: 0.00075057 Acc: 0.94090909\n",
      "New best validation loss: 0.0007505685091018677\n",
      "Epoch 30 of 500 took 0.100s\n",
      "Epoch 30/499\n",
      "----------\n",
      "train Loss: 0.00148246 Acc: 0.9375\n",
      "val Loss: 0.00074806 Acc: 0.94090909\n",
      "Epoch 31 of 500 took 0.102s\n",
      "Epoch 31/499\n",
      "----------\n",
      "train Loss: 0.00123301 Acc: 0.95\n",
      "val Loss: 0.00078962 Acc: 0.95\n",
      "Epoch 32 of 500 took 0.100s\n",
      "Epoch 32/499\n",
      "----------\n",
      "train Loss: 0.00135170 Acc: 0.93958333\n",
      "val Loss: 0.00156343 Acc: 0.88636364\n",
      "Epoch 33 of 500 took 0.102s\n",
      "Epoch 33/499\n",
      "----------\n",
      "train Loss: 0.00142390 Acc: 0.93385417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.00075766 Acc: 0.93636364\n",
      "Epoch 34 of 500 took 0.100s\n",
      "Epoch 34/499\n",
      "----------\n",
      "train Loss: 0.00151205 Acc: 0.9390625\n",
      "val Loss: 0.00074625 Acc: 0.94545455\n",
      "Epoch 35 of 500 took 0.102s\n",
      "Epoch 35/499\n",
      "----------\n",
      "train Loss: 0.00133586 Acc: 0.940625\n",
      "val Loss: 0.00071377 Acc: 0.94090909\n",
      "Epoch 36 of 500 took 0.099s\n",
      "Epoch 36/499\n",
      "----------\n",
      "train Loss: 0.00132596 Acc: 0.93229167\n",
      "val Loss: 0.00073921 Acc: 0.92727273\n",
      "Epoch 37 of 500 took 0.102s\n",
      "Epoch 37/499\n",
      "----------\n",
      "train Loss: 0.00142590 Acc: 0.93645833\n",
      "val Loss: 0.00070917 Acc: 0.94545455\n",
      "Epoch 38 of 500 took 0.098s\n",
      "Epoch 38/499\n",
      "----------\n",
      "train Loss: 0.00117150 Acc: 0.95520833\n",
      "val Loss: 0.00062989 Acc: 0.95\n",
      "New best validation loss: 0.0006298906423828818\n",
      "Epoch 39 of 500 took 0.103s\n",
      "Epoch 39/499\n",
      "----------\n",
      "train Loss: 0.00123696 Acc: 0.9484375\n",
      "val Loss: 0.00058737 Acc: 0.95909091\n",
      "Epoch 40 of 500 took 0.098s\n",
      "Epoch 40/499\n",
      "----------\n",
      "train Loss: 0.00122041 Acc: 0.94375\n",
      "val Loss: 0.00092949 Acc: 0.93636364\n",
      "Epoch 41 of 500 took 0.101s\n",
      "Epoch 41/499\n",
      "----------\n",
      "train Loss: 0.00123493 Acc: 0.9484375\n",
      "val Loss: 0.00064283 Acc: 0.95454545\n",
      "Epoch 42 of 500 took 0.099s\n",
      "Epoch 42/499\n",
      "----------\n",
      "train Loss: 0.00098233 Acc: 0.96197917\n",
      "val Loss: 0.00069709 Acc: 0.95454545\n",
      "Epoch 43 of 500 took 0.128s\n",
      "Epoch 43/499\n",
      "----------\n",
      "train Loss: 0.00118393 Acc: 0.95052083\n",
      "val Loss: 0.00050791 Acc: 0.95909091\n",
      "New best validation loss: 0.000507914884523912\n",
      "Epoch 44 of 500 took 0.116s\n",
      "Epoch 44/499\n",
      "----------\n",
      "train Loss: 0.00101280 Acc: 0.95520833\n",
      "val Loss: 0.00052486 Acc: 0.96363636\n",
      "Epoch 45 of 500 took 0.098s\n",
      "Epoch 45/499\n",
      "----------\n",
      "train Loss: 0.00089619 Acc: 0.96041667\n",
      "val Loss: 0.00054038 Acc: 0.95454545\n",
      "Epoch 46 of 500 took 0.098s\n",
      "Epoch 46/499\n",
      "----------\n",
      "train Loss: 0.00109064 Acc: 0.953125\n",
      "val Loss: 0.00058818 Acc: 0.95\n",
      "Epoch 47 of 500 took 0.103s\n",
      "Epoch 47/499\n",
      "----------\n",
      "train Loss: 0.00095515 Acc: 0.95208333\n",
      "val Loss: 0.00131289 Acc: 0.88636364\n",
      "Epoch 48 of 500 took 0.099s\n",
      "Epoch 48/499\n",
      "----------\n",
      "train Loss: 0.00103071 Acc: 0.95416667\n",
      "val Loss: 0.00056853 Acc: 0.95454545\n",
      "Epoch 49 of 500 took 0.102s\n",
      "Epoch 49/499\n",
      "----------\n",
      "train Loss: 0.00102106 Acc: 0.95885417\n",
      "val Loss: 0.00038272 Acc: 0.96818182\n",
      "New best validation loss: 0.0003827150233767249\n",
      "Epoch 50 of 500 took 0.100s\n",
      "Epoch 50/499\n",
      "----------\n",
      "train Loss: 0.00101275 Acc: 0.96041667\n",
      "val Loss: 0.00110898 Acc: 0.92272727\n",
      "Epoch 51 of 500 took 0.101s\n",
      "Epoch 51/499\n",
      "----------\n",
      "train Loss: 0.00087142 Acc: 0.96197917\n",
      "val Loss: 0.00077941 Acc: 0.94090909\n",
      "Epoch 52 of 500 took 0.098s\n",
      "Epoch 52/499\n",
      "----------\n",
      "train Loss: 0.00087002 Acc: 0.9640625\n",
      "val Loss: 0.00075098 Acc: 0.95\n",
      "Epoch 53 of 500 took 0.104s\n",
      "Epoch 53/499\n",
      "----------\n",
      "train Loss: 0.00105380 Acc: 0.95052083\n",
      "val Loss: 0.00068835 Acc: 0.95909091\n",
      "Epoch 54 of 500 took 0.097s\n",
      "Epoch 54/499\n",
      "----------\n",
      "train Loss: 0.00085031 Acc: 0.96510417\n",
      "val Loss: 0.00063448 Acc: 0.95\n",
      "Epoch 55 of 500 took 0.098s\n",
      "Epoch 55/499\n",
      "----------\n",
      "train Loss: 0.00095847 Acc: 0.95572917\n",
      "val Loss: 0.00068298 Acc: 0.96363636\n",
      "Epoch    56: reducing learning rate of group 0 to 5.0300e-04.\n",
      "Epoch 56 of 500 took 0.103s\n",
      "Epoch 56/499\n",
      "----------\n",
      "train Loss: 0.00085192 Acc: 0.965625\n",
      "val Loss: 0.00052222 Acc: 0.96363636\n",
      "Epoch 57 of 500 took 0.098s\n",
      "Epoch 57/499\n",
      "----------\n",
      "train Loss: 0.00066811 Acc: 0.9734375\n",
      "val Loss: 0.00039014 Acc: 0.97272727\n",
      "Epoch 58 of 500 took 0.098s\n",
      "Epoch 58/499\n",
      "----------\n",
      "train Loss: 0.00069267 Acc: 0.96666667\n",
      "val Loss: 0.00040878 Acc: 0.95909091\n",
      "Epoch 59 of 500 took 0.103s\n",
      "Epoch 59/499\n",
      "----------\n",
      "train Loss: 0.00062055 Acc: 0.9703125\n",
      "val Loss: 0.00045014 Acc: 0.96363636\n",
      "Epoch 60 of 500 took 0.098s\n",
      "Epoch 60/499\n",
      "----------\n",
      "train Loss: 0.00062437 Acc: 0.97291667\n",
      "val Loss: 0.00034158 Acc: 0.97727273\n",
      "Epoch 61 of 500 took 0.098s\n",
      "\n",
      "Training complete in 0m 6s\n",
      "Best val loss: 0.000383\n"
     ]
    }
   ],
   "source": [
    "train_fine_tuning(examples_datasets_train, labels_datasets_train,\n",
    "                  num_kernels=num_kernels, path_weight_to_save_to=path_to_save_to,\n",
    "                  number_of_classes=number_of_classes,\n",
    "                  batch_size=batch_size,\n",
    "                  feature_vector_input_length=feature_vector_input_length,\n",
    "                  learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET one participant_examples  (3, 40, 550, 252)\n",
      "   GET one training_index_examples  (40, 550, 252)  at  0\n",
      "   GOT one group XY  (2200, 252)    (2200,)\n",
      "       one group XY test  (550, 252)    (550, 252)\n",
      "       one group XY train (1980, 252)    (1980,)\n",
      "       one group XY valid (220, 252)    (220, 252)\n",
      "   GET one training_index_examples  (40, 550, 252)  at  1\n",
      "   GOT one group XY  (2200, 252)    (2200,)\n",
      "       one group XY test  (550, 252)    (550, 252)\n",
      "       one group XY train (1980, 252)    (1980,)\n",
      "       one group XY valid (220, 252)    (220, 252)\n",
      "   GET one training_index_examples  (40, 550, 252)  at  2\n",
      "   GOT one group XY  (2200, 252)    (2200,)\n",
      "       one group XY test  (550, 252)    (550, 252)\n",
      "       one group XY train (1980, 252)    (1980,)\n",
      "       one group XY valid (220, 252)    (220, 252)\n",
      "dataloaders: \n",
      "   train  (1, 3)\n",
      "   valid  (1, 3)\n",
      "   test  (1, 3)\n",
      "GET one participant_examples  (3, 40, 550, 252)\n",
      "   GET one training_index_examples  (40, 550, 252)  at  0\n",
      "   GOT one group XY  (2200, 252)    (2200,)\n",
      "       one group XY test  (550, 252)    (550, 252)\n",
      "       one group XY train (1980, 252)    (1980,)\n",
      "       one group XY valid (220, 252)    (220, 252)\n",
      "   GET one training_index_examples  (40, 550, 252)  at  1\n",
      "   GOT one group XY  (2200, 252)    (2200,)\n",
      "       one group XY test  (550, 252)    (550, 252)\n",
      "       one group XY train (1980, 252)    (1980,)\n",
      "       one group XY valid (220, 252)    (220, 252)\n",
      "   GET one training_index_examples  (40, 550, 252)  at  2\n",
      "   GOT one group XY  (2200, 252)    (2200,)\n",
      "       one group XY test  (550, 252)    (550, 252)\n",
      "       one group XY train (1980, 252)    (1980,)\n",
      "       one group XY valid (220, 252)    (220, 252)\n",
      "dataloaders: \n",
      "   train  (2, 3)\n",
      "   valid  (2, 3)\n",
      "   test  (2, 3)\n",
      "GET one participant_examples  (3, 40, 550, 252)\n",
      "   GET one training_index_examples  (40, 550, 252)  at  0\n",
      "   GOT one group XY  (2200, 252)    (2200,)\n",
      "       one group XY test  (550, 252)    (550, 252)\n",
      "       one group XY train (1980, 252)    (1980,)\n",
      "       one group XY valid (220, 252)    (220, 252)\n",
      "   GET one training_index_examples  (40, 550, 252)  at  1\n",
      "   GOT one group XY  (2200, 252)    (2200,)\n",
      "       one group XY test  (550, 252)    (550, 252)\n",
      "       one group XY train (1980, 252)    (1980,)\n",
      "       one group XY valid (220, 252)    (220, 252)\n",
      "   GET one training_index_examples  (40, 550, 252)  at  2\n",
      "   GOT one group XY  (2200, 252)    (2200,)\n",
      "       one group XY test  (550, 252)    (550, 252)\n",
      "       one group XY train (1980, 252)    (1980,)\n",
      "       one group XY valid (220, 252)    (220, 252)\n",
      "dataloaders: \n",
      "   train  (3, 3)\n",
      "   valid  (3, 3)\n",
      "   test  (3, 3)\n",
      "TSD_Network(\n",
      "  (_network): ModuleList(\n",
      "    (0): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=252, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=252, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (1): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (2): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (_output): Linear(in_features=200, out_features=22, bias=True)\n",
      "  (_output_discriminator): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "Number Parameters:  137624\n",
      "0  SESSION   data =  550\n",
      "Participant:  0  Accuracy:  0.9963636363636363\n",
      "1  SESSION   data =  550\n",
      "Participant:  0  Accuracy:  0.5181818181818182\n",
      "2  SESSION   data =  550\n",
      "Participant:  0  Accuracy:  0.509090909090909\n",
      "ACCURACY PARTICIPANT  0 :  [0.9963636363636363, 0.5181818181818182, 0.509090909090909]\n",
      "TSD_Network(\n",
      "  (_network): ModuleList(\n",
      "    (0): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=252, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=252, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (1): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (2): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (_output): Linear(in_features=200, out_features=22, bias=True)\n",
      "  (_output_discriminator): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "Number Parameters:  137624\n",
      "0  SESSION   data =  550\n",
      "Participant:  1  Accuracy:  0.9745454545454545\n",
      "1  SESSION   data =  550\n",
      "Participant:  1  Accuracy:  0.8672727272727273\n",
      "2  SESSION   data =  550\n",
      "Participant:  1  Accuracy:  0.6818181818181818\n",
      "ACCURACY PARTICIPANT  1 :  [0.9745454545454545, 0.8672727272727273, 0.6818181818181818]\n",
      "TSD_Network(\n",
      "  (_network): ModuleList(\n",
      "    (0): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=252, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=252, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (1): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (2): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (_output): Linear(in_features=200, out_features=22, bias=True)\n",
      "  (_output_discriminator): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "Number Parameters:  137624\n",
      "0  SESSION   data =  550\n",
      "Participant:  2  Accuracy:  0.9963636363636363\n",
      "1  SESSION   data =  550\n",
      "Participant:  2  Accuracy:  0.66\n",
      "2  SESSION   data =  550\n",
      "Participant:  2  Accuracy:  0.5709090909090909\n",
      "ACCURACY PARTICIPANT  2 :  [0.9963636363636363, 0.66, 0.5709090909090909]\n",
      "[array([0.99636364, 0.51818182, 0.50909091]), array([0.97454545, 0.86727273, 0.68181818]), array([0.99636364, 0.66      , 0.57090909])]\n",
      "OVERALL ACCURACY: 0.7527272727272727\n"
     ]
    }
   ],
   "source": [
    "save_path = \"/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/results_tsd\"\n",
    "algo_name = \"standard_TSD\"\n",
    "test_TSD_DNN_on_training_sessions(examples_datasets_train, labels_datasets_train,\n",
    "                                  num_neurons=num_kernels, use_only_first_training=True,\n",
    "                                  path_weights=path_to_save_to,\n",
    "                                  feature_vector_input_length=feature_vector_input_length,\n",
    "                                  save_path = save_path, algo_name=algo_name,\n",
    "                                  number_of_classes=number_of_classes, cycle_for_test=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant_0</th>\n",
       "      <th>Participant_1</th>\n",
       "      <th>Participant_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Session_0</th>\n",
       "      <td>0.996364</td>\n",
       "      <td>0.974545</td>\n",
       "      <td>0.996364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Session_1</th>\n",
       "      <td>0.518182</td>\n",
       "      <td>0.867273</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Session_2</th>\n",
       "      <td>0.509091</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.570909</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Participant_0 Participant_1 Participant_2\n",
       "Session_0      0.996364      0.974545      0.996364\n",
       "Session_1      0.518182      0.867273          0.66\n",
       "Session_2      0.509091      0.681818      0.570909"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_filename = save_path + '/predictions_' + algo_name + \"_no_retraining.npy\"\n",
    "results = np.load(results_filename, allow_pickle=True)\n",
    "TSD_acc = results[0]\n",
    "TSD_acc_overall = np.mean(TSD_acc)\n",
    "TSD_df = pd.DataFrame(TSD_acc.transpose(), \n",
    "                       index = [f'Session_{i}' for i in range(TSD_acc.shape[1])],\n",
    "                        columns = [f'Participant_{j}' for j in range(TSD_acc.shape[0])])\n",
    "TSD_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "TSD_df.transpose().plot.bar(rot=0, figsize=(10,5))\n",
    "plt.title(\"TSD Accuracies\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. DANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TrainingsAndEvaluations.ForTrainingSessions.train_tsd_dnn_DA import train_DANN, test_DANN_on_training_sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_save_to_DANN = \"Weights_TSD/DANN\"\n",
    "num_kernels=[200, 200, 200]                        # model layer size \n",
    "number_of_cycle_for_first_training=40               # #session\n",
    "number_of_cycles_rest_of_training=40     \n",
    "path_weights_standard_DNN =path_to_save_to\n",
    "path_weights_DANN =\"/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/DANN\"\n",
    "number_of_classes=22\n",
    "batch_size=128          \n",
    "feature_vector_input_length=252                     # size of one example \n",
    "learning_rate=0.002515"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET one participant_examples  (3, 40, 550, 252)\n",
      "   GET one training_index_examples  (40, 550, 252)  at  0\n",
      "   GOT one group XY  (2200, 252)    (2200,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (1980, 252)    (1980,)\n",
      "       one group XY valid (220, 252)    (220, 252)\n",
      "   GET one training_index_examples  (40, 550, 252)  at  1\n",
      "   GOT one group XY  (2200, 252)    (2200,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (1980, 252)    (1980,)\n",
      "       one group XY valid (220, 252)    (220, 252)\n",
      "   GET one training_index_examples  (40, 550, 252)  at  2\n",
      "   GOT one group XY  (2200, 252)    (2200,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (1980, 252)    (1980,)\n",
      "       one group XY valid (220, 252)    (220, 252)\n",
      "dataloaders: \n",
      "   train  (1, 3)\n",
      "   valid  (1, 3)\n",
      "   test  (1, 0)\n",
      "GET one participant_examples  (3, 40, 550, 252)\n",
      "   GET one training_index_examples  (40, 550, 252)  at  0\n",
      "   GOT one group XY  (2200, 252)    (2200,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (1980, 252)    (1980,)\n",
      "       one group XY valid (220, 252)    (220, 252)\n",
      "   GET one training_index_examples  (40, 550, 252)  at  1\n",
      "   GOT one group XY  (2200, 252)    (2200,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (1980, 252)    (1980,)\n",
      "       one group XY valid (220, 252)    (220, 252)\n",
      "   GET one training_index_examples  (40, 550, 252)  at  2\n",
      "   GOT one group XY  (2200, 252)    (2200,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (1980, 252)    (1980,)\n",
      "       one group XY valid (220, 252)    (220, 252)\n",
      "dataloaders: \n",
      "   train  (2, 3)\n",
      "   valid  (2, 3)\n",
      "   test  (2, 0)\n",
      "GET one participant_examples  (3, 40, 550, 252)\n",
      "   GET one training_index_examples  (40, 550, 252)  at  0\n",
      "   GOT one group XY  (2200, 252)    (2200,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (1980, 252)    (1980,)\n",
      "       one group XY valid (220, 252)    (220, 252)\n",
      "   GET one training_index_examples  (40, 550, 252)  at  1\n",
      "   GOT one group XY  (2200, 252)    (2200,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (1980, 252)    (1980,)\n",
      "       one group XY valid (220, 252)    (220, 252)\n",
      "   GET one training_index_examples  (40, 550, 252)  at  2\n",
      "   GOT one group XY  (2200, 252)    (2200,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (1980, 252)    (1980,)\n",
      "       one group XY valid (220, 252)    (220, 252)\n",
      "dataloaders: \n",
      "   train  (3, 3)\n",
      "   valid  (3, 3)\n",
      "   test  (3, 0)\n",
      "SHAPE SESSIONS:  (3,)\n",
      "()\n",
      "TSD_Network(\n",
      "  (_network): ModuleList(\n",
      "    (0): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=252, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=252, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (1): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (2): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (_output): Linear(in_features=200, out_features=22, bias=True)\n",
      "  (_output_discriminator): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "Number Parameters:  137624\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/TSD/participant_0/best_state_0.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/TSD/participant_0/best_state_0.pt' (epoch 16)\n",
      "STARTING TRAINING\n",
      "Accuracy source 0.957812, main loss classifier 0.155032, source classification loss 0.146345, loss domain distinction 0.340154, accuracy domain distinction 0.487500\n",
      "VALIDATION Loss: 0.13667908 Acc: 0.93181818\n",
      "New best validation loss:  0.13667908310890198\n",
      "Epoch 1 of 500 took 0.234s\n",
      "Accuracy source 0.958854, main loss classifier 0.142708, source classification loss 0.137678, loss domain distinction 0.187354, accuracy domain distinction 0.500260\n",
      "VALIDATION Loss: 0.25575686 Acc: 0.90454545\n",
      "Epoch 2 of 500 took 0.210s\n",
      "Accuracy source 0.952083, main loss classifier 0.151696, source classification loss 0.159234, loss domain distinction 0.185877, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.04629594 Acc: 0.99090909\n",
      "New best validation loss:  0.04629594087600708\n",
      "Epoch 3 of 500 took 0.222s\n",
      "Accuracy source 0.951042, main loss classifier 0.148913, source classification loss 0.153975, loss domain distinction 0.187977, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.04837430 Acc: 0.98636364\n",
      "Epoch 4 of 500 took 0.212s\n",
      "Accuracy source 0.947396, main loss classifier 0.148822, source classification loss 0.155125, loss domain distinction 0.184204, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.18411954 Acc: 0.92272727\n",
      "Epoch 5 of 500 took 0.215s\n",
      "Accuracy source 0.957292, main loss classifier 0.140034, source classification loss 0.138684, loss domain distinction 0.184638, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.10491860 Acc: 0.96818182\n",
      "Epoch    22: reducing learning rate of group 0 to 5.0300e-04.\n",
      "Epoch 6 of 500 took 0.212s\n",
      "Accuracy source 0.956250, main loss classifier 0.131336, source classification loss 0.126265, loss domain distinction 0.182632, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.03994258 Acc: 0.99545455\n",
      "New best validation loss:  0.039942581206560135\n",
      "Epoch 7 of 500 took 0.264s\n",
      "Accuracy source 0.973437, main loss classifier 0.111334, source classification loss 0.085557, loss domain distinction 0.180949, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.03163732 Acc: 0.99090909\n",
      "New best validation loss:  0.031637318432331085\n",
      "Epoch 8 of 500 took 0.259s\n",
      "Accuracy source 0.979167, main loss classifier 0.104003, source classification loss 0.071818, loss domain distinction 0.181253, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.03065003 Acc: 0.98636364\n",
      "New best validation loss:  0.030650025233626366\n",
      "Epoch 9 of 500 took 0.259s\n",
      "Accuracy source 0.976042, main loss classifier 0.106726, source classification loss 0.077410, loss domain distinction 0.179958, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.06898869 Acc: 0.97727273\n",
      "Epoch 10 of 500 took 0.257s\n",
      "Accuracy source 0.985938, main loss classifier 0.098454, source classification loss 0.059308, loss domain distinction 0.182991, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.10331802 Acc: 0.96818182\n",
      "Epoch 11 of 500 took 0.260s\n",
      "Accuracy source 0.974479, main loss classifier 0.110332, source classification loss 0.084643, loss domain distinction 0.180675, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.05840314 Acc: 0.98181818\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0060e-04.\n",
      "Epoch 12 of 500 took 0.257s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy source 0.986979, main loss classifier 0.097795, source classification loss 0.060128, loss domain distinction 0.179884, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.04518892 Acc: 0.97727273\n",
      "Epoch 13 of 500 took 0.257s\n",
      "Accuracy source 0.985417, main loss classifier 0.095885, source classification loss 0.056279, loss domain distinction 0.180404, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.04117551 Acc: 0.98636364\n",
      "Epoch 14 of 500 took 0.248s\n",
      "Accuracy source 0.981771, main loss classifier 0.098109, source classification loss 0.060879, loss domain distinction 0.179543, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.06138015 Acc: 0.97727273\n",
      "Epoch 15 of 500 took 0.215s\n",
      "Accuracy source 0.977083, main loss classifier 0.101982, source classification loss 0.068672, loss domain distinction 0.180250, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.03065302 Acc: 0.99090909\n",
      "Epoch 16 of 500 took 0.213s\n",
      "Accuracy source 0.983854, main loss classifier 0.095986, source classification loss 0.056587, loss domain distinction 0.181023, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.05812438 Acc: 0.97727273\n",
      "Epoch 17 of 500 took 0.213s\n",
      "Accuracy source 0.979167, main loss classifier 0.100779, source classification loss 0.065693, loss domain distinction 0.180454, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.04142475 Acc: 0.98636364\n",
      "Epoch 18 of 500 took 0.213s\n",
      "Accuracy source 0.981771, main loss classifier 0.097024, source classification loss 0.059076, loss domain distinction 0.178216, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.04186713 Acc: 0.99090909\n",
      "Epoch 19 of 500 took 0.212s\n",
      "Accuracy source 0.984375, main loss classifier 0.096800, source classification loss 0.057636, loss domain distinction 0.180579, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.03381836 Acc: 0.99545455\n",
      "Training complete in 0m 5s\n",
      "()\n",
      "TSD_Network(\n",
      "  (_network): ModuleList(\n",
      "    (0): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=252, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=252, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (1): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (2): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (_output): Linear(in_features=200, out_features=22, bias=True)\n",
      "  (_output_discriminator): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "Number Parameters:  137624\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/TSD/participant_0/best_state_0.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/TSD/participant_0/best_state_0.pt' (epoch 16)\n",
      "STARTING TRAINING\n",
      "Accuracy source 0.954167, main loss classifier 0.154545, source classification loss 0.144045, loss domain distinction 0.342616, accuracy domain distinction 0.485938\n",
      "VALIDATION Loss: 0.07176952 Acc: 0.97272727\n",
      "New best validation loss:  0.07176952064037323\n",
      "Epoch 1 of 500 took 0.212s\n",
      "Accuracy source 0.946875, main loss classifier 0.161622, source classification loss 0.177316, loss domain distinction 0.187567, accuracy domain distinction 0.500260\n",
      "VALIDATION Loss: 0.07802471 Acc: 0.96818182\n",
      "Epoch 2 of 500 took 0.213s\n",
      "Accuracy source 0.957292, main loss classifier 0.148320, source classification loss 0.152926, loss domain distinction 0.185044, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.11639355 Acc: 0.94545455\n",
      "Epoch 3 of 500 took 0.211s\n",
      "Accuracy source 0.946875, main loss classifier 0.153761, source classification loss 0.164493, loss domain distinction 0.185685, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.06053587 Acc: 0.98636364\n",
      "New best validation loss:  0.06053587421774864\n",
      "Epoch 4 of 500 took 0.213s\n",
      "Accuracy source 0.958854, main loss classifier 0.135511, source classification loss 0.128643, loss domain distinction 0.184642, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.10493007 Acc: 0.96363636\n",
      "Epoch 5 of 500 took 0.223s\n",
      "Accuracy source 0.962500, main loss classifier 0.144383, source classification loss 0.147631, loss domain distinction 0.184415, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.04436455 Acc: 0.99090909\n",
      "Epoch    22: reducing learning rate of group 0 to 5.0300e-04.\n",
      "New best validation loss:  0.04436454921960831\n",
      "Epoch 6 of 500 took 0.213s\n",
      "Accuracy source 0.967187, main loss classifier 0.121547, source classification loss 0.106857, loss domain distinction 0.178089, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.02025233 Acc: 1.0\n",
      "New best validation loss:  0.02025233395397663\n",
      "Epoch 7 of 500 took 0.223s\n",
      "Accuracy source 0.965625, main loss classifier 0.119732, source classification loss 0.104232, loss domain distinction 0.179716, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.04404375 Acc: 0.99090909\n",
      "Epoch 8 of 500 took 0.216s\n",
      "Accuracy source 0.975000, main loss classifier 0.111934, source classification loss 0.087638, loss domain distinction 0.179958, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.04063011 Acc: 0.98636364\n",
      "Epoch 9 of 500 took 0.217s\n",
      "Accuracy source 0.983854, main loss classifier 0.099713, source classification loss 0.063761, loss domain distinction 0.178276, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.04338376 Acc: 0.99545455\n",
      "Epoch 10 of 500 took 0.212s\n",
      "Accuracy source 0.977604, main loss classifier 0.109798, source classification loss 0.083176, loss domain distinction 0.179228, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.04912164 Acc: 0.99090909\n",
      "Epoch 11 of 500 took 0.212s\n",
      "Accuracy source 0.980729, main loss classifier 0.100581, source classification loss 0.066025, loss domain distinction 0.177989, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.03313579 Acc: 0.99545455\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0060e-04.\n",
      "Epoch 12 of 500 took 0.213s\n",
      "Accuracy source 0.984375, main loss classifier 0.100779, source classification loss 0.066223, loss domain distinction 0.178450, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.02047178 Acc: 0.99545455\n",
      "Epoch 13 of 500 took 0.213s\n",
      "Accuracy source 0.979167, main loss classifier 0.102456, source classification loss 0.070168, loss domain distinction 0.178102, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.02222679 Acc: 0.99545455\n",
      "Epoch 14 of 500 took 0.216s\n",
      "Accuracy source 0.977083, main loss classifier 0.100322, source classification loss 0.065869, loss domain distinction 0.179467, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.02453941 Acc: 0.99545455\n",
      "Epoch 15 of 500 took 0.211s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy source 0.982292, main loss classifier 0.099381, source classification loss 0.064381, loss domain distinction 0.177820, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.04601528 Acc: 0.98181818\n",
      "Epoch 16 of 500 took 0.212s\n",
      "Accuracy source 0.988021, main loss classifier 0.094761, source classification loss 0.055053, loss domain distinction 0.179319, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.03406387 Acc: 0.99090909\n",
      "Epoch 17 of 500 took 0.237s\n",
      "Accuracy source 0.983333, main loss classifier 0.097324, source classification loss 0.059657, loss domain distinction 0.178020, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.03045976 Acc: 0.99545455\n",
      "Training complete in 0m 4s\n",
      "SHAPE SESSIONS:  (3,)\n",
      "()\n",
      "TSD_Network(\n",
      "  (_network): ModuleList(\n",
      "    (0): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=252, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=252, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (1): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (2): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (_output): Linear(in_features=200, out_features=22, bias=True)\n",
      "  (_output_discriminator): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "Number Parameters:  137624\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/TSD/participant_1/best_state_0.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/TSD/participant_1/best_state_0.pt' (epoch 18)\n",
      "STARTING TRAINING\n",
      "Accuracy source 0.968750, main loss classifier 0.145934, source classification loss 0.096416, loss domain distinction 0.752853, accuracy domain distinction 0.484635\n",
      "VALIDATION Loss: 0.07817271 Acc: 0.97272727\n",
      "New best validation loss:  0.0781727060675621\n",
      "Epoch 1 of 500 took 0.217s\n",
      "Accuracy source 0.968750, main loss classifier 0.126168, source classification loss 0.106785, loss domain distinction 0.298937, accuracy domain distinction 0.501563\n",
      "VALIDATION Loss: 0.07198079 Acc: 0.97272727\n",
      "New best validation loss:  0.0719807893037796\n",
      "Epoch 2 of 500 took 0.214s\n",
      "Accuracy source 0.972917, main loss classifier 0.119413, source classification loss 0.089500, loss domain distinction 0.223961, accuracy domain distinction 0.496875\n",
      "VALIDATION Loss: 0.11257518 Acc: 0.95\n",
      "Epoch 3 of 500 took 0.211s\n",
      "Accuracy source 0.975521, main loss classifier 0.117320, source classification loss 0.085188, loss domain distinction 0.201902, accuracy domain distinction 0.498437\n",
      "VALIDATION Loss: 0.06765704 Acc: 0.97727273\n",
      "New best validation loss:  0.06765703856945038\n",
      "Epoch 4 of 500 took 0.213s\n",
      "Accuracy source 0.978125, main loss classifier 0.115712, source classification loss 0.084531, loss domain distinction 0.189726, accuracy domain distinction 0.501302\n",
      "VALIDATION Loss: 0.07132325 Acc: 0.96363636\n",
      "Epoch 5 of 500 took 0.217s\n",
      "Accuracy source 0.978646, main loss classifier 0.114477, source classification loss 0.082300, loss domain distinction 0.191924, accuracy domain distinction 0.499740\n",
      "VALIDATION Loss: 0.11963860 Acc: 0.94090909\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0060e-04.\n",
      "Epoch 6 of 500 took 0.257s\n",
      "Accuracy source 0.977083, main loss classifier 0.114704, source classification loss 0.083737, loss domain distinction 0.191171, accuracy domain distinction 0.498958\n",
      "VALIDATION Loss: 0.09305754 Acc: 0.96363636\n",
      "Epoch 7 of 500 took 0.215s\n",
      "Accuracy source 0.977604, main loss classifier 0.108613, source classification loss 0.070276, loss domain distinction 0.192509, accuracy domain distinction 0.499479\n",
      "VALIDATION Loss: 0.06019668 Acc: 0.97727273\n",
      "New best validation loss:  0.060196682810783386\n",
      "Epoch 8 of 500 took 0.215s\n",
      "Accuracy source 0.979167, main loss classifier 0.108951, source classification loss 0.072222, loss domain distinction 0.188181, accuracy domain distinction 0.499740\n",
      "VALIDATION Loss: 0.04887059 Acc: 0.97727273\n",
      "New best validation loss:  0.04887058585882187\n",
      "Epoch 9 of 500 took 0.238s\n",
      "Accuracy source 0.976562, main loss classifier 0.111394, source classification loss 0.078338, loss domain distinction 0.188682, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.07412094 Acc: 0.96818182\n",
      "Epoch 10 of 500 took 0.220s\n",
      "Accuracy source 0.983333, main loss classifier 0.104957, source classification loss 0.066540, loss domain distinction 0.185779, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.07796872 Acc: 0.96818182\n",
      "Epoch 11 of 500 took 0.211s\n",
      "Accuracy source 0.981250, main loss classifier 0.105552, source classification loss 0.068059, loss domain distinction 0.186964, accuracy domain distinction 0.500260\n",
      "VALIDATION Loss: 0.08302291 Acc: 0.96363636\n",
      "Epoch 12 of 500 took 0.214s\n",
      "Accuracy source 0.982812, main loss classifier 0.108479, source classification loss 0.071168, loss domain distinction 0.191324, accuracy domain distinction 0.498958\n",
      "VALIDATION Loss: 0.07107323 Acc: 0.97272727\n",
      "Epoch 13 of 500 took 0.212s\n",
      "Accuracy source 0.988021, main loss classifier 0.100849, source classification loss 0.057291, loss domain distinction 0.187309, accuracy domain distinction 0.499479\n",
      "VALIDATION Loss: 0.08336080 Acc: 0.97272727\n",
      "Epoch 14 of 500 took 0.217s\n",
      "Accuracy source 0.981771, main loss classifier 0.105888, source classification loss 0.066153, loss domain distinction 0.192142, accuracy domain distinction 0.498958\n",
      "VALIDATION Loss: 0.10154591 Acc: 0.95909091\n",
      "Epoch 15 of 500 took 0.212s\n",
      "Accuracy source 0.979688, main loss classifier 0.106708, source classification loss 0.070435, loss domain distinction 0.185593, accuracy domain distinction 0.499479\n",
      "VALIDATION Loss: 0.08637377 Acc: 0.96818182\n",
      "Epoch 16 of 500 took 0.257s\n",
      "Accuracy source 0.981250, main loss classifier 0.104279, source classification loss 0.063529, loss domain distinction 0.188365, accuracy domain distinction 0.499740\n",
      "VALIDATION Loss: 0.09745898 Acc: 0.95909091\n",
      "Epoch 17 of 500 took 0.215s\n",
      "Accuracy source 0.983333, main loss classifier 0.103884, source classification loss 0.063653, loss domain distinction 0.187431, accuracy domain distinction 0.499479\n",
      "VALIDATION Loss: 0.07858020 Acc: 0.96818182\n",
      "Epoch 18 of 500 took 0.221s\n",
      "Accuracy source 0.990104, main loss classifier 0.100373, source classification loss 0.056896, loss domain distinction 0.186634, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.05849677 Acc: 0.97727273\n",
      "Epoch 19 of 500 took 0.217s\n",
      "Accuracy source 0.986979, main loss classifier 0.100116, source classification loss 0.056862, loss domain distinction 0.188053, accuracy domain distinction 0.500260\n",
      "VALIDATION Loss: 0.08208375 Acc: 0.96818182\n",
      "Training complete in 0m 4s\n",
      "()\n",
      "TSD_Network(\n",
      "  (_network): ModuleList(\n",
      "    (0): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=252, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=252, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (1): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (2): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (_output): Linear(in_features=200, out_features=22, bias=True)\n",
      "  (_output_discriminator): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "Number Parameters:  137624\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/TSD/participant_1/best_state_0.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/TSD/participant_1/best_state_0.pt' (epoch 18)\n",
      "STARTING TRAINING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy source 0.965625, main loss classifier 0.155689, source classification loss 0.117083, loss domain distinction 0.745969, accuracy domain distinction 0.491146\n",
      "VALIDATION Loss: 0.06633518 Acc: 0.98181818\n",
      "New best validation loss:  0.06633517891168594\n",
      "Epoch 1 of 500 took 0.261s\n",
      "Accuracy source 0.973958, main loss classifier 0.122521, source classification loss 0.097172, loss domain distinction 0.302482, accuracy domain distinction 0.489063\n",
      "VALIDATION Loss: 0.07927524 Acc: 0.97727273\n",
      "Epoch 2 of 500 took 0.218s\n",
      "Accuracy source 0.978125, main loss classifier 0.114792, source classification loss 0.082411, loss domain distinction 0.218465, accuracy domain distinction 0.496875\n",
      "VALIDATION Loss: 0.07688402 Acc: 0.98181818\n",
      "Epoch 3 of 500 took 0.218s\n",
      "Accuracy source 0.972917, main loss classifier 0.120137, source classification loss 0.090315, loss domain distinction 0.201345, accuracy domain distinction 0.496615\n",
      "VALIDATION Loss: 0.11392841 Acc: 0.96818182\n",
      "Epoch 4 of 500 took 0.211s\n",
      "Accuracy source 0.973958, main loss classifier 0.118391, source classification loss 0.087187, loss domain distinction 0.193016, accuracy domain distinction 0.498177\n",
      "VALIDATION Loss: 0.08240018 Acc: 0.97727273\n",
      "Epoch 5 of 500 took 0.214s\n",
      "Accuracy source 0.978125, main loss classifier 0.114471, source classification loss 0.083129, loss domain distinction 0.190783, accuracy domain distinction 0.498177\n",
      "VALIDATION Loss: 0.10382754 Acc: 0.96818182\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0060e-04.\n",
      "Epoch 6 of 500 took 0.213s\n",
      "Accuracy source 0.979688, main loss classifier 0.108693, source classification loss 0.072028, loss domain distinction 0.192027, accuracy domain distinction 0.500260\n",
      "VALIDATION Loss: 0.10704390 Acc: 0.96363636\n",
      "Epoch 7 of 500 took 0.212s\n",
      "Accuracy source 0.985417, main loss classifier 0.104360, source classification loss 0.064724, loss domain distinction 0.188616, accuracy domain distinction 0.499740\n",
      "VALIDATION Loss: 0.07884768 Acc: 0.97727273\n",
      "Epoch 8 of 500 took 0.223s\n",
      "Accuracy source 0.981250, main loss classifier 0.111826, source classification loss 0.077518, loss domain distinction 0.194568, accuracy domain distinction 0.499479\n",
      "VALIDATION Loss: 0.09573386 Acc: 0.96818182\n",
      "Epoch 9 of 500 took 0.214s\n",
      "Accuracy source 0.984896, main loss classifier 0.103611, source classification loss 0.061745, loss domain distinction 0.188835, accuracy domain distinction 0.500521\n",
      "VALIDATION Loss: 0.09206622 Acc: 0.97272727\n",
      "Epoch 10 of 500 took 0.212s\n",
      "Accuracy source 0.983333, main loss classifier 0.105997, source classification loss 0.067649, loss domain distinction 0.186814, accuracy domain distinction 0.500260\n",
      "VALIDATION Loss: 0.10270771 Acc: 0.96818182\n",
      "Epoch 11 of 500 took 0.214s\n",
      "Accuracy source 0.981771, main loss classifier 0.107049, source classification loss 0.071709, loss domain distinction 0.184477, accuracy domain distinction 0.499740\n",
      "VALIDATION Loss: 0.09909178 Acc: 0.97727273\n",
      "Training complete in 0m 3s\n",
      "SHAPE SESSIONS:  (3,)\n",
      "()\n",
      "TSD_Network(\n",
      "  (_network): ModuleList(\n",
      "    (0): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=252, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=252, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (1): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (2): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (_output): Linear(in_features=200, out_features=22, bias=True)\n",
      "  (_output_discriminator): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "Number Parameters:  137624\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/TSD/participant_2/best_state_0.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/TSD/participant_2/best_state_0.pt' (epoch 25)\n",
      "STARTING TRAINING\n",
      "Accuracy source 0.951562, main loss classifier 0.158463, source classification loss 0.158715, loss domain distinction 0.462821, accuracy domain distinction 0.484375\n",
      "VALIDATION Loss: 0.10475621 Acc: 0.94545455\n",
      "New best validation loss:  0.10475621372461319\n",
      "Epoch 1 of 500 took 0.215s\n",
      "Accuracy source 0.959375, main loss classifier 0.135201, source classification loss 0.124366, loss domain distinction 0.233940, accuracy domain distinction 0.495833\n",
      "VALIDATION Loss: 0.05598566 Acc: 0.98636364\n",
      "New best validation loss:  0.05598566308617592\n",
      "Epoch 2 of 500 took 0.215s\n",
      "Accuracy source 0.957812, main loss classifier 0.136605, source classification loss 0.126258, loss domain distinction 0.198900, accuracy domain distinction 0.498958\n",
      "VALIDATION Loss: 0.06958733 Acc: 0.96818182\n",
      "Epoch 3 of 500 took 0.215s\n",
      "Accuracy source 0.964063, main loss classifier 0.133965, source classification loss 0.121745, loss domain distinction 0.191516, accuracy domain distinction 0.499740\n",
      "VALIDATION Loss: 0.05632230 Acc: 0.98181818\n",
      "Epoch 4 of 500 took 0.212s\n",
      "Accuracy source 0.963021, main loss classifier 0.129329, source classification loss 0.111839, loss domain distinction 0.191204, accuracy domain distinction 0.499740\n",
      "VALIDATION Loss: 0.06435584 Acc: 0.97727273\n",
      "Epoch 5 of 500 took 0.220s\n",
      "Accuracy source 0.965625, main loss classifier 0.126354, source classification loss 0.107096, loss domain distinction 0.188736, accuracy domain distinction 0.499479\n",
      "VALIDATION Loss: 0.04752622 Acc: 0.97727273\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0060e-04.\n",
      "New best validation loss:  0.04752621799707413\n",
      "Epoch 6 of 500 took 0.258s\n",
      "Accuracy source 0.967708, main loss classifier 0.124614, source classification loss 0.107117, loss domain distinction 0.185165, accuracy domain distinction 0.499479\n",
      "VALIDATION Loss: 0.11197113 Acc: 0.95\n",
      "Epoch 7 of 500 took 0.258s\n",
      "Accuracy source 0.962500, main loss classifier 0.127706, source classification loss 0.112727, loss domain distinction 0.188030, accuracy domain distinction 0.500260\n",
      "VALIDATION Loss: 0.06127644 Acc: 0.99090909\n",
      "Epoch 8 of 500 took 0.255s\n",
      "Accuracy source 0.966146, main loss classifier 0.127792, source classification loss 0.111952, loss domain distinction 0.188331, accuracy domain distinction 0.500260\n",
      "VALIDATION Loss: 0.04804280 Acc: 0.99090909\n",
      "Epoch 9 of 500 took 0.217s\n",
      "Accuracy source 0.964583, main loss classifier 0.126023, source classification loss 0.109945, loss domain distinction 0.184818, accuracy domain distinction 0.499740\n",
      "VALIDATION Loss: 0.04793797 Acc: 0.98181818\n",
      "Epoch 10 of 500 took 0.214s\n",
      "Accuracy source 0.973437, main loss classifier 0.117942, source classification loss 0.091794, loss domain distinction 0.185357, accuracy domain distinction 0.499479\n",
      "VALIDATION Loss: 0.10389306 Acc: 0.95454545\n",
      "Epoch 11 of 500 took 0.216s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy source 0.973958, main loss classifier 0.118170, source classification loss 0.095085, loss domain distinction 0.182789, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.05778011 Acc: 0.97727273\n",
      "Epoch 12 of 500 took 0.214s\n",
      "Accuracy source 0.968229, main loss classifier 0.118598, source classification loss 0.095209, loss domain distinction 0.182607, accuracy domain distinction 0.499740\n",
      "VALIDATION Loss: 0.05029126 Acc: 0.99090909\n",
      "Epoch 13 of 500 took 0.215s\n",
      "Accuracy source 0.964063, main loss classifier 0.122090, source classification loss 0.101076, loss domain distinction 0.184983, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.04493857 Acc: 0.98636364\n",
      "New best validation loss:  0.04493856802582741\n",
      "Epoch 14 of 500 took 0.219s\n",
      "Accuracy source 0.966146, main loss classifier 0.122908, source classification loss 0.103195, loss domain distinction 0.186132, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.11403615 Acc: 0.95454545\n",
      "Epoch 15 of 500 took 0.212s\n",
      "Accuracy source 0.970313, main loss classifier 0.116033, source classification loss 0.092033, loss domain distinction 0.182330, accuracy domain distinction 0.500260\n",
      "VALIDATION Loss: 0.08586241 Acc: 0.96818182\n",
      "Epoch 16 of 500 took 0.214s\n",
      "Accuracy source 0.971875, main loss classifier 0.117513, source classification loss 0.091736, loss domain distinction 0.187100, accuracy domain distinction 0.499740\n",
      "VALIDATION Loss: 0.05449842 Acc: 0.97727273\n",
      "Epoch 17 of 500 took 0.213s\n",
      "Accuracy source 0.971875, main loss classifier 0.122490, source classification loss 0.103292, loss domain distinction 0.186559, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.04935836 Acc: 0.97727273\n",
      "Epoch 18 of 500 took 0.219s\n",
      "Accuracy source 0.977083, main loss classifier 0.116272, source classification loss 0.090960, loss domain distinction 0.184701, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.04948340 Acc: 0.98181818\n",
      "Epoch 19 of 500 took 0.215s\n",
      "Accuracy source 0.971354, main loss classifier 0.116352, source classification loss 0.091545, loss domain distinction 0.183570, accuracy domain distinction 0.500260\n",
      "VALIDATION Loss: 0.07195855 Acc: 0.96818182\n",
      "Epoch 20 of 500 took 0.213s\n",
      "Accuracy source 0.965625, main loss classifier 0.124270, source classification loss 0.106024, loss domain distinction 0.188452, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.04685977 Acc: 0.98181818\n",
      "Epoch 21 of 500 took 0.213s\n",
      "Accuracy source 0.967187, main loss classifier 0.119201, source classification loss 0.097711, loss domain distinction 0.184855, accuracy domain distinction 0.499219\n",
      "VALIDATION Loss: 0.06052736 Acc: 0.98181818\n",
      "Epoch 22 of 500 took 0.216s\n",
      "Accuracy source 0.975000, main loss classifier 0.115880, source classification loss 0.089652, loss domain distinction 0.185273, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.06228361 Acc: 0.97272727\n",
      "Epoch 23 of 500 took 0.225s\n",
      "Accuracy source 0.972396, main loss classifier 0.120840, source classification loss 0.100458, loss domain distinction 0.183898, accuracy domain distinction 0.500260\n",
      "VALIDATION Loss: 0.03812242 Acc: 0.99090909\n",
      "New best validation loss:  0.03812241554260254\n",
      "Epoch 24 of 500 took 0.214s\n",
      "Accuracy source 0.978646, main loss classifier 0.116686, source classification loss 0.091505, loss domain distinction 0.183984, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.05675437 Acc: 0.97727273\n",
      "Epoch 25 of 500 took 0.213s\n",
      "Accuracy source 0.974479, main loss classifier 0.115476, source classification loss 0.090507, loss domain distinction 0.183622, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.09597807 Acc: 0.95454545\n",
      "Epoch 26 of 500 took 0.211s\n",
      "Accuracy source 0.977083, main loss classifier 0.113378, source classification loss 0.085995, loss domain distinction 0.184265, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.04724995 Acc: 0.98181818\n",
      "Epoch 27 of 500 took 0.213s\n",
      "Accuracy source 0.971354, main loss classifier 0.117999, source classification loss 0.095428, loss domain distinction 0.184637, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.10412716 Acc: 0.95\n",
      "Epoch 28 of 500 took 0.218s\n",
      "Accuracy source 0.973958, main loss classifier 0.113525, source classification loss 0.087142, loss domain distinction 0.181730, accuracy domain distinction 0.500260\n",
      "VALIDATION Loss: 0.07294775 Acc: 0.96818182\n",
      "Epoch 29 of 500 took 0.212s\n",
      "Accuracy source 0.967187, main loss classifier 0.118508, source classification loss 0.097392, loss domain distinction 0.183772, accuracy domain distinction 0.499740\n",
      "VALIDATION Loss: 0.04591359 Acc: 0.97727273\n",
      "Epoch 30 of 500 took 0.215s\n",
      "Accuracy source 0.975000, main loss classifier 0.110972, source classification loss 0.081995, loss domain distinction 0.183378, accuracy domain distinction 0.499740\n",
      "VALIDATION Loss: 0.11282469 Acc: 0.96363636\n",
      "Epoch 31 of 500 took 0.212s\n",
      "Accuracy source 0.969792, main loss classifier 0.119421, source classification loss 0.099586, loss domain distinction 0.182061, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.07122269 Acc: 0.97272727\n",
      "Epoch 32 of 500 took 0.214s\n",
      "Accuracy source 0.978646, main loss classifier 0.109245, source classification loss 0.078243, loss domain distinction 0.183429, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.04914459 Acc: 0.97727273\n",
      "Epoch 33 of 500 took 0.226s\n",
      "Accuracy source 0.978646, main loss classifier 0.108759, source classification loss 0.078109, loss domain distinction 0.183003, accuracy domain distinction 0.499740\n",
      "VALIDATION Loss: 0.04935995 Acc: 0.98181818\n",
      "Epoch 34 of 500 took 0.211s\n",
      "Accuracy source 0.977604, main loss classifier 0.114049, source classification loss 0.087390, loss domain distinction 0.184154, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.07061255 Acc: 0.96818182\n",
      "Training complete in 0m 8s\n",
      "()\n",
      "TSD_Network(\n",
      "  (_network): ModuleList(\n",
      "    (0): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=252, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=252, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (1): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (2): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (_output): Linear(in_features=200, out_features=22, bias=True)\n",
      "  (_output_discriminator): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "Number Parameters:  137624\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/TSD/participant_2/best_state_0.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/TSD/participant_2/best_state_0.pt' (epoch 25)\n",
      "STARTING TRAINING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy source 0.945833, main loss classifier 0.155813, source classification loss 0.152956, loss domain distinction 0.459136, accuracy domain distinction 0.493229\n",
      "VALIDATION Loss: 0.22686878 Acc: 0.92272727\n",
      "New best validation loss:  0.22686877846717834\n",
      "Epoch 1 of 500 took 0.216s\n",
      "Accuracy source 0.957292, main loss classifier 0.138160, source classification loss 0.131113, loss domain distinction 0.232445, accuracy domain distinction 0.499479\n",
      "VALIDATION Loss: 0.15271020 Acc: 0.94090909\n",
      "New best validation loss:  0.1527101993560791\n",
      "Epoch 2 of 500 took 0.216s\n",
      "Accuracy source 0.957812, main loss classifier 0.136775, source classification loss 0.127756, loss domain distinction 0.201560, accuracy domain distinction 0.500781\n",
      "VALIDATION Loss: 0.13526992 Acc: 0.93181818\n",
      "New best validation loss:  0.1352699249982834\n",
      "Epoch 3 of 500 took 0.214s\n",
      "Accuracy source 0.952604, main loss classifier 0.143637, source classification loss 0.141133, loss domain distinction 0.190865, accuracy domain distinction 0.498958\n",
      "VALIDATION Loss: 0.19759177 Acc: 0.91818182\n",
      "Epoch 4 of 500 took 0.211s\n",
      "Accuracy source 0.961458, main loss classifier 0.131224, source classification loss 0.118360, loss domain distinction 0.186546, accuracy domain distinction 0.500521\n",
      "VALIDATION Loss: 0.17615496 Acc: 0.92272727\n",
      "Epoch 5 of 500 took 0.211s\n",
      "Accuracy source 0.966667, main loss classifier 0.131231, source classification loss 0.118694, loss domain distinction 0.187097, accuracy domain distinction 0.499479\n",
      "VALIDATION Loss: 0.19396926 Acc: 0.93181818\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0060e-04.\n",
      "Epoch 6 of 500 took 0.212s\n",
      "Accuracy source 0.977604, main loss classifier 0.116813, source classification loss 0.091344, loss domain distinction 0.187703, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.11957750 Acc: 0.94090909\n",
      "New best validation loss:  0.11957750469446182\n",
      "Epoch 7 of 500 took 0.269s\n",
      "Accuracy source 0.973437, main loss classifier 0.117320, source classification loss 0.091974, loss domain distinction 0.189268, accuracy domain distinction 0.499740\n",
      "VALIDATION Loss: 0.16258641 Acc: 0.94090909\n",
      "Epoch 8 of 500 took 0.215s\n",
      "Accuracy source 0.965104, main loss classifier 0.124752, source classification loss 0.106986, loss domain distinction 0.187231, accuracy domain distinction 0.499740\n",
      "VALIDATION Loss: 0.21817714 Acc: 0.91363636\n",
      "Epoch 9 of 500 took 0.214s\n",
      "Accuracy source 0.974479, main loss classifier 0.120814, source classification loss 0.099152, loss domain distinction 0.186167, accuracy domain distinction 0.498958\n",
      "VALIDATION Loss: 0.07496431 Acc: 0.98181818\n",
      "New best validation loss:  0.07496430724859238\n",
      "Epoch 10 of 500 took 0.219s\n",
      "Accuracy source 0.976562, main loss classifier 0.116458, source classification loss 0.090184, loss domain distinction 0.183823, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.08892754 Acc: 0.96363636\n",
      "Epoch 11 of 500 took 0.221s\n",
      "Accuracy source 0.965104, main loss classifier 0.126820, source classification loss 0.110412, loss domain distinction 0.187362, accuracy domain distinction 0.500260\n",
      "VALIDATION Loss: 0.11475397 Acc: 0.94545455\n",
      "Epoch 12 of 500 took 0.214s\n",
      "Accuracy source 0.979688, main loss classifier 0.113436, source classification loss 0.085101, loss domain distinction 0.185384, accuracy domain distinction 0.501042\n",
      "VALIDATION Loss: 0.12659450 Acc: 0.95\n",
      "Epoch 13 of 500 took 0.214s\n",
      "Accuracy source 0.976042, main loss classifier 0.112441, source classification loss 0.083406, loss domain distinction 0.182260, accuracy domain distinction 0.499740\n",
      "VALIDATION Loss: 0.10777975 Acc: 0.95909091\n",
      "Epoch 14 of 500 took 0.214s\n",
      "Accuracy source 0.971875, main loss classifier 0.117067, source classification loss 0.091670, loss domain distinction 0.187740, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.18124336 Acc: 0.91818182\n",
      "Epoch 15 of 500 took 0.215s\n",
      "Accuracy source 0.970313, main loss classifier 0.119638, source classification loss 0.096872, loss domain distinction 0.185580, accuracy domain distinction 0.499740\n",
      "VALIDATION Loss: 0.14718889 Acc: 0.94090909\n",
      "Epoch 16 of 500 took 0.217s\n",
      "Accuracy source 0.975521, main loss classifier 0.115318, source classification loss 0.087826, loss domain distinction 0.185811, accuracy domain distinction 0.500000\n",
      "VALIDATION Loss: 0.10438957 Acc: 0.95909091\n",
      "Epoch 17 of 500 took 0.214s\n",
      "Accuracy source 0.969271, main loss classifier 0.116884, source classification loss 0.091545, loss domain distinction 0.185273, accuracy domain distinction 0.500260\n",
      "VALIDATION Loss: 0.10532290 Acc: 0.95909091\n",
      "Epoch 18 of 500 took 0.214s\n",
      "Accuracy source 0.968229, main loss classifier 0.120733, source classification loss 0.099996, loss domain distinction 0.184041, accuracy domain distinction 0.500260\n",
      "VALIDATION Loss: 0.10174390 Acc: 0.95454545\n",
      "Epoch 19 of 500 took 0.213s\n",
      "Accuracy source 0.977604, main loss classifier 0.116317, source classification loss 0.091361, loss domain distinction 0.185031, accuracy domain distinction 0.499740\n",
      "VALIDATION Loss: 0.12744631 Acc: 0.94545455\n",
      "Epoch 20 of 500 took 0.215s\n",
      "Accuracy source 0.968229, main loss classifier 0.118131, source classification loss 0.094887, loss domain distinction 0.185014, accuracy domain distinction 0.500260\n",
      "VALIDATION Loss: 0.12828353 Acc: 0.95\n",
      "Training complete in 0m 5s\n"
     ]
    }
   ],
   "source": [
    "train_DANN(examples_datasets_train, labels_datasets_train, \n",
    "                      num_kernels=num_kernels,\n",
    "                      path_weights_fine_tuning=path_weights_standard_DNN,\n",
    "                      number_of_classes=number_of_classes,\n",
    "                      number_of_cycle_for_first_training=number_of_cycle_for_first_training,\n",
    "                      number_of_cycles_rest_of_training=number_of_cycles_rest_of_training,\n",
    "                      batch_size=batch_size,\n",
    "                      feature_vector_input_length=feature_vector_input_length,\n",
    "                      path_weights_to_save_to=path_weights_DANN, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET one participant_examples  (3, 40, 550, 252)\n",
      "   GET one training_index_examples  (40, 550, 252)  at  0\n",
      "   GOT one group XY  (2200, 252)    (2200,)\n",
      "       one group XY test  (550, 252)    (550, 252)\n",
      "       one group XY train (1980, 252)    (1980,)\n",
      "       one group XY valid (220, 252)    (220, 252)\n",
      "   GET one training_index_examples  (40, 550, 252)  at  1\n",
      "   GOT one group XY  (2200, 252)    (2200,)\n",
      "       one group XY test  (550, 252)    (550, 252)\n",
      "       one group XY train (1980, 252)    (1980,)\n",
      "       one group XY valid (220, 252)    (220, 252)\n",
      "   GET one training_index_examples  (40, 550, 252)  at  2\n",
      "   GOT one group XY  (2200, 252)    (2200,)\n",
      "       one group XY test  (550, 252)    (550, 252)\n",
      "       one group XY train (1980, 252)    (1980,)\n",
      "       one group XY valid (220, 252)    (220, 252)\n",
      "dataloaders: \n",
      "   train  (1, 3)\n",
      "   valid  (1, 3)\n",
      "   test  (1, 3)\n",
      "GET one participant_examples  (3, 40, 550, 252)\n",
      "   GET one training_index_examples  (40, 550, 252)  at  0\n",
      "   GOT one group XY  (2200, 252)    (2200,)\n",
      "       one group XY test  (550, 252)    (550, 252)\n",
      "       one group XY train (1980, 252)    (1980,)\n",
      "       one group XY valid (220, 252)    (220, 252)\n",
      "   GET one training_index_examples  (40, 550, 252)  at  1\n",
      "   GOT one group XY  (2200, 252)    (2200,)\n",
      "       one group XY test  (550, 252)    (550, 252)\n",
      "       one group XY train (1980, 252)    (1980,)\n",
      "       one group XY valid (220, 252)    (220, 252)\n",
      "   GET one training_index_examples  (40, 550, 252)  at  2\n",
      "   GOT one group XY  (2200, 252)    (2200,)\n",
      "       one group XY test  (550, 252)    (550, 252)\n",
      "       one group XY train (1980, 252)    (1980,)\n",
      "       one group XY valid (220, 252)    (220, 252)\n",
      "dataloaders: \n",
      "   train  (2, 3)\n",
      "   valid  (2, 3)\n",
      "   test  (2, 3)\n",
      "GET one participant_examples  (3, 40, 550, 252)\n",
      "   GET one training_index_examples  (40, 550, 252)  at  0\n",
      "   GOT one group XY  (2200, 252)    (2200,)\n",
      "       one group XY test  (550, 252)    (550, 252)\n",
      "       one group XY train (1980, 252)    (1980,)\n",
      "       one group XY valid (220, 252)    (220, 252)\n",
      "   GET one training_index_examples  (40, 550, 252)  at  1\n",
      "   GOT one group XY  (2200, 252)    (2200,)\n",
      "       one group XY test  (550, 252)    (550, 252)\n",
      "       one group XY train (1980, 252)    (1980,)\n",
      "       one group XY valid (220, 252)    (220, 252)\n",
      "   GET one training_index_examples  (40, 550, 252)  at  2\n",
      "   GOT one group XY  (2200, 252)    (2200,)\n",
      "       one group XY test  (550, 252)    (550, 252)\n",
      "       one group XY train (1980, 252)    (1980,)\n",
      "       one group XY valid (220, 252)    (220, 252)\n",
      "dataloaders: \n",
      "   train  (3, 3)\n",
      "   valid  (3, 3)\n",
      "   test  (3, 3)\n",
      "TSD_Network(\n",
      "  (_network): ModuleList(\n",
      "    (0): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=252, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=252, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (1): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (2): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (_output): Linear(in_features=200, out_features=22, bias=True)\n",
      "  (_output_discriminator): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "Number Parameters:  137624\n",
      "(3,)\n",
      "Participant ID:  0  Session ID:  0  Accuracy:  0.9963636363636363\n",
      "Participant ID:  0  Session ID:  1  Accuracy:  0.5654545454545454\n",
      "Participant ID:  0  Session ID:  2  Accuracy:  0.5509090909090909\n",
      "ACCURACY PARTICIPANT:  [0.9963636363636363, 0.5654545454545454, 0.5509090909090909]\n",
      "TSD_Network(\n",
      "  (_network): ModuleList(\n",
      "    (0): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=252, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=252, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (1): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (2): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (_output): Linear(in_features=200, out_features=22, bias=True)\n",
      "  (_output_discriminator): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "Number Parameters:  137624\n",
      "(3,)\n",
      "Participant ID:  1  Session ID:  0  Accuracy:  0.9745454545454545\n",
      "Participant ID:  1  Session ID:  1  Accuracy:  0.8963636363636364\n",
      "Participant ID:  1  Session ID:  2  Accuracy:  0.7509090909090909\n",
      "ACCURACY PARTICIPANT:  [0.9745454545454545, 0.8963636363636364, 0.7509090909090909]\n",
      "TSD_Network(\n",
      "  (_network): ModuleList(\n",
      "    (0): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=252, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=252, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (1): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (2): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (_output): Linear(in_features=200, out_features=22, bias=True)\n",
      "  (_output_discriminator): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "Number Parameters:  137624\n",
      "(3,)\n",
      "Participant ID:  2  Session ID:  0  Accuracy:  0.9963636363636363\n",
      "Participant ID:  2  Session ID:  1  Accuracy:  0.6927272727272727\n",
      "Participant ID:  2  Session ID:  2  Accuracy:  0.56\n",
      "ACCURACY PARTICIPANT:  [0.9963636363636363, 0.6927272727272727, 0.56]\n",
      "[[0.99636364 0.56545455 0.55090909]\n",
      " [0.97454545 0.89636364 0.75090909]\n",
      " [0.99636364 0.69272727 0.56      ]]\n",
      "[array([0.99636364, 0.56545455, 0.55090909]), array([0.97454545, 0.89636364, 0.75090909]), array([0.99636364, 0.69272727, 0.56      ])]\n",
      "OVERALL ACCURACY: 0.7759595959595958\n"
     ]
    }
   ],
   "source": [
    "save_path = \"/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/results_tsd\"\n",
    "algo_name = \"DANN\"\n",
    "test_DANN_on_training_sessions(examples_datasets_train, labels_datasets_train,\n",
    "                              feature_vector_input_length=feature_vector_input_length,\n",
    "                              num_neurons=num_kernels, path_weights_DA=path_weights_DANN,\n",
    "                              algo_name=algo_name, save_path = save_path,\n",
    "                              path_weights_normal=path_weights_standard_DNN, number_of_classes=number_of_classes,\n",
    "                              cycle_for_test=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant_0</th>\n",
       "      <th>Participant_1</th>\n",
       "      <th>Participant_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Session_0</th>\n",
       "      <td>0.996364</td>\n",
       "      <td>0.974545</td>\n",
       "      <td>0.996364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Session_1</th>\n",
       "      <td>0.565455</td>\n",
       "      <td>0.896364</td>\n",
       "      <td>0.692727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Session_2</th>\n",
       "      <td>0.550909</td>\n",
       "      <td>0.750909</td>\n",
       "      <td>0.56</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Participant_0 Participant_1 Participant_2\n",
       "Session_0      0.996364      0.974545      0.996364\n",
       "Session_1      0.565455      0.896364      0.692727\n",
       "Session_2      0.550909      0.750909          0.56"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_filename = save_path + '/predictions_' + algo_name + \".npy\"\n",
    "results = np.load(results_filename, allow_pickle=True)\n",
    "DANN_acc = results[0]\n",
    "DANN_acc_overall = np.mean(DANN_acc)\n",
    "DANN_df = pd.DataFrame(DANN_acc.transpose(), \n",
    "                       index = [f'Session_{i}' for i in range(DANN_acc.shape[1])],\n",
    "                        columns = [f'Participant_{j}' for j in range(DANN_acc.shape[0])])\n",
    "DANN_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "DANN_df.transpose().plot.bar(rot=0, figsize=(10,5))\n",
    "plt.title(\"DANN Accuracies\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. SCADANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TrainingsAndEvaluations.ForTrainingSessions.train_tsd_dnn_SCADANN import run_SCADANN_training_sessions, test_network_SLADANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_kernels=[200, 200, 200]                        # model layer size \n",
    "number_of_cycle_for_first_training=40               # #session\n",
    "number_of_cycles_rest_of_training=40     \n",
    "number_of_classes=22\n",
    "batch_size=128          \n",
    "feature_vector_input_length=252                     # size of one example \n",
    "learning_rate=0.002515\n",
    "percentage_same_gesture_stable = 0.75 \n",
    "path_weights_SCADANN =\"/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/SCADANN\"\n",
    "path_weight_to_save_to = path_weights_SCADANN\n",
    "path_weights_DANN =\"/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/DANN\"\n",
    "path_weights_start_with = path_weights_DANN\n",
    "path_weights_TSD =\"/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/TSD\"\n",
    "path_weights_Normal_training = path_weights_TSD\n",
    "algo_name = \"SCADANN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET one participant_examples  (3, 40, 550, 252)\n",
      "   GET one training_index_examples  (40, 550, 252)  at  0\n",
      "   GOT one group XY  (2200, 252)    (2200,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2200, 252)    (2200,)\n",
      "   GET one training_index_examples  (40, 550, 252)  at  1\n",
      "   GOT one group XY  (2200, 252)    (2200,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2200, 252)    (2200,)\n",
      "   GET one training_index_examples  (40, 550, 252)  at  2\n",
      "   GOT one group XY  (2200, 252)    (2200,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2200, 252)    (2200,)\n",
      "dataloaders: \n",
      "   train  (1, 3)\n",
      "   valid  (0,)\n",
      "   test  (1, 0)\n",
      "GET one participant_examples  (3, 40, 550, 252)\n",
      "   GET one training_index_examples  (40, 550, 252)  at  0\n",
      "   GOT one group XY  (2200, 252)    (2200,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2200, 252)    (2200,)\n",
      "   GET one training_index_examples  (40, 550, 252)  at  1\n",
      "   GOT one group XY  (2200, 252)    (2200,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2200, 252)    (2200,)\n",
      "   GET one training_index_examples  (40, 550, 252)  at  2\n",
      "   GOT one group XY  (2200, 252)    (2200,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2200, 252)    (2200,)\n",
      "dataloaders: \n",
      "   train  (2, 3)\n",
      "   valid  (0,)\n",
      "   test  (2, 0)\n",
      "GET one participant_examples  (3, 40, 550, 252)\n",
      "   GET one training_index_examples  (40, 550, 252)  at  0\n",
      "   GOT one group XY  (2200, 252)    (2200,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2200, 252)    (2200,)\n",
      "   GET one training_index_examples  (40, 550, 252)  at  1\n",
      "   GOT one group XY  (2200, 252)    (2200,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2200, 252)    (2200,)\n",
      "   GET one training_index_examples  (40, 550, 252)  at  2\n",
      "   GOT one group XY  (2200, 252)    (2200,)\n",
      "       one group XY test  (0,)    (0,)\n",
      "       one group XY train (2200, 252)    (2200,)\n",
      "dataloaders: \n",
      "   train  (3, 3)\n",
      "   valid  (0,)\n",
      "   test  (3, 0)\n",
      "participants_train =  3\n",
      "TSD_Network(\n",
      "  (_network): ModuleList(\n",
      "    (0): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=252, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=252, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (1): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (2): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (_output): Linear(in_features=200, out_features=22, bias=True)\n",
      "  (_output_discriminator): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "Number Parameters:  137624\n",
      "Optimizer =  <generator object Module.parameters at 0x7fb10a706ba0>\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/DANN/participant_0/best_state_1.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/DANN/participant_0/best_state_1.pt' (epoch 9)\n",
      "TSD_Network(\n",
      "  (_network): ModuleList(\n",
      "    (0): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=252, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=252, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (1): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (2): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (_output): Linear(in_features=200, out_features=22, bias=True)\n",
      "  (_output_discriminator): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "Number Parameters:  137624\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/TSD/participant_0/best_state_0.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/TSD/participant_0/best_state_0.pt' (epoch 16)\n",
      "TSD_Network(\n",
      "  (_network): ModuleList(\n",
      "    (0): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=252, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=252, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (1): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (2): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (_output): Linear(in_features=200, out_features=22, bias=True)\n",
      "  (_output_discriminator): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "Number Parameters:  137624\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/DANN/participant_0/best_state_1.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/DANN/participant_0/best_state_1.pt' (epoch 9)\n",
      "==== models_array =  (2,)  @ session  1\n",
      "HANDLING NEW SESSION  0\n",
      "HANDLING NEW SESSION  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.76   AFTER:  1.0  len before:  25   len after:  6\n",
      "BEFORE:  0.96   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.4   AFTER:  0.4  len before:  25   len after:  25\n",
      "BEFORE:  0.2   AFTER:  0.0  len before:  25   len after:  17\n",
      "BEFORE:  0.24   AFTER:  0.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.24   AFTER:  0.44  len before:  25   len after:  25\n",
      "BEFORE:  0.92   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  25   len after:  25\n",
      "BEFORE:  0.44   AFTER:  0.44  len before:  25   len after:  25\n",
      "BEFORE:  0.72   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.56   AFTER:  1.0  len before:  25   len after:  3\n",
      "BEFORE:  0.28   AFTER:  0.56  len before:  25   len after:  25\n",
      "BEFORE:  0.84   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.68   AFTER:  0.6  len before:  25   len after:  25\n",
      "BEFORE:  0.32   AFTER:  0.32  len before:  25   len after:  25\n",
      "BEFORE:  0.6   AFTER:  1.0  len before:  25   len after:  1\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.64   AFTER:  0.6  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  25   len after:  25\n",
      "BEFORE:  0.88   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.88   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.28   AFTER:  0.0  len before:  25   len after:  12\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  25   len after:  25\n",
      "BEFORE:  0.8   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.64   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.84   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.36   AFTER:  0.36  len before:  25   len after:  25\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  25   len after:  25\n",
      "BEFORE:  0.12   AFTER:  0.0  len before:  25   len after:  25\n",
      "BEFORE:  0.16   AFTER:  0.0  len before:  25   len after:  8\n",
      "BEFORE:  0.44   AFTER:  0.6  len before:  25   len after:  25\n",
      "BEFORE:  0.64   AFTER:  0.6  len before:  25   len after:  25\n",
      "BEFORE:  0.2   AFTER:  0.2  len before:  25   len after:  25\n",
      "BEFORE:  0.32   AFTER:  0.0  len before:  25   len after:  10\n",
      "BEFORE:  0.96   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.4   AFTER:  0.8888888888888888  len before:  25   len after:  9\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  25   len after:  25\n",
      "BEFORE:  0.88   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.4   AFTER:  1.0  len before:  25   len after:  2\n",
      "BEFORE:  0.28   AFTER:  0.0  len before:  25   len after:  10\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.24   AFTER:  0.0  len before:  25   len after:  10\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  25   len after:  25\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  25   len after:  25\n",
      "BEFORE:  0.56   AFTER:  0.0  len before:  25   len after:  15\n",
      "BEFORE:  0.4   AFTER:  0.52  len before:  25   len after:  25\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  25   len after:  25\n",
      "BEFORE:  0.88   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.24   AFTER:  0.16  len before:  25   len after:  25\n",
      "BEFORE:  0.36   AFTER:  0.0  len before:  25   len after:  6\n",
      "BEFORE:  0.32   AFTER:  0.0  len before:  25   len after:  25\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  25   len after:  25\n",
      "BEFORE:  0.96   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.48   AFTER:  1.0  len before:  25   len after:  3\n",
      "BEFORE:  0.72   AFTER:  1.0  len before:  25   len after:  17\n",
      "BEFORE:  0.84   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.04   AFTER:  0.0  len before:  25   len after:  16\n",
      "BEFORE:  0.36   AFTER:  0.0  len before:  25   len after:  11\n",
      "BEFORE:  0.88   AFTER:  1.0  len before:  25   len after:  15\n",
      "BEFORE:  0.8   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  25   len after:  15\n",
      "BEFORE:  0.2   AFTER:  0.56  len before:  25   len after:  25\n",
      "BEFORE:  0.52   AFTER:  0.0  len before:  25   len after:  11\n",
      "BEFORE:  0.24   AFTER:  0.44  len before:  25   len after:  25\n",
      "BEFORE:  0.04   AFTER:  0.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.56   AFTER:  0.56  len before:  25   len after:  25\n",
      "BEFORE:  0.24   AFTER:  0.7142857142857143  len before:  25   len after:  7\n",
      "BEFORE:  0.56   AFTER:  0.52  len before:  25   len after:  25\n",
      "BEFORE:  0.04   AFTER:  0.0  len before:  25   len after:  9\n",
      "BEFORE:  0.92   AFTER:  1.0  len before:  25   len after:  25\n",
      "ACCURACY MODEL:  0.5645454545454546   Accuracy pseudo: 0.6210413311862587  len pseudo:  1863    len predictions 2200\n",
      "STARTING TRAINING\n",
      "Accuracy total 0.868886, main loss classifier 0.558694, source accuracy 0.966033 source classification loss 0.114230, target accuracy 0.771739 target loss 0.787620 accuracy domain distinction 0.500000 loss domain distinction 1.077691,\n",
      "VALIDATION Loss: 0.31882201 Acc: 0.89544236\n",
      "New best validation loss:  0.3188220088680585\n",
      "Epoch 2 of 500 took 0.306s\n",
      "Accuracy total 0.887568, main loss classifier 0.484469, source accuracy 0.956522 source classification loss 0.132579, target accuracy 0.818614 target loss 0.626090 accuracy domain distinction 0.500000 loss domain distinction 1.051349,\n",
      "VALIDATION Loss: 0.39369399 Acc: 0.86863271\n",
      "Epoch 3 of 500 took 0.304s\n",
      "Accuracy total 0.896399, main loss classifier 0.448511, source accuracy 0.959239 source classification loss 0.126485, target accuracy 0.833560 target loss 0.560745 accuracy domain distinction 0.500000 loss domain distinction 1.048963,\n",
      "VALIDATION Loss: 0.26910499 Acc: 0.91689008\n",
      "New best validation loss:  0.2691049898664157\n",
      "Epoch 4 of 500 took 0.314s\n",
      "Accuracy total 0.906250, main loss classifier 0.405576, source accuracy 0.967391 source classification loss 0.111025, target accuracy 0.845109 target loss 0.490437 accuracy domain distinction 0.500000 loss domain distinction 1.048451,\n",
      "VALIDATION Loss: 0.26229791 Acc: 0.90884718\n",
      "New best validation loss:  0.2622979059815407\n",
      "Epoch 5 of 500 took 0.305s\n",
      "Accuracy total 0.906250, main loss classifier 0.383428, source accuracy 0.968750 source classification loss 0.113766, target accuracy 0.843750 target loss 0.442863 accuracy domain distinction 0.500000 loss domain distinction 1.051137,\n",
      "VALIDATION Loss: 0.28189031 Acc: 0.91689008\n",
      "Epoch 6 of 500 took 0.302s\n",
      "Accuracy total 0.918478, main loss classifier 0.360696, source accuracy 0.970109 source classification loss 0.101111, target accuracy 0.866848 target loss 0.410921 accuracy domain distinction 0.500000 loss domain distinction 1.046800,\n",
      "VALIDATION Loss: 0.21760840 Acc: 0.93297587\n",
      "New best validation loss:  0.2176083984474341\n",
      "Epoch 7 of 500 took 0.308s\n",
      "Accuracy total 0.930367, main loss classifier 0.341894, source accuracy 0.978261 source classification loss 0.088184, target accuracy 0.882473 target loss 0.387258 accuracy domain distinction 0.500000 loss domain distinction 1.041727,\n",
      "VALIDATION Loss: 0.23645768 Acc: 0.91152815\n",
      "Epoch 8 of 500 took 0.306s\n",
      "Accuracy total 0.918139, main loss classifier 0.359154, source accuracy 0.963315 source classification loss 0.103214, target accuracy 0.872962 target loss 0.406468 accuracy domain distinction 0.500000 loss domain distinction 1.043129,\n",
      "VALIDATION Loss: 0.17692492 Acc: 0.94101877\n",
      "New best validation loss:  0.17692491908868155\n",
      "Epoch 9 of 500 took 0.303s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy total 0.931726, main loss classifier 0.329933, source accuracy 0.978940 source classification loss 0.079149, target accuracy 0.884511 target loss 0.371053 accuracy domain distinction 0.500000 loss domain distinction 1.048322,\n",
      "VALIDATION Loss: 0.30001787 Acc: 0.89008043\n",
      "Epoch 10 of 500 took 0.303s\n",
      "Accuracy total 0.929008, main loss classifier 0.332596, source accuracy 0.969429 source classification loss 0.092397, target accuracy 0.888587 target loss 0.364989 accuracy domain distinction 0.500000 loss domain distinction 1.039031,\n",
      "VALIDATION Loss: 0.21787400 Acc: 0.93029491\n",
      "Epoch 11 of 500 took 0.301s\n",
      "Accuracy total 0.930027, main loss classifier 0.323317, source accuracy 0.974185 source classification loss 0.085984, target accuracy 0.885870 target loss 0.352147 accuracy domain distinction 0.500000 loss domain distinction 1.042517,\n",
      "VALIDATION Loss: 0.20190769 Acc: 0.93565684\n",
      "Epoch 12 of 500 took 0.302s\n",
      "Accuracy total 0.929348, main loss classifier 0.331878, source accuracy 0.980299 source classification loss 0.071228, target accuracy 0.878397 target loss 0.384825 accuracy domain distinction 0.500000 loss domain distinction 1.038513,\n",
      "VALIDATION Loss: 0.20028391 Acc: 0.93297587\n",
      "Epoch 13 of 500 took 0.301s\n",
      "Accuracy total 0.921875, main loss classifier 0.340349, source accuracy 0.967391 source classification loss 0.094700, target accuracy 0.876359 target loss 0.377338 accuracy domain distinction 0.500000 loss domain distinction 1.043298,\n",
      "VALIDATION Loss: 0.20853250 Acc: 0.91957105\n",
      "Epoch 14 of 500 took 0.301s\n",
      "Accuracy total 0.929688, main loss classifier 0.325243, source accuracy 0.972826 source classification loss 0.088247, target accuracy 0.886549 target loss 0.354352 accuracy domain distinction 0.500000 loss domain distinction 1.039435,\n",
      "VALIDATION Loss: 0.17801508 Acc: 0.93297587\n",
      "Epoch    14: reducing learning rate of group 0 to 1.0060e-04.\n",
      "Epoch 15 of 500 took 0.304s\n",
      "Accuracy total 0.936141, main loss classifier 0.299390, source accuracy 0.975543 source classification loss 0.087061, target accuracy 0.896739 target loss 0.306236 accuracy domain distinction 0.500000 loss domain distinction 1.027415,\n",
      "VALIDATION Loss: 0.17558079 Acc: 0.94101877\n",
      "New best validation loss:  0.17558079212903976\n",
      "Epoch 16 of 500 took 0.303s\n",
      "Accuracy total 0.938179, main loss classifier 0.284198, source accuracy 0.969429 source classification loss 0.096658, target accuracy 0.906929 target loss 0.264150 accuracy domain distinction 0.500000 loss domain distinction 1.037945,\n",
      "VALIDATION Loss: 0.18586384 Acc: 0.93297587\n",
      "Epoch 17 of 500 took 0.301s\n",
      "Accuracy total 0.937840, main loss classifier 0.299971, source accuracy 0.970788 source classification loss 0.099545, target accuracy 0.904891 target loss 0.292987 accuracy domain distinction 0.500000 loss domain distinction 1.037053,\n",
      "VALIDATION Loss: 0.19735809 Acc: 0.93029491\n",
      "Epoch 18 of 500 took 0.303s\n",
      "Accuracy total 0.939538, main loss classifier 0.284624, source accuracy 0.970788 source classification loss 0.079712, target accuracy 0.908288 target loss 0.283084 accuracy domain distinction 0.500000 loss domain distinction 1.032258,\n",
      "VALIDATION Loss: 0.21374459 Acc: 0.93297587\n",
      "Epoch 19 of 500 took 0.301s\n",
      "Accuracy total 0.936821, main loss classifier 0.294332, source accuracy 0.976902 source classification loss 0.069752, target accuracy 0.896739 target loss 0.311939 accuracy domain distinction 0.500000 loss domain distinction 1.034869,\n",
      "VALIDATION Loss: 0.17733352 Acc: 0.9383378\n",
      "Epoch 20 of 500 took 0.300s\n",
      "Accuracy total 0.938179, main loss classifier 0.284433, source accuracy 0.987092 source classification loss 0.058800, target accuracy 0.889266 target loss 0.304229 accuracy domain distinction 0.500000 loss domain distinction 1.029181,\n",
      "VALIDATION Loss: 0.16792345 Acc: 0.94369973\n",
      "New best validation loss:  0.1679234504699707\n",
      "Epoch 21 of 500 took 0.307s\n",
      "Accuracy total 0.937160, main loss classifier 0.297660, source accuracy 0.975543 source classification loss 0.082904, target accuracy 0.898777 target loss 0.306869 accuracy domain distinction 0.500000 loss domain distinction 1.027731,\n",
      "VALIDATION Loss: 0.23152450 Acc: 0.91420912\n",
      "Epoch 22 of 500 took 0.304s\n",
      "Accuracy total 0.938179, main loss classifier 0.290486, source accuracy 0.982337 source classification loss 0.067754, target accuracy 0.894022 target loss 0.307987 accuracy domain distinction 0.500000 loss domain distinction 1.026153,\n",
      "VALIDATION Loss: 0.15728194 Acc: 0.95442359\n",
      "New best validation loss:  0.1572819414238135\n",
      "Epoch 23 of 500 took 0.304s\n",
      "Accuracy total 0.944293, main loss classifier 0.283001, source accuracy 0.982337 source classification loss 0.066055, target accuracy 0.906250 target loss 0.294290 accuracy domain distinction 0.500000 loss domain distinction 1.028283,\n",
      "VALIDATION Loss: 0.21066167 Acc: 0.92761394\n",
      "Epoch 24 of 500 took 0.307s\n",
      "Accuracy total 0.948030, main loss classifier 0.270791, source accuracy 0.983696 source classification loss 0.067398, target accuracy 0.912364 target loss 0.267582 accuracy domain distinction 0.500000 loss domain distinction 1.033011,\n",
      "VALIDATION Loss: 0.18639762 Acc: 0.9383378\n",
      "Epoch 25 of 500 took 0.303s\n",
      "Accuracy total 0.946332, main loss classifier 0.262471, source accuracy 0.985734 source classification loss 0.061315, target accuracy 0.906929 target loss 0.256796 accuracy domain distinction 0.500000 loss domain distinction 1.034152,\n",
      "VALIDATION Loss: 0.15400686 Acc: 0.94906166\n",
      "New best validation loss:  0.15400686487555504\n",
      "Epoch 26 of 500 took 0.302s\n",
      "Accuracy total 0.947351, main loss classifier 0.280647, source accuracy 0.983016 source classification loss 0.068599, target accuracy 0.911685 target loss 0.286653 accuracy domain distinction 0.500000 loss domain distinction 1.030213,\n",
      "VALIDATION Loss: 0.20319832 Acc: 0.93565684\n",
      "Epoch 27 of 500 took 0.303s\n",
      "Accuracy total 0.950747, main loss classifier 0.263042, source accuracy 0.989810 source classification loss 0.056329, target accuracy 0.911685 target loss 0.264434 accuracy domain distinction 0.500000 loss domain distinction 1.026609,\n",
      "VALIDATION Loss: 0.18186490 Acc: 0.94369973\n",
      "Epoch 28 of 500 took 0.304s\n",
      "Accuracy total 0.945312, main loss classifier 0.268063, source accuracy 0.984375 source classification loss 0.058950, target accuracy 0.906250 target loss 0.270167 accuracy domain distinction 0.500000 loss domain distinction 1.035043,\n",
      "VALIDATION Loss: 0.18084173 Acc: 0.9383378\n",
      "Epoch 29 of 500 took 0.302s\n",
      "Accuracy total 0.945652, main loss classifier 0.281208, source accuracy 0.981658 source classification loss 0.074709, target accuracy 0.909647 target loss 0.281315 accuracy domain distinction 0.500000 loss domain distinction 1.031953,\n",
      "VALIDATION Loss: 0.22092290 Acc: 0.92761394\n",
      "Epoch 30 of 500 took 0.301s\n",
      "Accuracy total 0.949049, main loss classifier 0.278603, source accuracy 0.986413 source classification loss 0.056910, target accuracy 0.911685 target loss 0.293801 accuracy domain distinction 0.500000 loss domain distinction 1.032481,\n",
      "VALIDATION Loss: 0.16439377 Acc: 0.95710456\n",
      "Epoch 31 of 500 took 0.302s\n",
      "Accuracy total 0.957880, main loss classifier 0.244937, source accuracy 0.992527 source classification loss 0.048554, target accuracy 0.923234 target loss 0.236206 accuracy domain distinction 0.500000 loss domain distinction 1.025570,\n",
      "VALIDATION Loss: 0.17887296 Acc: 0.94906166\n",
      "Epoch    31: reducing learning rate of group 0 to 2.0120e-05.\n",
      "Epoch 32 of 500 took 0.304s\n",
      "Accuracy total 0.950747, main loss classifier 0.269266, source accuracy 0.980978 source classification loss 0.069157, target accuracy 0.920516 target loss 0.264963 accuracy domain distinction 0.500000 loss domain distinction 1.022059,\n",
      "VALIDATION Loss: 0.15184986 Acc: 0.95710456\n",
      "New best validation loss:  0.15184985970457396\n",
      "Epoch 33 of 500 took 0.303s\n",
      "Accuracy total 0.953804, main loss classifier 0.250972, source accuracy 0.983016 source classification loss 0.059059, target accuracy 0.924592 target loss 0.237740 accuracy domain distinction 0.500000 loss domain distinction 1.025721,\n",
      "VALIDATION Loss: 0.15414799 Acc: 0.9463807\n",
      "Epoch 34 of 500 took 0.303s\n",
      "Accuracy total 0.951766, main loss classifier 0.250465, source accuracy 0.984375 source classification loss 0.059972, target accuracy 0.919158 target loss 0.235755 accuracy domain distinction 0.500000 loss domain distinction 1.026010,\n",
      "VALIDATION Loss: 0.14913074 Acc: 0.95442359\n",
      "New best validation loss:  0.14913074051340422\n",
      "Epoch 35 of 500 took 0.327s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy total 0.957201, main loss classifier 0.246717, source accuracy 0.985734 source classification loss 0.055297, target accuracy 0.928668 target loss 0.232004 accuracy domain distinction 0.500000 loss domain distinction 1.030661,\n",
      "VALIDATION Loss: 0.13835669 Acc: 0.95710456\n",
      "New best validation loss:  0.13835668501754603\n",
      "Epoch 36 of 500 took 0.304s\n",
      "Accuracy total 0.949049, main loss classifier 0.257777, source accuracy 0.980299 source classification loss 0.065718, target accuracy 0.917799 target loss 0.244676 accuracy domain distinction 0.500000 loss domain distinction 1.025796,\n",
      "VALIDATION Loss: 0.16292043 Acc: 0.9463807\n",
      "Epoch 37 of 500 took 0.305s\n",
      "Accuracy total 0.950747, main loss classifier 0.255634, source accuracy 0.987772 source classification loss 0.052835, target accuracy 0.913723 target loss 0.253952 accuracy domain distinction 0.500000 loss domain distinction 1.022400,\n",
      "VALIDATION Loss: 0.17122754 Acc: 0.94101877\n",
      "Epoch 38 of 500 took 0.303s\n",
      "Accuracy total 0.952785, main loss classifier 0.255098, source accuracy 0.980978 source classification loss 0.067607, target accuracy 0.924592 target loss 0.237421 accuracy domain distinction 0.500000 loss domain distinction 1.025843,\n",
      "VALIDATION Loss: 0.16044356 Acc: 0.95174263\n",
      "Epoch 39 of 500 took 0.302s\n",
      "Accuracy total 0.948030, main loss classifier 0.262744, source accuracy 0.985734 source classification loss 0.057299, target accuracy 0.910326 target loss 0.262935 accuracy domain distinction 0.500000 loss domain distinction 1.026273,\n",
      "VALIDATION Loss: 0.15305890 Acc: 0.95442359\n",
      "Epoch 40 of 500 took 0.304s\n",
      "Accuracy total 0.952446, main loss classifier 0.257146, source accuracy 0.985054 source classification loss 0.058771, target accuracy 0.919837 target loss 0.249969 accuracy domain distinction 0.500000 loss domain distinction 1.027754,\n",
      "VALIDATION Loss: 0.20334387 Acc: 0.9383378\n",
      "Epoch 41 of 500 took 0.301s\n",
      "Accuracy total 0.952106, main loss classifier 0.252650, source accuracy 0.987772 source classification loss 0.050458, target accuracy 0.916440 target loss 0.249894 accuracy domain distinction 0.500000 loss domain distinction 1.024743,\n",
      "VALIDATION Loss: 0.16647010 Acc: 0.94369973\n",
      "Epoch    41: reducing learning rate of group 0 to 4.0240e-06.\n",
      "Epoch 42 of 500 took 0.305s\n",
      "Accuracy total 0.951087, main loss classifier 0.254281, source accuracy 0.983696 source classification loss 0.064134, target accuracy 0.918478 target loss 0.238916 accuracy domain distinction 0.500000 loss domain distinction 1.027559,\n",
      "VALIDATION Loss: 0.16175824 Acc: 0.95442359\n",
      "Epoch 43 of 500 took 0.300s\n",
      "Accuracy total 0.953465, main loss classifier 0.257149, source accuracy 0.983016 source classification loss 0.069344, target accuracy 0.923913 target loss 0.239646 accuracy domain distinction 0.500000 loss domain distinction 1.026539,\n",
      "VALIDATION Loss: 0.15403644 Acc: 0.94101877\n",
      "Epoch 44 of 500 took 0.300s\n",
      "Accuracy total 0.954144, main loss classifier 0.253403, source accuracy 0.984375 source classification loss 0.074579, target accuracy 0.923913 target loss 0.226466 accuracy domain distinction 0.500000 loss domain distinction 1.028809,\n",
      "VALIDATION Loss: 0.19178649 Acc: 0.93565684\n",
      "Epoch 45 of 500 took 0.303s\n",
      "Accuracy total 0.952106, main loss classifier 0.249607, source accuracy 0.980299 source classification loss 0.066120, target accuracy 0.923913 target loss 0.227766 accuracy domain distinction 0.500000 loss domain distinction 1.026638,\n",
      "VALIDATION Loss: 0.15835249 Acc: 0.9463807\n",
      "Epoch 46 of 500 took 0.300s\n",
      "Accuracy total 0.949389, main loss classifier 0.272825, source accuracy 0.985734 source classification loss 0.063553, target accuracy 0.913043 target loss 0.276894 accuracy domain distinction 0.500000 loss domain distinction 1.026016,\n",
      "VALIDATION Loss: 0.17545994 Acc: 0.94101877\n",
      "Epoch 47 of 500 took 0.301s\n",
      "Training complete in 0m 14s\n",
      "['participant_1', 'participant_2', 'participant_0']\n",
      "TSD_Network(\n",
      "  (_network): ModuleList(\n",
      "    (0): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=252, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=252, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (1): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (2): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (_output): Linear(in_features=200, out_features=22, bias=True)\n",
      "  (_output_discriminator): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "Number Parameters:  137624\n",
      "Optimizer =  <generator object Module.parameters at 0x7fb108d9cf90>\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/DANN/participant_0/best_state_2.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/DANN/participant_0/best_state_2.pt' (epoch 7)\n",
      "TSD_Network(\n",
      "  (_network): ModuleList(\n",
      "    (0): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=252, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=252, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (1): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (2): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (_output): Linear(in_features=200, out_features=22, bias=True)\n",
      "  (_output_discriminator): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "Number Parameters:  137624\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/TSD/participant_0/best_state_0.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/TSD/participant_0/best_state_0.pt' (epoch 16)\n",
      "TSD_Network(\n",
      "  (_network): ModuleList(\n",
      "    (0): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=252, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=252, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (1): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (2): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (_output): Linear(in_features=200, out_features=22, bias=True)\n",
      "  (_output_discriminator): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "Number Parameters:  137624\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/DANN/participant_0/best_state_1.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/DANN/participant_0/best_state_1.pt' (epoch 9)\n",
      "TSD_Network(\n",
      "  (_network): ModuleList(\n",
      "    (0): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=252, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=252, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (1): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (2): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (_output): Linear(in_features=200, out_features=22, bias=True)\n",
      "  (_output_discriminator): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "Number Parameters:  137624\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/DANN/participant_0/best_state_2.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/DANN/participant_0/best_state_2.pt' (epoch 7)\n",
      "==== models_array =  (3,)  @ session  2\n",
      "HANDLING NEW SESSION  0\n",
      "HANDLING NEW SESSION  1\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.76   AFTER:  1.0  len before:  25   len after:  6\n",
      "BEFORE:  0.96   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.4   AFTER:  0.4  len before:  25   len after:  25\n",
      "BEFORE:  0.2   AFTER:  0.0  len before:  25   len after:  17\n",
      "BEFORE:  0.24   AFTER:  0.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.24   AFTER:  0.44  len before:  25   len after:  25\n",
      "BEFORE:  0.92   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  25   len after:  25\n",
      "BEFORE:  0.44   AFTER:  0.44  len before:  25   len after:  25\n",
      "BEFORE:  0.72   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.56   AFTER:  1.0  len before:  25   len after:  3\n",
      "BEFORE:  0.28   AFTER:  0.56  len before:  25   len after:  25\n",
      "BEFORE:  0.84   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.68   AFTER:  0.6  len before:  25   len after:  25\n",
      "BEFORE:  0.32   AFTER:  0.32  len before:  25   len after:  25\n",
      "BEFORE:  0.6   AFTER:  1.0  len before:  25   len after:  1\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.64   AFTER:  0.6  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  25   len after:  25\n",
      "BEFORE:  0.88   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.88   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.28   AFTER:  0.0  len before:  25   len after:  12\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  25   len after:  25\n",
      "BEFORE:  0.8   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.64   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.84   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.36   AFTER:  0.36  len before:  25   len after:  25\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  25   len after:  25\n",
      "BEFORE:  0.12   AFTER:  0.0  len before:  25   len after:  25\n",
      "BEFORE:  0.16   AFTER:  0.0  len before:  25   len after:  8\n",
      "BEFORE:  0.44   AFTER:  0.6  len before:  25   len after:  25\n",
      "BEFORE:  0.64   AFTER:  0.6  len before:  25   len after:  25\n",
      "BEFORE:  0.2   AFTER:  0.2  len before:  25   len after:  25\n",
      "BEFORE:  0.32   AFTER:  0.0  len before:  25   len after:  10\n",
      "BEFORE:  0.96   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.4   AFTER:  0.8888888888888888  len before:  25   len after:  9\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  25   len after:  25\n",
      "BEFORE:  0.88   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.4   AFTER:  1.0  len before:  25   len after:  2\n",
      "BEFORE:  0.28   AFTER:  0.0  len before:  25   len after:  10\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.24   AFTER:  0.0  len before:  25   len after:  10\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  25   len after:  25\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  25   len after:  25\n",
      "BEFORE:  0.56   AFTER:  0.0  len before:  25   len after:  15\n",
      "BEFORE:  0.4   AFTER:  0.52  len before:  25   len after:  25\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  25   len after:  25\n",
      "BEFORE:  0.88   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.24   AFTER:  0.16  len before:  25   len after:  25\n",
      "BEFORE:  0.36   AFTER:  0.0  len before:  25   len after:  6\n",
      "BEFORE:  0.32   AFTER:  0.0  len before:  25   len after:  25\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  25   len after:  25\n",
      "BEFORE:  0.96   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.48   AFTER:  1.0  len before:  25   len after:  3\n",
      "BEFORE:  0.72   AFTER:  1.0  len before:  25   len after:  17\n",
      "BEFORE:  0.84   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.04   AFTER:  0.0  len before:  25   len after:  16\n",
      "BEFORE:  0.36   AFTER:  0.0  len before:  25   len after:  11\n",
      "BEFORE:  0.88   AFTER:  1.0  len before:  25   len after:  15\n",
      "BEFORE:  0.8   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  25   len after:  15\n",
      "BEFORE:  0.2   AFTER:  0.56  len before:  25   len after:  25\n",
      "BEFORE:  0.52   AFTER:  0.0  len before:  25   len after:  11\n",
      "BEFORE:  0.24   AFTER:  0.44  len before:  25   len after:  25\n",
      "BEFORE:  0.04   AFTER:  0.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.56   AFTER:  0.56  len before:  25   len after:  25\n",
      "BEFORE:  0.24   AFTER:  0.7142857142857143  len before:  25   len after:  7\n",
      "BEFORE:  0.56   AFTER:  0.52  len before:  25   len after:  25\n",
      "BEFORE:  0.04   AFTER:  0.0  len before:  25   len after:  9\n",
      "BEFORE:  0.92   AFTER:  1.0  len before:  25   len after:  25\n",
      "ACCURACY MODEL:  0.5645454545454546   Accuracy pseudo: 0.6210413311862587  len pseudo:  1863    len predictions 2200\n",
      "HANDLING NEW SESSION  2\n",
      "Finish segment dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.88   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.96   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.6   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.92   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.48   AFTER:  0.52  len before:  25   len after:  25\n",
      "BEFORE:  0.12   AFTER:  0.0  len before:  25   len after:  25\n",
      "BEFORE:  0.48   AFTER:  0.24  len before:  25   len after:  25\n",
      "BEFORE:  0.6   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.16   AFTER:  0.0  len before:  25   len after:  1\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  25   len after:  7\n",
      "BEFORE:  0.92   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.76   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  25   len after:  25\n",
      "BEFORE:  0.28   AFTER:  1.0  len before:  25   len after:  10\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.04   AFTER:  nan  len before:  25   len after:  0\n",
      "BEFORE:  0.52   AFTER:  1.0  len before:  25   len after:  10\n",
      "BEFORE:  0.44   AFTER:  1.0  len before:  25   len after:  1\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.72   AFTER:  0.56  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.2   AFTER:  0.0  len before:  25   len after:  25\n",
      "BEFORE:  0.8   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.56   AFTER:  0.8  len before:  25   len after:  25\n",
      "BEFORE:  0.16   AFTER:  0.0  len before:  25   len after:  3\n",
      "BEFORE:  0.4   AFTER:  0.36  len before:  25   len after:  25\n",
      "BEFORE:  0.32   AFTER:  0.0  len before:  25   len after:  15\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.12   AFTER:  0.0  len before:  25   len after:  25\n",
      "BEFORE:  0.2   AFTER:  0.0  len before:  25   len after:  16\n",
      "BEFORE:  0.36   AFTER:  0.52  len before:  25   len after:  25\n",
      "BEFORE:  0.84   AFTER:  1.0  len before:  25   len after:  16\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  25   len after:  5\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.96   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  25   len after:  6\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.96   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.4   AFTER:  0.24  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.24   AFTER:  0.0  len before:  25   len after:  25\n",
      "BEFORE:  0.96   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.4   AFTER:  nan  len before:  25   len after:  0\n",
      "BEFORE:  0.2   AFTER:  0.0  len before:  25   len after:  1\n",
      "BEFORE:  0.56   AFTER:  0.44  len before:  25   len after:  25\n",
      "BEFORE:  0.32   AFTER:  1.0  len before:  25   len after:  6\n",
      "BEFORE:  0.92   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.0   AFTER:  nan  len before:  25   len after:  0\n",
      "BEFORE:  0.04   AFTER:  0.0  len before:  25   len after:  25\n",
      "BEFORE:  0.4   AFTER:  0.4  len before:  25   len after:  25\n",
      "BEFORE:  0.32   AFTER:  1.0  len before:  25   len after:  2\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  25   len after:  2\n",
      "BEFORE:  0.04   AFTER:  0.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  25   len after:  4\n",
      "BEFORE:  0.88   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.88   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.44   AFTER:  1.0  len before:  25   len after:  7\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.4   AFTER:  1.0  len before:  25   len after:  15\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.28   AFTER:  0.0  len before:  25   len after:  14\n",
      "BEFORE:  0.12   AFTER:  0.0  len before:  25   len after:  17\n",
      "BEFORE:  0.96   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.36   AFTER:  0.4  len before:  25   len after:  25\n",
      "BEFORE:  0.84   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  25   len after:  9\n",
      "BEFORE:  0.24   AFTER:  1.0  len before:  25   len after:  12\n",
      "BEFORE:  0.36   AFTER:  0.4  len before:  25   len after:  25\n",
      "BEFORE:  0.28   AFTER:  0.0  len before:  25   len after:  25\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  25   len after:  9\n",
      "BEFORE:  0.08   AFTER:  0.0  len before:  25   len after:  14\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  25   len after:  10\n",
      "BEFORE:  0.92   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.84   AFTER:  1.0  len before:  25   len after:  25\n",
      "ACCURACY MODEL:  0.5618181818181818   Accuracy pseudo: 0.701518691588785  len pseudo:  1712    len predictions 2200\n",
      "STARTING TRAINING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laiy/.local/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/laiy/.local/lib/python3.8/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy total 0.822917, main loss classifier 0.757164, source accuracy 0.861607 source classification loss 0.496487, target accuracy 0.784226 target loss 0.801350 accuracy domain distinction 0.500000 loss domain distinction 1.082458,\n",
      "VALIDATION Loss: 0.33678210 Acc: 0.90670554\n",
      "New best validation loss:  0.336782102783521\n",
      "Epoch 2 of 500 took 0.331s\n",
      "Accuracy total 0.835193, main loss classifier 0.631437, source accuracy 0.864583 source classification loss 0.439209, target accuracy 0.805804 target loss 0.612495 accuracy domain distinction 0.500000 loss domain distinction 1.055850,\n",
      "VALIDATION Loss: 0.26353565 Acc: 0.90379009\n",
      "New best validation loss:  0.26353564858436584\n",
      "Epoch 3 of 500 took 0.279s\n",
      "Accuracy total 0.850074, main loss classifier 0.579084, source accuracy 0.872768 source classification loss 0.376787, target accuracy 0.827381 target loss 0.572096 accuracy domain distinction 0.500000 loss domain distinction 1.046429,\n",
      "VALIDATION Loss: 0.29285687 Acc: 0.90087464\n",
      "Epoch 4 of 500 took 0.277s\n",
      "Accuracy total 0.860863, main loss classifier 0.536850, source accuracy 0.895833 source classification loss 0.311331, target accuracy 0.825893 target loss 0.552524 accuracy domain distinction 0.500000 loss domain distinction 1.049223,\n",
      "VALIDATION Loss: 0.25383593 Acc: 0.91253644\n",
      "New best validation loss:  0.2538359264532725\n",
      "Epoch 5 of 500 took 0.281s\n",
      "Accuracy total 0.850074, main loss classifier 0.563108, source accuracy 0.875000 source classification loss 0.388130, target accuracy 0.825149 target loss 0.529749 accuracy domain distinction 0.500000 loss domain distinction 1.041685,\n",
      "VALIDATION Loss: 0.31310651 Acc: 0.90087464\n",
      "Epoch 6 of 500 took 0.276s\n",
      "Accuracy total 0.871280, main loss classifier 0.499764, source accuracy 0.898065 source classification loss 0.319969, target accuracy 0.844494 target loss 0.468362 accuracy domain distinction 0.500000 loss domain distinction 1.055980,\n",
      "VALIDATION Loss: 0.22877047 Acc: 0.9271137\n",
      "New best validation loss:  0.22877046962579092\n",
      "Epoch 7 of 500 took 0.283s\n",
      "Accuracy total 0.875000, main loss classifier 0.491511, source accuracy 0.895089 source classification loss 0.299969, target accuracy 0.854911 target loss 0.473456 accuracy domain distinction 0.500000 loss domain distinction 1.047988,\n",
      "VALIDATION Loss: 0.30402399 Acc: 0.90087464\n",
      "Epoch 8 of 500 took 0.279s\n",
      "Accuracy total 0.877976, main loss classifier 0.478071, source accuracy 0.901786 source classification loss 0.311941, target accuracy 0.854167 target loss 0.434496 accuracy domain distinction 0.500000 loss domain distinction 1.048523,\n",
      "VALIDATION Loss: 0.34168715 Acc: 0.89212828\n",
      "Epoch 9 of 500 took 0.277s\n",
      "Accuracy total 0.875744, main loss classifier 0.489789, source accuracy 0.900298 source classification loss 0.344871, target accuracy 0.851190 target loss 0.426307 accuracy domain distinction 0.500000 loss domain distinction 1.041998,\n",
      "VALIDATION Loss: 0.18859220 Acc: 0.94460641\n",
      "New best validation loss:  0.18859220296144485\n",
      "Epoch 10 of 500 took 0.285s\n",
      "Accuracy total 0.880580, main loss classifier 0.476670, source accuracy 0.900298 source classification loss 0.295742, target accuracy 0.860863 target loss 0.449304 accuracy domain distinction 0.500000 loss domain distinction 1.041470,\n",
      "VALIDATION Loss: 0.22372938 Acc: 0.94752187\n",
      "Epoch 11 of 500 took 0.322s\n",
      "Accuracy total 0.877232, main loss classifier 0.455482, source accuracy 0.909226 source classification loss 0.279695, target accuracy 0.845238 target loss 0.422561 accuracy domain distinction 0.500000 loss domain distinction 1.043536,\n",
      "VALIDATION Loss: 0.22927663 Acc: 0.9212828\n",
      "Epoch 12 of 500 took 0.297s\n",
      "Accuracy total 0.886905, main loss classifier 0.434758, source accuracy 0.904762 source classification loss 0.262533, target accuracy 0.869048 target loss 0.398496 accuracy domain distinction 0.500000 loss domain distinction 1.042432,\n",
      "VALIDATION Loss: 0.22130028 Acc: 0.93294461\n",
      "Epoch 13 of 500 took 0.280s\n",
      "Accuracy total 0.885045, main loss classifier 0.456472, source accuracy 0.920387 source classification loss 0.271375, target accuracy 0.849702 target loss 0.433134 accuracy domain distinction 0.500000 loss domain distinction 1.042169,\n",
      "VALIDATION Loss: 0.20019699 Acc: 0.94460641\n",
      "Epoch 14 of 500 took 0.285s\n",
      "Accuracy total 0.890253, main loss classifier 0.454454, source accuracy 0.919643 source classification loss 0.271969, target accuracy 0.860863 target loss 0.427543 accuracy domain distinction 0.500000 loss domain distinction 1.046983,\n",
      "VALIDATION Loss: 0.19824772 Acc: 0.93002915\n",
      "Epoch 15 of 500 took 0.279s\n",
      "Accuracy total 0.886905, main loss classifier 0.448267, source accuracy 0.894345 source classification loss 0.307167, target accuracy 0.879464 target loss 0.381410 accuracy domain distinction 0.500000 loss domain distinction 1.039783,\n",
      "VALIDATION Loss: 0.25657213 Acc: 0.90087464\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0060e-04.\n",
      "Epoch 16 of 500 took 0.279s\n",
      "Accuracy total 0.898810, main loss classifier 0.419549, source accuracy 0.919643 source classification loss 0.257977, target accuracy 0.877976 target loss 0.374719 accuracy domain distinction 0.500000 loss domain distinction 1.032006,\n",
      "VALIDATION Loss: 0.23925130 Acc: 0.91836735\n",
      "Epoch 17 of 500 took 0.276s\n",
      "Accuracy total 0.899926, main loss classifier 0.405807, source accuracy 0.910714 source classification loss 0.276328, target accuracy 0.889137 target loss 0.329604 accuracy domain distinction 0.500000 loss domain distinction 1.028415,\n",
      "VALIDATION Loss: 0.21188666 Acc: 0.9154519\n",
      "Epoch 18 of 500 took 0.279s\n",
      "Accuracy total 0.899926, main loss classifier 0.406502, source accuracy 0.908482 source classification loss 0.262912, target accuracy 0.891369 target loss 0.342710 accuracy domain distinction 0.500000 loss domain distinction 1.036908,\n",
      "VALIDATION Loss: 0.15424504 Acc: 0.95626822\n",
      "New best validation loss:  0.15424504006902376\n",
      "Epoch 19 of 500 took 0.281s\n",
      "Accuracy total 0.908482, main loss classifier 0.391717, source accuracy 0.927083 source classification loss 0.249699, target accuracy 0.889881 target loss 0.327492 accuracy domain distinction 0.500000 loss domain distinction 1.031218,\n",
      "VALIDATION Loss: 0.40647024 Acc: 0.88046647\n",
      "Epoch 20 of 500 took 0.278s\n",
      "Accuracy total 0.905134, main loss classifier 0.400608, source accuracy 0.909226 source classification loss 0.273071, target accuracy 0.901042 target loss 0.321289 accuracy domain distinction 0.500000 loss domain distinction 1.034283,\n",
      "VALIDATION Loss: 0.20603669 Acc: 0.93586006\n",
      "Epoch 21 of 500 took 0.278s\n",
      "Accuracy total 0.909970, main loss classifier 0.381212, source accuracy 0.926339 source classification loss 0.223689, target accuracy 0.893601 target loss 0.332801 accuracy domain distinction 0.500000 loss domain distinction 1.029667,\n",
      "VALIDATION Loss: 0.18673667 Acc: 0.93586006\n",
      "Epoch 22 of 500 took 0.278s\n",
      "Accuracy total 0.912202, main loss classifier 0.388926, source accuracy 0.930804 source classification loss 0.228382, target accuracy 0.893601 target loss 0.343333 accuracy domain distinction 0.500000 loss domain distinction 1.030690,\n",
      "VALIDATION Loss: 0.16340346 Acc: 0.95335277\n",
      "Epoch 23 of 500 took 0.281s\n",
      "Accuracy total 0.916295, main loss classifier 0.359679, source accuracy 0.935268 source classification loss 0.201170, target accuracy 0.897321 target loss 0.312622 accuracy domain distinction 0.500000 loss domain distinction 1.027831,\n",
      "VALIDATION Loss: 0.19087089 Acc: 0.92419825\n",
      "Epoch 24 of 500 took 0.279s\n",
      "Accuracy total 0.908854, main loss classifier 0.379489, source accuracy 0.918899 source classification loss 0.251847, target accuracy 0.898810 target loss 0.301375 accuracy domain distinction 0.500000 loss domain distinction 1.028782,\n",
      "VALIDATION Loss: 0.14821508 Acc: 0.94752187\n",
      "New best validation loss:  0.1482150765756766\n",
      "Epoch 25 of 500 took 0.285s\n",
      "Accuracy total 0.917783, main loss classifier 0.356891, source accuracy 0.927083 source classification loss 0.225832, target accuracy 0.908482 target loss 0.281822 accuracy domain distinction 0.500000 loss domain distinction 1.030642,\n",
      "VALIDATION Loss: 0.19251973 Acc: 0.93002915\n",
      "Epoch 26 of 500 took 0.280s\n",
      "Accuracy total 0.912574, main loss classifier 0.357850, source accuracy 0.929315 source classification loss 0.220941, target accuracy 0.895833 target loss 0.288867 accuracy domain distinction 0.500000 loss domain distinction 1.029467,\n",
      "VALIDATION Loss: 0.20395661 Acc: 0.9271137\n",
      "Epoch 27 of 500 took 0.280s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy total 0.914062, main loss classifier 0.384759, source accuracy 0.935268 source classification loss 0.235121, target accuracy 0.892857 target loss 0.327926 accuracy domain distinction 0.500000 loss domain distinction 1.032353,\n",
      "VALIDATION Loss: 0.15271799 Acc: 0.95043732\n",
      "Epoch 28 of 500 took 0.284s\n",
      "Accuracy total 0.913690, main loss classifier 0.371400, source accuracy 0.927083 source classification loss 0.221703, target accuracy 0.900298 target loss 0.314886 accuracy domain distinction 0.500000 loss domain distinction 1.031058,\n",
      "VALIDATION Loss: 0.20227192 Acc: 0.93586006\n",
      "Epoch 29 of 500 took 0.277s\n",
      "Accuracy total 0.912574, main loss classifier 0.367236, source accuracy 0.919643 source classification loss 0.235847, target accuracy 0.905506 target loss 0.292307 accuracy domain distinction 0.500000 loss domain distinction 1.031592,\n",
      "VALIDATION Loss: 0.22328232 Acc: 0.9154519\n",
      "Epoch 30 of 500 took 0.280s\n",
      "Accuracy total 0.914807, main loss classifier 0.373799, source accuracy 0.935268 source classification loss 0.234541, target accuracy 0.894345 target loss 0.306564 accuracy domain distinction 0.500000 loss domain distinction 1.032459,\n",
      "VALIDATION Loss: 0.14605424 Acc: 0.95335277\n",
      "New best validation loss:  0.14605424428979555\n",
      "Epoch 31 of 500 took 0.280s\n",
      "Accuracy total 0.917411, main loss classifier 0.372087, source accuracy 0.934524 source classification loss 0.234004, target accuracy 0.900298 target loss 0.302729 accuracy domain distinction 0.500000 loss domain distinction 1.037204,\n",
      "VALIDATION Loss: 0.17825119 Acc: 0.9271137\n",
      "Epoch 32 of 500 took 0.277s\n",
      "Accuracy total 0.915923, main loss classifier 0.363027, source accuracy 0.934524 source classification loss 0.203512, target accuracy 0.897321 target loss 0.316180 accuracy domain distinction 0.500000 loss domain distinction 1.031805,\n",
      "VALIDATION Loss: 0.20430340 Acc: 0.9271137\n",
      "Epoch 33 of 500 took 0.277s\n",
      "Accuracy total 0.909598, main loss classifier 0.372259, source accuracy 0.929315 source classification loss 0.215093, target accuracy 0.889881 target loss 0.323609 accuracy domain distinction 0.500000 loss domain distinction 1.029081,\n",
      "VALIDATION Loss: 0.17142155 Acc: 0.94460641\n",
      "Epoch 34 of 500 took 0.279s\n",
      "Accuracy total 0.915923, main loss classifier 0.370793, source accuracy 0.922619 source classification loss 0.245614, target accuracy 0.909226 target loss 0.290147 accuracy domain distinction 0.500000 loss domain distinction 1.029119,\n",
      "VALIDATION Loss: 0.20226557 Acc: 0.89795918\n",
      "Epoch 35 of 500 took 0.277s\n",
      "Accuracy total 0.917411, main loss classifier 0.368055, source accuracy 0.931548 source classification loss 0.211168, target accuracy 0.903274 target loss 0.318230 accuracy domain distinction 0.500000 loss domain distinction 1.033561,\n",
      "VALIDATION Loss: 0.17933831 Acc: 0.9271137\n",
      "Epoch 36 of 500 took 0.278s\n",
      "Accuracy total 0.913318, main loss classifier 0.368595, source accuracy 0.931548 source classification loss 0.222506, target accuracy 0.895089 target loss 0.309079 accuracy domain distinction 0.500000 loss domain distinction 1.028027,\n",
      "VALIDATION Loss: 0.15262133 Acc: 0.95043732\n",
      "Epoch    36: reducing learning rate of group 0 to 2.0120e-05.\n",
      "Epoch 37 of 500 took 0.288s\n",
      "Accuracy total 0.909970, main loss classifier 0.365451, source accuracy 0.921875 source classification loss 0.228417, target accuracy 0.898065 target loss 0.297411 accuracy domain distinction 0.500000 loss domain distinction 1.025366,\n",
      "VALIDATION Loss: 0.15152483 Acc: 0.95335277\n",
      "Epoch 38 of 500 took 0.321s\n",
      "Accuracy total 0.918527, main loss classifier 0.350149, source accuracy 0.939732 source classification loss 0.195659, target accuracy 0.897321 target loss 0.299760 accuracy domain distinction 0.500000 loss domain distinction 1.024399,\n",
      "VALIDATION Loss: 0.19538262 Acc: 0.93002915\n",
      "Epoch 39 of 500 took 0.321s\n",
      "Accuracy total 0.920759, main loss classifier 0.344998, source accuracy 0.940476 source classification loss 0.199735, target accuracy 0.901042 target loss 0.284671 accuracy domain distinction 0.500000 loss domain distinction 1.027949,\n",
      "VALIDATION Loss: 0.13762860 Acc: 0.94752187\n",
      "New best validation loss:  0.13762859627604485\n",
      "Epoch 40 of 500 took 0.325s\n",
      "Accuracy total 0.921875, main loss classifier 0.341620, source accuracy 0.925595 source classification loss 0.217418, target accuracy 0.918155 target loss 0.260135 accuracy domain distinction 0.500000 loss domain distinction 1.028436,\n",
      "VALIDATION Loss: 0.12565876 Acc: 0.96209913\n",
      "New best validation loss:  0.12565876419345537\n",
      "Epoch 41 of 500 took 0.293s\n",
      "Accuracy total 0.919271, main loss classifier 0.354335, source accuracy 0.936756 source classification loss 0.206053, target accuracy 0.901786 target loss 0.296574 accuracy domain distinction 0.500000 loss domain distinction 1.030217,\n",
      "VALIDATION Loss: 0.11626494 Acc: 0.96501458\n",
      "New best validation loss:  0.1162649430334568\n",
      "Epoch 42 of 500 took 0.284s\n",
      "Accuracy total 0.927827, main loss classifier 0.342349, source accuracy 0.941964 source classification loss 0.202149, target accuracy 0.913690 target loss 0.276706 accuracy domain distinction 0.500000 loss domain distinction 1.029210,\n",
      "VALIDATION Loss: 0.14918987 Acc: 0.95043732\n",
      "Epoch 43 of 500 took 0.278s\n",
      "Accuracy total 0.921503, main loss classifier 0.356020, source accuracy 0.940476 source classification loss 0.210085, target accuracy 0.902530 target loss 0.296349 accuracy domain distinction 0.500000 loss domain distinction 1.028030,\n",
      "VALIDATION Loss: 0.17660398 Acc: 0.93002915\n",
      "Epoch 44 of 500 took 0.282s\n",
      "Accuracy total 0.915179, main loss classifier 0.370122, source accuracy 0.929315 source classification loss 0.234207, target accuracy 0.901042 target loss 0.300704 accuracy domain distinction 0.500000 loss domain distinction 1.026661,\n",
      "VALIDATION Loss: 0.14356337 Acc: 0.95626822\n",
      "Epoch 45 of 500 took 0.284s\n",
      "Accuracy total 0.921131, main loss classifier 0.352727, source accuracy 0.931548 source classification loss 0.216349, target accuracy 0.910714 target loss 0.284073 accuracy domain distinction 0.500000 loss domain distinction 1.025159,\n",
      "VALIDATION Loss: 0.15046823 Acc: 0.95918367\n",
      "Epoch 46 of 500 took 0.278s\n",
      "Accuracy total 0.922247, main loss classifier 0.346625, source accuracy 0.929315 source classification loss 0.214461, target accuracy 0.915179 target loss 0.272198 accuracy domain distinction 0.500000 loss domain distinction 1.032955,\n",
      "VALIDATION Loss: 0.11653816 Acc: 0.96793003\n",
      "Epoch 47 of 500 took 0.278s\n",
      "Accuracy total 0.918899, main loss classifier 0.356700, source accuracy 0.927083 source classification loss 0.231143, target accuracy 0.910714 target loss 0.276317 accuracy domain distinction 0.500000 loss domain distinction 1.029701,\n",
      "VALIDATION Loss: 0.13909183 Acc: 0.95626822\n",
      "Epoch    47: reducing learning rate of group 0 to 4.0240e-06.\n",
      "Epoch 48 of 500 took 0.279s\n",
      "Accuracy total 0.921131, main loss classifier 0.348959, source accuracy 0.936756 source classification loss 0.212614, target accuracy 0.905506 target loss 0.279802 accuracy domain distinction 0.500000 loss domain distinction 1.027505,\n",
      "VALIDATION Loss: 0.17733511 Acc: 0.93586006\n",
      "Epoch 49 of 500 took 0.278s\n",
      "Accuracy total 0.926711, main loss classifier 0.338551, source accuracy 0.941964 source classification loss 0.198422, target accuracy 0.911458 target loss 0.273116 accuracy domain distinction 0.500000 loss domain distinction 1.027829,\n",
      "VALIDATION Loss: 0.13777741 Acc: 0.95335277\n",
      "Epoch 50 of 500 took 0.277s\n",
      "Accuracy total 0.921875, main loss classifier 0.347245, source accuracy 0.941964 source classification loss 0.185322, target accuracy 0.901786 target loss 0.303452 accuracy domain distinction 0.500000 loss domain distinction 1.028580,\n",
      "VALIDATION Loss: 0.13410009 Acc: 0.95043732\n",
      "Epoch 51 of 500 took 0.278s\n",
      "Accuracy total 0.916667, main loss classifier 0.358427, source accuracy 0.925595 source classification loss 0.224445, target accuracy 0.907738 target loss 0.285984 accuracy domain distinction 0.500000 loss domain distinction 1.032132,\n",
      "VALIDATION Loss: 0.15255949 Acc: 0.95043732\n",
      "Epoch 52 of 500 took 0.279s\n",
      "Accuracy total 0.920759, main loss classifier 0.342119, source accuracy 0.933036 source classification loss 0.206213, target accuracy 0.908482 target loss 0.272728 accuracy domain distinction 0.500000 loss domain distinction 1.026488,\n",
      "VALIDATION Loss: 0.13499207 Acc: 0.95918367\n",
      "Epoch 53 of 500 took 0.277s\n",
      "Training complete in 0m 15s\n",
      "['participant_1', 'participant_2', 'participant_0']\n",
      "TSD_Network(\n",
      "  (_network): ModuleList(\n",
      "    (0): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=252, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=252, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (1): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (2): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (_output): Linear(in_features=200, out_features=22, bias=True)\n",
      "  (_output_discriminator): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "Number Parameters:  137624\n",
      "Optimizer =  <generator object Module.parameters at 0x7fb0acd5b890>\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/DANN/participant_1/best_state_1.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/DANN/participant_1/best_state_1.pt' (epoch 9)\n",
      "TSD_Network(\n",
      "  (_network): ModuleList(\n",
      "    (0): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=252, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=252, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (1): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (2): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (_output): Linear(in_features=200, out_features=22, bias=True)\n",
      "  (_output_discriminator): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "Number Parameters:  137624\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/TSD/participant_1/best_state_0.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/TSD/participant_1/best_state_0.pt' (epoch 18)\n",
      "TSD_Network(\n",
      "  (_network): ModuleList(\n",
      "    (0): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=252, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=252, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (1): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (2): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (_output): Linear(in_features=200, out_features=22, bias=True)\n",
      "  (_output_discriminator): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "Number Parameters:  137624\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/DANN/participant_1/best_state_1.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/DANN/participant_1/best_state_1.pt' (epoch 9)\n",
      "==== models_array =  (2,)  @ session  1\n",
      "HANDLING NEW SESSION  0\n",
      "HANDLING NEW SESSION  1\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.96   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.52   AFTER:  0.0  len before:  25   len after:  11\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.96   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.8   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.92   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.84   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.88   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.52   AFTER:  1.0  len before:  25   len after:  10\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.56   AFTER:  0.0  len before:  25   len after:  12\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.96   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.6   AFTER:  1.0  len before:  25   len after:  8\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.92   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.32   AFTER:  0.0  len before:  25   len after:  17\n",
      "BEFORE:  0.84   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.96   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.04   AFTER:  0.0  len before:  25   len after:  9\n",
      "BEFORE:  0.52   AFTER:  1.0  len before:  25   len after:  16\n",
      "BEFORE:  0.96   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.96   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.76   AFTER:  0.96  len before:  25   len after:  25\n",
      "BEFORE:  0.52   AFTER:  0.52  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.56   AFTER:  0.6  len before:  25   len after:  25\n",
      "BEFORE:  0.88   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.96   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.84   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.12   AFTER:  0.0  len before:  25   len after:  25\n",
      "BEFORE:  0.6   AFTER:  0.56  len before:  25   len after:  25\n",
      "BEFORE:  0.6   AFTER:  0.56  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.2   AFTER:  0.0  len before:  25   len after:  25\n",
      "BEFORE:  0.28   AFTER:  0.0  len before:  25   len after:  17\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  25   len after:  25\n",
      "BEFORE:  0.64   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.96   AFTER:  1.0  len before:  25   len after:  17\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.48   AFTER:  0.0  len before:  25   len after:  13\n",
      "BEFORE:  0.64   AFTER:  0.72  len before:  25   len after:  25\n",
      "ACCURACY MODEL:  0.8645454545454545   Accuracy pseudo: 0.9009615384615385  len pseudo:  2080    len predictions 2200\n",
      "STARTING TRAINING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy total 0.920373, main loss classifier 0.383602, source accuracy 0.972356 source classification loss 0.093593, target accuracy 0.868389 target loss 0.428674 accuracy domain distinction 0.497596 loss domain distinction 1.224688,\n",
      "VALIDATION Loss: 0.12108456 Acc: 0.95432692\n",
      "New best validation loss:  0.12108456476458482\n",
      "Epoch 2 of 500 took 0.345s\n",
      "Accuracy total 0.924279, main loss classifier 0.344631, source accuracy 0.967548 source classification loss 0.096964, target accuracy 0.881010 target loss 0.363061 accuracy domain distinction 0.498498 loss domain distinction 1.146188,\n",
      "VALIDATION Loss: 0.13890789 Acc: 0.95432692\n",
      "Epoch 3 of 500 took 0.342s\n",
      "Accuracy total 0.940805, main loss classifier 0.319007, source accuracy 0.977163 source classification loss 0.080367, target accuracy 0.904447 target loss 0.332481 accuracy domain distinction 0.499399 loss domain distinction 1.125831,\n",
      "VALIDATION Loss: 0.09809347 Acc: 0.96875\n",
      "New best validation loss:  0.0980934692280633\n",
      "Epoch 4 of 500 took 0.341s\n",
      "Accuracy total 0.933293, main loss classifier 0.315115, source accuracy 0.971154 source classification loss 0.090604, target accuracy 0.895433 target loss 0.318248 accuracy domain distinction 0.497897 loss domain distinction 1.106891,\n",
      "VALIDATION Loss: 0.13695865 Acc: 0.95913462\n",
      "Epoch 5 of 500 took 0.342s\n",
      "Accuracy total 0.940505, main loss classifier 0.302170, source accuracy 0.967548 source classification loss 0.097902, target accuracy 0.913462 target loss 0.287307 accuracy domain distinction 0.496995 loss domain distinction 1.095654,\n",
      "VALIDATION Loss: 0.10543875 Acc: 0.96634615\n",
      "Epoch 6 of 500 took 0.345s\n",
      "Accuracy total 0.949219, main loss classifier 0.271399, source accuracy 0.974159 source classification loss 0.086386, target accuracy 0.924279 target loss 0.238056 accuracy domain distinction 0.496094 loss domain distinction 1.091773,\n",
      "VALIDATION Loss: 0.10272229 Acc: 0.96875\n",
      "Epoch 7 of 500 took 0.340s\n",
      "Accuracy total 0.948317, main loss classifier 0.287738, source accuracy 0.978966 source classification loss 0.082370, target accuracy 0.917668 target loss 0.275505 accuracy domain distinction 0.499399 loss domain distinction 1.088008,\n",
      "VALIDATION Loss: 0.09436151 Acc: 0.96875\n",
      "New best validation loss:  0.09436150640249252\n",
      "Epoch 8 of 500 took 0.346s\n",
      "Accuracy total 0.951923, main loss classifier 0.275100, source accuracy 0.978966 source classification loss 0.079270, target accuracy 0.924880 target loss 0.251552 accuracy domain distinction 0.500000 loss domain distinction 1.096893,\n",
      "VALIDATION Loss: 0.10280673 Acc: 0.96153846\n",
      "Epoch 9 of 500 took 0.345s\n",
      "Accuracy total 0.952224, main loss classifier 0.274792, source accuracy 0.978365 source classification loss 0.082080, target accuracy 0.926082 target loss 0.250004 accuracy domain distinction 0.500000 loss domain distinction 1.087502,\n",
      "VALIDATION Loss: 0.11402823 Acc: 0.95913462\n",
      "Epoch 10 of 500 took 0.341s\n",
      "Accuracy total 0.953726, main loss classifier 0.262698, source accuracy 0.977163 source classification loss 0.087783, target accuracy 0.930288 target loss 0.219373 accuracy domain distinction 0.499099 loss domain distinction 1.091200,\n",
      "VALIDATION Loss: 0.06837290 Acc: 0.97115385\n",
      "New best validation loss:  0.06837290498827185\n",
      "Epoch 11 of 500 took 0.346s\n",
      "Accuracy total 0.953726, main loss classifier 0.253998, source accuracy 0.974159 source classification loss 0.082426, target accuracy 0.933293 target loss 0.210308 accuracy domain distinction 0.499700 loss domain distinction 1.076311,\n",
      "VALIDATION Loss: 0.08213300 Acc: 0.96875\n",
      "Epoch 12 of 500 took 0.341s\n",
      "Accuracy total 0.946815, main loss classifier 0.266933, source accuracy 0.975962 source classification loss 0.080313, target accuracy 0.917668 target loss 0.238798 accuracy domain distinction 0.498798 loss domain distinction 1.073777,\n",
      "VALIDATION Loss: 0.08627070 Acc: 0.97355769\n",
      "Epoch 13 of 500 took 0.340s\n",
      "Accuracy total 0.956430, main loss classifier 0.248093, source accuracy 0.978365 source classification loss 0.077670, target accuracy 0.934495 target loss 0.203993 accuracy domain distinction 0.500901 loss domain distinction 1.072618,\n",
      "VALIDATION Loss: 0.12068246 Acc: 0.95673077\n",
      "Epoch 14 of 500 took 0.342s\n",
      "Accuracy total 0.945913, main loss classifier 0.276680, source accuracy 0.971755 source classification loss 0.097177, target accuracy 0.920072 target loss 0.239806 accuracy domain distinction 0.500000 loss domain distinction 1.081889,\n",
      "VALIDATION Loss: 0.08374634 Acc: 0.96153846\n",
      "Epoch 15 of 500 took 0.342s\n",
      "Accuracy total 0.955529, main loss classifier 0.243788, source accuracy 0.979567 source classification loss 0.072477, target accuracy 0.931490 target loss 0.199456 accuracy domain distinction 0.499099 loss domain distinction 1.078221,\n",
      "VALIDATION Loss: 0.09582585 Acc: 0.95913462\n",
      "Epoch 16 of 500 took 0.339s\n",
      "Accuracy total 0.952825, main loss classifier 0.254885, source accuracy 0.974159 source classification loss 0.084940, target accuracy 0.931490 target loss 0.211165 accuracy domain distinction 0.500901 loss domain distinction 1.068326,\n",
      "VALIDATION Loss: 0.12317637 Acc: 0.95913462\n",
      "Epoch    16: reducing learning rate of group 0 to 2.0120e-05.\n",
      "Epoch 17 of 500 took 0.343s\n",
      "Accuracy total 0.954026, main loss classifier 0.261421, source accuracy 0.977163 source classification loss 0.081575, target accuracy 0.930889 target loss 0.227362 accuracy domain distinction 0.499399 loss domain distinction 1.069527,\n",
      "VALIDATION Loss: 0.06962715 Acc: 0.97596154\n",
      "Epoch 18 of 500 took 0.340s\n",
      "Accuracy total 0.958834, main loss classifier 0.240149, source accuracy 0.978365 source classification loss 0.085660, target accuracy 0.939303 target loss 0.182427 accuracy domain distinction 0.499399 loss domain distinction 1.061057,\n",
      "VALIDATION Loss: 0.06317409 Acc: 0.97596154\n",
      "New best validation loss:  0.06317409181169101\n",
      "Epoch 19 of 500 took 0.341s\n",
      "Accuracy total 0.959736, main loss classifier 0.234446, source accuracy 0.984976 source classification loss 0.064850, target accuracy 0.934495 target loss 0.191092 accuracy domain distinction 0.500300 loss domain distinction 1.064754,\n",
      "VALIDATION Loss: 0.10450962 Acc: 0.96153846\n",
      "Epoch 20 of 500 took 0.344s\n",
      "Accuracy total 0.954928, main loss classifier 0.255235, source accuracy 0.976562 source classification loss 0.085594, target accuracy 0.933293 target loss 0.211252 accuracy domain distinction 0.500601 loss domain distinction 1.068118,\n",
      "VALIDATION Loss: 0.07918197 Acc: 0.96875\n",
      "Epoch 21 of 500 took 0.346s\n",
      "Accuracy total 0.957031, main loss classifier 0.248750, source accuracy 0.977764 source classification loss 0.074835, target accuracy 0.936298 target loss 0.207005 accuracy domain distinction 0.499700 loss domain distinction 1.078303,\n",
      "VALIDATION Loss: 0.13208100 Acc: 0.95432692\n",
      "Epoch 22 of 500 took 0.342s\n",
      "Accuracy total 0.954327, main loss classifier 0.245034, source accuracy 0.977163 source classification loss 0.080227, target accuracy 0.931490 target loss 0.195851 accuracy domain distinction 0.499399 loss domain distinction 1.069949,\n",
      "VALIDATION Loss: 0.07154548 Acc: 0.97836538\n",
      "Epoch 23 of 500 took 0.342s\n",
      "Accuracy total 0.966046, main loss classifier 0.224607, source accuracy 0.980769 source classification loss 0.069112, target accuracy 0.951322 target loss 0.167595 accuracy domain distinction 0.500901 loss domain distinction 1.062531,\n",
      "VALIDATION Loss: 0.08473464 Acc: 0.96634615\n",
      "Epoch 24 of 500 took 0.346s\n",
      "Accuracy total 0.951923, main loss classifier 0.252217, source accuracy 0.972957 source classification loss 0.084441, target accuracy 0.930889 target loss 0.207178 accuracy domain distinction 0.499700 loss domain distinction 1.064078,\n",
      "VALIDATION Loss: 0.10008958 Acc: 0.96634615\n",
      "Epoch    24: reducing learning rate of group 0 to 4.0240e-06.\n",
      "Epoch 25 of 500 took 0.341s\n",
      "Accuracy total 0.953425, main loss classifier 0.258331, source accuracy 0.978966 source classification loss 0.080272, target accuracy 0.927885 target loss 0.223522 accuracy domain distinction 0.499700 loss domain distinction 1.064336,\n",
      "VALIDATION Loss: 0.10824340 Acc: 0.96875\n",
      "Epoch 26 of 500 took 0.342s\n",
      "Accuracy total 0.960637, main loss classifier 0.245148, source accuracy 0.976562 source classification loss 0.081649, target accuracy 0.944712 target loss 0.196171 accuracy domain distinction 0.499399 loss domain distinction 1.062383,\n",
      "VALIDATION Loss: 0.14918065 Acc: 0.95192308\n",
      "Epoch 27 of 500 took 0.343s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy total 0.956430, main loss classifier 0.242077, source accuracy 0.975962 source classification loss 0.078769, target accuracy 0.936899 target loss 0.192050 accuracy domain distinction 0.500300 loss domain distinction 1.066671,\n",
      "VALIDATION Loss: 0.12297426 Acc: 0.95913462\n",
      "Epoch 28 of 500 took 0.340s\n",
      "Accuracy total 0.959736, main loss classifier 0.225441, source accuracy 0.977764 source classification loss 0.071691, target accuracy 0.941707 target loss 0.165411 accuracy domain distinction 0.499099 loss domain distinction 1.068898,\n",
      "VALIDATION Loss: 0.11866431 Acc: 0.95432692\n",
      "Epoch 29 of 500 took 0.345s\n",
      "Accuracy total 0.955829, main loss classifier 0.246464, source accuracy 0.981370 source classification loss 0.069437, target accuracy 0.930288 target loss 0.211382 accuracy domain distinction 0.499399 loss domain distinction 1.060540,\n",
      "VALIDATION Loss: 0.07786741 Acc: 0.97596154\n",
      "Epoch 30 of 500 took 0.342s\n",
      "Training complete in 0m 10s\n",
      "['participant_1', 'participant_2', 'participant_0']\n",
      "TSD_Network(\n",
      "  (_network): ModuleList(\n",
      "    (0): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=252, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=252, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (1): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (2): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (_output): Linear(in_features=200, out_features=22, bias=True)\n",
      "  (_output_discriminator): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "Number Parameters:  137624\n",
      "Optimizer =  <generator object Module.parameters at 0x7fb0acd5b740>\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/DANN/participant_1/best_state_2.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/DANN/participant_1/best_state_2.pt' (epoch 1)\n",
      "TSD_Network(\n",
      "  (_network): ModuleList(\n",
      "    (0): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=252, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=252, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (1): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (2): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (_output): Linear(in_features=200, out_features=22, bias=True)\n",
      "  (_output_discriminator): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "Number Parameters:  137624\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/TSD/participant_1/best_state_0.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/TSD/participant_1/best_state_0.pt' (epoch 18)\n",
      "TSD_Network(\n",
      "  (_network): ModuleList(\n",
      "    (0): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=252, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=252, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (1): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (2): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (_output): Linear(in_features=200, out_features=22, bias=True)\n",
      "  (_output_discriminator): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "Number Parameters:  137624\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/DANN/participant_1/best_state_1.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/DANN/participant_1/best_state_1.pt' (epoch 9)\n",
      "TSD_Network(\n",
      "  (_network): ModuleList(\n",
      "    (0): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=252, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=252, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (1): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (2): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (_output): Linear(in_features=200, out_features=22, bias=True)\n",
      "  (_output_discriminator): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "Number Parameters:  137624\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/DANN/participant_1/best_state_2.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/DANN/participant_1/best_state_2.pt' (epoch 1)\n",
      "==== models_array =  (3,)  @ session  2\n",
      "HANDLING NEW SESSION  0\n",
      "HANDLING NEW SESSION  1\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.96   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.52   AFTER:  0.0  len before:  25   len after:  11\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.96   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.8   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.92   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.84   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.88   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.52   AFTER:  1.0  len before:  25   len after:  10\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.56   AFTER:  0.0  len before:  25   len after:  12\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.96   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.6   AFTER:  1.0  len before:  25   len after:  8\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.92   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.32   AFTER:  0.0  len before:  25   len after:  17\n",
      "BEFORE:  0.84   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.96   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.04   AFTER:  0.0  len before:  25   len after:  9\n",
      "BEFORE:  0.52   AFTER:  1.0  len before:  25   len after:  16\n",
      "BEFORE:  0.96   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.96   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.76   AFTER:  0.96  len before:  25   len after:  25\n",
      "BEFORE:  0.52   AFTER:  0.52  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.56   AFTER:  0.6  len before:  25   len after:  25\n",
      "BEFORE:  0.88   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.96   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.84   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.12   AFTER:  0.0  len before:  25   len after:  25\n",
      "BEFORE:  0.6   AFTER:  0.56  len before:  25   len after:  25\n",
      "BEFORE:  0.6   AFTER:  0.56  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.2   AFTER:  0.0  len before:  25   len after:  25\n",
      "BEFORE:  0.28   AFTER:  0.0  len before:  25   len after:  17\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  25   len after:  25\n",
      "BEFORE:  0.64   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.96   AFTER:  1.0  len before:  25   len after:  17\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.48   AFTER:  0.0  len before:  25   len after:  13\n",
      "BEFORE:  0.64   AFTER:  0.72  len before:  25   len after:  25\n",
      "ACCURACY MODEL:  0.8645454545454545   Accuracy pseudo: 0.9009615384615385  len pseudo:  2080    len predictions 2200\n",
      "HANDLING NEW SESSION  2\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  0.88   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.24   AFTER:  0.4  len before:  25   len after:  25\n",
      "BEFORE:  0.64   AFTER:  0.0  len before:  25   len after:  8\n",
      "BEFORE:  0.48   AFTER:  1.0  len before:  25   len after:  25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE:  0.92   AFTER:  1.0  len before:  25   len after:  16\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.96   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.96   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.76   AFTER:  1.0  len before:  25   len after:  17\n",
      "BEFORE:  0.24   AFTER:  0.04  len before:  25   len after:  25\n",
      "BEFORE:  0.2   AFTER:  0.0  len before:  25   len after:  25\n",
      "BEFORE:  0.84   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.88   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.84   AFTER:  1.0  len before:  25   len after:  17\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.56   AFTER:  1.0  len before:  25   len after:  8\n",
      "BEFORE:  0.56   AFTER:  1.0  len before:  25   len after:  5\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.4   AFTER:  0.2222222222222222  len before:  25   len after:  9\n",
      "BEFORE:  0.88   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.72   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.92   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.92   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.92   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.84   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.92   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.16   AFTER:  0.0  len before:  25   len after:  25\n",
      "BEFORE:  0.56   AFTER:  0.48  len before:  25   len after:  25\n",
      "BEFORE:  0.72   AFTER:  1.0  len before:  25   len after:  14\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.96   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.48   AFTER:  1.0  len before:  25   len after:  1\n",
      "BEFORE:  0.84   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.88   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.32   AFTER:  0.0  len before:  25   len after:  25\n",
      "BEFORE:  0.76   AFTER:  1.0  len before:  25   len after:  9\n",
      "BEFORE:  0.68   AFTER:  0.42857142857142855  len before:  25   len after:  7\n",
      "BEFORE:  0.8   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.96   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.92   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.92   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.12   AFTER:  0.0  len before:  25   len after:  25\n",
      "BEFORE:  0.32   AFTER:  0.0  len before:  25   len after:  13\n",
      "BEFORE:  0.8   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.68   AFTER:  0.8  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.96   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.64   AFTER:  0.56  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.8   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.84   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.8   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.68   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.48   AFTER:  nan  len before:  25   len after:  0\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.96   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.76   AFTER:  1.0  len before:  25   len after:  17\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.52   AFTER:  0.6470588235294118  len before:  25   len after:  17\n",
      "BEFORE:  0.16   AFTER:  0.0  len before:  25   len after:  25\n",
      "BEFORE:  0.2   AFTER:  0.0  len before:  25   len after:  25\n",
      "BEFORE:  0.44   AFTER:  1.0  len before:  25   len after:  11\n",
      "BEFORE:  0.96   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.84   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.92   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.8   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.16   AFTER:  0.0  len before:  25   len after:  25\n",
      "ACCURACY MODEL:  0.7872727272727272   Accuracy pseudo: 0.8572879634332148  len pseudo:  1969    len predictions 2200\n",
      "STARTING TRAINING\n",
      "Accuracy total 0.867188, main loss classifier 0.508131, source accuracy 0.888021 source classification loss 0.326090, target accuracy 0.846354 target loss 0.460869 accuracy domain distinction 0.494792 loss domain distinction 1.146510,\n",
      "VALIDATION Loss: 0.17272903 Acc: 0.93654822\n",
      "New best validation loss:  0.17272903078368732\n",
      "Epoch 2 of 500 took 0.336s\n",
      "Accuracy total 0.891927, main loss classifier 0.429265, source accuracy 0.917318 source classification loss 0.253449, target accuracy 0.866536 target loss 0.376078 accuracy domain distinction 0.497396 loss domain distinction 1.145017,\n",
      "VALIDATION Loss: 0.12883409 Acc: 0.95177665\n",
      "New best validation loss:  0.12883408793381282\n",
      "Epoch 3 of 500 took 0.319s\n",
      "Accuracy total 0.901042, main loss classifier 0.404306, source accuracy 0.921224 source classification loss 0.249080, target accuracy 0.880859 target loss 0.337807 accuracy domain distinction 0.497396 loss domain distinction 1.108626,\n",
      "VALIDATION Loss: 0.20307117 Acc: 0.91624365\n",
      "Epoch 4 of 500 took 0.318s\n",
      "Accuracy total 0.918945, main loss classifier 0.365077, source accuracy 0.934245 source classification loss 0.204388, target accuracy 0.903646 target loss 0.308054 accuracy domain distinction 0.499023 loss domain distinction 1.088565,\n",
      "VALIDATION Loss: 0.12496715 Acc: 0.94162437\n",
      "New best validation loss:  0.1249671475296574\n",
      "Epoch 5 of 500 took 0.320s\n",
      "Accuracy total 0.920573, main loss classifier 0.361865, source accuracy 0.933594 source classification loss 0.220246, target accuracy 0.907552 target loss 0.286342 accuracy domain distinction 0.499023 loss domain distinction 1.085708,\n",
      "VALIDATION Loss: 0.16512972 Acc: 0.94162437\n",
      "Epoch 6 of 500 took 0.321s\n",
      "Accuracy total 0.915365, main loss classifier 0.375625, source accuracy 0.934245 source classification loss 0.215377, target accuracy 0.896484 target loss 0.319624 accuracy domain distinction 0.499349 loss domain distinction 1.081242,\n",
      "VALIDATION Loss: 0.12150769 Acc: 0.94416244\n",
      "New best validation loss:  0.12150769185141794\n",
      "Epoch 7 of 500 took 0.320s\n",
      "Accuracy total 0.919271, main loss classifier 0.344411, source accuracy 0.929036 source classification loss 0.193393, target accuracy 0.909505 target loss 0.282020 accuracy domain distinction 0.499674 loss domain distinction 1.067051,\n",
      "VALIDATION Loss: 0.12166321 Acc: 0.93654822\n",
      "Epoch 8 of 500 took 0.319s\n",
      "Accuracy total 0.932292, main loss classifier 0.324616, source accuracy 0.951172 source classification loss 0.174577, target accuracy 0.913411 target loss 0.261651 accuracy domain distinction 0.500000 loss domain distinction 1.065025,\n",
      "VALIDATION Loss: 0.17248735 Acc: 0.93401015\n",
      "Epoch 9 of 500 took 0.324s\n",
      "Accuracy total 0.923828, main loss classifier 0.327158, source accuracy 0.934896 source classification loss 0.187368, target accuracy 0.912760 target loss 0.253859 accuracy domain distinction 0.500000 loss domain distinction 1.065449,\n",
      "VALIDATION Loss: 0.10503935 Acc: 0.96192893\n",
      "New best validation loss:  0.10503934643098287\n",
      "Epoch 10 of 500 took 0.319s\n",
      "Accuracy total 0.934245, main loss classifier 0.303915, source accuracy 0.944010 source classification loss 0.169237, target accuracy 0.924479 target loss 0.227341 accuracy domain distinction 0.500000 loss domain distinction 1.056259,\n",
      "VALIDATION Loss: 0.09633869 Acc: 0.95685279\n",
      "New best validation loss:  0.09633868560194969\n",
      "Epoch 11 of 500 took 0.321s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy total 0.931966, main loss classifier 0.305469, source accuracy 0.945312 source classification loss 0.160226, target accuracy 0.918620 target loss 0.239925 accuracy domain distinction 0.500000 loss domain distinction 1.053933,\n",
      "VALIDATION Loss: 0.23833833 Acc: 0.9213198\n",
      "Epoch 12 of 500 took 0.318s\n",
      "Accuracy total 0.931641, main loss classifier 0.311261, source accuracy 0.942708 source classification loss 0.172966, target accuracy 0.920573 target loss 0.239000 accuracy domain distinction 0.500000 loss domain distinction 1.052780,\n",
      "VALIDATION Loss: 0.10964042 Acc: 0.95177665\n",
      "Epoch 13 of 500 took 0.316s\n",
      "Accuracy total 0.933594, main loss classifier 0.298367, source accuracy 0.936198 source classification loss 0.185789, target accuracy 0.930990 target loss 0.201040 accuracy domain distinction 0.500000 loss domain distinction 1.049525,\n",
      "VALIDATION Loss: 0.11569609 Acc: 0.95177665\n",
      "Epoch 14 of 500 took 0.321s\n",
      "Accuracy total 0.935547, main loss classifier 0.295257, source accuracy 0.939453 source classification loss 0.174019, target accuracy 0.931641 target loss 0.206591 accuracy domain distinction 0.500000 loss domain distinction 1.049524,\n",
      "VALIDATION Loss: 0.13926088 Acc: 0.94923858\n",
      "Epoch 15 of 500 took 0.315s\n",
      "Accuracy total 0.936849, main loss classifier 0.290909, source accuracy 0.947266 source classification loss 0.167258, target accuracy 0.926432 target loss 0.204520 accuracy domain distinction 0.500000 loss domain distinction 1.050192,\n",
      "VALIDATION Loss: 0.12022426 Acc: 0.94923858\n",
      "Epoch 16 of 500 took 0.317s\n",
      "Accuracy total 0.949219, main loss classifier 0.273355, source accuracy 0.957031 source classification loss 0.137504, target accuracy 0.941406 target loss 0.200461 accuracy domain distinction 0.500000 loss domain distinction 1.043730,\n",
      "VALIDATION Loss: 0.08568171 Acc: 0.96192893\n",
      "New best validation loss:  0.08568171305315835\n",
      "Epoch 17 of 500 took 0.321s\n",
      "Accuracy total 0.939453, main loss classifier 0.285774, source accuracy 0.940755 source classification loss 0.168998, target accuracy 0.938151 target loss 0.193292 accuracy domain distinction 0.500000 loss domain distinction 1.046286,\n",
      "VALIDATION Loss: 0.17151292 Acc: 0.95177665\n",
      "Epoch 18 of 500 took 0.318s\n",
      "Accuracy total 0.936523, main loss classifier 0.281473, source accuracy 0.944661 source classification loss 0.160005, target accuracy 0.928385 target loss 0.193728 accuracy domain distinction 0.500000 loss domain distinction 1.046067,\n",
      "VALIDATION Loss: 0.08753434 Acc: 0.96446701\n",
      "Epoch 19 of 500 took 0.319s\n",
      "Accuracy total 0.944987, main loss classifier 0.269392, source accuracy 0.956380 source classification loss 0.138291, target accuracy 0.933594 target loss 0.191072 accuracy domain distinction 0.500000 loss domain distinction 1.047102,\n",
      "VALIDATION Loss: 0.07461800 Acc: 0.97969543\n",
      "New best validation loss:  0.0746179994727884\n",
      "Epoch 20 of 500 took 0.319s\n",
      "Accuracy total 0.947591, main loss classifier 0.264345, source accuracy 0.954427 source classification loss 0.150556, target accuracy 0.940755 target loss 0.170451 accuracy domain distinction 0.500000 loss domain distinction 1.038418,\n",
      "VALIDATION Loss: 0.14180935 Acc: 0.94923858\n",
      "Epoch 21 of 500 took 0.324s\n",
      "Accuracy total 0.943685, main loss classifier 0.266821, source accuracy 0.945964 source classification loss 0.155540, target accuracy 0.941406 target loss 0.169916 accuracy domain distinction 0.500000 loss domain distinction 1.040934,\n",
      "VALIDATION Loss: 0.09123064 Acc: 0.97715736\n",
      "Epoch 22 of 500 took 0.329s\n",
      "Accuracy total 0.945312, main loss classifier 0.273944, source accuracy 0.958984 source classification loss 0.130690, target accuracy 0.931641 target loss 0.207840 accuracy domain distinction 0.500000 loss domain distinction 1.046793,\n",
      "VALIDATION Loss: 0.18737076 Acc: 0.94162437\n",
      "Epoch 23 of 500 took 0.364s\n",
      "Accuracy total 0.945312, main loss classifier 0.264169, source accuracy 0.956380 source classification loss 0.118052, target accuracy 0.934245 target loss 0.201761 accuracy domain distinction 0.500000 loss domain distinction 1.042621,\n",
      "VALIDATION Loss: 0.10472528 Acc: 0.97461929\n",
      "Epoch 24 of 500 took 0.326s\n",
      "Accuracy total 0.954753, main loss classifier 0.243272, source accuracy 0.958984 source classification loss 0.125782, target accuracy 0.950521 target loss 0.153399 accuracy domain distinction 0.500000 loss domain distinction 1.036816,\n",
      "VALIDATION Loss: 0.08392038 Acc: 0.96446701\n",
      "Epoch 25 of 500 took 0.317s\n",
      "Accuracy total 0.944336, main loss classifier 0.265986, source accuracy 0.955729 source classification loss 0.132886, target accuracy 0.932943 target loss 0.190939 accuracy domain distinction 0.500000 loss domain distinction 1.040739,\n",
      "VALIDATION Loss: 0.07864401 Acc: 0.96954315\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0060e-04.\n",
      "Epoch 26 of 500 took 0.317s\n",
      "Accuracy total 0.946940, main loss classifier 0.259986, source accuracy 0.953776 source classification loss 0.145410, target accuracy 0.940104 target loss 0.168586 accuracy domain distinction 0.500000 loss domain distinction 1.029876,\n",
      "VALIDATION Loss: 0.05512682 Acc: 0.9822335\n",
      "New best validation loss:  0.05512681603431702\n",
      "Epoch 27 of 500 took 0.322s\n",
      "Accuracy total 0.948893, main loss classifier 0.241196, source accuracy 0.955078 source classification loss 0.125829, target accuracy 0.942708 target loss 0.150609 accuracy domain distinction 0.500000 loss domain distinction 1.029774,\n",
      "VALIDATION Loss: 0.07553558 Acc: 0.97208122\n",
      "Epoch 28 of 500 took 0.317s\n",
      "Accuracy total 0.957031, main loss classifier 0.235162, source accuracy 0.961589 source classification loss 0.121699, target accuracy 0.952474 target loss 0.143724 accuracy domain distinction 0.500000 loss domain distinction 1.024502,\n",
      "VALIDATION Loss: 0.08344671 Acc: 0.96192893\n",
      "Epoch 29 of 500 took 0.317s\n",
      "Accuracy total 0.957031, main loss classifier 0.238349, source accuracy 0.962240 source classification loss 0.123595, target accuracy 0.951823 target loss 0.146728 accuracy domain distinction 0.500000 loss domain distinction 1.031879,\n",
      "VALIDATION Loss: 0.11685647 Acc: 0.95431472\n",
      "Epoch 30 of 500 took 0.320s\n",
      "Accuracy total 0.958659, main loss classifier 0.225988, source accuracy 0.963542 source classification loss 0.105954, target accuracy 0.953776 target loss 0.140288 accuracy domain distinction 0.500000 loss domain distinction 1.028669,\n",
      "VALIDATION Loss: 0.06694753 Acc: 0.97969543\n",
      "Epoch 31 of 500 took 0.319s\n",
      "Accuracy total 0.958984, main loss classifier 0.228215, source accuracy 0.963542 source classification loss 0.108021, target accuracy 0.954427 target loss 0.142428 accuracy domain distinction 0.500000 loss domain distinction 1.029903,\n",
      "VALIDATION Loss: 0.09215087 Acc: 0.96954315\n",
      "Epoch 32 of 500 took 0.320s\n",
      "Accuracy total 0.960612, main loss classifier 0.227956, source accuracy 0.964193 source classification loss 0.114962, target accuracy 0.957031 target loss 0.135715 accuracy domain distinction 0.500000 loss domain distinction 1.026179,\n",
      "VALIDATION Loss: 0.06090322 Acc: 0.96700508\n",
      "Epoch    32: reducing learning rate of group 0 to 2.0120e-05.\n",
      "Epoch 33 of 500 took 0.320s\n",
      "Accuracy total 0.960612, main loss classifier 0.216697, source accuracy 0.964844 source classification loss 0.107292, target accuracy 0.956380 target loss 0.119120 accuracy domain distinction 0.500000 loss domain distinction 1.034913,\n",
      "VALIDATION Loss: 0.05876078 Acc: 0.9822335\n",
      "Epoch 34 of 500 took 0.320s\n",
      "Accuracy total 0.962240, main loss classifier 0.225299, source accuracy 0.962240 source classification loss 0.120403, target accuracy 0.962240 target loss 0.123982 accuracy domain distinction 0.500000 loss domain distinction 1.031061,\n",
      "VALIDATION Loss: 0.05076512 Acc: 0.97969543\n",
      "New best validation loss:  0.05076512178805258\n",
      "Epoch 35 of 500 took 0.319s\n",
      "Accuracy total 0.956055, main loss classifier 0.232991, source accuracy 0.962240 source classification loss 0.110071, target accuracy 0.949870 target loss 0.150113 accuracy domain distinction 0.500000 loss domain distinction 1.028994,\n",
      "VALIDATION Loss: 0.06932138 Acc: 0.97208122\n",
      "Epoch 36 of 500 took 0.320s\n",
      "Accuracy total 0.961263, main loss classifier 0.217893, source accuracy 0.962240 source classification loss 0.105070, target accuracy 0.960286 target loss 0.124367 accuracy domain distinction 0.500000 loss domain distinction 1.031745,\n",
      "VALIDATION Loss: 0.07617845 Acc: 0.96700508\n",
      "Epoch 37 of 500 took 0.324s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy total 0.957357, main loss classifier 0.230963, source accuracy 0.961589 source classification loss 0.105411, target accuracy 0.953125 target loss 0.150597 accuracy domain distinction 0.500000 loss domain distinction 1.029593,\n",
      "VALIDATION Loss: 0.05713374 Acc: 0.97461929\n",
      "Epoch 38 of 500 took 0.317s\n",
      "Accuracy total 0.963867, main loss classifier 0.225168, source accuracy 0.966797 source classification loss 0.111527, target accuracy 0.960938 target loss 0.133342 accuracy domain distinction 0.500000 loss domain distinction 1.027338,\n",
      "VALIDATION Loss: 0.10505826 Acc: 0.95431472\n",
      "Epoch 39 of 500 took 0.319s\n",
      "Accuracy total 0.965169, main loss classifier 0.207786, source accuracy 0.970052 source classification loss 0.091462, target accuracy 0.960286 target loss 0.118010 accuracy domain distinction 0.500000 loss domain distinction 1.030500,\n",
      "VALIDATION Loss: 0.07052546 Acc: 0.97715736\n",
      "Epoch 40 of 500 took 0.324s\n",
      "Accuracy total 0.965820, main loss classifier 0.210851, source accuracy 0.970703 source classification loss 0.089065, target accuracy 0.960938 target loss 0.127180 accuracy domain distinction 0.500000 loss domain distinction 1.027284,\n",
      "VALIDATION Loss: 0.06381914 Acc: 0.96954315\n",
      "Epoch    40: reducing learning rate of group 0 to 4.0240e-06.\n",
      "Epoch 41 of 500 took 0.317s\n",
      "Accuracy total 0.960612, main loss classifier 0.224176, source accuracy 0.964193 source classification loss 0.107301, target accuracy 0.957031 target loss 0.135653 accuracy domain distinction 0.500000 loss domain distinction 1.026990,\n",
      "VALIDATION Loss: 0.08965559 Acc: 0.96954315\n",
      "Epoch 42 of 500 took 0.317s\n",
      "Accuracy total 0.958333, main loss classifier 0.223842, source accuracy 0.961589 source classification loss 0.111450, target accuracy 0.955078 target loss 0.130799 accuracy domain distinction 0.500000 loss domain distinction 1.027179,\n",
      "VALIDATION Loss: 0.06824019 Acc: 0.97715736\n",
      "Epoch 43 of 500 took 0.319s\n",
      "Accuracy total 0.955078, main loss classifier 0.234949, source accuracy 0.957682 source classification loss 0.128380, target accuracy 0.952474 target loss 0.135230 accuracy domain distinction 0.500000 loss domain distinction 1.031432,\n",
      "VALIDATION Loss: 0.05964500 Acc: 0.97969543\n",
      "Epoch 44 of 500 took 0.316s\n",
      "Accuracy total 0.960938, main loss classifier 0.221075, source accuracy 0.966146 source classification loss 0.112016, target accuracy 0.955729 target loss 0.124620 accuracy domain distinction 0.500000 loss domain distinction 1.027572,\n",
      "VALIDATION Loss: 0.05889554 Acc: 0.97208122\n",
      "Epoch 45 of 500 took 0.317s\n",
      "Accuracy total 0.958008, main loss classifier 0.232276, source accuracy 0.961589 source classification loss 0.117678, target accuracy 0.954427 target loss 0.141558 accuracy domain distinction 0.500000 loss domain distinction 1.026575,\n",
      "VALIDATION Loss: 0.09535201 Acc: 0.95939086\n",
      "Epoch 46 of 500 took 0.322s\n",
      "Training complete in 0m 14s\n",
      "['participant_1', 'participant_2', 'participant_0']\n",
      "TSD_Network(\n",
      "  (_network): ModuleList(\n",
      "    (0): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=252, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=252, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (1): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (2): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (_output): Linear(in_features=200, out_features=22, bias=True)\n",
      "  (_output_discriminator): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "Number Parameters:  137624\n",
      "Optimizer =  <generator object Module.parameters at 0x7fb0acd5b5f0>\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/DANN/participant_2/best_state_1.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/DANN/participant_2/best_state_1.pt' (epoch 24)\n",
      "TSD_Network(\n",
      "  (_network): ModuleList(\n",
      "    (0): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=252, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=252, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (1): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (2): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (_output): Linear(in_features=200, out_features=22, bias=True)\n",
      "  (_output_discriminator): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "Number Parameters:  137624\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/TSD/participant_2/best_state_0.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/TSD/participant_2/best_state_0.pt' (epoch 25)\n",
      "TSD_Network(\n",
      "  (_network): ModuleList(\n",
      "    (0): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=252, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=252, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (1): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (2): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (_output): Linear(in_features=200, out_features=22, bias=True)\n",
      "  (_output_discriminator): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "Number Parameters:  137624\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/DANN/participant_2/best_state_1.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/DANN/participant_2/best_state_1.pt' (epoch 24)\n",
      "==== models_array =  (2,)  @ session  1\n",
      "HANDLING NEW SESSION  0\n",
      "HANDLING NEW SESSION  1\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.08   AFTER:  0.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.56   AFTER:  0.56  len before:  25   len after:  25\n",
      "BEFORE:  0.8   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.24   AFTER:  0.24  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.96   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.64   AFTER:  0.6  len before:  25   len after:  25\n",
      "BEFORE:  0.84   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.8   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.04   AFTER:  0.0  len before:  25   len after:  1\n",
      "BEFORE:  0.2   AFTER:  0.48  len before:  25   len after:  25\n",
      "BEFORE:  0.68   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.92   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  25   len after:  6\n",
      "BEFORE:  0.2   AFTER:  0.0  len before:  25   len after:  15\n",
      "BEFORE:  0.24   AFTER:  0.0  len before:  25   len after:  25\n",
      "BEFORE:  0.16   AFTER:  0.0  len before:  25   len after:  16\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.52   AFTER:  1.0  len before:  25   len after:  4\n",
      "BEFORE:  0.88   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.16   AFTER:  0.0  len before:  25   len after:  25\n",
      "BEFORE:  0.24   AFTER:  0.4  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.96   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.88   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.88   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.64   AFTER:  0.72  len before:  25   len after:  25\n",
      "BEFORE:  0.16   AFTER:  0.0  len before:  25   len after:  25\n",
      "BEFORE:  0.64   AFTER:  1.0  len before:  25   len after:  6\n",
      "BEFORE:  0.72   AFTER:  0.6  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.88   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.4   AFTER:  0.6  len before:  25   len after:  25\n",
      "BEFORE:  0.04   AFTER:  0.0  len before:  25   len after:  25\n",
      "BEFORE:  0.44   AFTER:  0.28  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.64   AFTER:  0.52  len before:  25   len after:  25\n",
      "BEFORE:  0.88   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.72   AFTER:  1.0  len before:  25   len after:  16\n",
      "BEFORE:  0.16   AFTER:  0.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  25   len after:  25\n",
      "BEFORE:  0.96   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.64   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.88   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.76   AFTER:  0.88  len before:  25   len after:  25\n",
      "BEFORE:  0.28   AFTER:  0.44  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.48   AFTER:  0.6  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.76   AFTER:  1.0  len before:  25   len after:  9\n",
      "BEFORE:  0.12   AFTER:  0.0  len before:  25   len after:  25\n",
      "BEFORE:  0.56   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.68   AFTER:  0.68  len before:  25   len after:  25\n",
      "BEFORE:  0.88   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.52   AFTER:  0.56  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  25   len after:  25\n",
      "BEFORE:  0.88   AFTER:  1.0  len before:  25   len after:  17\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.64   AFTER:  0.84  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.84   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.96   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.04   AFTER:  0.0  len before:  25   len after:  16\n",
      "BEFORE:  0.2   AFTER:  nan  len before:  25   len after:  0\n",
      "BEFORE:  0.6   AFTER:  0.96  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.68   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.24   AFTER:  0.0  len before:  25   len after:  25\n",
      "BEFORE:  0.08   AFTER:  0.0  len before:  25   len after:  11\n",
      "ACCURACY MODEL:  0.6804545454545454   Accuracy pseudo: 0.7565691621219633  len pseudo:  2017    len predictions 2200\n",
      "STARTING TRAINING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy total 0.881250, main loss classifier 0.535353, source accuracy 0.960000 source classification loss 0.121401, target accuracy 0.802500 target loss 0.711161 accuracy domain distinction 0.499688 loss domain distinction 1.190726,\n",
      "VALIDATION Loss: 0.31359766 Acc: 0.89356436\n",
      "New best validation loss:  0.31359765997954775\n",
      "Epoch 2 of 500 took 0.333s\n",
      "Accuracy total 0.890625, main loss classifier 0.494684, source accuracy 0.961250 source classification loss 0.120492, target accuracy 0.820000 target loss 0.648502 accuracy domain distinction 0.499688 loss domain distinction 1.101870,\n",
      "VALIDATION Loss: 0.33323558 Acc: 0.88118812\n",
      "Epoch 3 of 500 took 0.331s\n",
      "Accuracy total 0.891875, main loss classifier 0.474388, source accuracy 0.965625 source classification loss 0.114041, target accuracy 0.818125 target loss 0.619601 accuracy domain distinction 0.500313 loss domain distinction 1.075672,\n",
      "VALIDATION Loss: 0.35297051 Acc: 0.87871287\n",
      "Epoch 4 of 500 took 0.330s\n",
      "Accuracy total 0.892188, main loss classifier 0.449185, source accuracy 0.963125 source classification loss 0.124243, target accuracy 0.821250 target loss 0.559575 accuracy domain distinction 0.500625 loss domain distinction 1.072760,\n",
      "VALIDATION Loss: 0.26774262 Acc: 0.90346535\n",
      "New best validation loss:  0.2677426189184189\n",
      "Epoch 5 of 500 took 0.329s\n",
      "Accuracy total 0.895625, main loss classifier 0.436211, source accuracy 0.960625 source classification loss 0.118165, target accuracy 0.830625 target loss 0.537943 accuracy domain distinction 0.498437 loss domain distinction 1.081571,\n",
      "VALIDATION Loss: 0.32616532 Acc: 0.90594059\n",
      "Epoch 6 of 500 took 0.330s\n",
      "Accuracy total 0.902813, main loss classifier 0.414532, source accuracy 0.965000 source classification loss 0.118148, target accuracy 0.840625 target loss 0.496061 accuracy domain distinction 0.499375 loss domain distinction 1.074272,\n",
      "VALIDATION Loss: 0.26518360 Acc: 0.91831683\n",
      "New best validation loss:  0.26518360312495914\n",
      "Epoch 7 of 500 took 0.341s\n",
      "Accuracy total 0.909062, main loss classifier 0.406860, source accuracy 0.965000 source classification loss 0.111554, target accuracy 0.853125 target loss 0.487477 accuracy domain distinction 0.499688 loss domain distinction 1.073443,\n",
      "VALIDATION Loss: 0.28355708 Acc: 0.8960396\n",
      "Epoch 8 of 500 took 0.364s\n",
      "Accuracy total 0.911250, main loss classifier 0.389896, source accuracy 0.960625 source classification loss 0.110523, target accuracy 0.861875 target loss 0.455542 accuracy domain distinction 0.498437 loss domain distinction 1.068637,\n",
      "VALIDATION Loss: 0.27886124 Acc: 0.88366337\n",
      "Epoch 9 of 500 took 0.330s\n",
      "Accuracy total 0.905937, main loss classifier 0.405792, source accuracy 0.964375 source classification loss 0.119187, target accuracy 0.847500 target loss 0.477257 accuracy domain distinction 0.499375 loss domain distinction 1.075697,\n",
      "VALIDATION Loss: 0.22412736 Acc: 0.91831683\n",
      "New best validation loss:  0.2241273560960378\n",
      "Epoch 10 of 500 took 0.335s\n",
      "Accuracy total 0.905000, main loss classifier 0.402135, source accuracy 0.960625 source classification loss 0.120427, target accuracy 0.849375 target loss 0.470753 accuracy domain distinction 0.499375 loss domain distinction 1.065445,\n",
      "VALIDATION Loss: 0.24339145 Acc: 0.92079208\n",
      "Epoch 11 of 500 took 0.328s\n",
      "Accuracy total 0.913125, main loss classifier 0.371948, source accuracy 0.971250 source classification loss 0.104721, target accuracy 0.855000 target loss 0.427186 accuracy domain distinction 0.500625 loss domain distinction 1.059945,\n",
      "VALIDATION Loss: 0.33751697 Acc: 0.85148515\n",
      "Epoch 12 of 500 took 0.329s\n",
      "Accuracy total 0.909687, main loss classifier 0.401429, source accuracy 0.966875 source classification loss 0.111910, target accuracy 0.852500 target loss 0.478342 accuracy domain distinction 0.499688 loss domain distinction 1.063029,\n",
      "VALIDATION Loss: 0.24650488 Acc: 0.91336634\n",
      "Epoch 13 of 500 took 0.331s\n",
      "Accuracy total 0.914375, main loss classifier 0.373796, source accuracy 0.970625 source classification loss 0.108317, target accuracy 0.858125 target loss 0.425974 accuracy domain distinction 0.499375 loss domain distinction 1.066502,\n",
      "VALIDATION Loss: 0.27976456 Acc: 0.89108911\n",
      "Epoch 14 of 500 took 0.345s\n",
      "Accuracy total 0.911250, main loss classifier 0.393330, source accuracy 0.966250 source classification loss 0.110143, target accuracy 0.856250 target loss 0.463491 accuracy domain distinction 0.500313 loss domain distinction 1.065130,\n",
      "VALIDATION Loss: 0.21214630 Acc: 0.92574257\n",
      "New best validation loss:  0.21214629603283747\n",
      "Epoch 15 of 500 took 0.332s\n",
      "Accuracy total 0.921562, main loss classifier 0.363176, source accuracy 0.970000 source classification loss 0.106147, target accuracy 0.873125 target loss 0.407764 accuracy domain distinction 0.500313 loss domain distinction 1.062202,\n",
      "VALIDATION Loss: 0.25760703 Acc: 0.90346535\n",
      "Epoch 16 of 500 took 0.328s\n",
      "Accuracy total 0.917813, main loss classifier 0.365150, source accuracy 0.965000 source classification loss 0.115210, target accuracy 0.870625 target loss 0.404473 accuracy domain distinction 0.499375 loss domain distinction 1.053088,\n",
      "VALIDATION Loss: 0.20825668 Acc: 0.92574257\n",
      "New best validation loss:  0.20825667679309845\n",
      "Epoch 17 of 500 took 0.329s\n",
      "Accuracy total 0.916875, main loss classifier 0.360656, source accuracy 0.963125 source classification loss 0.117732, target accuracy 0.870625 target loss 0.390973 accuracy domain distinction 0.499688 loss domain distinction 1.063030,\n",
      "VALIDATION Loss: 0.23925870 Acc: 0.91336634\n",
      "Epoch 18 of 500 took 0.328s\n",
      "Accuracy total 0.914375, main loss classifier 0.361739, source accuracy 0.966250 source classification loss 0.105812, target accuracy 0.862500 target loss 0.406614 accuracy domain distinction 0.500000 loss domain distinction 1.055256,\n",
      "VALIDATION Loss: 0.21204344 Acc: 0.90841584\n",
      "Epoch 19 of 500 took 0.332s\n",
      "Accuracy total 0.908750, main loss classifier 0.400587, source accuracy 0.968125 source classification loss 0.107070, target accuracy 0.849375 target loss 0.481789 accuracy domain distinction 0.500000 loss domain distinction 1.061578,\n",
      "VALIDATION Loss: 0.26301772 Acc: 0.92326733\n",
      "Epoch 20 of 500 took 0.329s\n",
      "Accuracy total 0.922188, main loss classifier 0.349386, source accuracy 0.973125 source classification loss 0.092815, target accuracy 0.871250 target loss 0.394499 accuracy domain distinction 0.499688 loss domain distinction 1.057285,\n",
      "VALIDATION Loss: 0.28993423 Acc: 0.91831683\n",
      "Epoch 21 of 500 took 0.333s\n",
      "Accuracy total 0.914375, main loss classifier 0.356296, source accuracy 0.966250 source classification loss 0.106024, target accuracy 0.862500 target loss 0.396324 accuracy domain distinction 0.499688 loss domain distinction 1.051215,\n",
      "VALIDATION Loss: 0.21077547 Acc: 0.91831683\n",
      "Epoch 22 of 500 took 0.338s\n",
      "Accuracy total 0.919063, main loss classifier 0.363021, source accuracy 0.967500 source classification loss 0.115489, target accuracy 0.870625 target loss 0.400768 accuracy domain distinction 0.500000 loss domain distinction 1.048923,\n",
      "VALIDATION Loss: 0.24505235 Acc: 0.91089109\n",
      "Epoch    22: reducing learning rate of group 0 to 2.0120e-05.\n",
      "Epoch 23 of 500 took 0.328s\n",
      "Accuracy total 0.923125, main loss classifier 0.347347, source accuracy 0.971875 source classification loss 0.093071, target accuracy 0.874375 target loss 0.390992 accuracy domain distinction 0.500000 loss domain distinction 1.053157,\n",
      "VALIDATION Loss: 0.37602181 Acc: 0.85891089\n",
      "Epoch 24 of 500 took 0.329s\n",
      "Accuracy total 0.914687, main loss classifier 0.369369, source accuracy 0.966250 source classification loss 0.101982, target accuracy 0.863125 target loss 0.426950 accuracy domain distinction 0.500000 loss domain distinction 1.049036,\n",
      "VALIDATION Loss: 0.30192469 Acc: 0.89108911\n",
      "Epoch 25 of 500 took 0.336s\n",
      "Accuracy total 0.915312, main loss classifier 0.373862, source accuracy 0.971875 source classification loss 0.097614, target accuracy 0.858750 target loss 0.440704 accuracy domain distinction 0.499688 loss domain distinction 1.047033,\n",
      "VALIDATION Loss: 0.21324320 Acc: 0.92326733\n",
      "Epoch 26 of 500 took 0.330s\n",
      "Accuracy total 0.918438, main loss classifier 0.355158, source accuracy 0.967500 source classification loss 0.107171, target accuracy 0.869375 target loss 0.392200 accuracy domain distinction 0.500313 loss domain distinction 1.054731,\n",
      "VALIDATION Loss: 0.17804308 Acc: 0.93564356\n",
      "New best validation loss:  0.17804308448519027\n",
      "Epoch 27 of 500 took 0.331s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy total 0.921875, main loss classifier 0.351128, source accuracy 0.962500 source classification loss 0.114344, target accuracy 0.881250 target loss 0.378711 accuracy domain distinction 0.500000 loss domain distinction 1.046006,\n",
      "VALIDATION Loss: 0.26314661 Acc: 0.90841584\n",
      "Epoch 28 of 500 took 0.332s\n",
      "Accuracy total 0.924063, main loss classifier 0.345464, source accuracy 0.970000 source classification loss 0.100203, target accuracy 0.878125 target loss 0.380157 accuracy domain distinction 0.500000 loss domain distinction 1.052842,\n",
      "VALIDATION Loss: 0.22579571 Acc: 0.91584158\n",
      "Epoch 29 of 500 took 0.327s\n",
      "Accuracy total 0.915000, main loss classifier 0.373391, source accuracy 0.968750 source classification loss 0.104624, target accuracy 0.861250 target loss 0.433431 accuracy domain distinction 0.500000 loss domain distinction 1.043636,\n",
      "VALIDATION Loss: 0.25402844 Acc: 0.90594059\n",
      "Epoch 30 of 500 took 0.330s\n",
      "Accuracy total 0.922813, main loss classifier 0.349884, source accuracy 0.969375 source classification loss 0.104137, target accuracy 0.876250 target loss 0.385785 accuracy domain distinction 0.500000 loss domain distinction 1.049223,\n",
      "VALIDATION Loss: 0.36718181 Acc: 0.87623762\n",
      "Epoch 31 of 500 took 0.331s\n",
      "Accuracy total 0.924063, main loss classifier 0.339491, source accuracy 0.971875 source classification loss 0.103078, target accuracy 0.876250 target loss 0.367908 accuracy domain distinction 0.500313 loss domain distinction 1.039985,\n",
      "VALIDATION Loss: 0.18825738 Acc: 0.92574257\n",
      "Epoch 32 of 500 took 0.328s\n",
      "Accuracy total 0.924063, main loss classifier 0.347540, source accuracy 0.967500 source classification loss 0.107528, target accuracy 0.880625 target loss 0.377663 accuracy domain distinction 0.500000 loss domain distinction 1.049447,\n",
      "VALIDATION Loss: 0.23302948 Acc: 0.92326733\n",
      "Epoch    32: reducing learning rate of group 0 to 4.0240e-06.\n",
      "Epoch 33 of 500 took 0.328s\n",
      "Accuracy total 0.926250, main loss classifier 0.332930, source accuracy 0.968125 source classification loss 0.110512, target accuracy 0.884375 target loss 0.346934 accuracy domain distinction 0.500000 loss domain distinction 1.042074,\n",
      "VALIDATION Loss: 0.24647421 Acc: 0.9009901\n",
      "Epoch 34 of 500 took 0.332s\n",
      "Accuracy total 0.923750, main loss classifier 0.333232, source accuracy 0.970000 source classification loss 0.102466, target accuracy 0.877500 target loss 0.354905 accuracy domain distinction 0.500000 loss domain distinction 1.045462,\n",
      "VALIDATION Loss: 0.19510199 Acc: 0.92079208\n",
      "Epoch 35 of 500 took 0.329s\n",
      "Accuracy total 0.924687, main loss classifier 0.342302, source accuracy 0.966875 source classification loss 0.109137, target accuracy 0.882500 target loss 0.366245 accuracy domain distinction 0.500000 loss domain distinction 1.046108,\n",
      "VALIDATION Loss: 0.25249544 Acc: 0.89108911\n",
      "Epoch 36 of 500 took 0.328s\n",
      "Accuracy total 0.919063, main loss classifier 0.346056, source accuracy 0.968125 source classification loss 0.105796, target accuracy 0.870000 target loss 0.378127 accuracy domain distinction 0.499688 loss domain distinction 1.040940,\n",
      "VALIDATION Loss: 0.25353928 Acc: 0.93564356\n",
      "Epoch 37 of 500 took 0.332s\n",
      "Accuracy total 0.927500, main loss classifier 0.329555, source accuracy 0.972500 source classification loss 0.096226, target accuracy 0.882500 target loss 0.355053 accuracy domain distinction 0.499688 loss domain distinction 1.039157,\n",
      "VALIDATION Loss: 0.21547990 Acc: 0.91584158\n",
      "Epoch 38 of 500 took 0.328s\n",
      "Training complete in 0m 12s\n",
      "['participant_1', 'participant_2', 'participant_0']\n",
      "TSD_Network(\n",
      "  (_network): ModuleList(\n",
      "    (0): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=252, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=252, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (1): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (2): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (_output): Linear(in_features=200, out_features=22, bias=True)\n",
      "  (_output_discriminator): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "Number Parameters:  137624\n",
      "Optimizer =  <generator object Module.parameters at 0x7fb0acd5b740>\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/DANN/participant_2/best_state_2.pt'\n",
      "Loading Optimizer\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/DANN/participant_2/best_state_2.pt' (epoch 10)\n",
      "TSD_Network(\n",
      "  (_network): ModuleList(\n",
      "    (0): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=252, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=252, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (1): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (2): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (_output): Linear(in_features=200, out_features=22, bias=True)\n",
      "  (_output_discriminator): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "Number Parameters:  137624\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/TSD/participant_2/best_state_0.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/TSD/participant_2/best_state_0.pt' (epoch 25)\n",
      "TSD_Network(\n",
      "  (_network): ModuleList(\n",
      "    (0): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=252, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=252, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (1): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (2): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (_output): Linear(in_features=200, out_features=22, bias=True)\n",
      "  (_output_discriminator): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "Number Parameters:  137624\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/DANN/participant_2/best_state_1.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/DANN/participant_2/best_state_1.pt' (epoch 24)\n",
      "TSD_Network(\n",
      "  (_network): ModuleList(\n",
      "    (0): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=252, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=252, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (1): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (2): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (_output): Linear(in_features=200, out_features=22, bias=True)\n",
      "  (_output_discriminator): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "Number Parameters:  137624\n",
      "=> loading checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/DANN/participant_2/best_state_2.pt'\n",
      "=> loaded checkpoint '/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/Weights_TSD/DANN/participant_2/best_state_2.pt' (epoch 10)\n",
      "==== models_array =  (3,)  @ session  2\n",
      "HANDLING NEW SESSION  0\n",
      "HANDLING NEW SESSION  1\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.08   AFTER:  0.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.56   AFTER:  0.56  len before:  25   len after:  25\n",
      "BEFORE:  0.8   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.24   AFTER:  0.24  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.96   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.64   AFTER:  0.6  len before:  25   len after:  25\n",
      "BEFORE:  0.84   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.8   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.04   AFTER:  0.0  len before:  25   len after:  1\n",
      "BEFORE:  0.2   AFTER:  0.48  len before:  25   len after:  25\n",
      "BEFORE:  0.68   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.92   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  25   len after:  6\n",
      "BEFORE:  0.2   AFTER:  0.0  len before:  25   len after:  15\n",
      "BEFORE:  0.24   AFTER:  0.0  len before:  25   len after:  25\n",
      "BEFORE:  0.16   AFTER:  0.0  len before:  25   len after:  16\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.52   AFTER:  1.0  len before:  25   len after:  4\n",
      "BEFORE:  0.88   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.16   AFTER:  0.0  len before:  25   len after:  25\n",
      "BEFORE:  0.24   AFTER:  0.4  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.96   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.88   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.88   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.64   AFTER:  0.72  len before:  25   len after:  25\n",
      "BEFORE:  0.16   AFTER:  0.0  len before:  25   len after:  25\n",
      "BEFORE:  0.64   AFTER:  1.0  len before:  25   len after:  6\n",
      "BEFORE:  0.72   AFTER:  0.6  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.88   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.4   AFTER:  0.6  len before:  25   len after:  25\n",
      "BEFORE:  0.04   AFTER:  0.0  len before:  25   len after:  25\n",
      "BEFORE:  0.44   AFTER:  0.28  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.64   AFTER:  0.52  len before:  25   len after:  25\n",
      "BEFORE:  0.88   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.72   AFTER:  1.0  len before:  25   len after:  16\n",
      "BEFORE:  0.16   AFTER:  0.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  25   len after:  25\n",
      "BEFORE:  0.96   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.64   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.88   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.76   AFTER:  0.88  len before:  25   len after:  25\n",
      "BEFORE:  0.28   AFTER:  0.44  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.48   AFTER:  0.6  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.76   AFTER:  1.0  len before:  25   len after:  9\n",
      "BEFORE:  0.12   AFTER:  0.0  len before:  25   len after:  25\n",
      "BEFORE:  0.56   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.68   AFTER:  0.68  len before:  25   len after:  25\n",
      "BEFORE:  0.88   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.52   AFTER:  0.56  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  25   len after:  25\n",
      "BEFORE:  0.88   AFTER:  1.0  len before:  25   len after:  17\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.64   AFTER:  0.84  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.84   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.96   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.04   AFTER:  0.0  len before:  25   len after:  16\n",
      "BEFORE:  0.2   AFTER:  nan  len before:  25   len after:  0\n",
      "BEFORE:  0.6   AFTER:  0.96  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.68   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.24   AFTER:  0.0  len before:  25   len after:  25\n",
      "BEFORE:  0.08   AFTER:  0.0  len before:  25   len after:  11\n",
      "ACCURACY MODEL:  0.6804545454545454   Accuracy pseudo: 0.7565691621219633  len pseudo:  2017    len predictions 2200\n",
      "HANDLING NEW SESSION  2\n",
      "Finish segment dataset\n",
      "Finish pseudo_labels\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.2   AFTER:  0.0  len before:  25   len after:  25\n",
      "BEFORE:  0.8   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.72   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.76   AFTER:  0.76  len before:  25   len after:  25\n",
      "BEFORE:  0.96   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.52   AFTER:  1.0  len before:  25   len after:  12\n",
      "BEFORE:  0.12   AFTER:  0.0  len before:  25   len after:  25\n",
      "BEFORE:  0.92   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.2   AFTER:  0.0  len before:  25   len after:  25\n",
      "BEFORE:  0.24   AFTER:  0.24  len before:  25   len after:  25\n",
      "BEFORE:  0.72   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.72   AFTER:  0.88  len before:  25   len after:  25\n",
      "BEFORE:  0.44   AFTER:  0.2  len before:  25   len after:  25\n",
      "BEFORE:  0.44   AFTER:  0.6  len before:  25   len after:  25\n",
      "BEFORE:  0.96   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.56   AFTER:  0.4117647058823529  len before:  25   len after:  17\n",
      "BEFORE:  0.8   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.72   AFTER:  0.72  len before:  25   len after:  25\n",
      "BEFORE:  0.12   AFTER:  0.4  len before:  25   len after:  5\n",
      "BEFORE:  0.56   AFTER:  0.6  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.84   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.04   AFTER:  0.0  len before:  25   len after:  25\n",
      "BEFORE:  0.8   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.36   AFTER:  0.0  len before:  25   len after:  10\n",
      "BEFORE:  0.88   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.16   AFTER:  0.0  len before:  25   len after:  25\n",
      "BEFORE:  0.04   AFTER:  0.0  len before:  25   len after:  25\n",
      "BEFORE:  0.48   AFTER:  0.0  len before:  25   len after:  16\n",
      "BEFORE:  0.36   AFTER:  0.48  len before:  25   len after:  25\n",
      "BEFORE:  0.32   AFTER:  0.44  len before:  25   len after:  25\n",
      "BEFORE:  0.56   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.84   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.96   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.04   AFTER:  0.0  len before:  25   len after:  7\n",
      "BEFORE:  0.72   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.72   AFTER:  1.0  len before:  25   len after:  15\n",
      "BEFORE:  0.64   AFTER:  1.0  len before:  25   len after:  9\n",
      "BEFORE:  0.88   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.44   AFTER:  1.0  len before:  25   len after:  10\n",
      "BEFORE:  0.2   AFTER:  0.44  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.8   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.16   AFTER:  0.0  len before:  25   len after:  25\n",
      "BEFORE:  0.88   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.72   AFTER:  0.8  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.88   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  25   len after:  25\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  25   len after:  25\n",
      "BEFORE:  0.52   AFTER:  0.6  len before:  25   len after:  25\n",
      "BEFORE:  0.36   AFTER:  nan  len before:  25   len after:  0\n",
      "BEFORE:  0.24   AFTER:  0.24  len before:  25   len after:  25\n",
      "BEFORE:  0.68   AFTER:  0.8  len before:  25   len after:  25\n",
      "BEFORE:  0.96   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.28   AFTER:  1.0  len before:  25   len after:  10\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.84   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.56   AFTER:  0.84  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.2   AFTER:  0.0  len before:  25   len after:  25\n",
      "BEFORE:  0.12   AFTER:  0.0  len before:  25   len after:  7\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.64   AFTER:  0.6  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  1.0   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.48   AFTER:  1.0  len before:  25   len after:  11\n",
      "BEFORE:  0.12   AFTER:  0.0  len before:  25   len after:  25\n",
      "BEFORE:  0.48   AFTER:  0.4  len before:  25   len after:  25\n",
      "BEFORE:  0.84   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  25   len after:  25\n",
      "BEFORE:  0.6   AFTER:  0.76  len before:  25   len after:  25\n",
      "BEFORE:  0.92   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.24   AFTER:  0.375  len before:  25   len after:  16\n",
      "BEFORE:  0.16   AFTER:  0.625  len before:  25   len after:  16\n",
      "BEFORE:  0.6   AFTER:  0.6  len before:  25   len after:  25\n",
      "BEFORE:  0.88   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.52   AFTER:  0.52  len before:  25   len after:  25\n",
      "BEFORE:  0.6   AFTER:  1.0  len before:  25   len after:  25\n",
      "BEFORE:  0.24   AFTER:  0.0  len before:  25   len after:  14\n",
      "BEFORE:  0.0   AFTER:  0.0  len before:  25   len after:  25\n",
      "ACCURACY MODEL:  0.5713636363636364   Accuracy pseudo: 0.660759493670886  len pseudo:  1975    len predictions 2200\n",
      "STARTING TRAINING\n",
      "Accuracy total 0.802083, main loss classifier 0.803020, source accuracy 0.857422 source classification loss 0.470444, target accuracy 0.746745 target loss 0.891465 accuracy domain distinction 0.498372 loss domain distinction 1.220656,\n",
      "VALIDATION Loss: 0.50270031 Acc: 0.8\n",
      "New best validation loss:  0.5027003139257431\n",
      "Epoch 2 of 500 took 0.327s\n",
      "Accuracy total 0.828451, main loss classifier 0.700178, source accuracy 0.880859 source classification loss 0.384152, target accuracy 0.776042 target loss 0.790504 accuracy domain distinction 0.499023 loss domain distinction 1.128499,\n",
      "VALIDATION Loss: 0.36141824 Acc: 0.86329114\n",
      "New best validation loss:  0.36141824296542574\n",
      "Epoch 3 of 500 took 0.317s\n",
      "Accuracy total 0.835612, main loss classifier 0.694997, source accuracy 0.884115 source classification loss 0.389186, target accuracy 0.787109 target loss 0.776539 accuracy domain distinction 0.498372 loss domain distinction 1.121352,\n",
      "VALIDATION Loss: 0.35615675 Acc: 0.87088608\n",
      "New best validation loss:  0.35615674938474384\n",
      "Epoch 4 of 500 took 0.318s\n",
      "Accuracy total 0.828125, main loss classifier 0.652110, source accuracy 0.876953 source classification loss 0.385147, target accuracy 0.779297 target loss 0.699404 accuracy domain distinction 0.499674 loss domain distinction 1.098348,\n",
      "VALIDATION Loss: 0.47598289 Acc: 0.83797468\n",
      "Epoch 5 of 500 took 0.317s\n",
      "Accuracy total 0.825195, main loss classifier 0.659402, source accuracy 0.867839 source classification loss 0.428224, target accuracy 0.782552 target loss 0.672469 accuracy domain distinction 0.500326 loss domain distinction 1.090550,\n",
      "VALIDATION Loss: 0.33135882 Acc: 0.85063291\n",
      "New best validation loss:  0.3313588170068605\n",
      "Epoch 6 of 500 took 0.315s\n",
      "Accuracy total 0.842448, main loss classifier 0.636186, source accuracy 0.889974 source classification loss 0.361914, target accuracy 0.794922 target loss 0.693187 accuracy domain distinction 0.500326 loss domain distinction 1.086352,\n",
      "VALIDATION Loss: 0.30669678 Acc: 0.88101266\n",
      "New best validation loss:  0.30669678109032766\n",
      "Epoch 7 of 500 took 0.316s\n",
      "Accuracy total 0.840169, main loss classifier 0.610696, source accuracy 0.884115 source classification loss 0.376746, target accuracy 0.796224 target loss 0.628062 accuracy domain distinction 0.500326 loss domain distinction 1.082923,\n",
      "VALIDATION Loss: 0.45171388 Acc: 0.81265823\n",
      "Epoch 8 of 500 took 0.319s\n",
      "Accuracy total 0.845703, main loss classifier 0.615735, source accuracy 0.884766 source classification loss 0.375036, target accuracy 0.806641 target loss 0.638887 accuracy domain distinction 0.500000 loss domain distinction 1.087730,\n",
      "VALIDATION Loss: 0.30855994 Acc: 0.88101266\n",
      "Epoch 9 of 500 took 0.336s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy total 0.849609, main loss classifier 0.572490, source accuracy 0.893229 source classification loss 0.335486, target accuracy 0.805990 target loss 0.592197 accuracy domain distinction 0.502279 loss domain distinction 1.086489,\n",
      "VALIDATION Loss: 0.21862037 Acc: 0.90886076\n",
      "New best validation loss:  0.2186203705413001\n",
      "Epoch 10 of 500 took 0.318s\n",
      "Accuracy total 0.845052, main loss classifier 0.576406, source accuracy 0.877604 source classification loss 0.341942, target accuracy 0.812500 target loss 0.596416 accuracy domain distinction 0.500326 loss domain distinction 1.072276,\n",
      "VALIDATION Loss: 0.33463891 Acc: 0.84556962\n",
      "Epoch 11 of 500 took 0.317s\n",
      "Accuracy total 0.862305, main loss classifier 0.544476, source accuracy 0.884766 source classification loss 0.353357, target accuracy 0.839844 target loss 0.517150 accuracy domain distinction 0.500000 loss domain distinction 1.092223,\n",
      "VALIDATION Loss: 0.30939912 Acc: 0.88860759\n",
      "Epoch 12 of 500 took 0.317s\n",
      "Accuracy total 0.839193, main loss classifier 0.595634, source accuracy 0.878906 source classification loss 0.362744, target accuracy 0.799479 target loss 0.612804 accuracy domain distinction 0.498698 loss domain distinction 1.078594,\n",
      "VALIDATION Loss: 0.39644281 Acc: 0.85822785\n",
      "Epoch 13 of 500 took 0.314s\n",
      "Accuracy total 0.844076, main loss classifier 0.578969, source accuracy 0.885417 source classification loss 0.359457, target accuracy 0.802734 target loss 0.585418 accuracy domain distinction 0.501302 loss domain distinction 1.065318,\n",
      "VALIDATION Loss: 0.25777706 Acc: 0.88860759\n",
      "Epoch 14 of 500 took 0.316s\n",
      "Accuracy total 0.843750, main loss classifier 0.569792, source accuracy 0.880208 source classification loss 0.354368, target accuracy 0.807292 target loss 0.571681 accuracy domain distinction 0.500326 loss domain distinction 1.067670,\n",
      "VALIDATION Loss: 0.26634188 Acc: 0.87088608\n",
      "Epoch 15 of 500 took 0.323s\n",
      "Accuracy total 0.860026, main loss classifier 0.538466, source accuracy 0.895833 source classification loss 0.334820, target accuracy 0.824219 target loss 0.528847 accuracy domain distinction 0.501953 loss domain distinction 1.066326,\n",
      "VALIDATION Loss: 0.33303658 Acc: 0.85822785\n",
      "Epoch    15: reducing learning rate of group 0 to 2.0120e-05.\n",
      "Epoch 16 of 500 took 0.314s\n",
      "Accuracy total 0.861979, main loss classifier 0.520703, source accuracy 0.896484 source classification loss 0.311729, target accuracy 0.827474 target loss 0.516420 accuracy domain distinction 0.499349 loss domain distinction 1.066284,\n",
      "VALIDATION Loss: 0.33133128 Acc: 0.89873418\n",
      "Epoch 17 of 500 took 0.317s\n",
      "Accuracy total 0.859049, main loss classifier 0.523368, source accuracy 0.901042 source classification loss 0.310956, target accuracy 0.817057 target loss 0.520126 accuracy domain distinction 0.500326 loss domain distinction 1.078276,\n",
      "VALIDATION Loss: 0.30881774 Acc: 0.88860759\n",
      "Epoch 18 of 500 took 0.322s\n",
      "Accuracy total 0.856771, main loss classifier 0.541946, source accuracy 0.895182 source classification loss 0.333814, target accuracy 0.818359 target loss 0.536113 accuracy domain distinction 0.499023 loss domain distinction 1.069826,\n",
      "VALIDATION Loss: 0.36695575 Acc: 0.84810127\n",
      "Epoch 19 of 500 took 0.317s\n",
      "Accuracy total 0.856771, main loss classifier 0.528784, source accuracy 0.890625 source classification loss 0.339420, target accuracy 0.822917 target loss 0.505325 accuracy domain distinction 0.500977 loss domain distinction 1.064113,\n",
      "VALIDATION Loss: 0.25968516 Acc: 0.9164557\n",
      "Epoch 20 of 500 took 0.314s\n",
      "Accuracy total 0.867188, main loss classifier 0.513856, source accuracy 0.894531 source classification loss 0.311667, target accuracy 0.839844 target loss 0.503031 accuracy domain distinction 0.499349 loss domain distinction 1.065072,\n",
      "VALIDATION Loss: 0.25536611 Acc: 0.90379747\n",
      "Epoch 21 of 500 took 0.316s\n",
      "Training complete in 0m 6s\n",
      "['participant_1', 'participant_2', 'participant_0']\n"
     ]
    }
   ],
   "source": [
    "run_SCADANN_training_sessions(examples_datasets=examples_datasets_train, labels_datasets=labels_datasets_train,\n",
    "                              num_kernels=num_kernels, feature_vector_input_length=feature_vector_input_length,\n",
    "                              path_weights_to_save_to=path_weight_to_save_to,\n",
    "                              path_weights_Adversarial_training=path_weights_start_with,\n",
    "                              path_weights_Normal_training=path_weights_Normal_training,\n",
    "                              number_of_cycle_for_first_training=40, number_of_cycles_rest_of_training=40,\n",
    "                              number_of_classes=number_of_classes,\n",
    "                              learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET one participant_examples  (3, 40, 550, 252)\n",
      "   GET one training_index_examples  (40, 550, 252)  at  0\n",
      "   GOT one group XY  (2200, 252)    (2200,)\n",
      "       one group XY test  (550, 252)    (550, 252)\n",
      "       one group XY train (1980, 252)    (1980,)\n",
      "       one group XY valid (220, 252)    (220, 252)\n",
      "   GET one training_index_examples  (40, 550, 252)  at  1\n",
      "   GOT one group XY  (2200, 252)    (2200,)\n",
      "       one group XY test  (550, 252)    (550, 252)\n",
      "       one group XY train (1980, 252)    (1980,)\n",
      "       one group XY valid (220, 252)    (220, 252)\n",
      "   GET one training_index_examples  (40, 550, 252)  at  2\n",
      "   GOT one group XY  (2200, 252)    (2200,)\n",
      "       one group XY test  (550, 252)    (550, 252)\n",
      "       one group XY train (1980, 252)    (1980,)\n",
      "       one group XY valid (220, 252)    (220, 252)\n",
      "dataloaders: \n",
      "   train  (1, 3)\n",
      "   valid  (1, 3)\n",
      "   test  (1, 3)\n",
      "GET one participant_examples  (3, 40, 550, 252)\n",
      "   GET one training_index_examples  (40, 550, 252)  at  0\n",
      "   GOT one group XY  (2200, 252)    (2200,)\n",
      "       one group XY test  (550, 252)    (550, 252)\n",
      "       one group XY train (1980, 252)    (1980,)\n",
      "       one group XY valid (220, 252)    (220, 252)\n",
      "   GET one training_index_examples  (40, 550, 252)  at  1\n",
      "   GOT one group XY  (2200, 252)    (2200,)\n",
      "       one group XY test  (550, 252)    (550, 252)\n",
      "       one group XY train (1980, 252)    (1980,)\n",
      "       one group XY valid (220, 252)    (220, 252)\n",
      "   GET one training_index_examples  (40, 550, 252)  at  2\n",
      "   GOT one group XY  (2200, 252)    (2200,)\n",
      "       one group XY test  (550, 252)    (550, 252)\n",
      "       one group XY train (1980, 252)    (1980,)\n",
      "       one group XY valid (220, 252)    (220, 252)\n",
      "dataloaders: \n",
      "   train  (2, 3)\n",
      "   valid  (2, 3)\n",
      "   test  (2, 3)\n",
      "GET one participant_examples  (3, 40, 550, 252)\n",
      "   GET one training_index_examples  (40, 550, 252)  at  0\n",
      "   GOT one group XY  (2200, 252)    (2200,)\n",
      "       one group XY test  (550, 252)    (550, 252)\n",
      "       one group XY train (1980, 252)    (1980,)\n",
      "       one group XY valid (220, 252)    (220, 252)\n",
      "   GET one training_index_examples  (40, 550, 252)  at  1\n",
      "   GOT one group XY  (2200, 252)    (2200,)\n",
      "       one group XY test  (550, 252)    (550, 252)\n",
      "       one group XY train (1980, 252)    (1980,)\n",
      "       one group XY valid (220, 252)    (220, 252)\n",
      "   GET one training_index_examples  (40, 550, 252)  at  2\n",
      "   GOT one group XY  (2200, 252)    (2200,)\n",
      "       one group XY test  (550, 252)    (550, 252)\n",
      "       one group XY train (1980, 252)    (1980,)\n",
      "       one group XY valid (220, 252)    (220, 252)\n",
      "dataloaders: \n",
      "   train  (3, 3)\n",
      "   valid  (3, 3)\n",
      "   test  (3, 3)\n",
      "TSD_Network(\n",
      "  (_network): ModuleList(\n",
      "    (0): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=252, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=252, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (1): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (2): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (_output): Linear(in_features=200, out_features=22, bias=True)\n",
      "  (_output_discriminator): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "Number Parameters:  137624\n",
      "Participant:  0  Accuracy:  0.9963636363636363\n",
      "Participant:  0  Accuracy:  0.58\n",
      "Participant:  0  Accuracy:  0.5636363636363636\n",
      "ACCURACY PARTICIPANT:  [0.9963636363636363, 0.58, 0.5636363636363636]\n",
      "TSD_Network(\n",
      "  (_network): ModuleList(\n",
      "    (0): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=252, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=252, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (1): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (2): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (_output): Linear(in_features=200, out_features=22, bias=True)\n",
      "  (_output_discriminator): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "Number Parameters:  137624\n",
      "Participant:  1  Accuracy:  0.9745454545454545\n",
      "Participant:  1  Accuracy:  0.9072727272727272\n",
      "Participant:  1  Accuracy:  0.8272727272727273\n",
      "ACCURACY PARTICIPANT:  [0.9745454545454545, 0.9072727272727272, 0.8272727272727273]\n",
      "TSD_Network(\n",
      "  (_network): ModuleList(\n",
      "    (0): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=252, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=252, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (1): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (2): LinearBlocks(\n",
      "      (fully_connected_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (batch_norm1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "      (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): BatchNorm1d(200, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (_output): Linear(in_features=200, out_features=22, bias=True)\n",
      "  (_output_discriminator): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "Number Parameters:  137624\n",
      "Participant:  2  Accuracy:  0.9963636363636363\n",
      "Participant:  2  Accuracy:  0.7327272727272728\n",
      "Participant:  2  Accuracy:  0.5890909090909091\n",
      "ACCURACY PARTICIPANT:  [0.9963636363636363, 0.7327272727272728, 0.5890909090909091]\n",
      "[[0.99636364 0.58       0.56363636]\n",
      " [0.97454545 0.90727273 0.82727273]\n",
      " [0.99636364 0.73272727 0.58909091]]\n",
      "[array([0.99636364, 0.58      , 0.56363636]), array([0.97454545, 0.90727273, 0.82727273]), array([0.99636364, 0.73272727, 0.58909091])]\n",
      "OVERALL ACCURACY: 0.7963636363636364\n"
     ]
    }
   ],
   "source": [
    "save_path = \"/home/laiy/gitrepos/msr_final/LongTermEMG_myo/TrainingsAndEvaluations/ForTrainingSessions/results_tsd\"\n",
    "test_network_SLADANN(examples_datasets_train=examples_datasets_train, labels_datasets_train=labels_datasets_train,\n",
    "                     num_neurons=num_kernels, feature_vector_input_length=feature_vector_input_length,\n",
    "                     path_weights_SCADANN =path_weight_to_save_to, path_weights_normal=path_weights_Normal_training,\n",
    "                     algo_name=algo_name, cycle_test=3, \n",
    "                     number_of_classes=number_of_classes, save_path = save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant_0</th>\n",
       "      <th>Participant_1</th>\n",
       "      <th>Participant_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Session_0</th>\n",
       "      <td>0.996364</td>\n",
       "      <td>0.974545</td>\n",
       "      <td>0.996364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Session_1</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0.907273</td>\n",
       "      <td>0.732727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Session_2</th>\n",
       "      <td>0.563636</td>\n",
       "      <td>0.827273</td>\n",
       "      <td>0.589091</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Participant_0 Participant_1 Participant_2\n",
       "Session_0      0.996364      0.974545      0.996364\n",
       "Session_1          0.58      0.907273      0.732727\n",
       "Session_2      0.563636      0.827273      0.589091"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_filename = save_path + '/predictions_' + algo_name + \".npy\"\n",
    "results = np.load(results_filename, allow_pickle=True)\n",
    "SCADANN_acc = results[0]\n",
    "SCADANN_acc_overall = np.mean(SCADANN_acc)\n",
    "SCADANN_df = pd.DataFrame(SCADANN_acc.transpose(), \n",
    "                       index = [f'Session_{i}' for i in range(SCADANN_acc.shape[1])],\n",
    "                        columns = [f'Participant_{j}' for j in range(SCADANN_acc.shape[0])])\n",
    "SCADANN_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "SCADANN_df.transpose().plot.bar(rot=0, figsize=(10,5))\n",
    "plt.title(\"SCADANN Accuracies\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TSD\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant_0</th>\n",
       "      <th>Participant_1</th>\n",
       "      <th>Participant_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Session_0</th>\n",
       "      <td>0.996364</td>\n",
       "      <td>0.974545</td>\n",
       "      <td>0.996364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Session_1</th>\n",
       "      <td>0.518182</td>\n",
       "      <td>0.867273</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Session_2</th>\n",
       "      <td>0.509091</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.570909</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Participant_0 Participant_1 Participant_2\n",
       "Session_0      0.996364      0.974545      0.996364\n",
       "Session_1      0.518182      0.867273          0.66\n",
       "Session_2      0.509091      0.681818      0.570909"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DANN\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant_0</th>\n",
       "      <th>Participant_1</th>\n",
       "      <th>Participant_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Session_0</th>\n",
       "      <td>0.996364</td>\n",
       "      <td>0.974545</td>\n",
       "      <td>0.996364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Session_1</th>\n",
       "      <td>0.565455</td>\n",
       "      <td>0.896364</td>\n",
       "      <td>0.692727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Session_2</th>\n",
       "      <td>0.550909</td>\n",
       "      <td>0.750909</td>\n",
       "      <td>0.56</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Participant_0 Participant_1 Participant_2\n",
       "Session_0      0.996364      0.974545      0.996364\n",
       "Session_1      0.565455      0.896364      0.692727\n",
       "Session_2      0.550909      0.750909          0.56"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCADANN\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant_0</th>\n",
       "      <th>Participant_1</th>\n",
       "      <th>Participant_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Session_0</th>\n",
       "      <td>0.996364</td>\n",
       "      <td>0.974545</td>\n",
       "      <td>0.996364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Session_1</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0.907273</td>\n",
       "      <td>0.732727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Session_2</th>\n",
       "      <td>0.563636</td>\n",
       "      <td>0.827273</td>\n",
       "      <td>0.589091</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Participant_0 Participant_1 Participant_2\n",
       "Session_0      0.996364      0.974545      0.996364\n",
       "Session_1          0.58      0.907273      0.732727\n",
       "Session_2      0.563636      0.827273      0.589091"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"TSD\")\n",
    "display(TSD_df)\n",
    "print(\"DANN\")\n",
    "display(DANN_df)\n",
    "print(\"SCADANN\")\n",
    "display(SCADANN_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Overall_Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TSD</th>\n",
       "      <td>0.752727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DANN</th>\n",
       "      <td>0.775960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SCADANN</th>\n",
       "      <td>0.796364</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Overall_Acc\n",
       "TSD         0.752727\n",
       "DANN        0.775960\n",
       "SCADANN     0.796364"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_acc_df = pd.DataFrame([TSD_acc_overall, DANN_acc_overall, SCADANN_acc_overall],\n",
    "                             index = [\"TSD\", \"DANN\", \"SCADANN\"],\n",
    "                             columns = [\"Overall_Acc\"])\n",
    "overall_acc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x1008 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(14,14))\n",
    "acc_list = [TSD_df, DANN_df, SCADANN_df, overall_acc_df]\n",
    "title_list = [\"TSD\", \"DANN\", \"SCADANN\", \"Overall\"]\n",
    "for idx, ax in enumerate(axes.reshape(-1)): \n",
    "    acc_list[idx].transpose().plot.bar(ax = ax, rot=0)\n",
    "    ax.set_title(title_list[idx])\n",
    "    ax.set_ylim([0, 1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
